diff --git a/src/samples/Samples/0_Introduction/c++11_cuda/c++11_cuda.cu.hip b/src/samples/Samples/0_Introduction/c++11_cuda/c++11_cuda.cu.hip
index e69de29..87d47b9 100755
--- a/src/samples/Samples/0_Introduction/c++11_cuda/c++11_cuda.cu.hip
+++ b/src/samples/Samples/0_Introduction/c++11_cuda/c++11_cuda.cu.hip
@@ -0,0 +1,145 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <thrust/device_ptr.h>
+#include <thrust/count.h>
+#include <thrust/execution_policy.h>
+
+#include <iostream>
+#include "helper_cuda_hipified.h"
+
+/////////////////////////////////////////////////////////////////
+// Some utility code to define grid_stride_range
+// Normally this would be in a header but it's here
+// for didactic purposes. Uses
+#include "range.hpp"
+using namespace util::lang;
+
+// type alias to simplify typing...
+template <typename T>
+using step_range = typename range_proxy<T>::step_range_proxy;
+
+template <typename T>
+__device__ step_range<T> grid_stride_range(T begin, T end) {
+  begin += blockDim.x * blockIdx.x + threadIdx.x;
+  return range(begin, end).step(gridDim.x * blockDim.x);
+}
+/////////////////////////////////////////////////////////////////
+
+template <typename T, typename Predicate>
+__device__ void count_if(int *count, T *data, int n, Predicate p) {
+  for (auto i : grid_stride_range(0, n)) {
+    if (p(data[i])) atomicAdd(count, 1);
+  }
+}
+
+// Use count_if with a lambda function that searches for x, y, z or w
+// Note the use of range-based for loop and initializer_list inside the functor
+// We use auto so we don't have to know the type of the functor or array
+__global__ void xyzw_frequency(int *count, char *text, int n) {
+  const char letters[]{'x', 'y', 'z', 'w'};
+
+  count_if(count, text, n, [&](char c) {
+    for (const auto x : letters)
+      if (c == x) return true;
+    return false;
+  });
+}
+
+__global__ void xyzw_frequency_thrust_device(int *count, char *text, int n) {
+  const char letters[]{'x', 'y', 'z', 'w'};
+  *count = thrust::count_if(thrust::device, text, text + n, [=](char c) {
+    for (const auto x : letters)
+      if (c == x) return true;
+    return false;
+  });
+}
+
+// a bug in Thrust 1.8 causes warnings when this is uncommented
+// so commented out by default -- fixed in Thrust master branch
+#if 0 
+void xyzw_frequency_thrust_host(int *count, char *text, int n)
+{
+  const char letters[] { 'x','y','z','w' };
+  *count = thrust::count_if(thrust::host, text, text+n, [&](char c) {
+    for (const auto x : letters) 
+      if (c == x) return true;
+    return false;
+  });
+}
+#endif
+
+int main(int argc, char **argv) {
+  const char *filename = sdkFindFilePath("warandpeace.txt", argv[0]);
+
+  int numBytes = 16 * 1048576;
+  char *h_text = (char *)malloc(numBytes);
+
+  // find first CUDA device
+  int devID = findCudaDevice(argc, (const char **)argv);
+
+  char *d_text;
+  HIPCHECK(hipMalloc((void **)&d_text, numBytes));
+
+  FILE *fp = fopen(filename, "r");
+  if (fp == NULL) {
+    printf("Cannot find the input text file\n. Exiting..\n");
+    return EXIT_FAILURE;
+  }
+  int len = (int)fread(h_text, sizeof(char), numBytes, fp);
+  fclose(fp);
+  std::cout << "Read " << len << " byte corpus from " << filename << std::endl;
+
+  HIPCHECK(hipMemcpy(d_text, h_text, len, hipMemcpyHostToDevice));
+
+  int count = 0;
+  int *d_count;
+  HIPCHECK(hipMalloc(&d_count, sizeof(int)));
+  HIPCHECK(hipMemset(d_count, 0, sizeof(int)));
+
+  // Try uncommenting one kernel call at a time
+  xyzw_frequency<<<8, 256>>>(d_count, d_text, len);
+  xyzw_frequency_thrust_device<<<1, 1>>>(d_count, d_text, len);
+  HIPCHECK(
+      hipMemcpy(&count, d_count, sizeof(int), hipMemcpyDeviceToHost));
+
+  // xyzw_frequency_thrust_host(&count, h_text, len);
+
+  std::cout << "counted " << count
+            << " instances of 'x', 'y', 'z', or 'w' in \"" << filename << "\""
+            << std::endl;
+
+  HIPCHECK(hipFree(d_count));
+  HIPCHECK(hipFree(d_text));
+
+  return EXIT_SUCCESS;
+}
+rors(hipFree(d_text));
+
+  return EXIT_SUCCESS;
+}
diff --git a/src/samples/Samples/0_Introduction/cppOverload/cppOverload.cu.hip b/src/samples/Samples/0_Introduction/cppOverload/cppOverload.cu.hip
index e69de29..5aa864c 100755
--- a/src/samples/Samples/0_Introduction/cppOverload/cppOverload.cu.hip
+++ b/src/samples/Samples/0_Introduction/cppOverload/cppOverload.cu.hip
@@ -0,0 +1,195 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#define THREAD_N 256
+#define N 1024
+#define DIV_UP(a, b) (((a) + (b) - 1) / (b))
+
+// Includes, system
+#include <stdio.h>
+#include "helper_cuda_hipified.h"
+#include <helper_string.h>
+#include <helper_math.h>
+#include "cppOverload_kernel.cuh"
+
+const char *sampleName = "C++ Function Overloading";
+
+#define OUTPUT_ATTR(attr)                                         \
+  printf("Shared Size:   %d\n", (int)attr.sharedSizeBytes);       \
+  printf("Constant Size: %d\n", (int)attr.constSizeBytes);        \
+  printf("Local Size:    %d\n", (int)attr.localSizeBytes);        \
+  printf("Max Threads Per Block: %d\n", attr.maxThreadsPerBlock); \
+  printf("Number of Registers: %d\n", attr.numRegs);              \
+  printf("PTX Version: %d\n", attr.ptxVersion);                   \
+  printf("Binary Version: %d\n", attr.binaryVersion);
+
+bool check_func1(int *hInput, int *hOutput, int a) {
+  for (int i = 0; i < N; ++i) {
+    int cpuRes = hInput[i] * a + i;
+
+    if (hOutput[i] != cpuRes) {
+      return false;
+    }
+  }
+
+  return true;
+}
+
+bool check_func2(int2 *hInput, int *hOutput, int a) {
+  for (int i = 0; i < N; i++) {
+    int cpuRes = (hInput[i].x + hInput[i].y) * a + i;
+
+    if (hOutput[i] != cpuRes) {
+      return false;
+    }
+  }
+
+  return true;
+}
+
+bool check_func3(int *hInput1, int *hInput2, int *hOutput, int a) {
+  for (int i = 0; i < N; i++) {
+    if (hOutput[i] != (hInput1[i] + hInput2[i]) * a + i) {
+      return false;
+    }
+  }
+
+  return true;
+}
+
+int main(int argc, const char *argv[]) {
+  int *hInput = NULL;
+  int *hOutput = NULL;
+  int *dInput = NULL;
+  int *dOutput = NULL;
+
+  printf("%s starting...\n", sampleName);
+
+  int deviceCount;
+  HIPCHECK(hipGetDeviceCount(&deviceCount));
+  printf("Device Count: %d\n", deviceCount);
+
+  int deviceID = findCudaDevice(argc, argv);
+  hipDeviceProp_t prop;
+  HIPCHECK(hipGetDeviceProperties(&prop, deviceID));
+  if (prop.major < 2) {
+    printf(
+        "ERROR: cppOverload requires GPU devices with compute SM 2.0 or "
+        "higher.\n");
+    printf("Current GPU device has compute SM%d.%d, Exiting...", prop.major,
+           prop.minor);
+    exit(EXIT_WAIVED);
+  }
+
+  HIPCHECK(hipSetDevice(deviceID));
+
+  // Allocate device memory
+  HIPCHECK(hipMalloc(&dInput, sizeof(int) * N * 2));
+  HIPCHECK(hipMalloc(&dOutput, sizeof(int) * N));
+
+  // Allocate host memory
+  HIPCHECK(hipHostMalloc(&hInput, sizeof(int) * N * 2));
+  HIPCHECK(hipHostMalloc(&hOutput, sizeof(int) * N));
+
+  for (int i = 0; i < N * 2; i++) {
+    hInput[i] = i;
+  }
+
+  // Copy data from host to device
+  HIPCHECK(
+      hipMemcpy(dInput, hInput, sizeof(int) * N * 2, hipMemcpyHostToDevice));
+
+  // Test C++ overloading
+  bool testResult = true;
+  bool funcResult = true;
+  int a = 1;
+
+  void (*func1)(const int *, int *, int);
+  void (*func2)(const int2 *, int *, int);
+  void (*func3)(const int *, const int *, int *, int);
+  struct hipFuncAttributes attr;
+
+  // overload function 1
+  func1 = simple_kernel;
+  memset(&attr, 0, sizeof(attr));
+  HIPCHECK(hipFuncSetCacheConfig(*func1, hipFuncCachePreferShared));
+  HIPCHECK(hipFuncGetAttributes(&attr, *func1));
+  OUTPUT_ATTR(attr);
+  (*func1)<<<DIV_UP(N, THREAD_N), THREAD_N>>>(dInput, dOutput, a);
+  HIPCHECK(
+      hipMemcpy(hOutput, dOutput, sizeof(int) * N, hipMemcpyDeviceToHost));
+  funcResult = check_func1(hInput, hOutput, a);
+  printf("simple_kernel(const int *pIn, int *pOut, int a) %s\n\n",
+         funcResult ? "PASSED" : "FAILED");
+  testResult &= funcResult;
+
+  // overload function 2
+  func2 = simple_kernel;
+  memset(&attr, 0, sizeof(attr));
+  HIPCHECK(hipFuncSetCacheConfig(*func2, hipFuncCachePreferShared));
+  HIPCHECK(hipFuncGetAttributes(&attr, *func2));
+  OUTPUT_ATTR(attr);
+  (*func2)<<<DIV_UP(N, THREAD_N), THREAD_N>>>((int2 *)dInput, dOutput, a);
+  HIPCHECK(
+      hipMemcpy(hOutput, dOutput, sizeof(int) * N, hipMemcpyDeviceToHost));
+  funcResult = check_func2(reinterpret_cast<int2 *>(hInput), hOutput, a);
+  printf("simple_kernel(const int2 *pIn, int *pOut, int a) %s\n\n",
+         funcResult ? "PASSED" : "FAILED");
+  testResult &= funcResult;
+
+  // overload function 3
+  func3 = simple_kernel;
+  memset(&attr, 0, sizeof(attr));
+  HIPCHECK(hipFuncSetCacheConfig(*func3, hipFuncCachePreferShared));
+  HIPCHECK(hipFuncGetAttributes(&attr, *func3));
+  OUTPUT_ATTR(attr);
+  (*func3)<<<DIV_UP(N, THREAD_N), THREAD_N>>>(dInput, dInput + N, dOutput, a);
+  HIPCHECK(
+      hipMemcpy(hOutput, dOutput, sizeof(int) * N, hipMemcpyDeviceToHost));
+  funcResult = check_func3(&hInput[0], &hInput[N], hOutput, a);
+  printf(
+      "simple_kernel(const int *pIn1, const int *pIn2, int *pOut, int a) "
+      "%s\n\n",
+      funcResult ? "PASSED" : "FAILED");
+  testResult &= funcResult;
+
+  HIPCHECK(hipFree(dInput));
+  HIPCHECK(hipFree(dOutput));
+  HIPCHECK(hipHostFree(hOutput));
+  HIPCHECK(hipHostFree(hInput));
+
+  HIPCHECK(hipDeviceSynchronize());
+
+  exit(testResult ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+tFree(hOutput));
+  checkCudaErrors(hipHostFree(hInput));
+
+  checkCudaErrors(hipDeviceSynchronize());
+
+  exit(testResult ? EXIT_SUCCESS : EXIT_FAILURE);
+}
diff --git a/src/samples/Samples/0_Introduction/matrixMulDrv/matrixMul_kernel.cu.hip b/src/samples/Samples/0_Introduction/matrixMulDrv/matrixMul_kernel.cu.hip
index e69de29..331a272 100755
--- a/src/samples/Samples/0_Introduction/matrixMulDrv/matrixMul_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/matrixMulDrv/matrixMul_kernel.cu.hip
@@ -0,0 +1,128 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* Matrix multiplication: C = A * B.
+ * Device code.
+ */
+
+#ifndef _MATRIXMUL_KERNEL_H_
+#define _MATRIXMUL_KERNEL_H_
+
+#include <stdio.h>
+
+#define AS(i, j) As[i][j]
+#define BS(i, j) Bs[i][j]
+
+////////////////////////////////////////////////////////////////////////////////
+//! Matrix multiplication on the device: C = A * B
+//! wA is A's width and wB is B's width
+////////////////////////////////////////////////////////////////////////////////
+template <int block_size, typename size_type>
+__device__ void matrixMul(float *C, float *A, float *B, size_type wA,
+                          size_type wB) {
+  // Block index
+  size_type bx = blockIdx.x;
+  size_type by = blockIdx.y;
+
+  // Thread index
+  size_type tx = threadIdx.x;
+  size_type ty = threadIdx.y;
+
+  // Index of the first sub-matrix of A processed by the block
+  size_type aBegin = wA * block_size * by;
+
+  // Index of the last sub-matrix of A processed by the block
+  size_type aEnd = aBegin + wA - 1;
+
+  // Step size used to iterate through the sub-matrices of A
+  size_type aStep = block_size;
+
+  // Index of the first sub-matrix of B processed by the block
+  size_type bBegin = block_size * bx;
+
+  // Step size used to iterate through the sub-matrices of B
+  size_type bStep = block_size * wB;
+
+  // Csub is used to store the element of the block sub-matrix
+  // that is computed by the thread
+  float Csub = 0;
+
+  // Loop over all the sub-matrices of A and B
+  // required to compute the block sub-matrix
+  for (size_type a = aBegin, b = bBegin; a <= aEnd; a += aStep, b += bStep) {
+    // Declaration of the shared memory array As used to
+    // store the sub-matrix of A
+    __shared__ float As[block_size][block_size];
+
+    // Declaration of the shared memory array Bs used to
+    // store the sub-matrix of B
+    __shared__ float Bs[block_size][block_size];
+
+    // Load the matrices from device memory
+    // to shared memory; each thread loads
+    // one element of each matrix
+    AS(ty, tx) = A[a + wA * ty + tx];
+    BS(ty, tx) = B[b + wB * ty + tx];
+
+    // Synchronize to make sure the matrices are loaded
+    __syncthreads();
+
+    // Multiply the two matrices together;
+    // each thread computes one element
+    // of the block sub-matrix
+#pragma unroll
+
+    for (size_type k = 0; k < block_size; ++k) Csub += AS(ty, k) * BS(k, tx);
+
+    // Synchronize to make sure that the preceding
+    // computation is done before loading two new
+    // sub-matrices of A and B in the next iteration
+    __syncthreads();
+  }
+
+  // Write the block sub-matrix to device memory;
+  // each thread writes one element
+  size_type c = wB * block_size * by + block_size * bx;
+  C[c + wB * ty + tx] = Csub;
+}
+
+// C wrappers around our template kernel
+extern "C" __global__ void matrixMul_bs8_64bit(float *C, float *A, float *B,
+                                               size_t wA, size_t wB) {
+  matrixMul<8, size_t>(C, A, B, wA, wB);
+}
+extern "C" __global__ void matrixMul_bs16_64bit(float *C, float *A, float *B,
+                                                size_t wA, size_t wB) {
+  matrixMul<16, size_t>(C, A, B, wA, wB);
+}
+extern "C" __global__ void matrixMul_bs32_64bit(float *C, float *A, float *B,
+                                                size_t wA, size_t wB) {
+  matrixMul<32, size_t>(C, A, B, wA, wB);
+}
+
+#endif  // #ifndef _MATRIXMUL_KERNEL_H_
diff --git a/src/samples/Samples/0_Introduction/matrixMul_nvrtc/matrixMul_kernel.cu.hip b/src/samples/Samples/0_Introduction/matrixMul_nvrtc/matrixMul_kernel.cu.hip
index e69de29..b365b74 100755
--- a/src/samples/Samples/0_Introduction/matrixMul_nvrtc/matrixMul_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/matrixMul_nvrtc/matrixMul_kernel.cu.hip
@@ -0,0 +1,132 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/**
+ * Matrix multiplication: C = A * B.
+ * Host code.
+ *
+ * This sample implements matrix multiplication as described in Chapter 3
+ * of the programming guide.
+ * It has been written for clarity of exposition to illustrate various CUDA
+ * programming principles, not with the goal of providing the most
+ * performant generic kernel for matrix multiplication.
+ *
+ * See also:
+ * V. Volkov and J. Demmel, "Benchmarking GPUs to tune dense linear algebra,"
+ * in Proc. 2008 ACM/IEEE Conf. on Supercomputing (SC '08),
+ * Piscataway, NJ: IEEE Press, 2008, pp. Art. 31:1-11.
+ */
+
+/**
+ * Matrix multiplication (CUDA Kernel) on the device: C = A * B
+ * wA is A's width and wB is B's width
+ */
+
+#include <hip/hip_cooperative_groups.h>
+
+template <int BLOCK_SIZE>
+__device__ void matrixMulCUDA(float *C, float *A, float *B, int wA, int wB) {
+  // Handle to thread block group
+  cooperative_groups::thread_block cta =
+      cooperative_groups::this_thread_block();
+  // Block index
+  int bx = blockIdx.x;
+  int by = blockIdx.y;
+
+  // Thread index
+  int tx = threadIdx.x;
+  int ty = threadIdx.y;
+
+  // Index of the first sub-matrix of A processed by the block
+  int aBegin = wA * BLOCK_SIZE * by;
+
+  // Index of the last sub-matrix of A processed by the block
+  int aEnd = aBegin + wA - 1;
+
+  // Step size used to iterate through the sub-matrices of A
+  int aStep = BLOCK_SIZE;
+
+  // Index of the first sub-matrix of B processed by the block
+  int bBegin = BLOCK_SIZE * bx;
+
+  // Step size used to iterate through the sub-matrices of B
+  int bStep = BLOCK_SIZE * wB;
+
+  // Csub is used to store the element of the block sub-matrix
+  // that is computed by the thread
+  float Csub = 0;
+
+  // Loop over all the sub-matrices of A and B
+  // required to compute the block sub-matrix
+  for (int a = aBegin, b = bBegin; a <= aEnd; a += aStep, b += bStep) {
+    // Declaration of the shared memory array As used to
+    // store the sub-matrix of A
+    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
+
+    // Declaration of the shared memory array Bs used to
+    // store the sub-matrix of B
+    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];
+
+    // Load the matrices from device memory
+    // to shared memory; each thread loads
+    // one element of each matrix
+    As[ty][tx] = A[a + wA * ty + tx];
+    Bs[ty][tx] = B[b + wB * ty + tx];
+
+    // Synchronize to make sure the matrices are loaded
+    cooperative_groups::sync(cta);
+
+// Multiply the two matrices together;
+// each thread computes one element
+// of the block sub-matrix
+#pragma unroll
+    for (int k = 0; k < BLOCK_SIZE; ++k) {
+      Csub += As[ty][k] * Bs[k][tx];
+    }
+
+    // Synchronize to make sure that the preceding
+    // computation is done before loading two new
+    // sub-matrices of A and B in the next iteration
+    cooperative_groups::sync(cta);
+  }
+
+  // Write the block sub-matrix to device memory;
+  // each thread writes one element
+  int c = wB * BLOCK_SIZE * by + BLOCK_SIZE * bx;
+  C[c + wB * ty + tx] = Csub;
+}
+
+extern "C" __global__ void matrixMulCUDA_block16(float *C, float *A, float *B,
+                                                 int wA, int wB) {
+  matrixMulCUDA<16>(C, A, B, wA, wB);
+}
+
+extern "C" __global__ void matrixMulCUDA_block32(float *C, float *A, float *B,
+                                                 int wA, int wB) {
+  matrixMulCUDA<32>(C, A, B, wA, wB);
+}
diff --git a/src/samples/Samples/0_Introduction/simpleAWBarrier/simpleAWBarrier.cu.hip b/src/samples/Samples/0_Introduction/simpleAWBarrier/simpleAWBarrier.cu.hip
index 097e148..6918f18 100755
--- a/src/samples/Samples/0_Introduction/simpleAWBarrier/simpleAWBarrier.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleAWBarrier/simpleAWBarrier.cu.hip
@@ -145,7 +145,7 @@ int main(int argc, char **argv) {
   int dev = findCudaDevice(argc, (const char **)argv);
 
   int major = 0;
-  checkCudaErrors(
+  HIPCHECK(
       hipDeviceGetAttribute(&major, hipDeviceAttributeComputeCapabilityMajor, dev));
 
   // Arrive-Wait Barrier require a GPU of Volta (SM7X) architecture or higher.
@@ -155,7 +155,7 @@ int main(int argc, char **argv) {
   }
 
   int supportsCooperativeLaunch = 0;
-  checkCudaErrors(hipDeviceGetAttribute(&supportsCooperativeLaunch,
+  HIPCHECK(hipDeviceGetAttribute(&supportsCooperativeLaunch,
                                          hipDeviceAttributeCooperativeLaunch, dev));
 
   if (!supportsCooperativeLaunch) {
@@ -178,11 +178,11 @@ int runNormVecByDotProductAWBarrier(int argc, char **argv, int deviceId) {
   double *d_partialResults;
   int size = 10000000;
 
-  checkCudaErrors(hipHostMalloc(&vecA, sizeof(float) * size));
-  checkCudaErrors(hipHostMalloc(&vecB, sizeof(float) * size));
+  HIPCHECK(hipHostMalloc(&vecA, sizeof(float) * size));
+  HIPCHECK(hipHostMalloc(&vecB, sizeof(float) * size));
 
-  checkCudaErrors(hipMalloc(&d_vecA, sizeof(float) * size));
-  checkCudaErrors(hipMalloc(&d_vecB, sizeof(float) * size));
+  HIPCHECK(hipMalloc(&d_vecA, sizeof(float) * size));
+  HIPCHECK(hipMalloc(&d_vecB, sizeof(float) * size));
 
   float baseVal = 2.0;
   for (int i = 0; i < size; i++) {
@@ -190,31 +190,31 @@ int runNormVecByDotProductAWBarrier(int argc, char **argv, int deviceId) {
   }
 
   hipStream_t stream;
-  checkCudaErrors(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
+  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
 
-  checkCudaErrors(hipMemcpyAsync(d_vecA, vecA, sizeof(float) * size,
+  HIPCHECK(hipMemcpyAsync(d_vecA, vecA, sizeof(float) * size,
                                   hipMemcpyHostToDevice, stream));
-  checkCudaErrors(hipMemcpyAsync(d_vecB, vecB, sizeof(float) * size,
+  HIPCHECK(hipMemcpyAsync(d_vecB, vecB, sizeof(float) * size,
                                   hipMemcpyHostToDevice, stream));
 
   // Kernel configuration, where a one-dimensional
   // grid and one-dimensional blocks are configured.
   int minGridSize = 0, blockSize = 0;
-  checkCudaErrors(hipOccupancyMaxPotentialBlockSize(
+  HIPCHECK(hipOccupancyMaxPotentialBlockSize(
       &minGridSize, &blockSize, (void *)normVecByDotProductAWBarrier, 0, size));
 
   int smemSize = ((blockSize / 32) + 1) * sizeof(double);
 
   int numBlocksPerSm = 0;
-  checkCudaErrors(hipOccupancyMaxActiveBlocksPerMultiprocessor(
+  HIPCHECK(hipOccupancyMaxActiveBlocksPerMultiprocessor(
       &numBlocksPerSm, normVecByDotProductAWBarrier, blockSize, smemSize));
 
   int multiProcessorCount = 0;
-  checkCudaErrors(hipDeviceGetAttribute(
+  HIPCHECK(hipDeviceGetAttribute(
       &multiProcessorCount, hipDeviceAttributeMultiprocessorCount, deviceId));
 
   minGridSize = multiProcessorCount * numBlocksPerSm;
-  checkCudaErrors(hipMalloc(&d_partialResults, minGridSize * sizeof(double)));
+  HIPCHECK(hipMalloc(&d_partialResults, minGridSize * sizeof(double)));
 
   printf(
       "Launching normVecByDotProductAWBarrier kernel with numBlocks = %d "
@@ -226,13 +226,13 @@ int runNormVecByDotProductAWBarrier(int argc, char **argv, int deviceId) {
   void *kernelArgs[] = {(void *)&d_vecA, (void *)&d_vecB,
                         (void *)&d_partialResults, (void *)&size};
 
-  checkCudaErrors(
+  HIPCHECK(
       hipLaunchCooperativeKernel((void *)normVecByDotProductAWBarrier, dimGrid,
                                   dimBlock, kernelArgs, smemSize, stream));
 
-  checkCudaErrors(hipMemcpyAsync(vecA, d_vecA, sizeof(float) * size,
+  HIPCHECK(hipMemcpyAsync(vecA, d_vecA, sizeof(float) * size,
                                   hipMemcpyDeviceToHost, stream));
-  checkCudaErrors(hipStreamSynchronize(stream));
+  HIPCHECK(hipStreamSynchronize(stream));
 
   float expectedResult = (baseVal / sqrt(size * baseVal * baseVal));
   unsigned int matches = 0;
@@ -246,9 +246,15 @@ int runNormVecByDotProductAWBarrier(int argc, char **argv, int deviceId) {
   }
 
   printf("Result = %s\n", matches == size ? "PASSED" : "FAILED");
-  checkCudaErrors(hipFree(d_vecA));
-  checkCudaErrors(hipFree(d_vecB));
-  checkCudaErrors(hipFree(d_partialResults));
+  HIPCHECK(hipFree(d_vecA));
+  HIPCHECK(hipFree(d_vecB));
+  HIPCHECK(hipFree(d_partialResults));
+
+  HIPCHECK(hipHostFree(vecA));
+  HIPCHECK(hipHostFree(vecB));
+  return matches == size;
+}
+eckCudaErrors(hipFree(d_partialResults));
 
   checkCudaErrors(hipHostFree(vecA));
   checkCudaErrors(hipHostFree(vecB));
diff --git a/src/samples/Samples/0_Introduction/simpleAssert/simpleAssert.cu.hip b/src/samples/Samples/0_Introduction/simpleAssert/simpleAssert.cu.hip
index e69de29..91432f5 100755
--- a/src/samples/Samples/0_Introduction/simpleAssert/simpleAssert.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleAssert/simpleAssert.cu.hip
@@ -0,0 +1,129 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifdef _WIN32
+#define WINDOWS_LEAN_AND_MEAN
+#define NOMINMAX
+#include <windows.h>
+#else
+#include <sys/utsname.h>
+#endif
+
+// Includes, system
+#include <stdio.h>
+#include <cassert>
+
+// Includes CUDA
+#include <hip/hip_runtime.h>
+
+// Utilities and timing functions
+#include "helper_functions.h"  // includes hip/hip_runtime.h and hip/hip_runtime_api.h
+
+// CUDA helper functions
+#include "helper_cuda_hipified.h"  // helper functions for CUDA error check
+
+const char *sampleName = "simpleAssert";
+
+////////////////////////////////////////////////////////////////////////////////
+// Auto-Verification Code
+bool testResult = true;
+
+////////////////////////////////////////////////////////////////////////////////
+// Kernels
+////////////////////////////////////////////////////////////////////////////////
+//! Tests assert function.
+//! Thread whose id > N will print assertion failed error message.
+////////////////////////////////////////////////////////////////////////////////
+__global__ void testKernel(int N) {
+  int gtid = blockIdx.x * blockDim.x + threadIdx.x;
+  assert(gtid < N);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Declaration, forward
+void runTest(int argc, char **argv);
+
+////////////////////////////////////////////////////////////////////////////////
+// Program main
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv) {
+  printf("%s starting...\n", sampleName);
+
+  runTest(argc, argv);
+
+  printf("%s completed, returned %s\n", sampleName,
+         testResult ? "OK" : "ERROR!");
+  exit(testResult ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+
+void runTest(int argc, char **argv) {
+  int Nblocks = 2;
+  int Nthreads = 32;
+  hipError_t error;
+
+#ifndef _WIN32
+  utsname OS_System_Type;
+  uname(&OS_System_Type);
+
+  printf("OS_System_Type.release = %s\n", OS_System_Type.release);
+
+  if (!strcasecmp(OS_System_Type.sysname, "Darwin")) {
+    printf("simpleAssert is not current supported on Mac OSX\n\n");
+    exit(EXIT_SUCCESS);
+  } else {
+    printf("OS Info: <%s>\n\n", OS_System_Type.version);
+  }
+
+#endif
+
+  // This will pick the best possible CUDA capable device
+  findCudaDevice(argc, (const char **)argv);
+
+  // Kernel configuration, where a one-dimensional
+  // grid and one-dimensional blocks are configured.
+  dim3 dimGrid(Nblocks);
+  dim3 dimBlock(Nthreads);
+
+  printf("Launch kernel to generate assertion failures\n");
+  testKernel<<<dimGrid, dimBlock>>>(60);
+
+  // Synchronize (flushes assert output).
+  printf("\n-- Begin assert output\n\n");
+  error = hipDeviceSynchronize();
+  printf("\n-- End assert output\n\n");
+
+  // Check for errors and failed asserts in asynchronous kernel launch.
+  if (error == hipErrorAssert) {
+    printf(
+        "Device assert failed as expected, "
+        "CUDA error message is: %s\n\n",
+        hipGetErrorString(error));
+  }
+
+  testResult = error == hipErrorAssert;
+}
diff --git a/src/samples/Samples/0_Introduction/simpleAtomicIntrinsics/simpleAtomicIntrinsics.cu.hip b/src/samples/Samples/0_Introduction/simpleAtomicIntrinsics/simpleAtomicIntrinsics.cu.hip
index e69de29..24cfa48 100755
--- a/src/samples/Samples/0_Introduction/simpleAtomicIntrinsics/simpleAtomicIntrinsics.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleAtomicIntrinsics/simpleAtomicIntrinsics.cu.hip
@@ -0,0 +1,137 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* A simple program demonstrating trivial use of global memory atomic
+ * device functions (atomic*() functions).
+ */
+
+// includes, system
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <math.h>
+
+#ifdef _WIN32
+#define WINDOWS_LEAN_AND_MEAN
+#define NOMINMAX
+#include <windows.h>
+#endif
+
+// Includes CUDA
+#include <hip/hip_runtime.h>
+
+// Utilities and timing functions
+#include <helper_functions.h>  // includes hip/hip_runtime.h and hip/hip_runtime_api.h
+
+// CUDA helper functions
+#include <helper_cuda.h>  // helper functions for CUDA error check
+
+// Includes, kernels
+#include "simpleAtomicIntrinsics_kernel.cuh"
+
+const char *sampleName = "simpleAtomicIntrinsics";
+
+////////////////////////////////////////////////////////////////////////////////
+// Auto-Verification Code
+bool testResult = true;
+
+////////////////////////////////////////////////////////////////////////////////
+// Declaration, forward
+void runTest(int argc, char **argv);
+
+extern "C" bool computeGold(int *gpuData, const int len);
+
+////////////////////////////////////////////////////////////////////////////////
+// Program main
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv) {
+  printf("%s starting...\n", sampleName);
+
+  runTest(argc, argv);
+
+  printf("%s completed, returned %s\n", sampleName,
+         testResult ? "OK" : "ERROR!");
+  exit(testResult ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run a simple test for CUDA
+////////////////////////////////////////////////////////////////////////////////
+void runTest(int argc, char **argv) {
+  hipStream_t stream;
+  // This will pick the best possible CUDA capable device
+  findCudaDevice(argc, (const char **)argv);
+
+  StopWatchInterface *timer;
+  sdkCreateTimer(&timer);
+  sdkStartTimer(&timer);
+
+  unsigned int numThreads = 256;
+  unsigned int numBlocks = 64;
+  unsigned int numData = 11;
+  unsigned int memSize = sizeof(int) * numData;
+
+  // allocate mem for the result on host side
+  int *hOData;
+  HIPCHECK(hipHostMalloc(&hOData, memSize));
+
+  // initialize the memory
+  for (unsigned int i = 0; i < numData; i++) hOData[i] = 0;
+
+  // To make the AND and XOR tests generate something other than 0...
+  hOData[8] = hOData[10] = 0xff;
+
+  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
+  // allocate device memory for result
+  int *dOData;
+  HIPCHECK(hipMalloc((void **)&dOData, memSize));
+  // copy host memory to device to initialize to zero
+  HIPCHECK(
+      hipMemcpyAsync(dOData, hOData, memSize, hipMemcpyHostToDevice, stream));
+
+  // execute the kernel
+  testKernel<<<numBlocks, numThreads, 0, stream>>>(dOData);
+
+  // Copy result from device to host
+  HIPCHECK(
+      hipMemcpyAsync(hOData, dOData, memSize, hipMemcpyDeviceToHost, stream));
+  HIPCHECK(hipStreamSynchronize(stream));
+
+  sdkStopTimer(&timer);
+  printf("Processing time: %f (ms)\n", sdkGetTimerValue(&timer));
+  sdkDeleteTimer(&timer);
+
+  // Compute reference solution
+  testResult = computeGold(hOData, numThreads * numBlocks);
+
+  // Cleanup memory
+  HIPCHECK(hipHostFree(hOData));
+  HIPCHECK(hipFree(dOData));
+}
+ostFree(hOData));
+  checkCudaErrors(hipFree(dOData));
+}
diff --git a/src/samples/Samples/0_Introduction/simpleAttributes/simpleAttributes.cu.hip b/src/samples/Samples/0_Introduction/simpleAttributes/simpleAttributes.cu.hip
index e69de29..68e0ff5 100755
--- a/src/samples/Samples/0_Introduction/simpleAttributes/simpleAttributes.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleAttributes/simpleAttributes.cu.hip
@@ -0,0 +1,220 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// includes, system
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <math.h>
+
+// includes CUDA
+#include <hip/hip_runtime.h>
+
+// includes, project
+#include "helper_cuda_hipified.h"
+#include <helper_functions.h>  // helper functions for SDK examples
+
+////////////////////////////////////////////////////////////////////////////////
+// declaration, forward
+void runTest(int argc, char **argv);
+
+hipAccessPolicyWindow initAccessPolicyWindow(void) {
+  hipAccessPolicyWindow accessPolicyWindow = {0};
+  accessPolicyWindow.base_ptr = (void *)0;
+  accessPolicyWindow.num_bytes = 0;
+  accessPolicyWindow.hitRatio = 0.f;
+  accessPolicyWindow.hitProp = hipAccessPropertyNormal;
+  accessPolicyWindow.missProp = hipAccessPropertyStreaming;
+  return accessPolicyWindow;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Simple test kernel for device functionality
+//! @param data  input data in global memory
+//! @param dataSize  input data size
+//! @param bigData  input bigData in global memory
+//! @param bigDataSize  input bigData size
+//! @param hitcount how many data access are done within block
+////////////////////////////////////////////////////////////////////////////////
+static __global__ void kernCacheSegmentTest(int *data, int dataSize, int *trash,
+                                            int bigDataSize, int hitCount) {
+  __shared__ unsigned int hit;
+  int row = blockIdx.y * blockDim.y + threadIdx.y;
+  int col = blockIdx.x * blockDim.x + threadIdx.x;
+  int tID = row * blockDim.y + col;
+  uint32_t psRand = tID;
+
+  atomicExch(&hit, 0);
+  __syncthreads();
+  while (hit < hitCount) {
+    psRand ^= psRand << 13;
+    psRand ^= psRand >> 17;
+    psRand ^= psRand << 5;
+
+    int idx = tID - psRand;
+    if (idx < 0) {
+      idx = -idx;
+    }
+
+    if ((tID % 2) == 0) {
+      data[psRand % dataSize] = data[psRand % dataSize] + data[idx % dataSize];
+    } else {
+      trash[psRand % bigDataSize] =
+          trash[psRand % bigDataSize] + trash[idx % bigDataSize];
+    }
+
+    atomicAdd(&hit, 1);
+  }
+}
+////////////////////////////////////////////////////////////////////////////////
+// Program main
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv) { runTest(argc, argv); }
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run a simple test for CUDA
+////////////////////////////////////////////////////////////////////////////////
+void runTest(int argc, char **argv) {
+  bool bTestResult = true;
+  hipAccessPolicyWindow accessPolicyWindow;
+  hipDeviceProp_t deviceProp;
+  cudaStreamAttrValue streamAttrValue;
+  hipStream_t stream;
+  cudaStreamAttrID streamAttrID;
+  dim3 threads(32, 32);
+  int *dataDevicePointer;
+  int *dataHostPointer;
+  int dataSize;
+  int *bigDataDevicePointer;
+  int *bigDataHostPointer;
+  int bigDataSize;
+  StopWatchInterface *timer = 0;
+
+  printf("%s Starting...\n\n", argv[0]);
+
+  // use command-line specified CUDA device, otherwise use device with highest
+  // Gflops/s
+  int devID = findCudaDevice(argc, (const char **)argv);
+  sdkCreateTimer(&timer);
+  sdkStartTimer(&timer);
+  // Get device properties
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, devID));
+  dim3 blocks(deviceProp.maxGridSize[1], 1);
+
+  // Make sure device the l2 optimization
+  if (deviceProp.persistingL2CacheMaxSize == 0) {
+    printf(
+        "Waiving execution as device %d does not support persisting L2 "
+        "Caching\n",
+        devID);
+    exit(EXIT_WAIVED);
+  }
+
+  // Create stream to assiocate with window
+  HIPCHECK(hipStreamCreate(&stream));
+
+  // Set the amount of l2 cache that will be persisting to maximum the device
+  // can support
+  HIPCHECK(hipDeviceSetLimit(cudaLimitPersistingL2CacheSize,
+                                     deviceProp.persistingL2CacheMaxSize));
+
+  // Stream attribute to set
+  streamAttrID = cudaStreamAttributeAccessPolicyWindow;
+
+  // Default window
+  streamAttrValue.accessPolicyWindow = initAccessPolicyWindow();
+  accessPolicyWindow = initAccessPolicyWindow();
+
+  // Allocate size of both buffers
+  bigDataSize = (deviceProp.l2CacheSize * 4) / sizeof(int);
+  dataSize = (deviceProp.l2CacheSize / 4) / sizeof(int);
+
+  // Allocate data
+  HIPCHECK(hipHostMalloc(&dataHostPointer, dataSize * sizeof(int)));
+  HIPCHECK(
+      hipHostMalloc(&bigDataHostPointer, bigDataSize * sizeof(int)));
+
+  for (int i = 0; i < bigDataSize; ++i) {
+    if (i < dataSize) {
+      dataHostPointer[i] = i;
+    }
+
+    bigDataHostPointer[bigDataSize - i - 1] = i;
+  }
+
+  HIPCHECK(
+      hipMalloc((void **)&dataDevicePointer, dataSize * sizeof(int)));
+  HIPCHECK(
+      hipMalloc((void **)&bigDataDevicePointer, bigDataSize * sizeof(int)));
+  HIPCHECK(hipMemcpyAsync(dataDevicePointer, dataHostPointer,
+                                  dataSize * sizeof(int),
+                                  hipMemcpyHostToDevice, stream));
+  HIPCHECK(hipMemcpyAsync(bigDataDevicePointer, bigDataHostPointer,
+                                  bigDataSize * sizeof(int),
+                                  hipMemcpyHostToDevice, stream));
+
+  // Make a window for the buffer of interest
+  accessPolicyWindow.base_ptr = (void *)dataDevicePointer;
+  accessPolicyWindow.num_bytes = dataSize * sizeof(int);
+  accessPolicyWindow.hitRatio = 1.f;
+  accessPolicyWindow.hitProp = hipAccessPropertyPersisting;
+  accessPolicyWindow.missProp = hipAccessPropertyNormal;
+  streamAttrValue.accessPolicyWindow = accessPolicyWindow;
+
+  // Assign window to stream
+  HIPCHECK(
+      cudaStreamSetAttribute(stream, streamAttrID, &streamAttrValue));
+
+  // Demote any previous persisting lines
+  HIPCHECK(cudaCtxResetPersistingL2Cache());
+
+  HIPCHECK(hipStreamSynchronize(stream));
+  kernCacheSegmentTest<<<blocks, threads, 0, stream>>>(
+      dataDevicePointer, dataSize, bigDataDevicePointer, bigDataSize, 0xAFFFF);
+
+  HIPCHECK(hipStreamSynchronize(stream));
+  // check if kernel execution generated and error
+  getLastCudaError("Kernel execution failed");
+
+  // Free memory
+  HIPCHECK(hipHostFree(dataHostPointer));
+  HIPCHECK(hipHostFree(bigDataHostPointer));
+  HIPCHECK(hipFree(dataDevicePointer));
+  HIPCHECK(hipFree(bigDataDevicePointer));
+
+  sdkStopTimer(&timer);
+  printf("Processing time: %f (ms)\n", sdkGetTimerValue(&timer));
+  sdkDeleteTimer(&timer);
+
+  exit(bTestResult ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+%f (ms)\n", sdkGetTimerValue(&timer));
+  sdkDeleteTimer(&timer);
+
+  exit(bTestResult ? EXIT_SUCCESS : EXIT_FAILURE);
+}
diff --git a/src/samples/Samples/0_Introduction/simpleCUDA2GL/simpleCUDA2GL.cu.hip b/src/samples/Samples/0_Introduction/simpleCUDA2GL/simpleCUDA2GL.cu.hip
index e69de29..7f0c62c 100755
--- a/src/samples/Samples/0_Introduction/simpleCUDA2GL/simpleCUDA2GL.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleCUDA2GL/simpleCUDA2GL.cu.hip
@@ -0,0 +1,63 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// Utilities and system includes
+
+#include "helper_cuda_hipified.h"
+
+// clamp x to range [a, b]
+__device__ float clamp(float x, float a, float b) { return max(a, min(b, x)); }
+
+__device__ int clamp(int x, int a, int b) { return max(a, min(b, x)); }
+
+// convert floating point rgb color to 8-bit integer
+__device__ int rgbToInt(float r, float g, float b) {
+  r = clamp(r, 0.0f, 255.0f);
+  g = clamp(g, 0.0f, 255.0f);
+  b = clamp(b, 0.0f, 255.0f);
+  return (int(b) << 16) | (int(g) << 8) | int(r);
+}
+
+__global__ void cudaProcess(unsigned int *g_odata, int imgw) {
+  extern __shared__ uchar4 sdata[];
+
+  int tx = threadIdx.x;
+  int ty = threadIdx.y;
+  int bw = blockDim.x;
+  int bh = blockDim.y;
+  int x = blockIdx.x * bw + tx;
+  int y = blockIdx.y * bh + ty;
+
+  uchar4 c4 = make_uchar4((x & 0x20) ? 100 : 0, 0, (y & 0x20) ? 100 : 0, 0);
+  g_odata[y * imgw + x] = rgbToInt(c4.z, c4.y, c4.x);
+}
+
+extern "C" void launch_cudaProcess(dim3 grid, dim3 block, int sbytes,
+                                   unsigned int *g_odata, int imgw) {
+  cudaProcess<<<grid, block, sbytes>>>(g_odata, imgw);
+}
diff --git a/src/samples/Samples/0_Introduction/simpleDrvRuntime/vectorAdd_kernel.cu.hip b/src/samples/Samples/0_Introduction/simpleDrvRuntime/vectorAdd_kernel.cu.hip
index e69de29..72e20f6 100755
--- a/src/samples/Samples/0_Introduction/simpleDrvRuntime/vectorAdd_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleDrvRuntime/vectorAdd_kernel.cu.hip
@@ -0,0 +1,43 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* Vector addition: C = A + B.
+ *
+ * This sample is a very basic sample that implements element by element
+ * vector addition. It is the same as the sample illustrating Chapter 3
+ * of the programming guide with some additions like error checking.
+ *
+ */
+
+// Device code
+extern "C" __global__ void VecAdd_kernel(const float *A, const float *B,
+                                         float *C, int N) {
+  int i = blockDim.x * blockIdx.x + threadIdx.x;
+
+  if (i < N) C[i] = A[i] + B[i];
+}
diff --git a/src/samples/Samples/0_Introduction/simplePitchLinearTexture/simplePitchLinearTexture.cu.hip b/src/samples/Samples/0_Introduction/simplePitchLinearTexture/simplePitchLinearTexture.cu.hip
index e69de29..38e1b2b 100755
--- a/src/samples/Samples/0_Introduction/simplePitchLinearTexture/simplePitchLinearTexture.cu.hip
+++ b/src/samples/Samples/0_Introduction/simplePitchLinearTexture/simplePitchLinearTexture.cu.hip
@@ -0,0 +1,319 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* pitchLinearTexture
+*
+* This example demonstrates how to use textures bound to pitch linear memory.
+* It performs a shift of matrix elements using wrap addressing mode (aka
+* periodic boundary conditions) on two arrays, a pitch linear and a CUDA array,
+* in order to highlight the differences in using each.
+*
+* Textures binding to pitch linear memory is a new feature in CUDA 2.2,
+* and allows use of texture features such as wrap addressing mode and
+* filtering which are not possible with textures bound to regular linear memory
+*/
+
+// includes, system
+#include <stdio.h>
+
+#ifdef _WIN32
+#define WINDOWS_LEAN_AND_MEAN
+#define NOMINMAX
+#include <windows.h>
+#endif
+
+// Includes CUDA
+#include <hip/hip_runtime.h>
+
+// Utilities and timing functions
+#include <helper_functions.h>  // includes hip/hip_runtime.h and hip/hip_runtime_api.h
+
+// CUDA helper functions
+#include <helper_cuda.h>  // helper functions for CUDA error check
+
+#define NUM_REPS 100  // number of repetitions performed
+#define TILE_DIM 16   // tile/block size
+
+const char *sSDKsample = "simplePitchLinearTexture";
+
+// Auto-Verification Code
+bool bTestResult = true;
+
+////////////////////////////////////////////////////////////////////////////////
+// NB: (1) The second argument "pitch" is in elements, not bytes
+//     (2) normalized coordinates are used (required for wrap address mode)
+////////////////////////////////////////////////////////////////////////////////
+//! Shifts matrix elements using pitch linear array
+//! @param odata  output data in global memory
+////////////////////////////////////////////////////////////////////////////////
+__global__ void shiftPitchLinear(float *odata, int pitch, int width, int height,
+                                 int shiftX, int shiftY,
+                                 hipTextureObject_t texRefPL) {
+  int xid = blockIdx.x * blockDim.x + threadIdx.x;
+  int yid = blockIdx.y * blockDim.y + threadIdx.y;
+
+  odata[yid * pitch + xid] = tex2D<float>(
+      texRefPL, (xid + shiftX) / (float)width, (yid + shiftY) / (float)height);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Shifts matrix elements using regular array
+//! @param odata  output data in global memory
+////////////////////////////////////////////////////////////////////////////////
+__global__ void shiftArray(float *odata, int pitch, int width, int height,
+                           int shiftX, int shiftY,
+                           hipTextureObject_t texRefArray) {
+  int xid = blockIdx.x * blockDim.x + threadIdx.x;
+  int yid = blockIdx.y * blockDim.y + threadIdx.y;
+
+  odata[yid * pitch + xid] =
+      tex2D<float>(texRefArray, (xid + shiftX) / (float)width,
+                   (yid + shiftY) / (float)height);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Declaration, forward
+void runTest(int argc, char **argv);
+
+////////////////////////////////////////////////////////////////////////////////
+// Program main
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv) {
+  printf("%s starting...\n\n", sSDKsample);
+
+  runTest(argc, argv);
+
+  printf("%s completed, returned %s\n", sSDKsample,
+         bTestResult ? "OK" : "ERROR!");
+  exit(bTestResult ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run a simple test for CUDA
+////////////////////////////////////////////////////////////////////////////////
+void runTest(int argc, char **argv) {
+  // Set array size
+  const int nx = 2048;
+  const int ny = 2048;
+
+  // Setup shifts applied to x and y data
+  const int x_shift = 5;
+  const int y_shift = 7;
+
+  if ((nx % TILE_DIM != 0) || (ny % TILE_DIM != 0)) {
+    printf("nx and ny must be multiples of TILE_DIM\n");
+    exit(EXIT_FAILURE);
+  }
+
+  // Setup execution configuration parameters
+  dim3 dimGrid(nx / TILE_DIM, ny / TILE_DIM), dimBlock(TILE_DIM, TILE_DIM);
+
+  // This will pick the best possible CUDA capable device
+  int devID = findCudaDevice(argc, (const char **)argv);
+
+  // CUDA events for timing
+  hipEvent_t start, stop;
+  hipEventCreate(&start);
+  hipEventCreate(&stop);
+
+  // Host allocation and initialization
+  float *h_idata = (float *)malloc(sizeof(float) * nx * ny);
+  float *h_odata = (float *)malloc(sizeof(float) * nx * ny);
+  float *gold = (float *)malloc(sizeof(float) * nx * ny);
+
+  for (int i = 0; i < nx * ny; ++i) {
+    h_idata[i] = (float)i;
+  }
+
+  // Device memory allocation
+  // Pitch linear input data
+  float *d_idataPL;
+  size_t d_pitchBytes;
+
+  HIPCHECK(hipMallocPitch((void **)&d_idataPL, &d_pitchBytes,
+                                  nx * sizeof(float), ny));
+
+  // Array input data
+  hipArray *d_idataArray;
+  hipChannelFormatDesc channelDesc = hipCreateChannelDesc<float>();
+
+  HIPCHECK(hipMallocArray(&d_idataArray, &channelDesc, nx, ny));
+
+  // Pitch linear output data
+  float *d_odata;
+  HIPCHECK(hipMallocPitch((void **)&d_odata, &d_pitchBytes,
+                                  nx * sizeof(float), ny));
+
+  // Copy host data to device
+  // Pitch linear
+  size_t h_pitchBytes = nx * sizeof(float);
+
+  HIPCHECK(hipMemcpy2D(d_idataPL, d_pitchBytes, h_idata, h_pitchBytes,
+                               nx * sizeof(float), ny, hipMemcpyHostToDevice));
+
+  // Array
+  HIPCHECK(hipMemcpyToArray(d_idataArray, 0, 0, h_idata,
+                                    nx * ny * sizeof(float),
+                                    hipMemcpyHostToDevice));
+
+  hipTextureObject_t texRefPL;
+  hipTextureObject_t texRefArray;
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypePitch2D;
+  texRes.res.pitch2D.devPtr = d_idataPL;
+  texRes.res.pitch2D.desc = channelDesc;
+  texRes.res.pitch2D.width = nx;
+  texRes.res.pitch2D.height = ny;
+  texRes.res.pitch2D.pitchInBytes = h_pitchBytes;
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = true;
+  texDescr.filterMode = hipFilterModePoint;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.addressMode[1] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(hipCreateTextureObject(&texRefPL, &texRes, &texDescr, NULL));
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = d_idataArray;
+  texDescr.normalizedCoords = true;
+  texDescr.filterMode = hipFilterModePoint;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.addressMode[1] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+  HIPCHECK(
+      hipCreateTextureObject(&texRefArray, &texRes, &texDescr, NULL));
+
+  // Reference calculation
+  for (int j = 0; j < ny; ++j) {
+    int jshift = (j + y_shift) % ny;
+
+    for (int i = 0; i < nx; ++i) {
+      int ishift = (i + x_shift) % nx;
+      gold[j * nx + i] = h_idata[jshift * nx + ishift];
+    }
+  }
+
+  // Run ShiftPitchLinear kernel
+  HIPCHECK(
+      hipMemset2D(d_odata, d_pitchBytes, 0, nx * sizeof(float), ny));
+
+  HIPCHECK(hipEventRecord(start, 0));
+
+  for (int i = 0; i < NUM_REPS; ++i) {
+    shiftPitchLinear<<<dimGrid, dimBlock>>>(d_odata,
+                                            (int)(d_pitchBytes / sizeof(float)),
+                                            nx, ny, x_shift, y_shift, texRefPL);
+  }
+
+  HIPCHECK(hipEventRecord(stop, 0));
+  HIPCHECK(hipEventSynchronize(stop));
+  float timePL;
+  HIPCHECK(hipEventElapsedTime(&timePL, start, stop));
+
+  // Check results
+  HIPCHECK(hipMemcpy2D(h_odata, h_pitchBytes, d_odata, d_pitchBytes,
+                               nx * sizeof(float), ny, hipMemcpyDeviceToHost));
+
+  bool res = compareData(gold, h_odata, nx * ny, 0.0f, 0.15f);
+
+  bTestResult = true;
+
+  if (res == false) {
+    printf("*** shiftPitchLinear failed ***\n");
+    bTestResult = false;
+  }
+
+  // Run ShiftArray kernel
+  HIPCHECK(
+      hipMemset2D(d_odata, d_pitchBytes, 0, nx * sizeof(float), ny));
+  HIPCHECK(hipEventRecord(start, 0));
+
+  for (int i = 0; i < NUM_REPS; ++i) {
+    shiftArray<<<dimGrid, dimBlock>>>(d_odata,
+                                      (int)(d_pitchBytes / sizeof(float)), nx,
+                                      ny, x_shift, y_shift, texRefArray);
+  }
+
+  HIPCHECK(hipEventRecord(stop, 0));
+  HIPCHECK(hipEventSynchronize(stop));
+  float timeArray;
+  HIPCHECK(hipEventElapsedTime(&timeArray, start, stop));
+
+  // Check results
+  HIPCHECK(hipMemcpy2D(h_odata, h_pitchBytes, d_odata, d_pitchBytes,
+                               nx * sizeof(float), ny, hipMemcpyDeviceToHost));
+  res = compareData(gold, h_odata, nx * ny, 0.0f, 0.15f);
+
+  if (res == false) {
+    printf("*** shiftArray failed ***\n");
+    bTestResult = false;
+  }
+
+  float bandwidthPL =
+      2.f * 1000.f * nx * ny * sizeof(float) / (1.e+9f) / (timePL / NUM_REPS);
+  float bandwidthArray = 2.f * 1000.f * nx * ny * sizeof(float) / (1.e+9f) /
+                         (timeArray / NUM_REPS);
+
+  printf("\nBandwidth (GB/s) for pitch linear: %.2e; for array: %.2e\n",
+         bandwidthPL, bandwidthArray);
+
+  float fetchRatePL = nx * ny / 1.e+6f / (timePL / (1000.0f * NUM_REPS));
+  float fetchRateArray = nx * ny / 1.e+6f / (timeArray / (1000.0f * NUM_REPS));
+
+  printf(
+      "\nTexture fetch rate (Mpix/s) for pitch linear: "
+      "%.2e; for array: %.2e\n\n",
+      fetchRatePL, fetchRateArray);
+
+  // Cleanup
+  free(h_idata);
+  free(h_odata);
+  free(gold);
+
+  HIPCHECK(hipDestroyTextureObject(texRefPL));
+  HIPCHECK(hipDestroyTextureObject(texRefArray));
+  HIPCHECK(hipFree(d_idataPL));
+  HIPCHECK(hipFreeArray(d_idataArray));
+  HIPCHECK(hipFree(d_odata));
+
+  HIPCHECK(hipEventDestroy(start));
+  HIPCHECK(hipEventDestroy(stop));
+}
+dataPL));
+  checkCudaErrors(hipFreeArray(d_idataArray));
+  checkCudaErrors(hipFree(d_odata));
+
+  checkCudaErrors(hipEventDestroy(start));
+  checkCudaErrors(hipEventDestroy(stop));
+}
diff --git a/src/samples/Samples/0_Introduction/simpleSurfaceWrite/simpleSurfaceWrite.cu.hip b/src/samples/Samples/0_Introduction/simpleSurfaceWrite/simpleSurfaceWrite.cu.hip
index e69de29..c00946a 100755
--- a/src/samples/Samples/0_Introduction/simpleSurfaceWrite/simpleSurfaceWrite.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleSurfaceWrite/simpleSurfaceWrite.cu.hip
@@ -0,0 +1,301 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * This sample demonstrates how use texture fetches in CUDA
+ *
+ * This sample takes an input PGM image (imageFilename) and generates
+ * an output PGM image (imageFilename_out).  This CUDA kernel performs
+ * a simple 2D transform (rotation) on the texture coordinates (u,v).
+ */
+
+// Includes, system
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <math.h>
+
+#ifdef _WIN32
+#define WINDOWS_LEAN_AND_MEAN
+#define NOMINMAX
+#include <windows.h>
+#endif
+
+// Includes CUDA
+#include <hip/hip_runtime.h>
+
+// Utilities and timing functions
+#include <helper_functions.h>  // includes hip/hip_runtime.h and hip/hip_runtime_api.h
+
+// CUDA helper functions
+#include <helper_cuda.h>  // helper functions for CUDA error check
+
+#define MIN_EPSILON_ERROR 5e-3f
+
+////////////////////////////////////////////////////////////////////////////////
+// Define the files that are to be save and the reference images for validation
+const char *imageFilename = "teapot512.pgm";
+const char *refFilename = "ref_rotated.pgm";
+float angle = 0.5f;  // angle to rotate image by (in radians)
+
+// Auto-Verification Code
+bool testResult = true;
+
+static const char *sampleName = "simpleSurfaceWrite";
+
+////////////////////////////////////////////////////////////////////////////////
+// Kernels
+////////////////////////////////////////////////////////////////////////////////
+//! Write to a cuArray (texture data source) using surface writes
+//! @param gIData input data in global memory
+////////////////////////////////////////////////////////////////////////////////
+__global__ void surfaceWriteKernel(float *gIData, int width, int height,
+                                   hipSurfaceObject_t outputSurface) {
+  // calculate surface coordinates
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  // read from global memory and write to cuarray (via surface reference)
+  surf2Dwrite(gIData[y * width + x], outputSurface, x * 4, y,
+              hipBoundaryModeTrap);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Transform an image using texture lookups
+//! @param gOData  output data in global memory
+////////////////////////////////////////////////////////////////////////////////
+__global__ void transformKernel(float *gOData, int width, int height,
+                                float theta, hipTextureObject_t tex) {
+  // calculate normalized texture coordinates
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  float u = x / (float)width;
+  float v = y / (float)height;
+
+  // transform coordinates
+  u -= 0.5f;
+  v -= 0.5f;
+  float tu = u * cosf(theta) - v * sinf(theta) + 0.5f;
+  float tv = v * cosf(theta) + u * sinf(theta) + 0.5f;
+
+  // read from texture and write to global memory
+  gOData[y * width + x] = tex2D<float>(tex, tu, tv);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Declaration, forward
+void runTest(int argc, char **argv);
+
+extern "C" void computeGold(float *reference, float *idata,
+                            const unsigned int len);
+
+////////////////////////////////////////////////////////////////////////////////
+// Program main
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv) {
+  printf("%s starting...\n", sampleName);
+
+  // Process command-line arguments
+  if (argc > 1) {
+    if (checkCmdLineFlag(argc, (const char **)argv, "input")) {
+      getCmdLineArgumentString(argc, (const char **)argv, "input",
+                               (char **)&imageFilename);
+
+      if (checkCmdLineFlag(argc, (const char **)argv, "reference")) {
+        getCmdLineArgumentString(argc, (const char **)argv, "reference",
+                                 (char **)&refFilename);
+      } else {
+        printf("-input flag should be used with -reference flag");
+        exit(EXIT_FAILURE);
+      }
+    } else if (checkCmdLineFlag(argc, (const char **)argv, "reference")) {
+      printf("-reference flag should be used with -input flag");
+      exit(EXIT_FAILURE);
+    }
+  }
+
+  runTest(argc, argv);
+
+  printf("%s completed, returned %s\n", sampleName,
+         testResult ? "OK" : "ERROR!");
+  exit(testResult ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run a simple test for CUDA
+////////////////////////////////////////////////////////////////////////////////
+void runTest(int argc, char **argv) {
+  // Use command-line specified CUDA device,
+  // otherwise use device with highest Gflops/s
+  int devID = findCudaDevice(argc, (const char **)argv);
+
+  // Get number of SMs on this GPU
+  hipDeviceProp_t deviceProps;
+
+  HIPCHECK(hipGetDeviceProperties(&deviceProps, devID));
+  printf("CUDA device [%s] has %d Multi-Processors, SM %d.%d\n",
+         deviceProps.name, deviceProps.multiProcessorCount, deviceProps.major,
+         deviceProps.minor);
+
+  // Load image from disk
+  float *hData = NULL;
+  unsigned int width, height;
+  char *imagePath = sdkFindFilePath(imageFilename, argv[0]);
+
+  if (imagePath == NULL) {
+    printf("Unable to source image input file: %s\n", imageFilename);
+    exit(EXIT_FAILURE);
+  }
+
+  sdkLoadPGM(imagePath, &hData, &width, &height);
+
+  unsigned int size = width * height * sizeof(float);
+  printf("Loaded '%s', %d x %d pixels\n", imageFilename, width, height);
+
+  // Load reference image from image (output)
+  float *hDataRef = (float *)malloc(size);
+  char *refPath = sdkFindFilePath(refFilename, argv[0]);
+
+  if (refPath == NULL) {
+    printf("Unable to find reference image file: %s\n", refFilename);
+    exit(EXIT_FAILURE);
+  }
+
+  sdkLoadPGM(refPath, &hDataRef, &width, &height);
+
+  // Allocate device memory for result
+  float *dData = NULL;
+  HIPCHECK(hipMalloc((void **)&dData, size));
+
+  // Allocate array and copy image data
+  hipChannelFormatDesc channelDesc =
+      hipCreateChannelDesc(32, 0, 0, 0, hipChannelFormatKindFloat);
+  hipArray *cuArray;
+  HIPCHECK(hipMallocArray(&cuArray, &channelDesc, width, height,
+                                  hipArraySurfaceLoadStore));
+
+  dim3 dimBlock(8, 8, 1);
+  dim3 dimGrid(width / dimBlock.x, height / dimBlock.y, 1);
+
+  hipSurfaceObject_t outputSurface;
+  hipResourceDesc surfRes;
+  memset(&surfRes, 0, sizeof(hipResourceDesc));
+  surfRes.resType = hipResourceTypeArray;
+  surfRes.res.array.array = cuArray;
+
+  HIPCHECK(hipCreateSurfaceObject(&outputSurface, &surfRes));
+#if 1
+  HIPCHECK(hipMemcpy(dData, hData, size, hipMemcpyHostToDevice));
+  surfaceWriteKernel<<<dimGrid, dimBlock>>>(dData, width, height,
+                                            outputSurface);
+#else  // This is what differs from the example simpleTexture
+  HIPCHECK(
+      hipMemcpyToArray(cuArray, 0, 0, hData, size, hipMemcpyHostToDevice));
+#endif
+
+  hipTextureObject_t tex;
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = cuArray;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = true;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.addressMode[1] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(hipCreateTextureObject(&tex, &texRes, &texDescr, NULL));
+
+  // Warmup
+  transformKernel<<<dimGrid, dimBlock, 0>>>(dData, width, height, angle, tex);
+
+  HIPCHECK(hipDeviceSynchronize());
+
+  StopWatchInterface *timer = NULL;
+  sdkCreateTimer(&timer);
+  sdkStartTimer(&timer);
+
+  // Execute the kernel
+  transformKernel<<<dimGrid, dimBlock, 0>>>(dData, width, height, angle, tex);
+
+  // Check if kernel execution generated an error
+  getLastCudaError("Kernel execution failed");
+
+  hipDeviceSynchronize();
+  sdkStopTimer(&timer);
+  printf("Processing time: %f (ms)\n", sdkGetTimerValue(&timer));
+  printf("%.2f Mpixels/sec\n",
+         (width * height / (sdkGetTimerValue(&timer) / 1000.0f)) / 1e6);
+  sdkDeleteTimer(&timer);
+
+  // Allocate mem for the result on host side
+  float *hOData = (float *)malloc(size);
+  // copy result from device to host
+  HIPCHECK(hipMemcpy(hOData, dData, size, hipMemcpyDeviceToHost));
+
+  // Write result to file
+  char outputFilename[1024];
+  strcpy(outputFilename, "output.pgm");
+  sdkSavePGM("output.pgm", hOData, width, height);
+  printf("Wrote '%s'\n", outputFilename);
+
+  // Write regression file if necessary
+  if (checkCmdLineFlag(argc, (const char **)argv, "regression")) {
+    // Write file for regression test
+    sdkWriteFile<float>("./data/regression.dat", hOData, width * height, 0.0f,
+                        false);
+  } else {
+    // We need to reload the data from disk,
+    // because it is inverted upon output
+    sdkLoadPGM(outputFilename, &hOData, &width, &height);
+
+    printf("Comparing files\n");
+    printf("\toutput:    <%s>\n", outputFilename);
+    printf("\treference: <%s>\n", refPath);
+    testResult =
+        compareData(hOData, hDataRef, width * height, MIN_EPSILON_ERROR, 0.0f);
+  }
+
+  HIPCHECK(hipDestroySurfaceObject(outputSurface));
+  HIPCHECK(hipDestroyTextureObject(tex));
+  HIPCHECK(hipFree(dData));
+  HIPCHECK(hipFreeArray(cuArray));
+  free(imagePath);
+  free(refPath);
+}
+e(dData));
+  checkCudaErrors(hipFreeArray(cuArray));
+  free(imagePath);
+  free(refPath);
+}
diff --git a/src/samples/Samples/0_Introduction/simpleTemplates_nvrtc/simpleTemplates_kernel.cu.hip b/src/samples/Samples/0_Introduction/simpleTemplates_nvrtc/simpleTemplates_kernel.cu.hip
index e69de29..89eed5d 100755
--- a/src/samples/Samples/0_Introduction/simpleTemplates_nvrtc/simpleTemplates_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleTemplates_nvrtc/simpleTemplates_kernel.cu.hip
@@ -0,0 +1,71 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// includes, kernels
+#include "sharedmem.cuh"
+
+////////////////////////////////////////////////////////////////////////////////
+//! Simple test kernel for device functionality
+//! @param g_idata  input data in global memory
+//! @param g_odata  output data in global memory
+////////////////////////////////////////////////////////////////////////////////
+
+template <class T>
+__device__ void testKernel(T *g_idata, T *g_odata) {
+  // Shared mem size is determined by the host app at run time
+  SharedMemory<T> smem;
+
+  T *sdata = smem.getPointer();
+
+  // access thread id
+  const unsigned int tid = threadIdx.x;
+
+  // access number of threads in this block
+  const unsigned int num_threads = blockDim.x;
+
+  // read in input data from global memory
+  sdata[tid] = g_idata[tid];
+
+  __syncthreads();
+
+  // perform some computations
+  sdata[tid] = (T)num_threads * sdata[tid];
+
+  __syncthreads();
+
+  // write data to global memory
+  g_odata[tid] = sdata[tid];
+}
+
+extern "C" __global__ void testFloat(float *p1, float *p2) {
+  testKernel<float>(p1, p2);
+}
+
+extern "C" __global__ void testInt(int *p1, int *p2) {
+  testKernel<int>(p1, p2);
+}
diff --git a/src/samples/Samples/0_Introduction/simpleTexture/simpleTexture.cu.hip b/src/samples/Samples/0_Introduction/simpleTexture/simpleTexture.cu.hip
index e69de29..8ecaad0 100755
--- a/src/samples/Samples/0_Introduction/simpleTexture/simpleTexture.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleTexture/simpleTexture.cu.hip
@@ -0,0 +1,256 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * This sample demonstrates how use texture fetches in CUDA
+ *
+ * This sample takes an input PGM image (image_filename) and generates
+ * an output PGM image (image_filename_out).  This CUDA kernel performs
+ * a simple 2D transform (rotation) on the texture coordinates (u,v).
+ */
+
+// Includes, system
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <math.h>
+
+#ifdef _WIN32
+#define WINDOWS_LEAN_AND_MEAN
+#define NOMINMAX
+#include <windows.h>
+#endif
+
+// Includes CUDA
+#include <hip/hip_runtime.h>
+
+// Utilities and timing functions
+#include <helper_functions.h>  // includes hip/hip_runtime.h and hip/hip_runtime_api.h
+
+// CUDA helper functions
+#include <helper_cuda.h>  // helper functions for CUDA error check
+
+#define MAX_EPSILON_ERROR 5e-3f
+
+// Define the files that are to be save and the reference images for validation
+const char *imageFilename = "teapot512.pgm";
+const char *refFilename = "ref_rotated.pgm";
+
+const char *sampleName = "simpleTexture";
+
+////////////////////////////////////////////////////////////////////////////////
+// Constants
+const float angle = 0.5f;  // angle to rotate image by (in radians)
+
+// Auto-Verification Code
+bool testResult = true;
+
+////////////////////////////////////////////////////////////////////////////////
+//! Transform an image using texture lookups
+//! @param outputData  output data in global memory
+////////////////////////////////////////////////////////////////////////////////
+__global__ void transformKernel(float *outputData, int width, int height,
+                                float theta, hipTextureObject_t tex) {
+  // calculate normalized texture coordinates
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  float u = (float)x - (float)width / 2;
+  float v = (float)y - (float)height / 2;
+  float tu = u * cosf(theta) - v * sinf(theta);
+  float tv = v * cosf(theta) + u * sinf(theta);
+
+  tu /= (float)width;
+  tv /= (float)height;
+
+  // read from texture and write to global memory
+  outputData[y * width + x] = tex2D<float>(tex, tu + 0.5f, tv + 0.5f);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Declaration, forward
+void runTest(int argc, char **argv);
+
+////////////////////////////////////////////////////////////////////////////////
+// Program main
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv) {
+  printf("%s starting...\n", sampleName);
+
+  // Process command-line arguments
+  if (argc > 1) {
+    if (checkCmdLineFlag(argc, (const char **)argv, "input")) {
+      getCmdLineArgumentString(argc, (const char **)argv, "input",
+                               (char **)&imageFilename);
+
+      if (checkCmdLineFlag(argc, (const char **)argv, "reference")) {
+        getCmdLineArgumentString(argc, (const char **)argv, "reference",
+                                 (char **)&refFilename);
+      } else {
+        printf("-input flag should be used with -reference flag");
+        exit(EXIT_FAILURE);
+      }
+    } else if (checkCmdLineFlag(argc, (const char **)argv, "reference")) {
+      printf("-reference flag should be used with -input flag");
+      exit(EXIT_FAILURE);
+    }
+  }
+
+  runTest(argc, argv);
+
+  printf("%s completed, returned %s\n", sampleName,
+         testResult ? "OK" : "ERROR!");
+  exit(testResult ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run a simple test for CUDA
+////////////////////////////////////////////////////////////////////////////////
+void runTest(int argc, char **argv) {
+  int devID = findCudaDevice(argc, (const char **)argv);
+
+  // load image from disk
+  float *hData = NULL;
+  unsigned int width, height;
+  char *imagePath = sdkFindFilePath(imageFilename, argv[0]);
+
+  if (imagePath == NULL) {
+    printf("Unable to source image file: %s\n", imageFilename);
+    exit(EXIT_FAILURE);
+  }
+
+  sdkLoadPGM(imagePath, &hData, &width, &height);
+
+  unsigned int size = width * height * sizeof(float);
+  printf("Loaded '%s', %d x %d pixels\n", imageFilename, width, height);
+
+  // Load reference image from image (output)
+  float *hDataRef = (float *)malloc(size);
+  char *refPath = sdkFindFilePath(refFilename, argv[0]);
+
+  if (refPath == NULL) {
+    printf("Unable to find reference image file: %s\n", refFilename);
+    exit(EXIT_FAILURE);
+  }
+
+  sdkLoadPGM(refPath, &hDataRef, &width, &height);
+
+  // Allocate device memory for result
+  float *dData = NULL;
+  HIPCHECK(hipMalloc((void **)&dData, size));
+
+  // Allocate array and copy image data
+  hipChannelFormatDesc channelDesc =
+      hipCreateChannelDesc(32, 0, 0, 0, hipChannelFormatKindFloat);
+  hipArray *cuArray;
+  HIPCHECK(hipMallocArray(&cuArray, &channelDesc, width, height));
+  HIPCHECK(
+      hipMemcpyToArray(cuArray, 0, 0, hData, size, hipMemcpyHostToDevice));
+
+  hipTextureObject_t tex;
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = cuArray;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = true;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.addressMode[1] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(hipCreateTextureObject(&tex, &texRes, &texDescr, NULL));
+
+  dim3 dimBlock(8, 8, 1);
+  dim3 dimGrid(width / dimBlock.x, height / dimBlock.y, 1);
+
+  // Warmup
+  transformKernel<<<dimGrid, dimBlock, 0>>>(dData, width, height, angle, tex);
+
+  HIPCHECK(hipDeviceSynchronize());
+  StopWatchInterface *timer = NULL;
+  sdkCreateTimer(&timer);
+  sdkStartTimer(&timer);
+
+  // Execute the kernel
+  transformKernel<<<dimGrid, dimBlock, 0>>>(dData, width, height, angle, tex);
+
+  // Check if kernel execution generated an error
+  getLastCudaError("Kernel execution failed");
+
+  HIPCHECK(hipDeviceSynchronize());
+  sdkStopTimer(&timer);
+  printf("Processing time: %f (ms)\n", sdkGetTimerValue(&timer));
+  printf("%.2f Mpixels/sec\n",
+         (width * height / (sdkGetTimerValue(&timer) / 1000.0f)) / 1e6);
+  sdkDeleteTimer(&timer);
+
+  // Allocate mem for the result on host side
+  float *hOutputData = (float *)malloc(size);
+  // copy result from device to host
+  HIPCHECK(hipMemcpy(hOutputData, dData, size, hipMemcpyDeviceToHost));
+
+  // Write result to file
+  char outputFilename[1024];
+  strcpy(outputFilename, imagePath);
+  strcpy(outputFilename + strlen(imagePath) - 4, "_out.pgm");
+  sdkSavePGM(outputFilename, hOutputData, width, height);
+  printf("Wrote '%s'\n", outputFilename);
+
+  // Write regression file if necessary
+  if (checkCmdLineFlag(argc, (const char **)argv, "regression")) {
+    // Write file for regression test
+    sdkWriteFile<float>("./data/regression.dat", hOutputData, width * height,
+                        0.0f, false);
+  } else {
+    // We need to reload the data from disk,
+    // because it is inverted upon output
+    sdkLoadPGM(outputFilename, &hOutputData, &width, &height);
+
+    printf("Comparing files\n");
+    printf("\toutput:    <%s>\n", outputFilename);
+    printf("\treference: <%s>\n", refPath);
+
+    testResult = compareData(hOutputData, hDataRef, width * height,
+                             MAX_EPSILON_ERROR, 0.15f);
+  }
+
+  HIPCHECK(hipDestroyTextureObject(tex));
+  HIPCHECK(hipFree(dData));
+  HIPCHECK(hipFreeArray(cuArray));
+  free(imagePath);
+  free(refPath);
+}
+aErrors(hipFreeArray(cuArray));
+  free(imagePath);
+  free(refPath);
+}
diff --git a/src/samples/Samples/0_Introduction/simpleTexture3D/simpleTexture3D_kernel.cu.hip b/src/samples/Samples/0_Introduction/simpleTexture3D/simpleTexture3D_kernel.cu.hip
index e69de29..f666dea 100755
--- a/src/samples/Samples/0_Introduction/simpleTexture3D/simpleTexture3D_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleTexture3D/simpleTexture3D_kernel.cu.hip
@@ -0,0 +1,142 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _SIMPLETEXTURE3D_KERNEL_CU_
+#define _SIMPLETEXTURE3D_KERNEL_CU_
+
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <math.h>
+
+#include "helper_cuda_hipified.h"
+#include <helper_math.h>
+
+typedef unsigned int uint;
+typedef unsigned char uchar;
+
+hipArray *d_volumeArray = 0;
+hipTextureObject_t tex;  // 3D texture
+
+__global__ void d_render(uint *d_output, uint imageW, uint imageH, float w,
+                         hipTextureObject_t texObj) {
+  uint x = __umul24(blockIdx.x, blockDim.x) + threadIdx.x;
+  uint y = __umul24(blockIdx.y, blockDim.y) + threadIdx.y;
+
+  float u = x / (float)imageW;
+  float v = y / (float)imageH;
+  // read from 3D texture
+  float voxel = tex3D<float>(texObj, u, v, w);
+
+  if ((x < imageW) && (y < imageH)) {
+    // write output color
+    uint i = __umul24(y, imageW) + x;
+    d_output[i] = voxel * 255;
+  }
+}
+
+extern "C" void setTextureFilterMode(bool bLinearFilter) {
+  if (tex) {
+    HIPCHECK(hipDestroyTextureObject(tex));
+  }
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = d_volumeArray;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = true;
+  texDescr.filterMode =
+      bLinearFilter ? hipFilterModeLinear : hipFilterModePoint;
+  ;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.addressMode[1] = hipAddressModeWrap;
+  texDescr.addressMode[2] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeNormalizedFloat;
+
+  HIPCHECK(hipCreateTextureObject(&tex, &texRes, &texDescr, NULL));
+}
+
+extern "C" void initCuda(const uchar *h_volume, hipExtent volumeSize) {
+  // create 3D array
+  hipChannelFormatDesc channelDesc = hipCreateChannelDesc<uchar>();
+  HIPCHECK(hipMalloc3DArray(&d_volumeArray, &channelDesc, volumeSize));
+
+  // copy data to 3D array
+  hipMemcpy3DParms copyParams = {0};
+  copyParams.srcPtr =
+      make_hipPitchedPtr((void *)h_volume, volumeSize.width * sizeof(uchar),
+                          volumeSize.width, volumeSize.height);
+  copyParams.dstArray = d_volumeArray;
+  copyParams.extent = volumeSize;
+  copyParams.kind = hipMemcpyHostToDevice;
+  HIPCHECK(hipMemcpy3D(&copyParams));
+
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = d_volumeArray;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  // access with normalized texture coordinates
+  texDescr.normalizedCoords = true;
+  // linear interpolation
+  texDescr.filterMode = hipFilterModeLinear;
+  // wrap texture coordinates
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.addressMode[1] = hipAddressModeWrap;
+  texDescr.addressMode[2] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeNormalizedFloat;
+
+  HIPCHECK(hipCreateTextureObject(&tex, &texRes, &texDescr, NULL));
+}
+
+extern "C" void render_kernel(dim3 gridSize, dim3 blockSize, uint *d_output,
+                              uint imageW, uint imageH, float w) {
+  d_render<<<gridSize, blockSize>>>(d_output, imageW, imageH, w, tex);
+}
+
+void cleanupCuda() {
+  if (tex) {
+    HIPCHECK(hipDestroyTextureObject(tex));
+  }
+  if (d_volumeArray) {
+    HIPCHECK(hipFreeArray(d_volumeArray));
+  }
+}
+
+#endif  // #ifndef _SIMPLETEXTURE3D_KERNEL_CU_
+
+
+#endif  // #ifndef _SIMPLETEXTURE3D_KERNEL_CU_
diff --git a/src/samples/Samples/0_Introduction/simpleTextureDrv/simpleTexture_kernel.cu.hip b/src/samples/Samples/0_Introduction/simpleTextureDrv/simpleTexture_kernel.cu.hip
index e69de29..d878b25 100755
--- a/src/samples/Samples/0_Introduction/simpleTextureDrv/simpleTexture_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleTextureDrv/simpleTexture_kernel.cu.hip
@@ -0,0 +1,56 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _SIMPLETEXTURE_KERNEL_H_
+#define _SIMPLETEXTURE_KERNEL_H_
+#include <hip/hip_runtime.h>
+
+////////////////////////////////////////////////////////////////////////////////
+//! Transform an image using texture lookups
+//! @param g_odata  output data in global memory
+////////////////////////////////////////////////////////////////////////////////
+extern "C" __global__ void transformKernel(float *g_odata, int width,
+                                           int height, float theta,
+                                           hipTextureObject_t tex) {
+  // calculate normalized texture coordinates
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  float u = (float)x - (float)width / 2;
+  float v = (float)y - (float)height / 2;
+  float tu = u * cosf(theta) - v * sinf(theta);
+  float tv = v * cosf(theta) + u * sinf(theta);
+
+  tu /= (float)width;
+  tv /= (float)height;
+
+  // read from texture and write to global memory
+  g_odata[y * width + x] = tex2D<float>(tex, tu + 0.5f, tv + 0.5f);
+}
+
+#endif  // #ifndef _SIMPLETEXTURE_KERNEL_H_
diff --git a/src/samples/Samples/0_Introduction/simpleVoteIntrinsics/simpleVoteIntrinsics.cu.hip b/src/samples/Samples/0_Introduction/simpleVoteIntrinsics/simpleVoteIntrinsics.cu.hip
index e69de29..7e6e57f 100755
--- a/src/samples/Samples/0_Introduction/simpleVoteIntrinsics/simpleVoteIntrinsics.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleVoteIntrinsics/simpleVoteIntrinsics.cu.hip
@@ -0,0 +1,313 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// System includes
+#include <assert.h>
+#include <stdio.h>
+
+// CUDA runtime
+#include <hip/hip_runtime.h>
+
+// helper functions and utilities to work with CUDA
+#include "helper_cuda_hipified.h"
+#include "helper_functions.h"
+
+#ifndef MAX
+#define MAX(a, b) (a > b ? a : b)
+#endif
+
+static const char *sSDKsample = "[simpleVoteIntrinsics]\0";
+
+////////////////////////////////////////////////////////////////////////////////
+// Global types and parameters
+////////////////////////////////////////////////////////////////////////////////
+#define VOTE_DATA_GROUP 4
+
+////////////////////////////////////////////////////////////////////////////////
+// CUDA Voting Kernel functions
+////////////////////////////////////////////////////////////////////////////////
+#include "simpleVote_kernel.cuh"
+
+// Generate the test pattern for Tests 1 and 2
+void genVoteTestPattern(unsigned int *VOTE_PATTERN, int size) {
+  // For testing VOTE.Any (all of these threads will return 0)
+  for (int i = 0; i < size / 4; i++) {
+    VOTE_PATTERN[i] = 0x00000000;
+  }
+
+  // For testing VOTE.Any (1/2 these threads will return 1)
+  for (int i = 2 * size / 8; i < 4 * size / 8; i++) {
+    VOTE_PATTERN[i] = (i & 0x01) ? i : 0;
+  }
+
+  // For testing VOTE.all (1/2 of these threads will return 0)
+  for (int i = 2 * size / 4; i < 3 * size / 4; i++) {
+    VOTE_PATTERN[i] = (i & 0x01) ? 0 : i;
+  }
+
+  // For testing VOTE.all (all of these threads will return 1)
+  for (int i = 3 * size / 4; i < 4 * size / 4; i++) {
+    VOTE_PATTERN[i] = 0xffffffff;
+  }
+}
+
+int checkErrors1(unsigned int *h_result, int start, int end, int warp_size,
+                 const char *voteType) {
+  int i, sum = 0;
+
+  for (sum = 0, i = start; i < end; i++) {
+    sum += h_result[i];
+  }
+
+  if (sum > 0) {
+    printf("\t<%s>[%d - %d] = ", voteType, start, end - 1);
+
+    for (i = start; i < end; i++) {
+      printf("%d", h_result[i]);
+    }
+
+    printf("%d values FAILED\n", sum);
+  }
+
+  return (sum > 0);
+}
+
+int checkErrors2(unsigned int *h_result, int start, int end, int warp_size,
+                 const char *voteType) {
+  int i, sum = 0;
+
+  for (sum = 0, i = start; i < end; i++) {
+    sum += h_result[i];
+  }
+
+  if (sum != warp_size) {
+    printf("\t<%s>[%d - %d] = ", voteType, start, end - 1);
+
+    for (i = start; i < end; i++) {
+      printf("%d", h_result[i]);
+    }
+
+    printf(" - FAILED\n");
+  }
+
+  return (sum != warp_size);
+}
+
+// Verification code for Kernel #1
+int checkResultsVoteAnyKernel1(unsigned int *h_result, int size,
+                               int warp_size) {
+  int error_count = 0;
+
+  error_count += checkErrors1(h_result, 0, VOTE_DATA_GROUP * warp_size / 4,
+                              warp_size, "Vote.Any");
+  error_count +=
+      checkErrors2(h_result, VOTE_DATA_GROUP * warp_size / 4,
+                   2 * VOTE_DATA_GROUP * warp_size / 4, warp_size, "Vote.Any");
+  error_count +=
+      checkErrors2(h_result, 2 * VOTE_DATA_GROUP * warp_size / 4,
+                   3 * VOTE_DATA_GROUP * warp_size / 4, warp_size, "Vote.Any");
+  error_count +=
+      checkErrors2(h_result, 3 * VOTE_DATA_GROUP * warp_size / 4,
+                   4 * VOTE_DATA_GROUP * warp_size / 4, warp_size, "Vote.Any");
+
+  printf((error_count == 0) ? "\tOK\n" : "\tERROR\n");
+  return error_count;
+}
+
+// Verification code for Kernel #2
+int checkResultsVoteAllKernel2(unsigned int *h_result, int size,
+                               int warp_size) {
+  int error_count = 0;
+
+  error_count += checkErrors1(h_result, 0, VOTE_DATA_GROUP * warp_size / 4,
+                              warp_size, "Vote.All");
+  error_count +=
+      checkErrors1(h_result, VOTE_DATA_GROUP * warp_size / 4,
+                   2 * VOTE_DATA_GROUP * warp_size / 4, warp_size, "Vote.All");
+  error_count +=
+      checkErrors1(h_result, 2 * VOTE_DATA_GROUP * warp_size / 4,
+                   3 * VOTE_DATA_GROUP * warp_size / 4, warp_size, "Vote.All");
+  error_count +=
+      checkErrors2(h_result, 3 * VOTE_DATA_GROUP * warp_size / 4,
+                   4 * VOTE_DATA_GROUP * warp_size / 4, warp_size, "Vote.All");
+
+  printf((error_count == 0) ? "\tOK\n" : "\tERROR\n");
+  return error_count;
+}
+
+// Verification code for Kernel #3
+int checkResultsVoteAnyKernel3(bool *hinfo, int size) {
+  int i, error_count = 0;
+
+  for (i = 0; i < size * 3; i++) {
+    switch (i % 3) {
+      case 0:
+
+        // First warp should be all zeros.
+        if (hinfo[i] != (i >= size * 1)) {
+          error_count++;
+        }
+
+        break;
+
+      case 1:
+
+        // First warp and half of second should be all zeros.
+        if (hinfo[i] != (i >= size * 3 / 2)) {
+          error_count++;
+        }
+
+        break;
+
+      case 2:
+
+        // First two warps should be all zeros.
+        if (hinfo[i] != (i >= size * 2)) {
+          error_count++;
+        }
+
+        break;
+    }
+  }
+
+  printf((error_count == 0) ? "\tOK\n" : "\tERROR\n");
+  return error_count;
+}
+
+int main(int argc, char **argv) {
+  unsigned int *h_input, *h_result;
+  unsigned int *d_input, *d_result;
+
+  bool *dinfo = NULL, *hinfo = NULL;
+  int error_count[3] = {0, 0, 0};
+
+  hipDeviceProp_t deviceProp;
+  int devID, warp_size = 32;
+
+  printf("%s\n", sSDKsample);
+
+  // This will pick the best possible CUDA capable device
+  devID = findCudaDevice(argc, (const char **)argv);
+
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, devID));
+
+  // Statistics about the GPU device
+  printf(
+      "> GPU device has %d Multi-Processors, SM %d.%d compute capabilities\n\n",
+      deviceProp.multiProcessorCount, deviceProp.major, deviceProp.minor);
+
+  h_input = (unsigned int *)malloc(VOTE_DATA_GROUP * warp_size *
+                                   sizeof(unsigned int));
+  h_result = (unsigned int *)malloc(VOTE_DATA_GROUP * warp_size *
+                                    sizeof(unsigned int));
+  HIPCHECK(
+      hipMalloc(reinterpret_cast<void **>(&d_input),
+                 VOTE_DATA_GROUP * warp_size * sizeof(unsigned int)));
+  HIPCHECK(
+      hipMalloc(reinterpret_cast<void **>(&d_result),
+                 VOTE_DATA_GROUP * warp_size * sizeof(unsigned int)));
+  genVoteTestPattern(h_input, VOTE_DATA_GROUP * warp_size);
+  HIPCHECK(hipMemcpy(d_input, h_input,
+                             VOTE_DATA_GROUP * warp_size * sizeof(unsigned int),
+                             hipMemcpyHostToDevice));
+
+  // Start of Vote Any Test Kernel #1
+  printf("[VOTE Kernel Test 1/3]\n");
+  printf("\tRunning <<Vote.Any>> kernel1 ...\n");
+  {
+    HIPCHECK(hipDeviceSynchronize());
+    dim3 gridBlock(1, 1);
+    dim3 threadBlock(VOTE_DATA_GROUP * warp_size, 1);
+    VoteAnyKernel1<<<gridBlock, threadBlock>>>(d_input, d_result,
+                                               VOTE_DATA_GROUP * warp_size);
+    getLastCudaError("VoteAnyKernel() execution failed\n");
+    HIPCHECK(hipDeviceSynchronize());
+  }
+  HIPCHECK(hipMemcpy(h_result, d_result,
+                             VOTE_DATA_GROUP * warp_size * sizeof(unsigned int),
+                             hipMemcpyDeviceToHost));
+  error_count[0] += checkResultsVoteAnyKernel1(
+      h_result, VOTE_DATA_GROUP * warp_size, warp_size);
+
+  // Start of Vote All Test Kernel #2
+  printf("\n[VOTE Kernel Test 2/3]\n");
+  printf("\tRunning <<Vote.All>> kernel2 ...\n");
+  {
+    HIPCHECK(hipDeviceSynchronize());
+    dim3 gridBlock(1, 1);
+    dim3 threadBlock(VOTE_DATA_GROUP * warp_size, 1);
+    VoteAllKernel2<<<gridBlock, threadBlock>>>(d_input, d_result,
+                                               VOTE_DATA_GROUP * warp_size);
+    getLastCudaError("VoteAllKernel() execution failed\n");
+    HIPCHECK(hipDeviceSynchronize());
+  }
+  HIPCHECK(hipMemcpy(h_result, d_result,
+                             VOTE_DATA_GROUP * warp_size * sizeof(unsigned int),
+                             hipMemcpyDeviceToHost));
+  error_count[1] += checkResultsVoteAllKernel2(
+      h_result, VOTE_DATA_GROUP * warp_size, warp_size);
+
+  // Second Vote Kernel Test #3 (both Any/All)
+  hinfo = reinterpret_cast<bool *>(calloc(warp_size * 3 * 3, sizeof(bool)));
+  hipMalloc(reinterpret_cast<void **>(&dinfo),
+             warp_size * 3 * 3 * sizeof(bool));
+  hipMemcpy(dinfo, hinfo, warp_size * 3 * 3 * sizeof(bool),
+             hipMemcpyHostToDevice);
+
+  printf("\n[VOTE Kernel Test 3/3]\n");
+  printf("\tRunning <<Vote.Any>> kernel3 ...\n");
+  {
+    HIPCHECK(hipDeviceSynchronize());
+    VoteAnyKernel3<<<1, warp_size * 3>>>(dinfo, warp_size);
+    HIPCHECK(hipDeviceSynchronize());
+  }
+
+  hipMemcpy(hinfo, dinfo, warp_size * 3 * 3 * sizeof(bool),
+             hipMemcpyDeviceToHost);
+
+  error_count[2] = checkResultsVoteAnyKernel3(hinfo, warp_size * 3);
+
+  // Now free these resources for Test #1,2
+  HIPCHECK(hipFree(d_input));
+  HIPCHECK(hipFree(d_result));
+  free(h_input);
+  free(h_result);
+
+  // Free resources from Test #3
+  free(hinfo);
+  hipFree(dinfo);
+
+  printf("\tShutting down...\n");
+
+  return (error_count[0] == 0 && error_count[1] == 0 && error_count[2] == 0)
+             ? EXIT_SUCCESS
+             : EXIT_FAILURE;
+}
+_count[1] == 0 && error_count[2] == 0)
+             ? EXIT_SUCCESS
+             : EXIT_FAILURE;
+}
diff --git a/src/samples/Samples/0_Introduction/systemWideAtomics/systemWideAtomics.cu.hip b/src/samples/Samples/0_Introduction/systemWideAtomics/systemWideAtomics.cu.hip
index e69de29..0f3196d 100755
--- a/src/samples/Samples/0_Introduction/systemWideAtomics/systemWideAtomics.cu.hip
+++ b/src/samples/Samples/0_Introduction/systemWideAtomics/systemWideAtomics.cu.hip
@@ -0,0 +1,345 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* A program demonstrating trivial use of system-wide atomics on migratable
+ * memory.
+ */
+
+#include <hip/hip_runtime.h>
+#include "helper_cuda_hipified.h"
+#include <math.h>
+#include <stdint.h>
+#include <cstdio>
+#include <ctime>
+
+#define min(a, b) (a) < (b) ? (a) : (b)
+#define max(a, b) (a) > (b) ? (a) : (b)
+
+#define LOOP_NUM 50
+
+__global__ void atomicKernel(int *atom_arr) {
+  unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;
+
+  for (int i = 0; i < LOOP_NUM; i++) {
+    // Atomic addition
+    atomicAdd_system(&atom_arr[0], 10);
+
+    // Atomic exchange
+    atomicExch_system(&atom_arr[1], tid);
+
+    // Atomic maximum
+    atomicMax_system(&atom_arr[2], tid);
+
+    // Atomic minimum
+    atomicMin_system(&atom_arr[3], tid);
+
+    // Atomic increment (modulo 17+1)
+    atomicInc_system((unsigned int *)&atom_arr[4], 17);
+
+    // Atomic decrement
+    atomicDec_system((unsigned int *)&atom_arr[5], 137);
+
+    // Atomic compare-and-swap
+    atomicCAS_system(&atom_arr[6], tid - 1, tid);
+
+    // Bitwise atomic instructions
+
+    // Atomic AND
+    atomicAnd_system(&atom_arr[7], 2 * tid + 7);
+
+    // Atomic OR
+    atomicOr_system(&atom_arr[8], 1 << tid);
+
+    // Atomic XOR
+    atomicXor_system(&atom_arr[9], tid);
+  }
+}
+
+void atomicKernel_CPU(int *atom_arr, int no_of_threads) {
+  for (int i = no_of_threads; i < 2 * no_of_threads; i++) {
+    for (int j = 0; j < LOOP_NUM; j++) {
+      // Atomic addition
+      __sync_fetch_and_add(&atom_arr[0], 10);
+
+      // Atomic exchange
+      __sync_lock_test_and_set(&atom_arr[1], i);
+
+      // Atomic maximum
+      int old, expected;
+      do {
+        expected = atom_arr[2];
+        old = __sync_val_compare_and_swap(&atom_arr[2], expected,
+                                          max(expected, i));
+      } while (old != expected);
+
+      // Atomic minimum
+      do {
+        expected = atom_arr[3];
+        old = __sync_val_compare_and_swap(&atom_arr[3], expected,
+                                          min(expected, i));
+      } while (old != expected);
+
+      // Atomic increment (modulo 17+1)
+      int limit = 17;
+      do {
+        expected = atom_arr[4];
+        old = __sync_val_compare_and_swap(
+            &atom_arr[4], expected, (expected >= limit) ? 0 : expected + 1);
+      } while (old != expected);
+
+      // Atomic decrement
+      limit = 137;
+      do {
+        expected = atom_arr[5];
+        old = __sync_val_compare_and_swap(
+            &atom_arr[5], expected,
+            ((expected == 0) || (expected > limit)) ? limit : expected - 1);
+      } while (old != expected);
+
+      // Atomic compare-and-swap
+      __sync_val_compare_and_swap(&atom_arr[6], i - 1, i);
+
+      // Bitwise atomic instructions
+
+      // Atomic AND
+      __sync_fetch_and_and(&atom_arr[7], 2 * i + 7);
+
+      // Atomic OR
+      __sync_fetch_and_or(&atom_arr[8], 1 << i);
+
+      // Atomic XOR
+      // 11th element should be 0xff
+      __sync_fetch_and_xor(&atom_arr[9], i);
+    }
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Compute reference data set
+//! Each element is multiplied with the number of threads / array length
+//! @param reference  reference data, computed but preallocated
+//! @param idata      input data as provided to device
+//! @param len        number of elements in reference / idata
+////////////////////////////////////////////////////////////////////////////////
+int verify(int *testData, const int len) {
+  int val = 0;
+
+  for (int i = 0; i < len * LOOP_NUM; ++i) {
+    val += 10;
+  }
+
+  if (val != testData[0]) {
+    printf("atomicAdd failed val = %d testData = %d\n", val, testData[0]);
+    return false;
+  }
+
+  val = 0;
+
+  bool found = false;
+
+  for (int i = 0; i < len; ++i) {
+    // second element should be a member of [0, len)
+    if (i == testData[1]) {
+      found = true;
+      break;
+    }
+  }
+
+  if (!found) {
+    printf("atomicExch failed\n");
+    return false;
+  }
+
+  val = -(1 << 8);
+
+  for (int i = 0; i < len; ++i) {
+    // third element should be len-1
+    val = max(val, i);
+  }
+
+  if (val != testData[2]) {
+    printf("atomicMax failed\n");
+    return false;
+  }
+
+  val = 1 << 8;
+
+  for (int i = 0; i < len; ++i) {
+    val = min(val, i);
+  }
+
+  if (val != testData[3]) {
+    printf("atomicMin failed\n");
+    return false;
+  }
+
+  int limit = 17;
+  val = 0;
+
+  for (int i = 0; i < len * LOOP_NUM; ++i) {
+    val = (val >= limit) ? 0 : val + 1;
+  }
+
+  if (val != testData[4]) {
+    printf("atomicInc failed\n");
+    return false;
+  }
+
+  limit = 137;
+  val = 0;
+
+  for (int i = 0; i < len * LOOP_NUM; ++i) {
+    val = ((val == 0) || (val > limit)) ? limit : val - 1;
+  }
+
+  if (val != testData[5]) {
+    printf("atomicDec failed\n");
+    return false;
+  }
+
+  found = false;
+
+  for (int i = 0; i < len; ++i) {
+    // seventh element should be a member of [0, len)
+    if (i == testData[6]) {
+      found = true;
+      break;
+    }
+  }
+
+  if (!found) {
+    printf("atomicCAS failed\n");
+    return false;
+  }
+
+  val = 0xff;
+
+  for (int i = 0; i < len; ++i) {
+    // 8th element should be 1
+    val &= (2 * i + 7);
+  }
+
+  if (val != testData[7]) {
+    printf("atomicAnd failed\n");
+    return false;
+  }
+
+  val = 0;
+
+  for (int i = 0; i < len; ++i) {
+    // 9th element should be 0xff
+    val |= (1 << i);
+  }
+
+  if (val != testData[8]) {
+    printf("atomicOr failed\n");
+    return false;
+  }
+
+  val = 0xff;
+
+  for (int i = 0; i < len; ++i) {
+    // 11th element should be 0xff
+    val ^= i;
+  }
+
+  if (val != testData[9]) {
+    printf("atomicXor failed\n");
+    return false;
+  }
+
+  return true;
+}
+
+int main(int argc, char **argv) {
+  // set device
+  hipDeviceProp_t device_prop;
+  int dev_id = findCudaDevice(argc, (const char **)argv);
+  HIPCHECK(hipGetDeviceProperties(&device_prop, dev_id));
+
+  if (!device_prop.managedMemory) {
+    // This samples requires being run on a device that supports Unified Memory
+    fprintf(stderr, "Unified Memory not supported on this device\n");
+    exit(EXIT_WAIVED);
+  }
+
+  if (device_prop.computeMode == hipComputeModeProhibited) {
+    // This sample requires being run with a default or process exclusive mode
+    fprintf(stderr,
+            "This sample requires a device in either default or process "
+            "exclusive mode\n");
+    exit(EXIT_WAIVED);
+  }
+
+  if (device_prop.major < 6) {
+    printf(
+        "%s: requires a minimum CUDA compute 6.0 capability, waiving "
+        "testing.\n",
+        argv[0]);
+    exit(EXIT_WAIVED);
+  }
+
+  unsigned int numThreads = 256;
+  unsigned int numBlocks = 64;
+  unsigned int numData = 10;
+
+  int *atom_arr;
+
+  if (device_prop.pageableMemoryAccess) {
+    printf("CAN access pageable memory\n");
+    atom_arr = (int *)malloc(sizeof(int) * numData);
+  } else {
+    printf("CANNOT access pageable memory\n");
+    HIPCHECK(hipMallocManaged(&atom_arr, sizeof(int) * numData));
+  }
+
+  for (unsigned int i = 0; i < numData; i++) atom_arr[i] = 0;
+
+  // To make the AND and XOR tests generate something other than 0...
+  atom_arr[7] = atom_arr[9] = 0xff;
+
+  atomicKernel<<<numBlocks, numThreads>>>(atom_arr);
+  atomicKernel_CPU(atom_arr, numBlocks * numThreads);
+
+  HIPCHECK(hipDeviceSynchronize());
+
+  // Compute & verify reference solution
+  int testResult = verify(atom_arr, 2 * numThreads * numBlocks);
+
+  if (device_prop.pageableMemoryAccess) {
+    free(atom_arr);
+  } else {
+    hipFree(atom_arr);
+  }
+
+  printf("systemWideAtomics completed, returned %s \n",
+         testResult ? "OK" : "ERROR!");
+  exit(testResult ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+S : EXIT_FAILURE);
+}
diff --git a/src/samples/Samples/0_Introduction/vectorAddDrv/vectorAdd_kernel.cu.hip b/src/samples/Samples/0_Introduction/vectorAddDrv/vectorAdd_kernel.cu.hip
index e69de29..8c2afb9 100755
--- a/src/samples/Samples/0_Introduction/vectorAddDrv/vectorAdd_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/vectorAddDrv/vectorAdd_kernel.cu.hip
@@ -0,0 +1,42 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+/* Vector addition: C = A + B.
+ *
+ * This sample is a very basic sample that implements element by element
+ * vector addition. It is the same as the sample illustrating Chapter 3
+ * of the programming guide with some additions like error checking.
+ *
+ */
+
+// Device code
+extern "C" __global__ void VecAdd_kernel(const float *A, const float *B,
+                                         float *C, int N) {
+  int i = blockDim.x * blockIdx.x + threadIdx.x;
+
+  if (i < N) C[i] = A[i] + B[i];
+}
diff --git a/src/samples/Samples/0_Introduction/vectorAddMMAP/vectorAdd_kernel.cu.hip b/src/samples/Samples/0_Introduction/vectorAddMMAP/vectorAdd_kernel.cu.hip
index e69de29..72e20f6 100755
--- a/src/samples/Samples/0_Introduction/vectorAddMMAP/vectorAdd_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/vectorAddMMAP/vectorAdd_kernel.cu.hip
@@ -0,0 +1,43 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* Vector addition: C = A + B.
+ *
+ * This sample is a very basic sample that implements element by element
+ * vector addition. It is the same as the sample illustrating Chapter 3
+ * of the programming guide with some additions like error checking.
+ *
+ */
+
+// Device code
+extern "C" __global__ void VecAdd_kernel(const float *A, const float *B,
+                                         float *C, int N) {
+  int i = blockDim.x * blockIdx.x + threadIdx.x;
+
+  if (i < N) C[i] = A[i] + B[i];
+}
diff --git a/src/samples/Samples/0_Introduction/vectorAdd_nvrtc/vectorAdd_kernel.cu.hip b/src/samples/Samples/0_Introduction/vectorAdd_nvrtc/vectorAdd_kernel.cu.hip
index e69de29..bb459dd 100755
--- a/src/samples/Samples/0_Introduction/vectorAdd_nvrtc/vectorAdd_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/vectorAdd_nvrtc/vectorAdd_kernel.cu.hip
@@ -0,0 +1,43 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/**
+ * CUDA Kernel Device code
+ *
+ * Computes the vector addition of A and B into C. The 3 vectors have the same
+ * number of elements numElements.
+ */
+
+extern "C" __global__ void vectorAdd(const float *A, const float *B, float *C,
+                                     int numElements) {
+  int i = blockDim.x * blockIdx.x + threadIdx.x;
+
+  if (i < numElements) {
+    C[i] = A[i] + B[i];
+  }
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/EGLStream_CUDA_CrossGPU/kernel.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/EGLStream_CUDA_CrossGPU/kernel.cu.hip
index e69de29..f06e484 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/EGLStream_CUDA_CrossGPU/kernel.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/EGLStream_CUDA_CrossGPU/kernel.cu.hip
@@ -0,0 +1,141 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+//
+// DESCRIPTION:   Simple CUDA consumer rendering sample app
+//
+
+#include <EGL/egl.h>
+#include <EGL/eglext.h>
+#include <hip/hip_runtime.h>
+#include <hip/hip_runtime.h>
+#include <stdio.h>
+#include <string.h>
+
+#include "eglstrm_common.h"
+
+extern bool isCrossDevice;
+
+__device__ static unsigned int numErrors = 0, errorFound = 0;
+__device__ void checkProducerDataGPU(char *data, int size, char expectedVal,
+                                     int frameNumber) {
+  if ((data[blockDim.x * blockIdx.x + threadIdx.x] != expectedVal) &&
+      (!errorFound)) {
+    printf("Producer FOUND:%d expected: %d at %d for trial %d %d\n",
+           data[blockDim.x * blockIdx.x + threadIdx.x], expectedVal,
+           (blockDim.x * blockIdx.x + threadIdx.x), frameNumber, numErrors);
+    numErrors++;
+    errorFound = 1;
+    return;
+  }
+}
+
+__device__ void checkConsumerDataGPU(char *data, int size, char expectedVal,
+                                     int frameNumber) {
+  if ((data[blockDim.x * blockIdx.x + threadIdx.x] != expectedVal) &&
+      (!errorFound)) {
+    printf("Consumer FOUND:%d expected: %d at %d for trial %d %d\n",
+           data[blockDim.x * blockIdx.x + threadIdx.x], expectedVal,
+           (blockDim.x * blockIdx.x + threadIdx.x), frameNumber, numErrors);
+    numErrors++;
+    errorFound = 1;
+    return;
+  }
+}
+
+__global__ void writeDataToBuffer(char *pSrc, char newVal) {
+  pSrc[blockDim.x * blockIdx.x + threadIdx.x] = newVal;
+}
+
+__global__ void testKernelConsumer(char *pSrc, char size, char expectedVal,
+                                   char newVal, int frameNumber) {
+  checkConsumerDataGPU(pSrc, size, expectedVal, frameNumber);
+}
+
+__global__ void testKernelProducer(char *pSrc, char size, char expectedVal,
+                                   char newVal, int frameNumber) {
+  checkProducerDataGPU(pSrc, size, expectedVal, frameNumber);
+}
+__global__ void getNumErrors(int *numErr) { *numErr = numErrors; }
+
+hipError_t cudaProducer_filter(hipStream_t pStream, char *pSrc, int width,
+                                int height, char expectedVal, char newVal,
+                                int frameNumber) {
+  // in case where consumer is on dgpu and producer is on igpu when return is
+  // called the frame is not copied back to igpu. So the consumer changes is not
+  // visible to producer
+  if (isCrossDevice == 0) {
+    testKernelProducer<<<(width * height) / 1024, 1024, 1, pStream>>>(
+        pSrc, width * height, expectedVal, newVal, frameNumber);
+  }
+  writeDataToBuffer<<<(width * height) / 1024, 1024, 1, pStream>>>(pSrc,
+                                                                   newVal);
+  return hipSuccess;
+};
+
+hipError_t cudaConsumer_filter(hipStream_t cStream, char *pSrc, int width,
+                                int height, char expectedVal, char newVal,
+                                int frameNumber) {
+  testKernelConsumer<<<(width * height) / 1024, 1024, 1, cStream>>>(
+      pSrc, width * height, expectedVal, newVal, frameNumber);
+  writeDataToBuffer<<<(width * height) / 1024, 1024, 1, cStream>>>(pSrc,
+                                                                   newVal);
+  return hipSuccess;
+};
+
+hipError_t cudaGetValueMismatch() {
+  int numErr_h;
+  int *numErr_d = NULL;
+  hipError_t err = hipSuccess;
+  err = hipMalloc(&numErr_d, sizeof(int));
+  if (err != hipSuccess) {
+    printf("Cuda Main: hipMalloc failed with %s\n", hipGetErrorString(err));
+    return err;
+  }
+  getNumErrors<<<1, 1>>>(numErr_d);
+  err = hipDeviceSynchronize();
+  if (err != hipSuccess) {
+    printf("Cuda Main: hipDeviceSynchronize failed with %s\n",
+           hipGetErrorString(err));
+  }
+  err = hipMemcpy(&numErr_h, numErr_d, sizeof(int), hipMemcpyDeviceToHost);
+  if (err != hipSuccess) {
+    printf("Cuda Main: hipMemcpy failed with %s\n", hipGetErrorString(err));
+    hipFree(numErr_d);
+    return err;
+  }
+  err = hipFree(numErr_d);
+  if (err != hipSuccess) {
+    printf("Cuda Main: hipFree failed with %s\n", hipGetErrorString(err));
+    return err;
+  }
+  if (numErr_h > 0) {
+    return hipErrorUnknown;
+  }
+  return hipSuccess;
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/EGLSync_CUDAEvent_Interop/EGLSync_CUDAEvent_Interop.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/EGLSync_CUDAEvent_Interop/EGLSync_CUDAEvent_Interop.cu.hip
index e69de29..773613f 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/EGLSync_CUDAEvent_Interop/EGLSync_CUDAEvent_Interop.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/EGLSync_CUDAEvent_Interop/EGLSync_CUDAEvent_Interop.cu.hip
@@ -0,0 +1,784 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// Simple interop app demonstrating EGLImage + EGLSync interop with CUDA.
+// Using EGLSync - CUDA Event interop one can achieve synchronization on GPU
+// itself for GL-EGL-CUDA operations instead of blocking CPU for
+// synchronization. This app requires GLES 3.2 or higher
+
+//---------------------------INCLUDES---------------------------------//
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <time.h>
+#include "graphics_interface.h"
+#include <hip/hip_runtime.h>
+#include <helper_cuda_drvapi.h>
+#include <cudaEGL.h>
+#include <EGL/egl.h>
+#include <EGL/eglext.h>
+#include <GLES3/gl32.h>
+#include "egl_common.h"
+
+//---------------------------DEFINES---------------------------------//
+#define MAX_ITR 100
+
+#define FAILURE 0
+#define SUCCESS 1
+#define WAIVED 2
+
+#define BLOCK_SIZE 16
+
+#define GL_READ 0
+#define GL_WRITE 1
+//---------------------------MACROS---------------------------------//
+
+// Error-checking wrapper around GL calls
+#define GL_SAFE_CALL(call)                                              \
+  {                                                                     \
+    GLenum err;                                                         \
+    call;                                                               \
+    err = glGetError();                                                 \
+    if (err != GL_NO_ERROR) {                                           \
+      fprintf(stderr, "%s:%d GL error: %d\n", __FILE__, __LINE__, err); \
+      cleanup(FAILURE);                                                 \
+    }                                                                   \
+  }
+
+#define GL_SAFE_CALL_NO_CLEANUP(call, err)                                 \
+  {                                                                        \
+    GLenum status;                                                         \
+    call;                                                                  \
+    status = glGetError();                                                 \
+    if (status != GL_NO_ERROR) {                                           \
+      fprintf(stderr, "%s:%d GL error: %d\n", __FILE__, __LINE__, status); \
+      err = status;                                                        \
+    }                                                                      \
+  }
+
+// Error-checking wrapper around CUDA calls (taken from cutil.h)
+#define CUDA_SAFE_CALL(call)                                                  \
+  do {                                                                        \
+    hipError_t err = call;                                                     \
+    if (hipSuccess != err) {                                                 \
+      fprintf(stderr, "Cuda error in file '%s' in line %i : %s.\n", __FILE__, \
+              __LINE__, hipGetErrorString(err));                             \
+      cleanup(FAILURE);                                                       \
+    }                                                                         \
+  } while (0)
+
+#define CUDA_SAFE_CALL_NO_CLEANUP(call, err)                                  \
+  do {                                                                        \
+    hipError_t status = call;                                                  \
+    if (hipSuccess != status) {                                              \
+      fprintf(stderr, "Cuda error in file '%s' in line %i : %s.\n", __FILE__, \
+              __LINE__, hipGetErrorString(status));                          \
+      err = status;                                                           \
+    }                                                                         \
+  } while (0)
+
+#if defined(EXTENSION_LIST)
+EXTENSION_LIST(EXTLST_DECL)
+typedef void (*extlst_fnptr_t)(void);
+static struct {
+  extlst_fnptr_t *fnptr;
+  char const *name;
+} extensionList[] = {EXTENSION_LIST(EXTLST_ENTRY)};
+
+int eglSetupExtensions(void) {
+  unsigned int i;
+
+  for (i = 0; i < (sizeof(extensionList) / sizeof(*extensionList)); i++) {
+    *extensionList[i].fnptr = eglGetProcAddress(extensionList[i].name);
+    if (*extensionList[i].fnptr == NULL) {
+      printf("Couldn't get address of %s()\n", extensionList[i].name);
+      return 0;
+    }
+  }
+
+  return 1;
+}
+#endif
+
+#if defined(EXTENSION_LIST)
+EXTENSION_LIST(EXTLST_EXTERN)
+#endif
+
+//------------------------GLOBAL VARIABLES--------------------------//
+
+// GL texture
+GLuint tex[2] = {0};
+
+// Used to catch unexpected termination from GLUT
+int cleanExit = 0;
+
+// Use CPU Sync or GPU sync; Default GPU
+int useGpu = 1;
+
+// CUDA Resource
+hipGraphicsResource_t writeResource = NULL;
+hipGraphicsResource_t readResource = NULL;
+hipArray_t writeArray, readArray;
+hipDevice_t device;
+hipCtx_t context;
+
+// Which device to run on
+unsigned int dev = 0;
+
+// Default width, height, and iterations value
+int width = 2048;
+int height = 2048;
+int itr = MAX_ITR;
+
+// Error check variable
+__device__ static unsigned int numErrors = 0;
+
+//-----------------------FUNCTION PROTOTYPES------------------------//
+
+void checkSync(int argc, char **argv);
+int parseCmdLine(int argc, char **argv);
+void printUsage(void);
+void cleanup(int status);
+void exitHandler(void);
+void printStatus(int status);
+void checkSyncOnCPU(void);
+void checkSyncOnGPU(EGLDisplay dpy);
+
+__global__ void verify_and_update_kernel(hipSurfaceObject_t write, hipSurfaceObject_t read,
+                                         char expected, char newval, int width,
+                                         int height);
+extern "C" hipError_t cudaGetValueMismatch();
+
+//-----------------------FUNCTION DEFINITIONS------------------------//
+
+int main(int argc, char *argv[]) {
+#if defined(__linux__)
+  setenv("DISPLAY", ":0", 0);
+#endif
+
+  parseCmdLine(argc, argv);
+  atexit(exitHandler);
+
+  checkSync(argc, argv);
+  return 0;
+}
+
+int parseCmdLine(int argc, char **argv) {
+  int i;
+  for (i = 1; i < argc; i++) {
+    if (strcmp(argv[i], "-cpu") == 0) {
+      useGpu = 0;
+    }
+
+    if (strcmp(argv[i], "-h") == 0) {
+      printUsage();
+      cleanup(SUCCESS);
+    }
+
+    if (strcmp(argv[i], "-width") == 0) {
+      ++i;
+      if (i == argc) {
+        printf("width option must be followed by value\n");
+        return FAILURE;
+      }
+      if (sscanf(argv[i], "%d", &width) != 1) {
+        printf("Error: invalid width value\n");
+        return FAILURE;
+      }
+    }
+
+    if (strcmp(argv[i], "-height") == 0) {
+      ++i;
+      if (i == argc) {
+        printf("height option must be followed by value\n");
+        return FAILURE;
+      }
+      if (sscanf(argv[i], "%d", &height) != 1) {
+        printf("Error: invalid height value\n");
+        return FAILURE;
+      }
+    }
+    if (strcmp(argv[i], "-itr") == 0) {
+      ++i;
+      if (i == argc) {
+        printf("itr option must be followed by iteration value\n");
+        return FAILURE;
+      }
+      if (sscanf(argv[i], "%d", &itr) != 1) {
+        printf("Error: invalid iteration value\n");
+        return FAILURE;
+      }
+    }
+  }
+
+  return SUCCESS;
+}
+
+void printUsage(void) {
+  printf("Usage:\n");
+  printf("\t-h\tPrint command line options\n");
+  printf("\t-cpu\tSync on the CPU instead of the GPU\n");
+  printf("\t-width w\tSet the width to w\n");
+  printf("\t-height h\tSet the height to h\n");
+  printf("\t-itr i\tSet number of iterations to i\n");
+}
+
+void checkSync(int argc, char **argv) {
+  int x, y;
+  int bufferSize = width * height * 4;
+  unsigned char *pSurf_read = NULL, *pSurf_write = NULL;
+  int integrated;
+
+  hipError_t status = hipSuccess;
+
+  // Init values for variables
+  x = y = 0;
+
+  if (hipSuccess != (status = hipInit(0))) {
+    printf("Failed to initialize CUDA\n");
+  }
+  device = findCudaDeviceDRV(argc, (const char **)argv);
+
+  if (hipSuccess != (status = hipCtxCreate(&context, 0, device))) {
+    printf("failed to create CUDA context\n");
+  }
+  hipCtxPushCurrent(context);
+
+  status =
+      hipDeviceGetAttribute(&integrated, hipDeviceAttributeIntegrated, device);
+  if (status != hipSuccess) {
+    printf("Failed to get device attribute hipDeviceAttributeIntegrated\n");
+    cleanup(FAILURE);
+  }
+
+  if (integrated != 1) {
+    printf(
+        "EGLSync_CUDAEvent_Interop does not support dGPU. Waiving sample.\n");
+    cleanup(WAIVED);
+  }
+
+#if (defined(__arm__) || defined(__aarch64__)) && defined(__linux__)
+  graphics_setup_window(0, 0, width, height, "EGLSync_CUDA_Interop");
+#endif
+
+  pSurf_read = (unsigned char *)malloc(bufferSize);
+  pSurf_write = (unsigned char *)malloc(bufferSize);
+  if (pSurf_read == NULL || pSurf_write == NULL) {
+    printf("malloc failed\n");
+    cleanup(FAILURE);
+  }
+
+  for (x = 0; x < width; x++) {
+    for (y = 0; y < height; y++) {
+      pSurf_read[(y * width + x) * 4] = 1;
+      pSurf_read[(y * width + x) * 4 + 1] = 1;
+      pSurf_read[(y * width + x) * 4 + 2] = 1;
+      pSurf_read[(y * width + x) * 4 + 3] = 1;
+      pSurf_write[(y * width + x) * 4] = 0;
+      pSurf_write[(y * width + x) * 4 + 1] = 0;
+      pSurf_write[(y * width + x) * 4 + 2] = 0;
+      pSurf_write[(y * width + x) * 4 + 3] = 0;
+    }
+  }
+
+  // NOP call to error-check the above glut calls
+  GL_SAFE_CALL({});
+
+  // Init texture
+  GL_SAFE_CALL(glGenTextures(2, tex));
+
+  GL_SAFE_CALL(glBindTexture(GL_TEXTURE_2D, tex[GL_READ]));
+  GL_SAFE_CALL(
+      glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST));
+  GL_SAFE_CALL(
+      glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST));
+  GL_SAFE_CALL(glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA8, width, height, 0,
+                            GL_RGBA, GL_UNSIGNED_BYTE, pSurf_read));
+  GL_SAFE_CALL(glBindTexture(GL_TEXTURE_2D, 0));
+
+  GL_SAFE_CALL(glBindTexture(GL_TEXTURE_2D, tex[GL_WRITE]));
+  GL_SAFE_CALL(
+      glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST));
+  GL_SAFE_CALL(
+      glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST));
+  GL_SAFE_CALL(glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA8, width, height, 0,
+                            GL_RGBA, GL_UNSIGNED_BYTE, pSurf_write));
+  GL_SAFE_CALL(glBindTexture(GL_TEXTURE_2D, 0));
+
+  glFinish();
+
+  EGLDisplay eglDisplayHandle = eglGetCurrentDisplay();
+  if (eglDisplayHandle == EGL_NO_DISPLAY) {
+    printf("eglDisplayHandle failed \n");
+    cleanup(FAILURE);
+  } else {
+    printf("eglDisplay Handle created \n");
+  }
+
+  if (!eglSetupExtensions()) {
+    printf("SetupExtentions failed \n");
+    cleanup(FAILURE);
+  }
+
+  EGLContext eglCtx = eglGetCurrentContext();
+  if (eglCtx == EGL_NO_CONTEXT) {
+    printf("Context1 create failed with error %d\n", eglGetError());
+    cleanup(FAILURE);
+  }
+
+  // Create the EGL_Image
+  EGLint eglImgAttrs[] = {EGL_IMAGE_PRESERVED_KHR, EGL_TRUE, EGL_NONE,
+                          EGL_NONE};
+
+  EGLImageKHR eglImage1 =
+      eglCreateImageKHR(eglDisplayHandle, eglCtx, EGL_GL_TEXTURE_2D_KHR,
+                        (EGLClientBuffer)(intptr_t)tex[GL_READ], eglImgAttrs);
+  if (eglImage1 == EGL_NO_IMAGE_KHR) {
+    printf("EGLImage create failed for read texture with error %d\n",
+           eglGetError());
+    cleanup(FAILURE);
+  } else {
+    printf("EGLImage1 created \n");
+  }
+
+  EGLImageKHR eglImage2 =
+      eglCreateImageKHR(eglDisplayHandle, eglCtx, EGL_GL_TEXTURE_2D_KHR,
+                        (EGLClientBuffer)(intptr_t)tex[GL_WRITE], eglImgAttrs);
+  if (eglImage2 == EGL_NO_IMAGE_KHR) {
+    printf("EGLImage create failed for write texture with error %d\n",
+           eglGetError());
+    cleanup(FAILURE);
+  } else {
+    printf("EGLImage2 created \n");
+  }
+
+  glFinish();
+
+  status = cuGraphicsEGLRegisterImage(&writeResource, eglImage1,
+                                      hipGraphicsRegisterFlagsNone);
+  if (status != hipSuccess) {
+    printf("cuGraphicsEGLRegisterImage failed with Texture 1\n");
+    cleanup(FAILURE);
+  } else {
+    printf(
+        "cuGraphicsEGLRegisterImage Passed, writeResource created with texture "
+        "1\n");
+  }
+
+  status =
+      hipGraphicsSubResourceGetMappedArray(&writeArray, writeResource, 0, 0);
+  if (status != hipSuccess) {
+    printf(
+        "hipGraphicsSubResourceGetMappedArray failed for writeResource with "
+        "texture 1\n");
+    cleanup(FAILURE);
+  }
+
+  status = cuGraphicsEGLRegisterImage(&readResource, eglImage2,
+                                      hipGraphicsRegisterFlagsNone);
+  if (status != hipSuccess) {
+    printf(
+        "cuGraphicsEGLRegisterImage failed for readResource with Texture 2\n");
+    cleanup(FAILURE);
+  } else {
+    printf(
+        "cuGraphicsEGLRegisterImage Passed, readResource created with texture "
+        "2\n");
+  }
+
+  status = hipGraphicsSubResourceGetMappedArray(&readArray, readResource, 0, 0);
+  if (status != hipSuccess) {
+    printf("hipGraphicsSubResourceGetMappedArray failed for texture 2\n");
+    cleanup(FAILURE);
+  }
+
+  if (useGpu) {
+    printf("Using GPU Sync path\n");
+    checkSyncOnGPU(eglDisplayHandle);
+  } else {
+    printf("Using CPU Sync path\n");
+    checkSyncOnCPU();
+  }
+
+  free(pSurf_read);
+  free(pSurf_write);
+  cleanup(SUCCESS);
+}
+
+void checkSyncOnCPU(void) {
+  int z = 0;
+  unsigned char expectedData, newData;
+  hipError_t status = hipSuccess;
+  HIP_RESOURCE_DESC wdsc, rdsc;
+  memset(&wdsc, 0, sizeof(wdsc));
+  memset(&rdsc, 0, sizeof(rdsc));
+
+  expectedData = 0;
+  newData = 1;
+
+  wdsc.resType = HIP_RESOURCE_TYPE_ARRAY;
+  wdsc.res.array.hArray = writeArray;
+  hipSurfaceObject_t writeSurface;
+  rdsc.resType = HIP_RESOURCE_TYPE_ARRAY;
+  rdsc.res.array.hArray = readArray;
+  hipSurfaceObject_t readSurface;
+
+  status = cuSurfObjectCreate(&writeSurface, &wdsc);
+  if (status != hipSuccess) {
+    printf("Surface bounding failed with status %d\n", status);
+    cleanup(FAILURE);
+  }
+  status = cuSurfObjectCreate(&readSurface, &rdsc);
+  if (status != hipSuccess) {
+    printf("Surface bounding failed\n");
+    cleanup(FAILURE);
+  }
+
+  for (z = 0; z < itr; z++) {
+    // GL call to copy from read texture to write texture
+    GL_SAFE_CALL(glCopyImageSubData(tex[GL_READ], GL_TEXTURE_2D, 0, 0, 0, 0,
+                                    tex[GL_WRITE], GL_TEXTURE_2D, 0, 0, 0, 0,
+                                    width, height, 1));
+
+    glFinish();
+
+    newData++;
+    expectedData++;
+
+    verify_and_update_kernel<<<(width * height) / 256, 256>>>(
+        writeSurface, readSurface, expectedData, newData, width, height);
+
+    status = hipCtxSynchronize();
+    if (status != hipSuccess) {
+      printf("hipCtxSynchronize failed \n");
+    }
+  }
+
+  hipError_t err = cudaGetValueMismatch();
+  if (err != hipSuccess) {
+    printf("Value mismatch seen when using CPU sync\n");
+    cleanup(FAILURE);
+  }
+
+  // Clean up CUDA writeResource
+  status = hipGraphicsUnregisterResource(writeResource);
+  if (status != hipSuccess) {
+    printf("Failed to unregister %d", status);
+    cleanup(FAILURE);
+  } else {
+    printf("Unregistered writeResource. \n");
+  }
+
+  // Clean up CUDA readResource
+  status = hipGraphicsUnregisterResource(readResource);
+  if (status != hipSuccess) {
+    printf("Failed to unregister %d", status);
+    cleanup(FAILURE);
+  } else {
+    printf("Unregistered readResource. \n");
+  }
+}
+
+/*
+    Performs same function as checkSyncOnCPU
+    Here instead of glFinish() and hipCtxSynchronize like in checkSyncOnCPU,
+    we make use of EGLSync, CUDA Event and hipStreamWaitEvent, eglWaitSyncKHR to
+   achieve the synchronization due to this CPU is not blocked for any
+   synchronization needed between GL-EGL & CUDA operations all synchronizations
+   happens on the GPU only.
+*/
+void checkSyncOnGPU(EGLDisplay dpy) {
+  int z = 0;
+  unsigned char expectedData, newData;
+  hipError_t err;
+  hipError_t status = hipSuccess;
+  hipStream_t stream;
+  hipEvent_t timingDisabledEvent;
+  HIP_RESOURCE_DESC wdsc, rdsc;
+  memset(&wdsc, 0, sizeof(wdsc));
+  memset(&rdsc, 0, sizeof(rdsc));
+
+  expectedData = 0;
+  newData = 1;
+
+  wdsc.resType = HIP_RESOURCE_TYPE_ARRAY;
+  wdsc.res.array.hArray = writeArray;
+  hipSurfaceObject_t writeSurface;
+  rdsc.resType = HIP_RESOURCE_TYPE_ARRAY;
+  rdsc.res.array.hArray = readArray;
+  hipSurfaceObject_t readSurface;
+
+  status = cuSurfObjectCreate(&writeSurface, &wdsc);
+  if (status != hipSuccess) {
+    printf("Surface bounding failed with status %d\n", status);
+    cleanup(FAILURE);
+  }
+  status = cuSurfObjectCreate(&readSurface, &rdsc);
+  if (status != hipSuccess) {
+    printf("Surface bounding failed\n");
+    cleanup(FAILURE);
+  }
+
+  status = hipStreamCreateWithFlags(&stream, hipStreamDefault);
+  if (status != hipSuccess) {
+    printf("Stream creation failed\n");
+    cleanup(FAILURE);
+  }
+
+  // Creates timing disabled event which uses non-blocking synchronization
+  status = hipEventCreateWithFlags(&timingDisabledEvent, hipEventDisableTiming);
+  if (status != hipSuccess) {
+    printf("Default event creation failed\n");
+    cleanup(FAILURE);
+  }
+
+  /*
+      1. We perform texture-to-texture copy in GLES which is async function
+      2. Followed by creating EGLSync and a CUDA Event from that EGLSync object
+      3. Using hipStreamWaitEvent() we wait in GPU for the GLES to finish texture
+     copy.
+      4. CUDA kernel verfiy_and_update_kernel verifies if the copied data by
+     GLES is correct, and it updates the buffer with new values.
+      5. This is followed by eglWaitSyncKHR() which waits for the cuda kernel to
+     finish, so that in the next iteration GLES can perform the copying of the
+     updated buffer to write texture,
+  */
+  for (z = 0; z < itr; z++) {
+    // GL call to copy from read texture to write texture
+    GL_SAFE_CALL(glCopyImageSubData(tex[GL_READ], GL_TEXTURE_2D, 0, 0, 0, 0,
+                                    tex[GL_WRITE], GL_TEXTURE_2D, 0, 0, 0, 0,
+                                    width, height, 1));
+
+    EGLSyncKHR eglSyncForGL, eglSyncForCuda;
+    EGLBoolean egl_status = EGL_TRUE;
+    EGLAttribKHR eglattrib[] = {EGL_CUDA_EVENT_HANDLE_NV,
+                                (EGLAttrib)timingDisabledEvent, EGL_NONE};
+
+    hipEvent_t cudaEGLSyncEvent;
+
+    eglSyncForGL = eglCreateSyncKHR(dpy, EGL_SYNC_FENCE_KHR, NULL);
+
+    if (eglSyncForGL == EGL_NO_SYNC_KHR) {
+      printf(" EGL Sync creation failed\n");
+      cleanup(FAILURE);
+    }
+
+    status = cuEventCreateFromEGLSync(&cudaEGLSyncEvent, eglSyncForGL,
+                                      hipEventDefault);
+    if (status != hipSuccess) {
+      printf("CUDA event creation from EGLSync failed\n");
+      cleanup(FAILURE);
+    }
+
+    // We wait from CUDA in GPU for GL-EGL operation completion
+    status = hipStreamWaitEvent(stream, cudaEGLSyncEvent, 0);
+    if (status != hipSuccess) {
+      printf("Stream wait for event created from EGLSync failed\n");
+      cleanup(FAILURE);
+    }
+
+    egl_status = eglDestroySyncKHR(dpy, eglSyncForGL);
+    if (egl_status != EGL_TRUE) {
+      printf("EGL sync object destruction failed\n");
+      cleanup(FAILURE);
+    }
+
+    newData++;
+    expectedData++;
+
+    // Verifies the values in readSurface which is copied by
+    // glCopyImageSubData() And writes value of newData into writeSurface
+    verify_and_update_kernel<<<(width * height) / 256, 256, 0, stream>>>(
+        writeSurface, readSurface, expectedData, newData, width, height);
+
+    status = hipEventDestroy(cudaEGLSyncEvent);
+    if (status != hipSuccess) {
+      printf("Event Destroy failed\n");
+      cleanup(FAILURE);
+    }
+
+    status = hipEventRecord(timingDisabledEvent, stream);
+    if (status != hipSuccess) {
+      printf("Event Record failed\n");
+      cleanup(FAILURE);
+    }
+
+    // creating an EGL sync object linked to a CUDA event object
+    eglSyncForCuda = eglCreateSync64KHR(dpy, EGL_SYNC_CUDA_EVENT_NV, eglattrib);
+
+    // We wait from EGL for CUDA operation completion
+    egl_status = eglWaitSyncKHR(dpy, eglSyncForCuda, 0);
+    if (egl_status != EGL_TRUE) {
+      printf("eglWaitSyncKHR failed\n");
+      cleanup(FAILURE);
+    }
+    egl_status = eglDestroySyncKHR(dpy, eglSyncForCuda);
+    if (egl_status != EGL_TRUE) {
+      printf("EGL sync object destruction failed\n");
+      cleanup(FAILURE);
+    }
+  }
+
+  err = cudaGetValueMismatch();
+  if (err != hipSuccess) {
+    printf("Value mismatch seen when using GPU sync\n");
+    cleanup(FAILURE);
+  }
+
+  // Clean up CUDA writeResource
+  status = hipGraphicsUnregisterResource(writeResource);
+  if (status != hipSuccess) {
+    printf("Failed to unregister %d", status);
+    cleanup(FAILURE);
+  } else {
+    printf("Unregistered writeResource. \n");
+  }
+
+  // Clean up CUDA readResource
+  status = hipGraphicsUnregisterResource(readResource);
+  if (status != hipSuccess) {
+    printf("Failed to unregister %d", status);
+    cleanup(FAILURE);
+  } else {
+    printf("Unregistered readResource. \n");
+  }
+}
+
+// Verifies the values in readSurface whether they are expected ones
+// And writes value of newData into writeSurface
+__global__ void verify_and_update_kernel(hipSurfaceObject_t write, hipSurfaceObject_t read,
+                                         char expected, char newval, int width,
+                                         int height) {
+  unsigned int x = blockDim.x * blockIdx.x + threadIdx.x;
+  unsigned int y = blockDim.y * blockIdx.y + threadIdx.y;
+
+  if (x < width && y < height) {
+    uchar4 check;
+    surf2Dread(&check, read, x * 4, y);
+    if (check.x != expected || check.y != expected || check.z != expected ||
+        check.w != expected) {
+      printf(
+          "Mismatch found in values read[0]= %u read[1]= %u read[2]= %u "
+          "read[3]= %u expected is %u\n",
+          check.x, check.y, check.z, check.w, expected);
+      numErrors++;
+      return;
+    }
+    uchar4 data = make_uchar4(newval, newval, newval, newval);
+    surf2Dwrite(data, write, x * 4, y);
+  }
+}
+
+__global__ void getNumErrors(int *numErr) { *numErr = numErrors; }
+
+extern "C" hipError_t cudaGetValueMismatch() {
+  int numErr_h;
+  int *numErr_d = NULL;
+  hipError_t err = hipSuccess;
+
+  err = hipMalloc(&numErr_d, sizeof(int));
+  if (err != hipSuccess) {
+    printf("Cuda Main: hipMemcpy failed with %s\n", hipGetErrorString(err));
+    hipFree(numErr_d);
+    return err;
+  }
+
+  getNumErrors<<<1, 1>>>(numErr_d);
+  err = hipDeviceSynchronize();
+  if (err != hipSuccess) {
+    printf("Cuda Main: hipDeviceSynchronize failed with %s\n",
+           hipGetErrorString(err));
+  }
+  err = hipMemcpy(&numErr_h, numErr_d, sizeof(int), hipMemcpyDeviceToHost);
+  if (err != hipSuccess) {
+    printf("Cuda Main: hipMemcpy failed with %s\n", hipGetErrorString(err));
+    hipFree(numErr_d);
+    return err;
+  }
+  err = hipFree(numErr_d);
+  if (err != hipSuccess) {
+    printf("Cuda Main: hipFree failed with %s\n", hipGetErrorString(err));
+    return err;
+  }
+  if (numErr_h > 0) {
+    return hipErrorUnknown;
+  }
+  return hipSuccess;
+}
+
+// Clean up state and exit. If status is SUCCESS, regression success is printed
+// to stdout. This will happen if the glut timer is triggered. If status is
+// anything else, the regression failure message is printed.
+void cleanup(int status) {
+  GLenum glErr = GL_NO_ERROR;
+  hipError_t cudaErr = hipSuccess;
+  int exitStatus = status;
+
+  // Clean up GL
+  if (*tex) {
+    GL_SAFE_CALL_NO_CLEANUP(glDeleteTextures(2, tex), glErr);
+  }
+
+  // Print test status and exit
+  if (glErr != GL_NO_ERROR || cudaErr != hipSuccess) exitStatus = FAILURE;
+
+  printStatus(exitStatus);
+
+  cleanExit = 1;
+
+  graphics_close_window();
+
+  if (exitStatus == FAILURE) exit(EXIT_FAILURE);
+
+  if (exitStatus == WAIVED) exit(EXIT_WAIVED);
+
+  exit(0);
+}
+
+void exitHandler(void) {
+  if (!cleanExit) {
+    printf("&&&& EGLSync_CUDAEvent_Interop unexpected failure \n");
+    printStatus(FAILURE);
+  }
+}
+
+// Print test success or fail for regression testing
+void printStatus(int status) {
+  switch (status) {
+    case SUCCESS:
+      printf("&&&& EGLSync_CUDAEvent_Interop PASSED\n");
+      break;
+    case WAIVED:
+      printf("&&&& EGLSync_CUDAEvent_Interop WAIVED\n");
+      break;
+    default:
+      printf("&&&& EGLSync_CUDAEvent_Interop FAILED\n");
+      break;
+  }
+  fflush(stdout);
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/FunctionPointers/FunctionPointers_kernels.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/FunctionPointers/FunctionPointers_kernels.cu.hip
index 57ad723..cc9e695 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/FunctionPointers/FunctionPointers_kernels.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/FunctionPointers/FunctionPointers_kernels.cu.hip
@@ -33,7 +33,7 @@
 namespace cg = cooperative_groups;
 
 #include "helper_cuda_hipified.h"
-#include "HIPCHECK.h"
+
 #include "FunctionPointers_kernels.h"
 
 // Texture object for reading image
@@ -400,4 +400,8 @@ extern "C" void sobelFilter(Pixel *odata, int iw, int ih,
           iw, ih, fScale, blockOperation, pPointOp, tex);
     } break;
   }
+}
+   iw, ih, fScale, blockOperation, pPointOp, tex);
+    } break;
+  }
 }
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiInlineP/src/piestimator.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiInlineP/src/piestimator.cu.hip
index e69de29..c56c703 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiInlineP/src/piestimator.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiInlineP/src/piestimator.cu.hip
@@ -0,0 +1,284 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "../inc/piestimator.h"
+
+#include <string>
+#include <vector>
+#include <numeric>
+#include <stdexcept>
+#include <typeinfo>
+#include <hip/hip_runtime.h>
+#include <hip/hip_cooperative_groups.h>
+
+namespace cg = cooperative_groups;
+#include <hiprand_kernel.h>
+
+using std::string;
+using std::vector;
+
+// RNG init kernel
+__global__ void initRNG(hiprandState *const rngStates, const unsigned int seed) {
+  // Determine thread ID
+  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
+
+  // Initialise the RNG
+  hiprand_init(seed, tid, 0, &rngStates[tid]);
+}
+
+__device__ unsigned int reduce_sum(unsigned int in, cg::thread_block cta) {
+  extern __shared__ unsigned int sdata[];
+
+  // Perform first level of reduction:
+  // - Write to shared memory
+  unsigned int ltid = threadIdx.x;
+
+  sdata[ltid] = in;
+  cg::sync(cta);
+
+  // Do reduction in shared mem
+  for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {
+    if (ltid < s) {
+      sdata[ltid] += sdata[ltid + s];
+    }
+
+    cg::sync(cta);
+  }
+
+  return sdata[0];
+}
+
+__device__ inline void getPoint(float &x, float &y, hiprandState &state) {
+  x = hiprand_uniform(&state);
+  y = hiprand_uniform(&state);
+}
+__device__ inline void getPoint(double &x, double &y, hiprandState &state) {
+  x = hiprand_uniform_double(&state);
+  y = hiprand_uniform_double(&state);
+}
+
+// Estimator kernel
+template <typename Real>
+__global__ void computeValue(unsigned int *const results,
+                             hiprandState *const rngStates,
+                             const unsigned int numSims) {
+  // Handle to thread block group
+  cg::thread_block cta = cg::this_thread_block();
+  // Determine thread ID
+  unsigned int bid = blockIdx.x;
+  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int step = gridDim.x * blockDim.x;
+
+  // Initialise the RNG
+  hiprandState localState = rngStates[tid];
+
+  // Count the number of points which lie inside the unit quarter-circle
+  unsigned int pointsInside = 0;
+
+  for (unsigned int i = tid; i < numSims; i += step) {
+    Real x;
+    Real y;
+    getPoint(x, y, localState);
+    Real l2norm2 = x * x + y * y;
+
+    if (l2norm2 < static_cast<Real>(1)) {
+      pointsInside++;
+    }
+  }
+
+  // Reduce within the block
+  pointsInside = reduce_sum(pointsInside, cta);
+
+  // Store the result
+  if (threadIdx.x == 0) {
+    results[bid] = pointsInside;
+  }
+}
+
+template <typename Real>
+PiEstimator<Real>::PiEstimator(unsigned int numSims, unsigned int device,
+                               unsigned int threadBlockSize, unsigned int seed)
+    : m_numSims(numSims),
+      m_device(device),
+      m_threadBlockSize(threadBlockSize),
+      m_seed(seed) {}
+
+template <typename Real>
+Real PiEstimator<Real>::operator()() {
+  hipError_t cudaResult = hipSuccess;
+  struct hipDeviceProp_t deviceProperties;
+  struct hipFuncAttributes funcAttributes;
+
+  // Get device properties
+  cudaResult = hipGetDeviceProperties(&deviceProperties, m_device);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not get device properties: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Check precision is valid
+  if (typeid(Real) == typeid(double) &&
+      (deviceProperties.major < 1 ||
+       (deviceProperties.major == 1 && deviceProperties.minor < 3))) {
+    throw std::runtime_error("Device does not have double precision support");
+  }
+
+  // Attach to GPU
+  cudaResult = hipSetDevice(m_device);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not set CUDA device: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Determine how to divide the work between cores
+  dim3 block;
+  dim3 grid;
+  block.x = m_threadBlockSize;
+  grid.x = (m_numSims + m_threadBlockSize - 1) / m_threadBlockSize;
+
+  // Aim to launch around ten or more times as many blocks as there
+  // are multiprocessors on the target device.
+  unsigned int blocksPerSM = 10;
+  unsigned int numSMs = deviceProperties.multiProcessorCount;
+
+  while (grid.x > 2 * blocksPerSM * numSMs) {
+    grid.x >>= 1;
+  }
+
+  // Get initRNG function properties and check the maximum block size
+  cudaResult = hipFuncGetAttributes(&funcAttributes, initRNG);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not get function attributes: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  if (block.x > (unsigned int)funcAttributes.maxThreadsPerBlock) {
+    throw std::runtime_error(
+        "Block X dimension is too large for initRNG kernel");
+  }
+
+  // Get computeValue function properties and check the maximum block size
+  cudaResult = hipFuncGetAttributes(&funcAttributes, computeValue<Real>);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not get function attributes: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  if (block.x > (unsigned int)funcAttributes.maxThreadsPerBlock) {
+    throw std::runtime_error(
+        "Block X dimension is too large for computeValue kernel");
+  }
+
+  // Check the dimensions are valid
+  if (block.x > (unsigned int)deviceProperties.maxThreadsDim[0]) {
+    throw std::runtime_error("Block X dimension is too large for device");
+  }
+
+  if (grid.x > (unsigned int)deviceProperties.maxGridSize[0]) {
+    throw std::runtime_error("Grid X dimension is too large for device");
+  }
+
+  // Allocate memory for RNG states
+  hiprandState *d_rngStates = 0;
+  cudaResult =
+      hipMalloc((void **)&d_rngStates, grid.x * block.x * sizeof(hiprandState));
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not allocate memory on device for RNG states: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Allocate memory for result
+  // Each thread block will produce one result
+  unsigned int *d_results = 0;
+  cudaResult = hipMalloc((void **)&d_results, grid.x * sizeof(unsigned int));
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not allocate memory on device for partial results: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Initialise RNG
+  initRNG<<<grid, block>>>(d_rngStates, m_seed);
+
+  // Count the points inside unit quarter-circle
+  computeValue<Real><<<grid, block, block.x * sizeof(unsigned int)>>>(
+      d_results, d_rngStates, m_numSims);
+
+  // Copy partial results back
+  vector<unsigned int> results(grid.x);
+  cudaResult = hipMemcpy(&results[0], d_results, grid.x * sizeof(unsigned int),
+                          hipMemcpyDeviceToHost);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not copy partial results to host: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Complete sum-reduction on host
+  Real value =
+      static_cast<Real>(std::accumulate(results.begin(), results.end(), 0));
+
+  // Determine the proportion of points inside the quarter-circle,
+  // i.e. the area of the unit quarter-circle
+  value /= m_numSims;
+
+  // Value is currently an estimate of the area of a unit quarter-circle, so we
+  // can scale to a full circle by multiplying by four. Now since the area of a
+  // circle is pi * r^2, and r is one, the value will be an estimate for the
+  // value of pi.
+  value *= 4;
+
+  // Cleanup
+  if (d_rngStates) {
+    hipFree(d_rngStates);
+    d_rngStates = 0;
+  }
+
+  if (d_results) {
+    hipFree(d_results);
+    d_results = 0;
+  }
+
+  return value;
+}
+
+// Explicit template instantiation
+template class PiEstimator<float>;
+template class PiEstimator<double>;
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiQ/src/piestimator.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiQ/src/piestimator.cu.hip
index e69de29..581aa2c 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiQ/src/piestimator.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiQ/src/piestimator.cu.hip
@@ -0,0 +1,312 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "../inc/piestimator.h"
+
+#include <string>
+#include <vector>
+#include <numeric>
+#include <stdexcept>
+#include <typeinfo>
+#include <hip/hip_runtime.h>
+#include <hip/hip_cooperative_groups.h>
+
+namespace cg = cooperative_groups;
+#include <hiprand.h>
+
+using std::string;
+using std::vector;
+
+__device__ unsigned int reduce_sum(unsigned int in, cg::thread_block cta) {
+  extern __shared__ unsigned int sdata[];
+
+  // Perform first level of reduction:
+  // - Write to shared memory
+  unsigned int ltid = threadIdx.x;
+
+  sdata[ltid] = in;
+  cg::sync(cta);
+
+  // Do reduction in shared mem
+  for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {
+    if (ltid < s) {
+      sdata[ltid] += sdata[ltid + s];
+    }
+
+    cg::sync(cta);
+  }
+
+  return sdata[0];
+}
+
+// Estimator kernel
+template <typename Real>
+__global__ void computeValue(unsigned int *const results,
+                             const Real *const points,
+                             const unsigned int numSims) {
+  // Handle to thread block group
+  cg::thread_block cta = cg::this_thread_block();
+  // Determine thread ID
+  unsigned int bid = blockIdx.x;
+  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int step = gridDim.x * blockDim.x;
+
+  // Shift the input/output pointers
+  const Real *pointx = points + tid;
+  const Real *pointy = pointx + numSims;
+
+  // Count the number of points which lie inside the unit quarter-circle
+  unsigned int pointsInside = 0;
+
+  for (unsigned int i = tid; i < numSims;
+       i += step, pointx += step, pointy += step) {
+    Real x = *pointx;
+    Real y = *pointy;
+    Real l2norm2 = x * x + y * y;
+
+    if (l2norm2 < static_cast<Real>(1)) {
+      pointsInside++;
+    }
+  }
+
+  // Reduce within the block
+  pointsInside = reduce_sum(pointsInside, cta);
+
+  // Store the result
+  if (threadIdx.x == 0) {
+    results[bid] = pointsInside;
+  }
+}
+
+template <typename Real>
+PiEstimator<Real>::PiEstimator(unsigned int numSims, unsigned int device,
+                               unsigned int threadBlockSize)
+    : m_numSims(numSims),
+      m_device(device),
+      m_threadBlockSize(threadBlockSize) {}
+
+template <typename Real>
+Real PiEstimator<Real>::operator()() {
+  hipError_t cudaResult = hipSuccess;
+  struct hipDeviceProp_t deviceProperties;
+  struct hipFuncAttributes funcAttributes;
+
+  // Get device properties
+  cudaResult = hipGetDeviceProperties(&deviceProperties, m_device);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not get device properties: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Check precision is valid
+  if (typeid(Real) == typeid(double) &&
+      (deviceProperties.major < 1 ||
+       (deviceProperties.major == 1 && deviceProperties.minor < 3))) {
+    throw std::runtime_error("Device does not have double precision support");
+  }
+
+  // Attach to GPU
+  cudaResult = hipSetDevice(m_device);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not set CUDA device: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Determine how to divide the work between cores
+  dim3 block;
+  dim3 grid;
+  block.x = m_threadBlockSize;
+  grid.x = (m_numSims + m_threadBlockSize - 1) / m_threadBlockSize;
+
+  // Aim to launch around ten or more times as many blocks as there
+  // are multiprocessors on the target device.
+  unsigned int blocksPerSM = 10;
+  unsigned int numSMs = deviceProperties.multiProcessorCount;
+
+  while (grid.x > 2 * blocksPerSM * numSMs) {
+    grid.x >>= 1;
+  }
+
+  // Get computeValue function properties and check the maximum block size
+  cudaResult = hipFuncGetAttributes(&funcAttributes, computeValue<Real>);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not get function attributes: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  if (block.x > (unsigned int)funcAttributes.maxThreadsPerBlock) {
+    throw std::runtime_error(
+        "Block X dimension is too large for computeValue kernel");
+  }
+
+  // Check the dimensions are valid
+  if (block.x > (unsigned int)deviceProperties.maxThreadsDim[0]) {
+    throw std::runtime_error("Block X dimension is too large for device");
+  }
+
+  if (grid.x > (unsigned int)deviceProperties.maxGridSize[0]) {
+    throw std::runtime_error("Grid X dimension is too large for device");
+  }
+
+  // Allocate memory for points
+  // Each simulation has two random numbers to give X and Y coordinate
+  Real *d_points = 0;
+  cudaResult = hipMalloc((void **)&d_points, 2 * m_numSims * sizeof(Real));
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not allocate memory on device for random numbers: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Allocate memory for result
+  // Each thread block will produce one result
+  unsigned int *d_results = 0;
+  cudaResult = hipMalloc((void **)&d_results, grid.x * sizeof(unsigned int));
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not allocate memory on device for partial results: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Generate random points in unit square
+  hiprandStatus_t curandResult;
+  hiprandGenerator_t qrng;
+
+  if (typeid(Real) == typeid(float)) {
+    curandResult = hiprandCreateGenerator(&qrng, HIPRAND_RNG_QUASI_SOBOL32);
+  } else if (typeid(Real) == typeid(double)) {
+    curandResult = hiprandCreateGenerator(&qrng, HIPRAND_RNG_QUASI_SOBOL64);
+  } else {
+    string msg("Could not create random number generator of specified type");
+    throw std::runtime_error(msg);
+  }
+
+  if (curandResult != HIPRAND_STATUS_SUCCESS) {
+    string msg("Could not create quasi-random number generator: ");
+    msg += curandResult;
+    throw std::runtime_error(msg);
+  }
+
+  curandResult = hiprandSetQuasiRandomGeneratorDimensions(qrng, 2);
+
+  if (curandResult != HIPRAND_STATUS_SUCCESS) {
+    string msg(
+        "Could not set number of dimensions for quasi-random number "
+        "generator: ");
+    msg += curandResult;
+    throw std::runtime_error(msg);
+  }
+
+  curandResult =
+      curandSetGeneratorOrdering(qrng, CURAND_ORDERING_QUASI_DEFAULT);
+
+  if (curandResult != HIPRAND_STATUS_SUCCESS) {
+    string msg("Could not set order for quasi-random number generator: ");
+    msg += curandResult;
+    throw std::runtime_error(msg);
+  }
+
+  if (typeid(Real) == typeid(float)) {
+    curandResult =
+        hiprandGenerateUniform(qrng, (float *)d_points, 2 * m_numSims);
+  } else if (typeid(Real) == typeid(double)) {
+    curandResult =
+        hiprandGenerateUniformDouble(qrng, (double *)d_points, 2 * m_numSims);
+  } else {
+    string msg("Could not generate random numbers of specified type");
+    throw std::runtime_error(msg);
+  }
+
+  if (curandResult != HIPRAND_STATUS_SUCCESS) {
+    string msg("Could not generate quasi-random numbers: ");
+    msg += curandResult;
+    throw std::runtime_error(msg);
+  }
+
+  curandResult = hiprandDestroyGenerator(qrng);
+
+  if (curandResult != HIPRAND_STATUS_SUCCESS) {
+    string msg("Could not destroy quasi-random number generator: ");
+    msg += curandResult;
+    throw std::runtime_error(msg);
+  }
+
+  // Count the points inside unit quarter-circle
+  computeValue<Real><<<grid, block, block.x * sizeof(unsigned int)>>>(
+      d_results, d_points, m_numSims);
+
+  // Copy partial results back
+  vector<unsigned int> results(grid.x);
+  cudaResult = hipMemcpy(&results[0], d_results, grid.x * sizeof(unsigned int),
+                          hipMemcpyDeviceToHost);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not copy partial results to host: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Complete sum-reduction on host
+  Real value =
+      static_cast<Real>(std::accumulate(results.begin(), results.end(), 0));
+
+  // Determine the proportion of points inside the quarter-circle,
+  // i.e. the area of the unit quarter-circle
+  value /= m_numSims;
+
+  // Value is currently an estimate of the area of a unit quarter-circle, so we
+  // can scale to a full circle by multiplying by four. Now since the area of a
+  // circle is pi * r^2, and r is one, the value will be an estimate for the
+  // value of pi.
+  value *= 4;
+
+  // Cleanup
+  if (d_points) {
+    hipFree(d_points);
+    d_points = 0;
+  }
+
+  if (d_results) {
+    hipFree(d_results);
+    d_results = 0;
+  }
+
+  return value;
+}
+
+// Explicit template instantiation
+template class PiEstimator<float>;
+template class PiEstimator<double>;
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/boxFilter/boxFilter_kernel.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/boxFilter/boxFilter_kernel.cu.hip
index e69de29..5645f19 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/boxFilter/boxFilter_kernel.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/boxFilter/boxFilter_kernel.cu.hip
@@ -0,0 +1,499 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _BOXFILTER_KERNEL_CH_
+#define _BOXFILTER_KERNEL_CH_
+
+#include <helper_math.h>
+#include "helper_functions.h"
+
+hipTextureObject_t tex;
+hipTextureObject_t texTempArray;
+hipTextureObject_t rgbaTex;
+hipTextureObject_t rgbaTexTempArray;
+hipArray *d_array, *d_tempArray;
+
+////////////////////////////////////////////////////////////////////////////////
+// These are CUDA Helper functions
+
+// This will output the proper CUDA error strings in the event that a CUDA host
+// call returns an error
+#define HIPCHECK(err) __HIPCHECK(err, __FILE__, __LINE__)
+
+inline void __HIPCHECK(hipError_t err, const char *file, const int line) {
+  if (hipSuccess != err) {
+    fprintf(stderr, "%s(%i) : CUDA Runtime API error %d: %s.\n", file, line,
+            (int)err, hipGetErrorString(err));
+    exit(EXIT_FAILURE);
+  }
+}
+
+/*
+  Perform a fast box filter using the sliding window method.
+
+  As the kernel moves from left to right, we add in the contribution of the
+  new sample on the right, and subtract the value of the exiting sample on the
+  left. This only requires 2 adds and a mul per output value, independent of the
+  filter radius. The box filter is separable, so to perform a 2D box filter we
+  perform the filter in the x direction, followed by the same filter in the y
+  direction. Applying multiple iterations of the box filter converges towards a
+  Gaussian blur. Using CUDA, rows or columns of the image are processed in
+  parallel. This version duplicates edge pixels.
+
+  Note that the x (row) pass suffers from uncoalesced global memory reads,
+  since each thread is reading from a different row. For this reason it is
+  better to use texture lookups for the x pass.
+  The y (column) pass is perfectly coalesced.
+
+  Parameters
+  id - pointer to input data in global memory
+  od - pointer to output data in global memory
+  w  - image width
+  h  - image height
+  r  - filter radius
+
+  e.g. for r = 2, w = 8:
+
+  0 1 2 3 4 5 6 7
+  x - -
+  - x - -
+  - - x - -
+    - - x - -
+      - - x - -
+        - - x - -
+          - - x -
+            - - x
+*/
+
+// process row
+__device__ void d_boxfilter_x(float *id, float *od, int w, int h, int r) {
+  float scale = 1.0f / (float)((r << 1) + 1);
+
+  float t;
+  // do left edge
+  t = id[0] * r;
+
+  for (int x = 0; x < (r + 1); x++) {
+    t += id[x];
+  }
+
+  od[0] = t * scale;
+
+  for (int x = 1; x < (r + 1); x++) {
+    t += id[x + r];
+    t -= id[0];
+    od[x] = t * scale;
+  }
+
+  // main loop
+  for (int x = (r + 1); x < w - r; x++) {
+    t += id[x + r];
+    t -= id[x - r - 1];
+    od[x] = t * scale;
+  }
+
+  // do right edge
+  for (int x = w - r; x < w; x++) {
+    t += id[w - 1];
+    t -= id[x - r - 1];
+    od[x] = t * scale;
+  }
+}
+
+// process column
+__device__ void d_boxfilter_y(float *id, float *od, int w, int h, int r) {
+  float scale = 1.0f / (float)((r << 1) + 1);
+
+  float t;
+  // do left edge
+  t = id[0] * r;
+
+  for (int y = 0; y < (r + 1); y++) {
+    t += id[y * w];
+  }
+
+  od[0] = t * scale;
+
+  for (int y = 1; y < (r + 1); y++) {
+    t += id[(y + r) * w];
+    t -= id[0];
+    od[y * w] = t * scale;
+  }
+
+  // main loop
+  for (int y = (r + 1); y < (h - r); y++) {
+    t += id[(y + r) * w];
+    t -= id[((y - r) * w) - w];
+    od[y * w] = t * scale;
+  }
+
+  // do right edge
+  for (int y = h - r; y < h; y++) {
+    t += id[(h - 1) * w];
+    t -= id[((y - r) * w) - w];
+    od[y * w] = t * scale;
+  }
+}
+
+__global__ void d_boxfilter_x_global(float *id, float *od, int w, int h,
+                                     int r) {
+  unsigned int y = blockIdx.x * blockDim.x + threadIdx.x;
+  d_boxfilter_x(&id[y * w], &od[y * w], w, h, r);
+}
+
+__global__ void d_boxfilter_y_global(float *id, float *od, int w, int h,
+                                     int r) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  d_boxfilter_y(&id[x], &od[x], w, h, r);
+}
+
+// texture version
+// texture fetches automatically clamp to edge of image
+__global__ void d_boxfilter_x_tex(float *od, int w, int h, int r,
+                                  hipTextureObject_t tex) {
+  float scale = 1.0f / (float)((r << 1) + 1);
+  unsigned int y = blockIdx.x * blockDim.x + threadIdx.x;
+
+  float t = 0.0f;
+
+  for (int x = -r; x <= r; x++) {
+    t += tex2D<float>(tex, x, y);
+  }
+
+  od[y * w] = t * scale;
+
+  for (int x = 1; x < w; x++) {
+    t += tex2D<float>(tex, x + r, y);
+    t -= tex2D<float>(tex, x - r - 1, y);
+    od[y * w + x] = t * scale;
+  }
+}
+
+__global__ void d_boxfilter_y_tex(float *od, int w, int h, int r,
+                                  hipTextureObject_t tex) {
+  float scale = 1.0f / (float)((r << 1) + 1);
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+
+  float t = 0.0f;
+
+  for (int y = -r; y <= r; y++) {
+    t += tex2D<float>(tex, x, y);
+  }
+
+  od[x] = t * scale;
+
+  for (int y = 1; y < h; y++) {
+    t += tex2D<float>(tex, x, y + r);
+    t -= tex2D<float>(tex, x, y - r - 1);
+    od[y * w + x] = t * scale;
+  }
+}
+
+// RGBA version
+// reads from 32-bit unsigned int array holding 8-bit RGBA
+
+// convert floating point rgba color to 32-bit integer
+__device__ unsigned int rgbaFloatToInt(float4 rgba) {
+  rgba.x = __saturatef(rgba.x);  // clamp to [0.0, 1.0]
+  rgba.y = __saturatef(rgba.y);
+  rgba.z = __saturatef(rgba.z);
+  rgba.w = __saturatef(rgba.w);
+  return ((unsigned int)(rgba.w * 255.0f) << 24) |
+         ((unsigned int)(rgba.z * 255.0f) << 16) |
+         ((unsigned int)(rgba.y * 255.0f) << 8) |
+         ((unsigned int)(rgba.x * 255.0f));
+}
+
+__device__ float4 rgbaIntToFloat(unsigned int c) {
+  float4 rgba;
+  rgba.x = (c & 0xff) * 0.003921568627f;          //  /255.0f;
+  rgba.y = ((c >> 8) & 0xff) * 0.003921568627f;   //  /255.0f;
+  rgba.z = ((c >> 16) & 0xff) * 0.003921568627f;  //  /255.0f;
+  rgba.w = ((c >> 24) & 0xff) * 0.003921568627f;  //  /255.0f;
+  return rgba;
+}
+
+// row pass using texture lookups
+__global__ void d_boxfilter_rgba_x(unsigned int *od, int w, int h, int r,
+                                   hipTextureObject_t rgbaTex) {
+  float scale = 1.0f / (float)((r << 1) + 1);
+  unsigned int y = blockIdx.x * blockDim.x + threadIdx.x;
+
+  // as long as address is always less than height, we do work
+  if (y < h) {
+    float4 t = make_float4(0.0f);
+
+    for (int x = -r; x <= r; x++) {
+      t += tex2D<float4>(rgbaTex, x, y);
+    }
+
+    od[y * w] = rgbaFloatToInt(t * scale);
+
+    for (int x = 1; x < w; x++) {
+      t += tex2D<float4>(rgbaTex, x + r, y);
+      t -= tex2D<float4>(rgbaTex, x - r - 1, y);
+      od[y * w + x] = rgbaFloatToInt(t * scale);
+    }
+  }
+}
+
+// column pass using coalesced global memory reads
+__global__ void d_boxfilter_rgba_y(unsigned int *id, unsigned int *od, int w,
+                                   int h, int r) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  id = &id[x];
+  od = &od[x];
+
+  float scale = 1.0f / (float)((r << 1) + 1);
+
+  float4 t;
+  // do left edge
+  t = rgbaIntToFloat(id[0]) * r;
+
+  for (int y = 0; y < (r + 1); y++) {
+    t += rgbaIntToFloat(id[y * w]);
+  }
+
+  od[0] = rgbaFloatToInt(t * scale);
+
+  for (int y = 1; y < (r + 1); y++) {
+    t += rgbaIntToFloat(id[(y + r) * w]);
+    t -= rgbaIntToFloat(id[0]);
+    od[y * w] = rgbaFloatToInt(t * scale);
+  }
+
+  // main loop
+  for (int y = (r + 1); y < (h - r); y++) {
+    t += rgbaIntToFloat(id[(y + r) * w]);
+    t -= rgbaIntToFloat(id[((y - r) * w) - w]);
+    od[y * w] = rgbaFloatToInt(t * scale);
+  }
+
+  // do right edge
+  for (int y = h - r; y < h; y++) {
+    t += rgbaIntToFloat(id[(h - 1) * w]);
+    t -= rgbaIntToFloat(id[((y - r) * w) - w]);
+    od[y * w] = rgbaFloatToInt(t * scale);
+  }
+}
+
+extern "C" void initTexture(int width, int height, void *pImage, bool useRGBA) {
+  // copy image data to array
+  hipChannelFormatDesc channelDesc;
+  if (useRGBA) {
+    channelDesc =
+        hipCreateChannelDesc(8, 8, 8, 8, hipChannelFormatKindUnsigned);
+  } else {
+    channelDesc =
+        hipCreateChannelDesc(32, 0, 0, 0, hipChannelFormatKindFloat);
+  }
+  HIPCHECK(hipMallocArray(&d_array, &channelDesc, width, height));
+
+  size_t bytesPerElem = (useRGBA ? sizeof(uchar4) : sizeof(float));
+  HIPCHECK(hipMemcpy2DToArray(
+      d_array, 0, 0, pImage, width * bytesPerElem, width * bytesPerElem, height,
+      hipMemcpyHostToDevice));
+
+  HIPCHECK(hipMallocArray(&d_tempArray, &channelDesc, width, height));
+
+  // set texture parameters
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = d_array;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.addressMode[1] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeNormalizedFloat;
+
+  HIPCHECK(hipCreateTextureObject(&rgbaTex, &texRes, &texDescr, NULL));
+
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = d_tempArray;
+
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeClamp;
+  texDescr.addressMode[1] = hipAddressModeClamp;
+  texDescr.readMode = hipReadModeNormalizedFloat;
+
+  HIPCHECK(
+      hipCreateTextureObject(&rgbaTexTempArray, &texRes, &texDescr, NULL));
+
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = d_array;
+
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = true;
+  texDescr.filterMode = hipFilterModePoint;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.addressMode[1] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(hipCreateTextureObject(&tex, &texRes, &texDescr, NULL));
+
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = d_tempArray;
+
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = true;
+  texDescr.filterMode = hipFilterModePoint;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.addressMode[1] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(
+      hipCreateTextureObject(&texTempArray, &texRes, &texDescr, NULL));
+}
+
+extern "C" void freeTextures() {
+  HIPCHECK(hipDestroyTextureObject(tex));
+  HIPCHECK(hipDestroyTextureObject(texTempArray));
+  HIPCHECK(hipDestroyTextureObject(rgbaTex));
+  HIPCHECK(hipDestroyTextureObject(rgbaTexTempArray));
+  HIPCHECK(hipFreeArray(d_array));
+  HIPCHECK(hipFreeArray(d_tempArray));
+}
+
+/*
+    Perform 2D box filter on image using CUDA
+
+    Parameters:
+    d_src  - pointer to input image in device memory
+    d_temp - pointer to temporary storage in device memory
+    d_dest - pointer to destination image in device memory
+    width  - image width
+    height - image height
+    radius - filter radius
+    iterations - number of iterations
+
+*/
+extern "C" double boxFilter(float *d_src, float *d_temp, float *d_dest,
+                            int width, int height, int radius, int iterations,
+                            int nthreads, StopWatchInterface *timer) {
+  // var for kernel timing
+  double dKernelTime = 0.0;
+
+  // sync host and start computation timer_kernel
+  HIPCHECK(hipDeviceSynchronize());
+
+  for (int i = 0; i < iterations; i++) {
+    sdkResetTimer(&timer);
+    // use texture for horizontal pass
+    if (iterations > 1) {
+      d_boxfilter_x_tex<<<height / nthreads, nthreads, 0>>>(
+          d_temp, width, height, radius, texTempArray);
+    } else {
+      d_boxfilter_x_tex<<<height / nthreads, nthreads, 0>>>(
+          d_temp, width, height, radius, tex);
+    }
+
+    d_boxfilter_y_global<<<width / nthreads, nthreads, 0>>>(
+        d_temp, d_dest, width, height, radius);
+
+    // sync host and stop computation timer_kernel
+    HIPCHECK(hipDeviceSynchronize());
+    dKernelTime += sdkGetTimerValue(&timer);
+
+    if (iterations > 1) {
+      // copy result back from global memory to array
+      HIPCHECK(hipMemcpy2DToArray(
+          d_tempArray, 0, 0, d_dest, width * sizeof(float),
+          width * sizeof(float), height, hipMemcpyDeviceToDevice));
+    }
+  }
+
+  return ((dKernelTime / 1000.) / (double)iterations);
+}
+
+// RGBA version
+extern "C" double boxFilterRGBA(unsigned int *d_src, unsigned int *d_temp,
+                                unsigned int *d_dest, int width, int height,
+                                int radius, int iterations, int nthreads,
+                                StopWatchInterface *timer) {
+  // var for kernel computation timing
+  double dKernelTime;
+
+  for (int i = 0; i < iterations; i++) {
+    // sync host and start kernel computation timer_kernel
+    dKernelTime = 0.0;
+    HIPCHECK(hipDeviceSynchronize());
+    sdkResetTimer(&timer);
+
+    // use texture for horizontal pass
+    if (iterations > 1) {
+      d_boxfilter_rgba_x<<<height / nthreads, nthreads, 0>>>(
+          d_temp, width, height, radius, rgbaTexTempArray);
+    } else {
+      d_boxfilter_rgba_x<<<height / nthreads, nthreads, 0>>>(
+          d_temp, width, height, radius, rgbaTex);
+    }
+
+    d_boxfilter_rgba_y<<<width / nthreads, nthreads, 0>>>(d_temp, d_dest, width,
+                                                          height, radius);
+
+    // sync host and stop computation timer_kernel
+    HIPCHECK(hipDeviceSynchronize());
+    dKernelTime += sdkGetTimerValue(&timer);
+
+    if (iterations > 1) {
+      // copy result back from global memory to array
+      HIPCHECK(hipMemcpy2DToArray(
+          d_tempArray, 0, 0, d_dest, width * sizeof(unsigned int),
+          width * sizeof(unsigned int), height, hipMemcpyDeviceToDevice));
+    }
+  }
+
+  return ((dKernelTime / 1000.) / (double)iterations);
+}
+
+#endif  // #ifndef _BOXFILTER_KERNEL_H_
+ned int), height, hipMemcpyDeviceToDevice));
+    }
+  }
+
+  return ((dKernelTime / 1000.) / (double)iterations);
+}
+
+#endif  // #ifndef _BOXFILTER_KERNEL_H_
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/convolutionTexture/convolutionTexture.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/convolutionTexture/convolutionTexture.cu.hip
index e69de29..602cda5 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/convolutionTexture/convolutionTexture.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/convolutionTexture/convolutionTexture.cu.hip
@@ -0,0 +1,163 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include "helper_cuda_hipified.h"
+
+#include "convolutionTexture_common.h"
+
+////////////////////////////////////////////////////////////////////////////////
+// GPU-specific defines
+////////////////////////////////////////////////////////////////////////////////
+// Maps to a single instruction on G8x / G9x / G10x
+#define IMAD(a, b, c) (__mul24((a), (b)) + (c))
+
+// Use unrolled innermost convolution loop
+#define UNROLL_INNER 1
+
+// Round a / b to nearest higher integer value
+inline int iDivUp(int a, int b) { return (a % b != 0) ? (a / b + 1) : (a / b); }
+
+// Align a to nearest higher multiple of b
+inline int iAlignUp(int a, int b) { return (a % b != 0) ? (a - a % b + b) : a; }
+
+////////////////////////////////////////////////////////////////////////////////
+// Convolution kernel and input array storage
+////////////////////////////////////////////////////////////////////////////////
+__constant__ float c_Kernel[KERNEL_LENGTH];
+
+extern "C" void setConvolutionKernel(float *h_Kernel) {
+  hipMemcpyToSymbol(HIP_SYMBOL(c_Kernel), h_Kernel, KERNEL_LENGTH * sizeof(float));
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Loop unrolling templates, needed for best performance
+////////////////////////////////////////////////////////////////////////////////
+template <int i>
+__device__ float convolutionRow(float x, float y, hipTextureObject_t texSrc) {
+  return tex2D<float>(texSrc, x + (float)(KERNEL_RADIUS - i), y) * c_Kernel[i] +
+         convolutionRow<i - 1>(x, y, texSrc);
+}
+
+template <>
+__device__ float convolutionRow<-1>(float x, float y,
+                                    hipTextureObject_t texSrc) {
+  return 0;
+}
+
+template <int i>
+__device__ float convolutionColumn(float x, float y,
+                                   hipTextureObject_t texSrc) {
+  return tex2D<float>(texSrc, x, y + (float)(KERNEL_RADIUS - i)) * c_Kernel[i] +
+         convolutionColumn<i - 1>(x, y, texSrc);
+}
+
+template <>
+__device__ float convolutionColumn<-1>(float x, float y,
+                                       hipTextureObject_t texSrc) {
+  return 0;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Row convolution filter
+////////////////////////////////////////////////////////////////////////////////
+__global__ void convolutionRowsKernel(float *d_Dst, int imageW, int imageH,
+                                      hipTextureObject_t texSrc) {
+  const int ix = IMAD(blockDim.x, blockIdx.x, threadIdx.x);
+  const int iy = IMAD(blockDim.y, blockIdx.y, threadIdx.y);
+  const float x = (float)ix + 0.5f;
+  const float y = (float)iy + 0.5f;
+
+  if (ix >= imageW || iy >= imageH) {
+    return;
+  }
+
+  float sum = 0;
+
+#if (UNROLL_INNER)
+  sum = convolutionRow<2 * KERNEL_RADIUS>(x, y, texSrc);
+#else
+
+  for (int k = -KERNEL_RADIUS; k <= KERNEL_RADIUS; k++) {
+    sum += tex2D<float>(texSrc, x + (float)k, y) * c_Kernel[KERNEL_RADIUS - k];
+  }
+
+#endif
+
+  d_Dst[IMAD(iy, imageW, ix)] = sum;
+}
+
+extern "C" void convolutionRowsGPU(float *d_Dst, hipArray *a_Src, int imageW,
+                                   int imageH, hipTextureObject_t texSrc) {
+  dim3 threads(16, 12);
+  dim3 blocks(iDivUp(imageW, threads.x), iDivUp(imageH, threads.y));
+
+  convolutionRowsKernel<<<blocks, threads>>>(d_Dst, imageW, imageH, texSrc);
+  getLastCudaError("convolutionRowsKernel() execution failed\n");
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Column convolution filter
+////////////////////////////////////////////////////////////////////////////////
+__global__ void convolutionColumnsKernel(float *d_Dst, int imageW, int imageH,
+                                         hipTextureObject_t texSrc) {
+  const int ix = IMAD(blockDim.x, blockIdx.x, threadIdx.x);
+  const int iy = IMAD(blockDim.y, blockIdx.y, threadIdx.y);
+  const float x = (float)ix + 0.5f;
+  const float y = (float)iy + 0.5f;
+
+  if (ix >= imageW || iy >= imageH) {
+    return;
+  }
+
+  float sum = 0;
+
+#if (UNROLL_INNER)
+  sum = convolutionColumn<2 * KERNEL_RADIUS>(x, y, texSrc);
+#else
+
+  for (int k = -KERNEL_RADIUS; k <= KERNEL_RADIUS; k++) {
+    sum += tex2D<float>(texSrc, x, y + (float)k) * c_Kernel[KERNEL_RADIUS - k];
+  }
+
+#endif
+
+  d_Dst[IMAD(iy, imageW, ix)] = sum;
+}
+
+extern "C" void convolutionColumnsGPU(float *d_Dst, hipArray *a_Src,
+                                      int imageW, int imageH,
+                                      hipTextureObject_t texSrc) {
+  dim3 threads(16, 12);
+  dim3 blocks(iDivUp(imageW, threads.x), iDivUp(imageH, threads.y));
+
+  convolutionColumnsKernel<<<blocks, threads>>>(d_Dst, imageW, imageH, texSrc);
+  getLastCudaError("convolutionColumnsKernel() execution failed\n");
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/dct8x8/dct8x8.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/dct8x8/dct8x8.cu.hip
index e69de29..f049d3c 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/dct8x8/dct8x8.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/dct8x8/dct8x8.cu.hip
@@ -0,0 +1,673 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/**
+**************************************************************************
+* \file dct8x8.cu
+* \brief Contains entry point, wrappers to host and device code and benchmark.
+*
+* This sample implements forward and inverse Discrete Cosine Transform to blocks
+* of image pixels (of 8x8 size), as in JPEG standard. The typical work flow is
+*as
+* follows:
+* 1. Run CPU version (Host code) and measure execution time;
+* 2. Run CUDA version (Device code) and measure execution time;
+* 3. Output execution timings and calculate CUDA speedup.
+*/
+
+#include "Common.h"
+#include "DCT8x8_Gold.h"
+#include "BmpUtil.h"
+
+/**
+*  The number of DCT kernel calls
+*/
+#define BENCHMARK_SIZE 10
+
+/**
+*  The PSNR values over this threshold indicate images equality
+*/
+#define PSNR_THRESHOLD_EQUAL 40
+
+// includes kernels
+#include "dct8x8_kernel1.cuh"
+#include "dct8x8_kernel2.cuh"
+#include "dct8x8_kernel_short.cuh"
+#include "dct8x8_kernel_quantization.cuh"
+
+/**
+**************************************************************************
+*  Wrapper function for 1st gold version of DCT, quantization and IDCT
+*implementations
+*
+* \param ImgSrc         [IN] - Source byte image plane
+* \param ImgDst         [IN] - Quantized result byte image plane
+* \param Stride         [IN] - Stride for both source and result planes
+* \param Size           [IN] - Size of both planes
+*
+* \return Execution time in milliseconds
+*/
+float WrapperGold1(byte *ImgSrc, byte *ImgDst, int Stride, ROI Size) {
+  // allocate float buffers for DCT and other data
+  int StrideF;
+  float *ImgF1 = MallocPlaneFloat(Size.width, Size.height, &StrideF);
+  float *ImgF2 = MallocPlaneFloat(Size.width, Size.height, &StrideF);
+
+  // convert source image to float representation
+  CopyByte2Float(ImgSrc, Stride, ImgF1, StrideF, Size);
+  AddFloatPlane(-128.0f, ImgF1, StrideF, Size);
+
+  // create and start CUDA timer
+  StopWatchInterface *timerGold = 0;
+  sdkCreateTimer(&timerGold);
+  sdkResetTimer(&timerGold);
+
+  // perform block-wise DCT processing and benchmarking
+  for (int i = 0; i < BENCHMARK_SIZE; i++) {
+    sdkStartTimer(&timerGold);
+    computeDCT8x8Gold1(ImgF1, ImgF2, StrideF, Size);
+    sdkStopTimer(&timerGold);
+  }
+
+  // stop and destroy CUDA timer
+  float TimerGoldSpan = sdkGetAverageTimerValue(&timerGold);
+  sdkDeleteTimer(&timerGold);
+
+  // perform quantization
+  quantizeGoldFloat(ImgF2, StrideF, Size);
+
+  // perform block-wise IDCT processing
+  computeIDCT8x8Gold1(ImgF2, ImgF1, StrideF, Size);
+
+  // convert image back to byte representation
+  AddFloatPlane(128.0f, ImgF1, StrideF, Size);
+  CopyFloat2Byte(ImgF1, StrideF, ImgDst, Stride, Size);
+
+  // free float buffers
+  FreePlane(ImgF1);
+  FreePlane(ImgF2);
+
+  // return time taken by the operation
+  return TimerGoldSpan;
+}
+
+/**
+**************************************************************************
+*  Wrapper function for 2nd gold version of DCT, quantization and IDCT
+*implementations
+*
+* \param ImgSrc         [IN] - Source byte image plane
+* \param ImgDst         [IN] - Quantized result byte image plane
+* \param Stride         [IN] - Stride for both source and result planes
+* \param Size           [IN] - Size of both planes
+*
+* \return Execution time in milliseconds
+*/
+float WrapperGold2(byte *ImgSrc, byte *ImgDst, int Stride, ROI Size) {
+  // allocate float buffers for DCT and other data
+  int StrideF;
+  float *ImgF1 = MallocPlaneFloat(Size.width, Size.height, &StrideF);
+  float *ImgF2 = MallocPlaneFloat(Size.width, Size.height, &StrideF);
+
+  // convert source image to float representation
+  CopyByte2Float(ImgSrc, Stride, ImgF1, StrideF, Size);
+  AddFloatPlane(-128.0f, ImgF1, StrideF, Size);
+
+  // create and start CUDA timer
+  StopWatchInterface *timerGold = 0;
+  sdkCreateTimer(&timerGold);
+  sdkResetTimer(&timerGold);
+
+  // perform block-wise DCT processing and benchmarking
+  for (int i = 0; i < BENCHMARK_SIZE; i++) {
+    sdkStartTimer(&timerGold);
+    computeDCT8x8Gold2(ImgF1, ImgF2, StrideF, Size);
+    sdkStopTimer(&timerGold);
+  }
+
+  // stop and destroy CUDA timer
+  float TimerGoldSpan = sdkGetAverageTimerValue(&timerGold);
+  sdkDeleteTimer(&timerGold);
+
+  // perform quantization
+  quantizeGoldFloat(ImgF2, StrideF, Size);
+
+  // perform block-wise IDCT processing
+  computeIDCT8x8Gold2(ImgF2, ImgF1, StrideF, Size);
+
+  // convert image back to byte representation
+  AddFloatPlane(128.0f, ImgF1, StrideF, Size);
+  CopyFloat2Byte(ImgF1, StrideF, ImgDst, Stride, Size);
+
+  // free float buffers
+  FreePlane(ImgF1);
+  FreePlane(ImgF2);
+
+  // return time taken by the operation
+  return TimerGoldSpan;
+}
+
+/**
+**************************************************************************
+*  Wrapper function for 1st CUDA version of DCT, quantization and IDCT
+*implementations
+*
+* \param ImgSrc         [IN] - Source byte image plane
+* \param ImgDst         [IN] - Quantized result byte image plane
+* \param Stride         [IN] - Stride for both source and result planes
+* \param Size           [IN] - Size of both planes
+*
+* \return Execution time in milliseconds
+*/
+float WrapperCUDA1(byte *ImgSrc, byte *ImgDst, int Stride, ROI Size) {
+  // prepare channel format descriptor for passing texture into kernels
+  hipChannelFormatDesc floattex = hipCreateChannelDesc<float>();
+
+  // allocate device memory
+  hipArray *Src;
+  float *Dst;
+  size_t DstStride;
+  HIPCHECK(hipMallocArray(&Src, &floattex, Size.width, Size.height));
+  HIPCHECK(hipMallocPitch((void **)(&Dst), &DstStride,
+                                  Size.width * sizeof(float), Size.height));
+  DstStride /= sizeof(float);
+
+  // convert source image to float representation
+  int ImgSrcFStride;
+  float *ImgSrcF = MallocPlaneFloat(Size.width, Size.height, &ImgSrcFStride);
+  CopyByte2Float(ImgSrc, Stride, ImgSrcF, ImgSrcFStride, Size);
+  AddFloatPlane(-128.0f, ImgSrcF, ImgSrcFStride, Size);
+
+  // copy from host memory to device
+  HIPCHECK(hipMemcpy2DToArray(
+      Src, 0, 0, ImgSrcF, ImgSrcFStride * sizeof(float),
+      Size.width * sizeof(float), Size.height, hipMemcpyHostToDevice));
+
+  // setup execution parameters
+  dim3 threads(BLOCK_SIZE, BLOCK_SIZE);
+  dim3 grid(Size.width / BLOCK_SIZE, Size.height / BLOCK_SIZE);
+
+  // create and start CUDA timer
+  StopWatchInterface *timerCUDA = 0;
+  sdkCreateTimer(&timerCUDA);
+  sdkResetTimer(&timerCUDA);
+
+  // execute DCT kernel and benchmark
+  hipTextureObject_t TexSrc;
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = Src;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.addressMode[1] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(hipCreateTextureObject(&TexSrc, &texRes, &texDescr, NULL));
+
+  for (int i = 0; i < BENCHMARK_SIZE; i++) {
+    sdkStartTimer(&timerCUDA);
+    CUDAkernel1DCT<<<grid, threads>>>(Dst, (int)DstStride, 0, 0, TexSrc);
+    HIPCHECK(hipDeviceSynchronize());
+    sdkStopTimer(&timerCUDA);
+  }
+
+  getLastCudaError("Kernel execution failed");
+
+  // finalize CUDA timer
+  float TimerCUDASpan = sdkGetAverageTimerValue(&timerCUDA);
+  sdkDeleteTimer(&timerCUDA);
+
+  // execute Quantization kernel
+  CUDAkernelQuantizationFloat<<<grid, threads>>>(Dst, (int)DstStride);
+  getLastCudaError("Kernel execution failed");
+
+  // copy quantized coefficients from host memory to device array
+  HIPCHECK(hipMemcpy2DToArray(Src, 0, 0, Dst, DstStride * sizeof(float),
+                                      Size.width * sizeof(float), Size.height,
+                                      hipMemcpyDeviceToDevice));
+
+  // execute IDCT kernel
+  CUDAkernel1IDCT<<<grid, threads>>>(Dst, (int)DstStride, 0, 0, TexSrc);
+  getLastCudaError("Kernel execution failed");
+
+  // copy quantized image block to host
+  HIPCHECK(hipMemcpy2D(
+      ImgSrcF, ImgSrcFStride * sizeof(float), Dst, DstStride * sizeof(float),
+      Size.width * sizeof(float), Size.height, hipMemcpyDeviceToHost));
+
+  // convert image back to byte representation
+  AddFloatPlane(128.0f, ImgSrcF, ImgSrcFStride, Size);
+  CopyFloat2Byte(ImgSrcF, ImgSrcFStride, ImgDst, Stride, Size);
+
+  // clean up memory
+  HIPCHECK(hipDestroyTextureObject(TexSrc));
+  HIPCHECK(hipFreeArray(Src));
+  HIPCHECK(hipFree(Dst));
+  FreePlane(ImgSrcF);
+
+  // return time taken by the operation
+  return TimerCUDASpan;
+}
+
+/**
+**************************************************************************
+*  Wrapper function for 2nd CUDA version of DCT, quantization and IDCT
+*implementations
+*
+* \param ImgSrc         [IN] - Source byte image plane
+* \param ImgDst         [IN] - Quantized result byte image plane
+* \param Stride         [IN] - Stride for both source and result planes
+* \param Size           [IN] - Size of both planes
+*
+* \return Execution time in milliseconds
+*/
+
+float WrapperCUDA2(byte *ImgSrc, byte *ImgDst, int Stride, ROI Size) {
+  // allocate host buffers for DCT and other data
+  int StrideF;
+  float *ImgF1 = MallocPlaneFloat(Size.width, Size.height, &StrideF);
+
+  // convert source image to float representation
+  CopyByte2Float(ImgSrc, Stride, ImgF1, StrideF, Size);
+  AddFloatPlane(-128.0f, ImgF1, StrideF, Size);
+
+  // allocate device memory
+  float *src, *dst;
+  size_t DeviceStride;
+  HIPCHECK(hipMallocPitch((void **)&src, &DeviceStride,
+                                  Size.width * sizeof(float), Size.height));
+  HIPCHECK(hipMallocPitch((void **)&dst, &DeviceStride,
+                                  Size.width * sizeof(float), Size.height));
+  DeviceStride /= sizeof(float);
+
+  // copy from host memory to device
+  HIPCHECK(hipMemcpy2D(
+      src, DeviceStride * sizeof(float), ImgF1, StrideF * sizeof(float),
+      Size.width * sizeof(float), Size.height, hipMemcpyHostToDevice));
+
+  // create and start CUDA timer
+  StopWatchInterface *timerCUDA = 0;
+  sdkCreateTimer(&timerCUDA);
+
+  // setup execution parameters
+  dim3 GridFullWarps(Size.width / KER2_BLOCK_WIDTH,
+                     Size.height / KER2_BLOCK_HEIGHT, 1);
+  dim3 ThreadsFullWarps(8, KER2_BLOCK_WIDTH / 8, KER2_BLOCK_HEIGHT / 8);
+
+  // perform block-wise DCT processing and benchmarking
+  const int numIterations = 100;
+
+  for (int i = -1; i < numIterations; i++) {
+    if (i == 0) {
+      HIPCHECK(hipDeviceSynchronize());
+      sdkResetTimer(&timerCUDA);
+      sdkStartTimer(&timerCUDA);
+    }
+
+    CUDAkernel2DCT<<<GridFullWarps, ThreadsFullWarps>>>(dst, src,
+                                                        (int)DeviceStride);
+    getLastCudaError("Kernel execution failed");
+  }
+
+  HIPCHECK(hipDeviceSynchronize());
+  sdkStopTimer(&timerCUDA);
+
+  // finalize timing of CUDA Kernels
+  float avgTime = (float)sdkGetTimerValue(&timerCUDA) / (float)numIterations;
+  sdkDeleteTimer(&timerCUDA);
+  printf("%f MPix/s //%f ms\n",
+         (1E-6 * (float)Size.width * (float)Size.height) / (1E-3 * avgTime),
+         avgTime);
+
+  // setup execution parameters for quantization
+  dim3 ThreadsSmallBlocks(BLOCK_SIZE, BLOCK_SIZE);
+  dim3 GridSmallBlocks(Size.width / BLOCK_SIZE, Size.height / BLOCK_SIZE);
+
+  // execute Quantization kernel
+  CUDAkernelQuantizationFloat<<<GridSmallBlocks, ThreadsSmallBlocks>>>(
+      dst, (int)DeviceStride);
+  getLastCudaError("Kernel execution failed");
+
+  // perform block-wise IDCT processing
+  CUDAkernel2IDCT<<<GridFullWarps, ThreadsFullWarps>>>(src, dst,
+                                                       (int)DeviceStride);
+  HIPCHECK(hipDeviceSynchronize());
+  getLastCudaError("Kernel execution failed");
+
+  // copy quantized image block to host
+  HIPCHECK(hipMemcpy2D(
+      ImgF1, StrideF * sizeof(float), src, DeviceStride * sizeof(float),
+      Size.width * sizeof(float), Size.height, hipMemcpyDeviceToHost));
+
+  // convert image back to byte representation
+  AddFloatPlane(128.0f, ImgF1, StrideF, Size);
+  CopyFloat2Byte(ImgF1, StrideF, ImgDst, Stride, Size);
+
+  // clean up memory
+  HIPCHECK(hipFree(dst));
+  HIPCHECK(hipFree(src));
+  FreePlane(ImgF1);
+
+  // return time taken by the operation
+  return avgTime;
+}
+
+/**
+**************************************************************************
+*  Wrapper function for short CUDA version of DCT, quantization and IDCT
+*implementations
+*
+* \param ImgSrc         [IN] - Source byte image plane
+* \param ImgDst         [IN] - Quantized result byte image plane
+* \param Stride         [IN] - Stride for both source and result planes
+* \param Size           [IN] - Size of both planes
+*
+* \return Execution time in milliseconds
+*/
+float WrapperCUDAshort(byte *ImgSrc, byte *ImgDst, int Stride, ROI Size) {
+  // allocate host buffers for DCT and other data
+  int StrideS;
+  short *ImgS1 = MallocPlaneShort(Size.width, Size.height, &StrideS);
+
+  // convert source image to short representation centered at 128
+  for (int i = 0; i < Size.height; i++) {
+    for (int j = 0; j < Size.width; j++) {
+      ImgS1[i * StrideS + j] = (short)ImgSrc[i * Stride + j] - 128;
+    }
+  }
+
+  // allocate device memory
+  short *SrcDst;
+  size_t DeviceStride;
+  HIPCHECK(hipMallocPitch((void **)(&SrcDst), &DeviceStride,
+                                  Size.width * sizeof(short), Size.height));
+  DeviceStride /= sizeof(short);
+
+  // copy from host memory to device
+  HIPCHECK(hipMemcpy2D(
+      SrcDst, DeviceStride * sizeof(short), ImgS1, StrideS * sizeof(short),
+      Size.width * sizeof(short), Size.height, hipMemcpyHostToDevice));
+
+  // create and start CUDA timer
+  StopWatchInterface *timerLibJpeg = 0;
+  sdkCreateTimer(&timerLibJpeg);
+  sdkResetTimer(&timerLibJpeg);
+
+  // setup execution parameters
+  dim3 GridShort(Size.width / KERS_BLOCK_WIDTH, Size.height / KERS_BLOCK_HEIGHT,
+                 1);
+  dim3 ThreadsShort(8, KERS_BLOCK_WIDTH / 8, KERS_BLOCK_HEIGHT / 8);
+
+  // perform block-wise DCT processing and benchmarking
+  sdkStartTimer(&timerLibJpeg);
+  CUDAkernelShortDCT<<<GridShort, ThreadsShort>>>(SrcDst, (int)DeviceStride);
+  HIPCHECK(hipDeviceSynchronize());
+  sdkStopTimer(&timerLibJpeg);
+  getLastCudaError("Kernel execution failed");
+
+  // stop and destroy CUDA timer
+  float TimerLibJpegSpan16b = sdkGetAverageTimerValue(&timerLibJpeg);
+  sdkDeleteTimer(&timerLibJpeg);
+
+  // setup execution parameters for quantization
+  dim3 ThreadsSmallBlocks(BLOCK_SIZE, BLOCK_SIZE);
+  dim3 GridSmallBlocks(Size.width / BLOCK_SIZE, Size.height / BLOCK_SIZE);
+
+  // execute Quantization kernel
+  CUDAkernelQuantizationShort<<<GridSmallBlocks, ThreadsSmallBlocks>>>(
+      SrcDst, (int)DeviceStride);
+  getLastCudaError("Kernel execution failed");
+
+  // perform block-wise IDCT processing
+  CUDAkernelShortIDCT<<<GridShort, ThreadsShort>>>(SrcDst, (int)DeviceStride);
+  HIPCHECK(hipDeviceSynchronize());
+  getLastCudaError("Kernel execution failed");
+
+  // copy quantized image block to host
+  HIPCHECK(hipMemcpy2D(
+      ImgS1, StrideS * sizeof(short), SrcDst, DeviceStride * sizeof(short),
+      Size.width * sizeof(short), Size.height, hipMemcpyDeviceToHost));
+
+  // convert image back to byte representation
+  for (int i = 0; i < Size.height; i++) {
+    for (int j = 0; j < Size.width; j++) {
+      ImgDst[i * Stride + j] = clamp_0_255(ImgS1[i * StrideS + j] + 128);
+    }
+  }
+
+  // free float buffers
+  HIPCHECK(hipFree(SrcDst));
+  FreePlane(ImgS1);
+
+  // return time taken by the operation
+  return TimerLibJpegSpan16b;
+}
+
+/**
+**************************************************************************
+*  Program entry point
+*
+* \param argc       [IN] - Number of command-line arguments
+* \param argv       [IN] - Array of command-line arguments
+*
+* \return Status code
+*/
+
+int main(int argc, char **argv) {
+  //
+  // Sample initialization
+  //
+  printf("%s Starting...\n\n", argv[0]);
+
+  // initialize CUDA
+  findCudaDevice(argc, (const char **)argv);
+
+  // source and results image filenames
+  char SampleImageFname[] = "teapot512.bmp";
+  char SampleImageFnameResGold1[] = "teapot512_gold1.bmp";
+  char SampleImageFnameResGold2[] = "teapot512_gold2.bmp";
+  char SampleImageFnameResCUDA1[] = "teapot512_cuda1.bmp";
+  char SampleImageFnameResCUDA2[] = "teapot512_cuda2.bmp";
+  char SampleImageFnameResCUDAshort[] = "teapot512_cuda_short.bmp";
+
+  char *pSampleImageFpath = sdkFindFilePath(SampleImageFname, argv[0]);
+
+  if (pSampleImageFpath == NULL) {
+    printf("dct8x8 could not locate Sample Image <%s>\nExiting...\n",
+           pSampleImageFpath);
+    exit(EXIT_FAILURE);
+  }
+
+  // preload image (acquire dimensions)
+  int ImgWidth, ImgHeight;
+  ROI ImgSize;
+  int res = PreLoadBmp(pSampleImageFpath, &ImgWidth, &ImgHeight);
+  ImgSize.width = ImgWidth;
+  ImgSize.height = ImgHeight;
+
+  // CONSOLE INFORMATION: saying hello to user
+  printf("CUDA sample DCT/IDCT implementation\n");
+  printf("===================================\n");
+  printf("Loading test image: %s... ", SampleImageFname);
+
+  if (res) {
+    printf("\nError: Image file not found or invalid!\n");
+    exit(EXIT_FAILURE);
+    return 1;
+  }
+
+  // check image dimensions are multiples of BLOCK_SIZE
+  if (ImgWidth % BLOCK_SIZE != 0 || ImgHeight % BLOCK_SIZE != 0) {
+    printf("\nError: Input image dimensions must be multiples of 8!\n");
+    exit(EXIT_FAILURE);
+    return 1;
+  }
+
+  printf("[%d x %d]... ", ImgWidth, ImgHeight);
+
+  // allocate image buffers
+  int ImgStride;
+  byte *ImgSrc = MallocPlaneByte(ImgWidth, ImgHeight, &ImgStride);
+  byte *ImgDstGold1 = MallocPlaneByte(ImgWidth, ImgHeight, &ImgStride);
+  byte *ImgDstGold2 = MallocPlaneByte(ImgWidth, ImgHeight, &ImgStride);
+  byte *ImgDstCUDA1 = MallocPlaneByte(ImgWidth, ImgHeight, &ImgStride);
+  byte *ImgDstCUDA2 = MallocPlaneByte(ImgWidth, ImgHeight, &ImgStride);
+  byte *ImgDstCUDAshort = MallocPlaneByte(ImgWidth, ImgHeight, &ImgStride);
+
+  // load sample image
+  LoadBmpAsGray(pSampleImageFpath, ImgStride, ImgSize, ImgSrc);
+
+  //
+  // RUNNING WRAPPERS
+  //
+
+  // compute Gold 1 version of DCT/quantization/IDCT
+  printf("Success\nRunning Gold 1 (CPU) version... ");
+  float TimeGold1 = WrapperGold1(ImgSrc, ImgDstGold1, ImgStride, ImgSize);
+
+  // compute Gold 2 version of DCT/quantization/IDCT
+  printf("Success\nRunning Gold 2 (CPU) version... ");
+  float TimeGold2 = WrapperGold2(ImgSrc, ImgDstGold2, ImgStride, ImgSize);
+
+  // compute CUDA 1 version of DCT/quantization/IDCT
+  printf("Success\nRunning CUDA 1 (GPU) version... ");
+  float TimeCUDA1 = WrapperCUDA1(ImgSrc, ImgDstCUDA1, ImgStride, ImgSize);
+
+  // compute CUDA 2 version of DCT/quantization/IDCT
+  printf("Success\nRunning CUDA 2 (GPU) version... ");
+  float TimeCUDA2 = WrapperCUDA2(ImgSrc, ImgDstCUDA2, ImgStride, ImgSize);
+
+  // compute CUDA short version of DCT/quantization/IDCT
+  printf("Success\nRunning CUDA short (GPU) version... ");
+  float TimeCUDAshort =
+      WrapperCUDAshort(ImgSrc, ImgDstCUDAshort, ImgStride, ImgSize);
+  //
+  // Execution statistics, result saving and validation
+  //
+
+  // dump result of Gold 1 processing
+  printf("Success\nDumping result to %s... ", SampleImageFnameResGold1);
+  DumpBmpAsGray(SampleImageFnameResGold1, ImgDstGold1, ImgStride, ImgSize);
+
+  // dump result of Gold 2 processing
+  printf("Success\nDumping result to %s... ", SampleImageFnameResGold2);
+  DumpBmpAsGray(SampleImageFnameResGold2, ImgDstGold2, ImgStride, ImgSize);
+
+  // dump result of CUDA 1 processing
+  printf("Success\nDumping result to %s... ", SampleImageFnameResCUDA1);
+  DumpBmpAsGray(SampleImageFnameResCUDA1, ImgDstCUDA1, ImgStride, ImgSize);
+
+  // dump result of CUDA 2 processing
+  printf("Success\nDumping result to %s... ", SampleImageFnameResCUDA2);
+  DumpBmpAsGray(SampleImageFnameResCUDA2, ImgDstCUDA2, ImgStride, ImgSize);
+
+  // dump result of CUDA short processing
+  printf("Success\nDumping result to %s... ", SampleImageFnameResCUDAshort);
+  DumpBmpAsGray(SampleImageFnameResCUDAshort, ImgDstCUDAshort, ImgStride,
+                ImgSize);
+  // print speed info
+  printf("Success\n");
+
+  printf("Processing time (CUDA 1)    : %f ms \n", TimeCUDA1);
+  printf("Processing time (CUDA 2)    : %f ms \n", TimeCUDA2);
+  printf("Processing time (CUDA short): %f ms \n", TimeCUDAshort);
+
+  // calculate PSNR between each pair of images
+  float PSNR_Src_DstGold1 =
+      CalculatePSNR(ImgSrc, ImgDstGold1, ImgStride, ImgSize);
+  float PSNR_Src_DstGold2 =
+      CalculatePSNR(ImgSrc, ImgDstGold2, ImgStride, ImgSize);
+  float PSNR_Src_DstCUDA1 =
+      CalculatePSNR(ImgSrc, ImgDstCUDA1, ImgStride, ImgSize);
+  float PSNR_Src_DstCUDA2 =
+      CalculatePSNR(ImgSrc, ImgDstCUDA2, ImgStride, ImgSize);
+  float PSNR_Src_DstCUDAshort =
+      CalculatePSNR(ImgSrc, ImgDstCUDAshort, ImgStride, ImgSize);
+  float PSNR_DstGold1_DstCUDA1 =
+      CalculatePSNR(ImgDstGold1, ImgDstCUDA1, ImgStride, ImgSize);
+  float PSNR_DstGold2_DstCUDA2 =
+      CalculatePSNR(ImgDstGold2, ImgDstCUDA2, ImgStride, ImgSize);
+  float PSNR_DstGold2_DstCUDA16b =
+      CalculatePSNR(ImgDstGold2, ImgDstCUDAshort, ImgStride, ImgSize);
+
+  printf("PSNR Original    <---> CPU(Gold 1)    : %f\n", PSNR_Src_DstGold1);
+  printf("PSNR Original    <---> CPU(Gold 2)    : %f\n", PSNR_Src_DstGold2);
+  printf("PSNR Original    <---> GPU(CUDA 1)    : %f\n", PSNR_Src_DstCUDA1);
+  printf("PSNR Original    <---> GPU(CUDA 2)    : %f\n", PSNR_Src_DstCUDA2);
+  printf("PSNR Original    <---> GPU(CUDA short): %f\n", PSNR_Src_DstCUDAshort);
+  printf("PSNR CPU(Gold 1) <---> GPU(CUDA 1)    : %f\n",
+         PSNR_DstGold1_DstCUDA1);
+  printf("PSNR CPU(Gold 2) <---> GPU(CUDA 2)    : %f\n",
+         PSNR_DstGold2_DstCUDA2);
+  printf("PSNR CPU(Gold 2) <---> GPU(CUDA short): %f\n",
+         PSNR_DstGold2_DstCUDA16b);
+
+  bool bTestResult = (PSNR_DstGold1_DstCUDA1 > PSNR_THRESHOLD_EQUAL &&
+                      PSNR_DstGold2_DstCUDA2 > PSNR_THRESHOLD_EQUAL &&
+                      PSNR_DstGold2_DstCUDA16b > PSNR_THRESHOLD_EQUAL);
+
+  //
+  // Finalization
+  //
+
+  // release byte planes
+  FreePlane(ImgSrc);
+  FreePlane(ImgDstGold1);
+  FreePlane(ImgDstGold2);
+  FreePlane(ImgDstCUDA1);
+  FreePlane(ImgDstCUDA2);
+  FreePlane(ImgDstCUDAshort);
+
+  // finalize
+  printf("\nTest Summary...\n");
+
+  if (!bTestResult) {
+    printf("Test failed!\n");
+    exit(EXIT_FAILURE);
+  }
+
+  printf("Test passed\n");
+  exit(EXIT_SUCCESS);
+}
+finalize
+  printf("\nTest Summary...\n");
+
+  if (!bTestResult) {
+    printf("Test failed!\n");
+    exit(EXIT_FAILURE);
+  }
+
+  printf("Test passed\n");
+  exit(EXIT_SUCCESS);
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/imageDenoising/imageDenoising.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/imageDenoising/imageDenoising.cu.hip
index e69de29..7860d27 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/imageDenoising/imageDenoising.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/imageDenoising/imageDenoising.cu.hip
@@ -0,0 +1,113 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * This sample demonstrates two adaptive image denoising techniques:
+ * KNN and NLM, based on computation of both geometric and color distance
+ * between texels. While both techniques are already implemented in the
+ * DirectX SDK using shaders, massively speeded up variation
+ * of the latter technique, taking advantage of shared memory, is implemented
+ * in addition to DirectX counterparts.
+ * See supplied whitepaper for more explanations.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include "helper_cuda_hipified.h"
+#include "imageDenoising.h"
+
+////////////////////////////////////////////////////////////////////////////////
+// Helper functions
+////////////////////////////////////////////////////////////////////////////////
+float Max(float x, float y) { return (x > y) ? x : y; }
+
+float Min(float x, float y) { return (x < y) ? x : y; }
+
+int iDivUp(int a, int b) { return ((a % b) != 0) ? (a / b + 1) : (a / b); }
+
+__device__ float lerpf(float a, float b, float c) { return a + (b - a) * c; }
+
+__device__ float vecLen(float4 a, float4 b) {
+  return ((b.x - a.x) * (b.x - a.x) + (b.y - a.y) * (b.y - a.y) +
+          (b.z - a.z) * (b.z - a.z));
+}
+
+__device__ TColor make_color(float r, float g, float b, float a) {
+  return ((int)(a * 255.0f) << 24) | ((int)(b * 255.0f) << 16) |
+         ((int)(g * 255.0f) << 8) | ((int)(r * 255.0f) << 0);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Global data handlers and parameters
+////////////////////////////////////////////////////////////////////////////////
+// Texture object and channel descriptor for image texture
+hipTextureObject_t texImage;
+hipChannelFormatDesc uchar4tex = hipCreateChannelDesc<uchar4>();
+
+// CUDA array descriptor
+hipArray *a_Src;
+
+////////////////////////////////////////////////////////////////////////////////
+// Filtering kernels
+////////////////////////////////////////////////////////////////////////////////
+#include "imageDenoising_copy_kernel.cuh"
+#include "imageDenoising_knn_kernel.cuh"
+#include "imageDenoising_nlm_kernel.cuh"
+#include "imageDenoising_nlm2_kernel.cuh"
+
+extern "C" hipError_t CUDA_MallocArray(uchar4 **h_Src, int imageW,
+                                        int imageH) {
+  hipError_t error;
+
+  error = hipMallocArray(&a_Src, &uchar4tex, imageW, imageH);
+  error = hipMemcpy2DToArray(a_Src, 0, 0, *h_Src, sizeof(uchar4) * imageW,
+                              sizeof(uchar4) * imageW, imageH,
+                              hipMemcpyHostToDevice);
+
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = a_Src;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.addressMode[1] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeNormalizedFloat;
+
+  HIPCHECK(hipCreateTextureObject(&texImage, &texRes, &texDescr, NULL));
+
+  return error;
+}
+
+extern "C" hipError_t CUDA_FreeArray() { return hipFreeArray(a_Src); }
+rc); }
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX/inlinePTX.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX/inlinePTX.cu.hip
index e69de29..a1a985a 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX/inlinePTX.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX/inlinePTX.cu.hip
@@ -0,0 +1,118 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * Demonstration of inline PTX (assembly language) usage in CUDA kernels
+ */
+
+// System includes
+#include <stdio.h>
+#include <assert.h>
+
+// CUDA runtime
+#include <hip/hip_runtime.h>
+
+// helper functions and utilities to work with CUDA
+#include "helper_functions.h"
+#include "helper_cuda_hipified.h"
+
+__global__ void sequence_gpu(int *d_ptr, int length)
+{
+    int elemID = blockIdx.x * blockDim.x + threadIdx.x;
+
+    if (elemID < length)
+    {
+        unsigned int laneid;
+        //This command gets the lane ID within the current warp
+        asm("mov.u32 %0, %%laneid;" : "=r"(laneid));
+        d_ptr[elemID] = laneid;
+    }
+}
+
+
+void sequence_cpu(int *h_ptr, int length)
+{
+    for (int elemID=0; elemID<length; elemID++)
+    {
+        h_ptr[elemID] = elemID % 32;
+    }
+}
+
+int main(int argc, char **argv)
+{
+    printf("CUDA inline PTX assembler sample\n");
+
+    const int N = 1000;
+
+    int dev = findCudaDevice(argc, (const char **) argv);
+
+    if (dev == -1)
+    {
+        return EXIT_FAILURE;
+    }
+
+    int *d_ptr;
+    HIPCHECK(hipMalloc(&d_ptr, N * sizeof(int)));
+
+    int *h_ptr;
+    HIPCHECK(hipHostMalloc(&h_ptr, N * sizeof(int)));
+
+    dim3 cudaBlockSize(256,1,1);
+    dim3 cudaGridSize((N + cudaBlockSize.x - 1) / cudaBlockSize.x, 1, 1);
+    sequence_gpu<<<cudaGridSize, cudaBlockSize>>>(d_ptr, N);
+    HIPCHECK(hipGetLastError());
+    HIPCHECK(hipDeviceSynchronize());
+
+    sequence_cpu(h_ptr, N);
+
+    int *h_d_ptr;
+    HIPCHECK(hipHostMalloc(&h_d_ptr, N *sizeof(int)));
+    HIPCHECK(hipMemcpy(h_d_ptr, d_ptr, N *sizeof(int), hipMemcpyDeviceToHost));
+
+    bool bValid = true;
+
+    for (int i=0; i<N && bValid; i++)
+    {
+        if (h_ptr[i] != h_d_ptr[i])
+        {
+            bValid = false;
+        }
+    }
+
+    printf("Test %s.\n", bValid ? "Successful" : "Failed");
+
+    HIPCHECK(hipFree(d_ptr));
+    HIPCHECK(hipHostFree(h_ptr));
+    HIPCHECK(hipHostFree(h_d_ptr));
+
+    return bValid ? EXIT_SUCCESS: EXIT_FAILURE;
+}
+(h_d_ptr));
+
+    return bValid ? EXIT_SUCCESS: EXIT_FAILURE;
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX_nvrtc/inlinePTX_kernel.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX_nvrtc/inlinePTX_kernel.cu.hip
index e69de29..9c714f1 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX_nvrtc/inlinePTX_kernel.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX_nvrtc/inlinePTX_kernel.cu.hip
@@ -0,0 +1,40 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+extern "C" __global__ void sequence_gpu(int *d_ptr, int length) {
+  int elemID = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (elemID < length) {
+    unsigned int laneid;
+
+    // This command gets the lane ID within the current warp
+    asm("mov.u32 %0, %%laneid;" : "=r"(laneid));
+
+    d_ptr[elemID] = laneid;
+  }
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/particles/particleSystem_cuda.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/particles/particleSystem_cuda.cu.hip
index e69de29..99e5691 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/particles/particleSystem_cuda.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/particles/particleSystem_cuda.cu.hip
@@ -0,0 +1,209 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// This file contains C wrappers around the some of the CUDA API and the
+// kernel functions so that they can be called from "particleSystem.cpp"
+
+#if defined(__APPLE__) || defined(MACOSX)
+#pragma clang diagnostic ignored "-Wdeprecated-declarations"
+#include <GLUT/glut.h>
+#else
+#include <GL/freeglut.h>
+#endif
+
+#include <cstdlib>
+#include <cstdio>
+#include <string.h>
+
+#include <hip/hip_runtime.h>
+#include <cuda_gl_interop.h>
+
+#include "helper_cuda_hipified.h"
+
+#include "helper_functions.h"
+#include "thrust/device_ptr.h"
+#include "thrust/for_each.h"
+#include "thrust/iterator/zip_iterator.h"
+#include "thrust/sort.h"
+
+#include "particles_kernel_impl.cuh"
+
+extern "C" {
+
+void cudaInit(int argc, char **argv) {
+  int devID;
+
+  // use command-line specified CUDA device, otherwise use device with highest
+  // Gflops/s
+  devID = findCudaDevice(argc, (const char **)argv);
+
+  if (devID < 0) {
+    printf("No CUDA Capable devices found, exiting...\n");
+    exit(EXIT_SUCCESS);
+  }
+}
+
+void allocateArray(void **devPtr, size_t size) {
+  HIPCHECK(hipMalloc(devPtr, size));
+}
+
+void freeArray(void *devPtr) { HIPCHECK(hipFree(devPtr)); }
+
+void threadSync() { HIPCHECK(hipDeviceSynchronize()); }
+
+void copyArrayToDevice(void *device, const void *host, int offset, int size) {
+  HIPCHECK(
+      hipMemcpy((char *)device + offset, host, size, hipMemcpyHostToDevice));
+}
+
+void registerGLBufferObject(uint vbo,
+                            struct hipGraphicsResource **cuda_vbo_resource) {
+  HIPCHECK(hipGraphicsGLRegisterBuffer(cuda_vbo_resource, vbo,
+                                               cudaGraphicsMapFlagsNone));
+}
+
+void unregisterGLBufferObject(struct hipGraphicsResource *cuda_vbo_resource) {
+  HIPCHECK(hipGraphicsUnregisterResource(cuda_vbo_resource));
+}
+
+void *mapGLBufferObject(struct hipGraphicsResource **cuda_vbo_resource) {
+  void *ptr;
+  HIPCHECK(hipGraphicsMapResources(1, cuda_vbo_resource, 0));
+  size_t num_bytes;
+  HIPCHECK(hipGraphicsResourceGetMappedPointer(
+      (void **)&ptr, &num_bytes, *cuda_vbo_resource));
+  return ptr;
+}
+
+void unmapGLBufferObject(struct hipGraphicsResource *cuda_vbo_resource) {
+  HIPCHECK(hipGraphicsUnmapResources(1, &cuda_vbo_resource, 0));
+}
+
+void copyArrayFromDevice(void *host, const void *device,
+                         struct hipGraphicsResource **cuda_vbo_resource,
+                         int size) {
+  if (cuda_vbo_resource) {
+    device = mapGLBufferObject(cuda_vbo_resource);
+  }
+
+  HIPCHECK(hipMemcpy(host, device, size, hipMemcpyDeviceToHost));
+
+  if (cuda_vbo_resource) {
+    unmapGLBufferObject(*cuda_vbo_resource);
+  }
+}
+
+void setParameters(SimParams *hostParams) {
+  // copy parameters to constant memory
+  HIPCHECK(hipMemcpyToSymbol(HIP_SYMBOL(params), hostParams, sizeof(SimParams)));
+}
+
+// Round a / b to nearest higher integer value
+uint iDivUp(uint a, uint b) { return (a % b != 0) ? (a / b + 1) : (a / b); }
+
+// compute grid and thread block size for a given number of elements
+void computeGridSize(uint n, uint blockSize, uint &numBlocks,
+                     uint &numThreads) {
+  numThreads = min(blockSize, n);
+  numBlocks = iDivUp(n, numThreads);
+}
+
+void integrateSystem(float *pos, float *vel, float deltaTime,
+                     uint numParticles) {
+  thrust::device_ptr<float4> d_pos4((float4 *)pos);
+  thrust::device_ptr<float4> d_vel4((float4 *)vel);
+
+  thrust::for_each(
+      thrust::make_zip_iterator(thrust::make_tuple(d_pos4, d_vel4)),
+      thrust::make_zip_iterator(
+          thrust::make_tuple(d_pos4 + numParticles, d_vel4 + numParticles)),
+      integrate_functor(deltaTime));
+}
+
+void calcHash(uint *gridParticleHash, uint *gridParticleIndex, float *pos,
+              int numParticles) {
+  uint numThreads, numBlocks;
+  computeGridSize(numParticles, 256, numBlocks, numThreads);
+
+  // execute the kernel
+  calcHashD<<<numBlocks, numThreads>>>(gridParticleHash, gridParticleIndex,
+                                       (float4 *)pos, numParticles);
+
+  // check if kernel invocation generated an error
+  getLastCudaError("Kernel execution failed");
+}
+
+void reorderDataAndFindCellStart(uint *cellStart, uint *cellEnd,
+                                 float *sortedPos, float *sortedVel,
+                                 uint *gridParticleHash,
+                                 uint *gridParticleIndex, float *oldPos,
+                                 float *oldVel, uint numParticles,
+                                 uint numCells) {
+  uint numThreads, numBlocks;
+  computeGridSize(numParticles, 256, numBlocks, numThreads);
+
+  // set all cells to empty
+  HIPCHECK(hipMemset(cellStart, 0xffffffff, numCells * sizeof(uint)));
+
+  uint smemSize = sizeof(uint) * (numThreads + 1);
+  reorderDataAndFindCellStartD<<<numBlocks, numThreads, smemSize>>>(
+      cellStart, cellEnd, (float4 *)sortedPos, (float4 *)sortedVel,
+      gridParticleHash, gridParticleIndex, (float4 *)oldPos, (float4 *)oldVel,
+      numParticles);
+  getLastCudaError("Kernel execution failed: reorderDataAndFindCellStartD");
+}
+
+void collide(float *newVel, float *sortedPos, float *sortedVel,
+             uint *gridParticleIndex, uint *cellStart, uint *cellEnd,
+             uint numParticles, uint numCells) {
+  // thread per particle
+  uint numThreads, numBlocks;
+  computeGridSize(numParticles, 64, numBlocks, numThreads);
+
+  // execute the kernel
+  collideD<<<numBlocks, numThreads>>>((float4 *)newVel, (float4 *)sortedPos,
+                                      (float4 *)sortedVel, gridParticleIndex,
+                                      cellStart, cellEnd, numParticles);
+
+  // check if kernel invocation generated an error
+  getLastCudaError("Kernel execution failed");
+}
+
+void sortParticles(uint *dGridParticleHash, uint *dGridParticleIndex,
+                   uint numParticles) {
+  thrust::sort_by_key(
+      thrust::device_ptr<uint>(dGridParticleHash),
+      thrust::device_ptr<uint>(dGridParticleHash + numParticles),
+      thrust::device_ptr<uint>(dGridParticleIndex));
+}
+
+}  // extern "C"
+articles),
+      thrust::device_ptr<uint>(dGridParticleIndex));
+}
+
+}  // extern "C"
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.cu.hip
index 8bd4923..9da52d3 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.cu.hip
@@ -34,7 +34,7 @@
 #include <thrust/generate.h>
 #include <thrust/detail/type_traits.h>
 
-#include <helper_cuda.h>
+#include "helper_cuda_hipified.h"
 
 #include <algorithm>
 #include <time.h>
@@ -142,8 +142,8 @@ bool testSort(int argc, char **argv) {
 
   // run multiple iterations to compute an average sort time
   hipEvent_t start_event, stop_event;
-  checkCudaErrors(hipEventCreate(&start_event));
-  checkCudaErrors(hipEventCreate(&stop_event));
+  HIPCHECK(hipEventCreate(&start_event));
+  HIPCHECK(hipEventCreate(&stop_event));
 
   float totalTime = 0;
 
@@ -153,18 +153,18 @@ bool testSort(int argc, char **argv) {
 
     if (!keysOnly) d_values = h_values;
 
-    checkCudaErrors(hipEventRecord(start_event, 0));
+    HIPCHECK(hipEventRecord(start_event, 0));
 
     if (keysOnly)
       thrust::sort(d_keys.begin(), d_keys.end());
     else
       thrust::sort_by_key(d_keys.begin(), d_keys.end(), d_values.begin());
 
-    checkCudaErrors(hipEventRecord(stop_event, 0));
-    checkCudaErrors(hipEventSynchronize(stop_event));
+    HIPCHECK(hipEventRecord(stop_event, 0));
+    HIPCHECK(hipEventSynchronize(stop_event));
 
     float time = 0;
-    checkCudaErrors(hipEventElapsedTime(&time, start_event, stop_event));
+    HIPCHECK(hipEventElapsedTime(&time, start_event, stop_event));
     totalTime += time;
   }
 
@@ -188,8 +188,8 @@ bool testSort(int argc, char **argv) {
   bool bTestResult =
       thrust::is_sorted(h_keysSorted.begin(), h_keysSorted.end());
 
-  checkCudaErrors(hipEventDestroy(start_event));
-  checkCudaErrors(hipEventDestroy(stop_event));
+  HIPCHECK(hipEventDestroy(start_event));
+  HIPCHECK(hipEventDestroy(stop_event));
 
   if (!bTestResult && !quiet) {
     return false;
@@ -213,3 +213,5 @@ int main(int argc, char **argv) {
 
   printf(bTestResult ? "Test passed\n" : "Test failed!\n");
 }
+tf(bTestResult ? "Test passed\n" : "Test failed!\n");
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/reductionMultiBlockCG/reductionMultiBlockCG.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/reductionMultiBlockCG/reductionMultiBlockCG.cu.hip
index 40f4bca..92013b5 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/reductionMultiBlockCG/reductionMultiBlockCG.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/reductionMultiBlockCG/reductionMultiBlockCG.cu.hip
@@ -63,7 +63,7 @@
 // includes, project
 #include "helper_functions.h"
 #include "helper_cuda_hipified.h"
-//#include <hipify/__clang_cuda_intrinsics.h>
+
 #include <hip/hip_runtime.h>
 
 const char *sSDKsample = "reductionMultiBlockCG";
@@ -177,7 +177,7 @@ int main(int argc, char **argv) {
   printf("%s Starting...\n\n", sSDKsample);
 
   dev = findCudaDevice(argc, (const char **)argv);
-  checkCudaErrors(hipGetDeviceProperties(&deviceProp, dev));
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, dev));
   if (!deviceProp.cooperativeLaunch) {
     printf(
         "\nSelected GPU (%d) does not support Cooperative Kernel Launch, "
@@ -235,7 +235,7 @@ void getNumBlocksAndThreads(int n, int maxBlocks, int maxThreads, int &blocks,
     threads = 1;
     blocks = 1;
   } else {
-    checkCudaErrors(hipOccupancyMaxPotentialBlockSize(
+    HIPCHECK(hipOccupancyMaxPotentialBlockSize(
         &blocks, &threads, reduceSinglePassMultiBlockCG));
   }
 
@@ -267,7 +267,7 @@ float benchmarkReduce(int n, int numThreads, int numBlocks, int maxThreads,
   // copy final sum from device to host
   error =
       hipMemcpy(&gpu_result, d_odata, sizeof(float), hipMemcpyDeviceToHost);
-  checkCudaErrors(error);
+  HIPCHECK(error);
 
   return gpu_result;
 }
@@ -287,8 +287,8 @@ bool runTest(int argc, char **argv, int device) {
 
   // Set the device to be used
   hipDeviceProp_t prop = {0};
-  checkCudaErrors(hipSetDevice(device));
-  checkCudaErrors(hipGetDeviceProperties(&prop, device));
+  HIPCHECK(hipSetDevice(device));
+  HIPCHECK(hipGetDeviceProperties(&prop, device));
 
   // create random input data on CPU
   unsigned int bytes = size * sizeof(float);
@@ -324,7 +324,7 @@ bool runTest(int argc, char **argv, int device) {
   // We calculate the occupancy to know how many block can actually fit on the
   // GPU
   int numBlocksPerSm = 0;
-  checkCudaErrors(hipOccupancyMaxActiveBlocksPerMultiprocessor(
+  HIPCHECK(hipOccupancyMaxActiveBlocksPerMultiprocessor(
       &numBlocksPerSm, reduceSinglePassMultiBlockCG, numThreads,
       numThreads * sizeof(double)));
 
@@ -342,12 +342,12 @@ bool runTest(int argc, char **argv, int device) {
   float *d_idata = NULL;
   float *d_odata = NULL;
 
-  checkCudaErrors(hipMalloc((void **)&d_idata, bytes));
-  checkCudaErrors(hipMalloc((void **)&d_odata, numBlocks * sizeof(float)));
+  HIPCHECK(hipMalloc((void **)&d_idata, bytes));
+  HIPCHECK(hipMalloc((void **)&d_odata, numBlocks * sizeof(float)));
 
   // copy data directly to device memory
-  checkCudaErrors(hipMemcpy(d_idata, h_idata, bytes, hipMemcpyHostToDevice));
-  checkCudaErrors(hipMemcpy(d_odata, h_idata, numBlocks * sizeof(float),
+  HIPCHECK(hipMemcpy(d_idata, h_idata, bytes, hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(d_odata, h_idata, numBlocks * sizeof(float),
                              hipMemcpyHostToDevice));
 
   int testIterations = 100;
@@ -385,3 +385,9 @@ bool runTest(int argc, char **argv, int device) {
 
   return bTestPassed;
 }
+ta);
+  hipFree(d_idata);
+  hipFree(d_odata);
+
+  return bTestPassed;
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/shfl_scan/shfl_scan.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/shfl_scan/shfl_scan.cu.hip
index 583de66..ba26509 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/shfl_scan/shfl_scan.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/shfl_scan/shfl_scan.cu.hip
@@ -41,6 +41,7 @@
 #include "helper_cuda_hipified.h"
 #include "helper_functions.h"
 #include "shfl_integral_image.cuh"
+
 // Scan using shfl - takes log2(n) steps
 // This function demonstrates basic use of the shuffle intrinsic, __shfl_up,
 // to perform a scan operation across a block.
@@ -212,9 +213,9 @@ bool shuffle_simple_test(int argc, char **argv) {
   cuda_device = findCudaDevice(argc, (const char **)argv);
 
   hipDeviceProp_t deviceProp;
-  checkCudaErrors(hipGetDevice(&cuda_device));
+  HIPCHECK(hipGetDevice(&cuda_device));
 
-  checkCudaErrors(hipGetDeviceProperties(&deviceProp, cuda_device));
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, cuda_device));
 
   printf("> Detected Compute SM %d.%d hardware with %d multi-processors\n",
          deviceProp.major, deviceProp.minor, deviceProp.multiProcessorCount);
@@ -226,9 +227,9 @@ bool shuffle_simple_test(int argc, char **argv) {
     exit(EXIT_WAIVED);
   }
 
-  checkCudaErrors(hipHostMalloc(reinterpret_cast<void **>(&h_data),
+  HIPCHECK(hipHostMalloc(reinterpret_cast<void **>(&h_data),
                                  sizeof(int) * n_elements));
-  checkCudaErrors(hipHostMalloc(reinterpret_cast<void **>(&h_result),
+  HIPCHECK(hipHostMalloc(reinterpret_cast<void **>(&h_result),
                                  sizeof(int) * n_elements));
 
   // initialize data:
@@ -258,32 +259,32 @@ bool shuffle_simple_test(int argc, char **argv) {
 
   // initialize a timer
   hipEvent_t start, stop;
-  checkCudaErrors(hipEventCreate(&start));
-  checkCudaErrors(hipEventCreate(&stop));
+  HIPCHECK(hipEventCreate(&start));
+  HIPCHECK(hipEventCreate(&stop));
   float et = 0;
   float inc = 0;
 
-  checkCudaErrors(hipMalloc(reinterpret_cast<void **>(&d_data), sz));
-  checkCudaErrors(
+  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_data), sz));
+  HIPCHECK(
       hipMalloc(reinterpret_cast<void **>(&d_partial_sums), partial_sz));
-  checkCudaErrors(hipMemset(d_partial_sums, 0, partial_sz));
+  HIPCHECK(hipMemset(d_partial_sums, 0, partial_sz));
 
-  checkCudaErrors(
+  HIPCHECK(
       hipHostMalloc(reinterpret_cast<void **>(&h_partial_sums), partial_sz));
-  checkCudaErrors(hipMemcpy(d_data, h_data, sz, hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(d_data, h_data, sz, hipMemcpyHostToDevice));
 
-  checkCudaErrors(hipEventRecord(start, 0));
+  HIPCHECK(hipEventRecord(start, 0));
   shfl_scan_test<<<gridSize, blockSize, shmem_sz>>>(d_data, 32, d_partial_sums);
   shfl_scan_test<<<p_gridSize, p_blockSize, shmem_sz>>>(d_partial_sums, 32);
   uniform_add<<<gridSize - 1, blockSize>>>(d_data + blockSize, d_partial_sums,
                                            n_elements);
-  checkCudaErrors(hipEventRecord(stop, 0));
-  checkCudaErrors(hipEventSynchronize(stop));
-  checkCudaErrors(hipEventElapsedTime(&inc, start, stop));
+  HIPCHECK(hipEventRecord(stop, 0));
+  HIPCHECK(hipEventSynchronize(stop));
+  HIPCHECK(hipEventElapsedTime(&inc, start, stop));
   et += inc;
 
-  checkCudaErrors(hipMemcpy(h_result, d_data, sz, hipMemcpyDeviceToHost));
-  checkCudaErrors(hipMemcpy(h_partial_sums, d_partial_sums, partial_sz,
+  HIPCHECK(hipMemcpy(h_result, d_data, sz, hipMemcpyDeviceToHost));
+  HIPCHECK(hipMemcpy(h_partial_sums, d_partial_sums, partial_sz,
                              hipMemcpyDeviceToHost));
 
   printf("Test Sum: %d\n", h_partial_sums[n_partialSums - 1]);
@@ -293,11 +294,11 @@ bool shuffle_simple_test(int argc, char **argv) {
 
   bool bTestResult = CPUverify(h_data, h_result, n_elements);
 
-  checkCudaErrors(hipHostFree(h_data));
-  checkCudaErrors(hipHostFree(h_result));
-  checkCudaErrors(hipHostFree(h_partial_sums));
-  checkCudaErrors(hipFree(d_data));
-  checkCudaErrors(hipFree(d_partial_sums));
+  HIPCHECK(hipHostFree(h_data));
+  HIPCHECK(hipHostFree(h_result));
+  HIPCHECK(hipHostFree(h_partial_sums));
+  HIPCHECK(hipFree(d_data));
+  HIPCHECK(hipFree(d_partial_sums));
 
   return bTestResult;
 }
@@ -316,7 +317,7 @@ bool shuffle_integral_image_test() {
   printf("\nComputing Integral Image Test on size %d x %d synthetic data\n", w,
          h);
   printf("---------------------------------------------------\n");
-  checkCudaErrors(hipHostMalloc(reinterpret_cast<void **>(&h_image), sz));
+  HIPCHECK(hipHostMalloc(reinterpret_cast<void **>(&h_image), sz));
   // fill test "image" with synthetic 1's data
   memset(h_image, 0, sz);
 
@@ -326,11 +327,11 @@ bool shuffle_integral_image_test() {
   int gridSize = h;
 
   // Create a synthetic image for testing
-  checkCudaErrors(hipMalloc(reinterpret_cast<void **>(&d_data), sz));
-  checkCudaErrors(hipMalloc(reinterpret_cast<void **>(&d_integral_image),
+  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_data), sz));
+  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_integral_image),
                              n_elements * sizeof(int) * 4));
-  checkCudaErrors(hipMemset(d_data, 1, sz));
-  checkCudaErrors(hipMemset(d_integral_image, 0, sz));
+  HIPCHECK(hipMemset(d_data, 1, sz));
+  HIPCHECK(hipMemset(d_integral_image, 0, sz));
 
   hipEvent_t start, stop;
   hipEventCreate(&start);
@@ -344,12 +345,12 @@ bool shuffle_integral_image_test() {
       reinterpret_cast<uint4 *>(d_data),
       reinterpret_cast<uint4 *>(d_integral_image));
   hipEventRecord(stop);
-  checkCudaErrors(hipEventSynchronize(stop));
-  checkCudaErrors(hipEventElapsedTime(&et, start, stop));
+  HIPCHECK(hipEventSynchronize(stop));
+  HIPCHECK(hipEventElapsedTime(&et, start, stop));
   printf("Method: Fast  Time (GPU Timer): %f ms ", et);
 
   // verify the scan line results
-  checkCudaErrors(
+  HIPCHECK(
       hipMemcpy(h_image, d_integral_image, sz, hipMemcpyDeviceToHost));
   err = verifyDataRowSums(h_image, w, h);
   printf("Diff = %d\n", err);
@@ -362,21 +363,21 @@ bool shuffle_integral_image_test() {
   shfl_vertical_shfl<<<testGrid, blockSz>>>((unsigned int *)d_integral_image, w,
                                             h);
   hipEventRecord(stop);
-  checkCudaErrors(hipEventSynchronize(stop));
-  checkCudaErrors(hipEventElapsedTime(&et, start, stop));
+  HIPCHECK(hipEventSynchronize(stop));
+  HIPCHECK(hipEventElapsedTime(&et, start, stop));
   printf("Method: Vertical Scan  Time (GPU Timer): %f ms ", et);
 
   // Verify the column results
-  checkCudaErrors(
+  HIPCHECK(
       hipMemcpy(h_image, d_integral_image, sz, hipMemcpyDeviceToHost));
   printf("\n");
 
   int finalSum = h_image[w * h - 1];
   printf("CheckSum: %d, (expect %dx%d=%d)\n", finalSum, w, h, w * h);
 
-  checkCudaErrors(hipFree(d_data));
-  checkCudaErrors(hipFree(d_integral_image));
-  checkCudaErrors(hipHostFree(h_image));
+  HIPCHECK(hipFree(d_data));
+  HIPCHECK(hipFree(d_integral_image));
+  HIPCHECK(hipHostFree(h_image));
   // verify final sum: if the final value in the corner is the same as the size
   // of the buffer (all 1's) then the integral image was generated successfully
   return (finalSum == w * h) ? true : false;
@@ -394,9 +395,9 @@ int main(int argc, char *argv[]) {
   cuda_device = findCudaDevice(argc, (const char **)argv);
 
   hipDeviceProp_t deviceProp;
-  checkCudaErrors(hipGetDevice(&cuda_device));
+  HIPCHECK(hipGetDevice(&cuda_device));
 
-  checkCudaErrors(hipGetDeviceProperties(&deviceProp, cuda_device));
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, cuda_device));
 
   printf("> Detected Compute SM %d.%d hardware with %d multi-processors\n",
          deviceProp.major, deviceProp.minor, deviceProp.multiProcessorCount);
@@ -416,3 +417,15 @@ int main(int argc, char *argv[]) {
 
   exit((bTestResult) ? EXIT_SUCCESS : EXIT_FAILURE);
 }
+ing test.\n");
+    exit(EXIT_WAIVED);
+  }
+
+  bool bTestResult = true;
+  bool simpleTest = shuffle_simple_test(argc, argv);
+  bool intTest = shuffle_integral_image_test();
+
+  bTestResult = simpleTest & intTest;
+
+  exit((bTestResult) ? EXIT_SUCCESS : EXIT_FAILURE);
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationIPC/streamOrderedAllocationIPC.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationIPC/streamOrderedAllocationIPC.cu.hip
index 2cd9b07..268b3af 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationIPC/streamOrderedAllocationIPC.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationIPC/streamOrderedAllocationIPC.cu.hip
@@ -36,10 +36,10 @@
 #include <vector>
 #include <hip/hip_runtime.h>
 #define CUDA_DRIVER_API 1
-#include "helper_cuda_hipified.h"
+#include "helper_cuda.h"
 #include "helper_cuda_drvapi.h"
 #include "helper_multiprocess.h"
-#include "HIPCHECK.h"
+
 static const char shmName[] = "streamOrderedAllocationIPCshm";
 static const char ipcName[] = "streamOrderedAllocationIPC_pipe";
 // For direct NVLINK and PCI-E peers, at max 8 simultaneous peers are allowed
@@ -439,3 +439,15 @@ int main(int argc, char **argv) {
   return EXIT_SUCCESS;
 #endif
 }
+d(_WIN32) || defined(WIN64) || defined(_WIN64)
+  printf("Not supported on ARM\n");
+  return EXIT_WAIVED;
+#else
+  if (argc == 1) {
+    parentProcess(argv[0]);
+  } else {
+    childProcess(atoi(argv[1]));
+  }
+  return EXIT_SUCCESS;
+#endif
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/threadMigration/threadMigration_kernel.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/threadMigration/threadMigration_kernel.cu.hip
index e69de29..42cc050 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/threadMigration/threadMigration_kernel.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/threadMigration/threadMigration_kernel.cu.hip
@@ -0,0 +1,31 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+extern "C" __global__ void kernelFunction(int *input) {
+  input[threadIdx.x] = 32 - threadIdx.x;
+}
diff --git a/src/samples/Samples/3_CUDA_Features/bf16TensorCoreGemm/bf16TensorCoreGemm.cu.hip b/src/samples/Samples/3_CUDA_Features/bf16TensorCoreGemm/bf16TensorCoreGemm.cu.hip
index 05c0694..11a668d 100755
--- a/src/samples/Samples/3_CUDA_Features/bf16TensorCoreGemm/bf16TensorCoreGemm.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/bf16TensorCoreGemm/bf16TensorCoreGemm.cu.hip
@@ -66,8 +66,8 @@
 #include <cuda/pipeline>
 
 // helper functions and utilities to work with CUDA
-#include <helper_functions.h>
-#include <helper_cuda.h>
+#include "helper_functions.h"
+#include "helper_cuda_hipified.h"
 
 // Externally configurable parameters.
 
@@ -171,12 +171,12 @@
 
 enum kernels
 {
-    bf16mma_shmem_gemm_async_copy  = 0, // __nv_bfloat16 MMA shmem using kernel with async_copy
+    bf16mma_shmem_gemm_async_copy  = 0, // __nv_bfloat16 MMA shmem using kernel with async_copy 
     bf16mma_shmem_gemm             = 1, // __nv_bfloat16 MMA shmem using kernel normal copy (without async_copy).
     simple_bf16mma_gemm            = 2  // __nv_bfloat16 MMA non-shmem using simple kernel.
 };
 
-const char* kernelNames[] = {"compute_bf16gemm_async_copy", "compute_bf16gemm",
+const char* kernelNames[] = {"compute_bf16gemm_async_copy", "compute_bf16gemm", 
                             "simple_wmma_bf16gemm"};
 
 using namespace nvcuda;
@@ -242,7 +242,7 @@ __global__ void compute_bf16gemm(const __nv_bfloat16 *A, const __nv_bfloat16 *B,
         // Stream multiple C tiles to shared memory.
 #pragma unroll
         for (int i = 0; i < N; i++) {
-            *((int4*)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId) =
+            *((int4*)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId) = 
                 *((int4*)(src_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) + laneId);
         }
 
@@ -287,7 +287,7 @@ __global__ void compute_bf16gemm(const __nv_bfloat16 *A, const __nv_bfloat16 *B,
         for (int tile_k = 0; tile_k < K_TILES; tile_k += CHUNK_K) {
             // Copy slices of the A and B matrices to shared memory.
             // The first half of the warps in the CTA copy the A matrix, the rest copy the B matrix.
-            size_t shmem_idx = warpId < (WARPS_PER_BLOCK/2) ? (M * (warpId % (WARPS_PER_BLOCK/2)) * 2) :
+            size_t shmem_idx = warpId < (WARPS_PER_BLOCK/2) ? (M * (warpId % (WARPS_PER_BLOCK/2)) * 2) : 
                                                               (N * (warpId % (WARPS_PER_BLOCK/2)) * 2 + shmem_idx_b_off);
 
             // First half of the warp copies the first row / column of the matrix,
@@ -563,7 +563,7 @@ __global__ void compute_bf16gemm_async_copy(const __nv_bfloat16 *A, const __nv_b
 
 // Performs an MxNxK bf16 GEMM (C=alpha*A*B + beta*C) assuming:
 //  1) Matrices are packed in memory.
-//  2) M, N and K are multiples of 16, 16 and 16 respectively.
+//  2) M, N and K are multiples of 16, 16 and 16 respectively. 
 //  3) A is row major, B is column major matrix.
 // Note: This is a less performant version of the compute_bf16gemm kernel. It is designed for
 //       demonstration purposes only to show the CUDA WMMA API use without relying on
@@ -579,7 +579,7 @@ __global__ void simple_wmma_bf16gemm(__nv_bfloat16 *a, __nv_bfloat16 *b, float *
    // Tile using a 2D grid
    int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / warpSize;
    int warpN = (blockIdx.y * blockDim.y + threadIdx.y);
-
+ 
    // Declare the fragments
    wmma::fragment<wmma::matrix_a, M, N, K, __nv_bfloat16, wmma::row_major> a_frag;
    wmma::fragment<wmma::matrix_b, M, N, K, __nv_bfloat16, wmma::col_major> b_frag;
@@ -590,7 +590,7 @@ __global__ void simple_wmma_bf16gemm(__nv_bfloat16 *a, __nv_bfloat16 *b, float *
 
    // Loop over k
    for (int i = 0; i < k_ld; i += K) {
-      int aCol = i;
+      int aCol = i; 
       int aRow = warpM * M;
 
       int bCol = i;
@@ -601,7 +601,7 @@ __global__ void simple_wmma_bf16gemm(__nv_bfloat16 *a, __nv_bfloat16 *b, float *
          // Load the inputs
          wmma::load_matrix_sync(a_frag, a + aCol + aRow * lda, lda);
          wmma::load_matrix_sync(b_frag, b + bRow + bCol * ldb, ldb);
-
+ 
          // Perform the matrix multiplication
          wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag);
 
@@ -651,7 +651,7 @@ int main(int argc, char **argv)
     int dev = findCudaDevice(argc, (const char **)argv);
 
     hipDeviceProp_t deviceProp;
-    checkCudaErrors(hipGetDeviceProperties(&deviceProp, dev));
+    HIPCHECK(hipGetDeviceProperties(&deviceProp, dev));
 
     // Tensor cores require a GPU of Volta (SM8X) architecture or higher.
     if (deviceProp.major < 8) {
@@ -684,10 +684,10 @@ int main(int argc, char **argv)
     float *C = NULL;
     float *D = NULL;
 
-    checkCudaErrors(hipMalloc((void**)&A, sizeof(__nv_bfloat16) * M_GLOBAL * K_GLOBAL));
-    checkCudaErrors(hipMalloc((void**)&B, sizeof(__nv_bfloat16) * N_GLOBAL * K_GLOBAL));
-    checkCudaErrors(hipMalloc((void**)&C, sizeof(float) * M_GLOBAL * N_GLOBAL));
-    checkCudaErrors(hipMalloc((void**)&D, sizeof(float) * M_GLOBAL * N_GLOBAL));
+    HIPCHECK(hipMalloc((void**)&A, sizeof(__nv_bfloat16) * M_GLOBAL * K_GLOBAL));
+    HIPCHECK(hipMalloc((void**)&B, sizeof(__nv_bfloat16) * N_GLOBAL * K_GLOBAL));
+    HIPCHECK(hipMalloc((void**)&C, sizeof(float) * M_GLOBAL * N_GLOBAL));
+    HIPCHECK(hipMalloc((void**)&D, sizeof(float) * M_GLOBAL * N_GLOBAL));
 
     assert(((unsigned long long)A) % 128 == 0);
     assert(((unsigned long long)B) % 128 == 0);
@@ -698,10 +698,10 @@ int main(int argc, char **argv)
 
     printf("Preparing data for GPU...\n");
 
-    checkCudaErrors(hipMemcpy(A, A_h, sizeof(__nv_bfloat16) * M_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
-    checkCudaErrors(hipMemcpy(B, B_h, sizeof(__nv_bfloat16) * N_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
-    checkCudaErrors(hipMemcpy(C, C_h, sizeof(float) * M_GLOBAL * N_GLOBAL, hipMemcpyHostToDevice));
-    checkCudaErrors(hipMemset(D, 0, sizeof(float) * M_GLOBAL * N_GLOBAL));
+    HIPCHECK(hipMemcpy(A, A_h, sizeof(__nv_bfloat16) * M_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
+    HIPCHECK(hipMemcpy(B, B_h, sizeof(__nv_bfloat16) * N_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
+    HIPCHECK(hipMemcpy(C, C_h, sizeof(float) * M_GLOBAL * N_GLOBAL, hipMemcpyHostToDevice));
+    HIPCHECK(hipMemset(D, 0, sizeof(float) * M_GLOBAL * N_GLOBAL));
 
     enum {
         // Compute the right amount of shared memory to request.
@@ -719,9 +719,9 @@ int main(int argc, char **argv)
 
     hipEvent_t start, stop;
 
-    checkCudaErrors(hipEventCreate(&start));
-    checkCudaErrors(hipEventCreate(&stop));
-    checkCudaErrors(hipEventRecord(start));
+    HIPCHECK(hipEventCreate(&start));    
+    HIPCHECK(hipEventCreate(&stop));
+    HIPCHECK(hipEventRecord(start));
 
     // kernel to run - default (b16mma_shmem_gemm_async_copy == 0)
     kernels selected_kernel = bf16mma_shmem_gemm_async_copy;
@@ -745,22 +745,22 @@ int main(int argc, char **argv)
         {
             case bf16mma_shmem_gemm_async_copy :
             default:
-                checkCudaErrors(hipFuncSetAttribute(compute_bf16gemm_async_copy, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
+                HIPCHECK(hipFuncSetAttribute(compute_bf16gemm_async_copy, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
                 checkKernelErrors((compute_bf16gemm_async_copy<<<deviceProp.multiProcessorCount*2, THREADS_PER_BLOCK, SHMEM_SZ>>>(A, B, C, D, alpha, beta)));
                 break;
             case bf16mma_shmem_gemm :
-                checkCudaErrors(hipFuncSetAttribute(compute_bf16gemm, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
+                HIPCHECK(hipFuncSetAttribute(compute_bf16gemm, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
                 checkKernelErrors((compute_bf16gemm<<<deviceProp.multiProcessorCount*2, THREADS_PER_BLOCK, SHMEM_SZ>>>(A, B, C, D, alpha, beta)));
                 break;
         }
 #if CPU_DEBUG
-        checkCudaErrors(hipMemcpy(result_hD, D, sizeof(float)*M_GLOBAL*N_GLOBAL, hipMemcpyDeviceToHost));
+        HIPCHECK(hipMemcpy(result_hD, D, sizeof(float)*M_GLOBAL*N_GLOBAL, hipMemcpyDeviceToHost));
 #endif
     }
     else {
         dim3 gridDim;
         dim3 blockDim;
-
+     
         // blockDim.x must be a multple of warpSize
         // 128x4 means we have 16 warps and a block computes a 64x64 output tile
         blockDim.x = 128;
@@ -772,12 +772,12 @@ int main(int argc, char **argv)
         printf("Computing... using simple_wmma_gemm kernel\n");
         simple_wmma_bf16gemm<<<gridDim, blockDim>>>(A, B, C, D, M_GLOBAL, N_GLOBAL, K_GLOBAL, alpha, beta);
 #if CPU_DEBUG
-        checkCudaErrors(hipMemcpy(result_hD, D, sizeof(float) * M_GLOBAL * N_GLOBAL, hipMemcpyDeviceToHost));
+        HIPCHECK(hipMemcpy(result_hD, D, sizeof(float) * M_GLOBAL * N_GLOBAL, hipMemcpyDeviceToHost));
 #endif
     }
 
-    checkCudaErrors(hipEventRecord(stop));
-    checkCudaErrors(hipEventSynchronize(stop));
+    HIPCHECK(hipEventRecord(stop));
+    HIPCHECK(hipEventSynchronize(stop));
 
 #if CPU_DEBUG
     printf("Verifying correctness of the computations...\n");
@@ -801,7 +801,7 @@ int main(int argc, char **argv)
 
     float milliseconds = 0;
 
-    checkCudaErrors(hipEventElapsedTime(&milliseconds, start, stop));
+    HIPCHECK(hipEventElapsedTime(&milliseconds, start, stop));
 
     printf("Time: %f ms\n", milliseconds);
     printf("TFLOPS: %.2f\n", (((double)M_GLOBAL * N_GLOBAL * K_GLOBAL * 2)/(milliseconds/1000.)) / 1e12);
@@ -809,7 +809,14 @@ int main(int argc, char **argv)
     free(A_h);
     free(B_h);
     free(C_h);
-    checkCudaErrors(hipFree((void*)A));
+    HIPCHECK(hipFree((void*)A));
+    HIPCHECK(hipFree((void*)B));
+    HIPCHECK(hipFree((void*)C));
+    HIPCHECK(hipFree((void*)D));
+
+    return 0;
+}
+ors(hipFree((void*)A));
     checkCudaErrors(hipFree((void*)B));
     checkCudaErrors(hipFree((void*)C));
     checkCudaErrors(hipFree((void*)D));
diff --git a/src/samples/Samples/3_CUDA_Features/binaryPartitionCG/binaryPartitionCG.cu.hip b/src/samples/Samples/3_CUDA_Features/binaryPartitionCG/binaryPartitionCG.cu.hip
index e69de29..68a5536 100755
--- a/src/samples/Samples/3_CUDA_Features/binaryPartitionCG/binaryPartitionCG.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/binaryPartitionCG/binaryPartitionCG.cu.hip
@@ -0,0 +1,166 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * This sample illustrates basic usage of binary partition cooperative groups
+ * within the thread block tile when divergent path exists.
+ * 1.) Each thread loads a value from random array.
+ * 2.) then checks if it is odd or even.
+ * 3.) create binary partition group based on the above predicate
+ * 4.) we count the number of odd/even in the group based on size of the binary
+       groups
+ * 5.) write it global counter of odd.
+ * 6.) sum the values loaded by individual threads(using reduce) and write it to
+       global even & odd elements sum.
+ *
+ * **NOTE** :
+ *    binary_partition results in splitting warp into divergent thread groups
+ *    this is not good from performance perspective, but in cases where warp
+ *    divergence is inevitable one can use binary_partition group.
+*/
+
+#include <stdio.h>
+#include <hip/hip_cooperative_groups.h>
+#include <cooperative_groups/reduce.h>
+#include "helper_cuda_hipified.h"
+
+namespace cg = cooperative_groups;
+
+void initOddEvenArr(int *inputArr, unsigned int size) {
+  for (int i = 0; i < size; i++) {
+    inputArr[i] = rand() % 50;
+  }
+}
+
+/**
+ * CUDA kernel device code
+ *
+ * Creates cooperative groups and performs odd/even counting & summation.
+ */
+__global__ void oddEvenCountAndSumCG(int *inputArr, int *numOfOdds,
+                                     int *sumOfOddAndEvens, unsigned int size) {
+  cg::thread_block cta = cg::this_thread_block();
+  cg::grid_group grid = cg::this_grid();
+  cg::thread_block_tile<32> tile32 = cg::tiled_partition<32>(cta);
+
+  for (int i = grid.thread_rank(); i < size; i += grid.size()) {
+    int elem = inputArr[i];
+    auto subTile = cg::binary_partition(tile32, elem & 1);
+    if (elem & 1)  // Odd numbers group
+    {
+      int oddGroupSum = cg::reduce(subTile, elem, cg::plus<int>());
+
+      if (subTile.thread_rank() == 0) {
+        // Add number of odds present in this group of Odds.
+        atomicAdd(numOfOdds, subTile.size());
+
+        // Add local reduction of odds present in this group of Odds.
+        atomicAdd(&sumOfOddAndEvens[0], oddGroupSum);
+      }
+    } else  // Even numbers group
+    {
+      int evenGroupSum = cg::reduce(subTile, elem, cg::plus<int>());
+
+      if (subTile.thread_rank() == 0) {
+        // Add local reduction of even present in this group of evens.
+        atomicAdd(&sumOfOddAndEvens[1], evenGroupSum);
+      }
+    }
+    // reconverge warp so for next loop iteration we ensure convergence of
+    // above diverged threads to perform coalesced loads of inputArr.
+    cg::sync(tile32);
+  }
+}
+
+/**
+ * Host main routine
+ */
+int main(int argc, const char **argv) {
+  int deviceId = findCudaDevice(argc, argv);
+  int *h_inputArr, *d_inputArr;
+  int *h_numOfOdds, *d_numOfOdds;
+  int *h_sumOfOddEvenElems, *d_sumOfOddEvenElems;
+  unsigned int arrSize = 1024 * 100;
+
+  HIPCHECK(hipHostMalloc(&h_inputArr, sizeof(int) * arrSize));
+  HIPCHECK(hipHostMalloc(&h_numOfOdds, sizeof(int)));
+  HIPCHECK(hipHostMalloc(&h_sumOfOddEvenElems, sizeof(int) * 2));
+  initOddEvenArr(h_inputArr, arrSize);
+
+  hipStream_t stream;
+  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
+  HIPCHECK(hipMalloc(&d_inputArr, sizeof(int) * arrSize));
+  HIPCHECK(hipMalloc(&d_numOfOdds, sizeof(int)));
+  HIPCHECK(hipMalloc(&d_sumOfOddEvenElems, sizeof(int) * 2));
+
+  HIPCHECK(hipMemcpyAsync(d_inputArr, h_inputArr, sizeof(int) * arrSize,
+                                  hipMemcpyHostToDevice, stream));
+  HIPCHECK(hipMemsetAsync(d_numOfOdds, 0, sizeof(int), stream));
+  HIPCHECK(
+      hipMemsetAsync(d_sumOfOddEvenElems, 0, 2 * sizeof(int), stream));
+
+  // Launch the kernel
+  int threadsPerBlock = 0;
+  int blocksPerGrid = 0;
+  HIPCHECK(hipOccupancyMaxPotentialBlockSize(
+      &blocksPerGrid, &threadsPerBlock, oddEvenCountAndSumCG, 0, 0));
+
+  printf("\nLaunching %d blocks with %d threads...\n\n", blocksPerGrid,
+         threadsPerBlock);
+
+  oddEvenCountAndSumCG<<<blocksPerGrid, threadsPerBlock, 0, stream>>>(
+      d_inputArr, d_numOfOdds, d_sumOfOddEvenElems, arrSize);
+
+  HIPCHECK(hipMemcpyAsync(h_numOfOdds, d_numOfOdds, sizeof(int),
+                                  hipMemcpyDeviceToHost, stream));
+  HIPCHECK(hipMemcpyAsync(h_sumOfOddEvenElems, d_sumOfOddEvenElems,
+                                  2 * sizeof(int), hipMemcpyDeviceToHost,
+                                  stream));
+  HIPCHECK(hipStreamSynchronize(stream));
+
+  printf("Array size = %d Num of Odds = %d Sum of Odds = %d Sum of Evens %d\n",
+         arrSize, h_numOfOdds[0], h_sumOfOddEvenElems[0],
+         h_sumOfOddEvenElems[1]);
+  printf("\n...Done.\n\n");
+
+  HIPCHECK(hipHostFree(h_inputArr));
+  HIPCHECK(hipHostFree(h_numOfOdds));
+  HIPCHECK(hipHostFree(h_sumOfOddEvenElems));
+
+  HIPCHECK(hipFree(d_inputArr));
+  HIPCHECK(hipFree(d_numOfOdds));
+  HIPCHECK(hipFree(d_sumOfOddEvenElems));
+
+  return EXIT_SUCCESS;
+}
+s(hipFree(d_inputArr));
+  checkCudaErrors(hipFree(d_numOfOdds));
+  checkCudaErrors(hipFree(d_sumOfOddEvenElems));
+
+  return EXIT_SUCCESS;
+}
diff --git a/src/samples/Samples/3_CUDA_Features/bindlessTexture/bindlessTexture_kernel.cu.hip b/src/samples/Samples/3_CUDA_Features/bindlessTexture/bindlessTexture_kernel.cu.hip
index e69de29..50acfeb 100755
--- a/src/samples/Samples/3_CUDA_Features/bindlessTexture/bindlessTexture_kernel.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/bindlessTexture/bindlessTexture_kernel.cu.hip
@@ -0,0 +1,437 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+  This sample has two kernels, one doing the rendering every frame, and
+  another one used to generate the mip map levels at startup.
+
+  For rendering we use a "virtual" texturing approach, where one 2d texture
+  stores pointers to the actual textures used. This can be achieved by the
+  new cudaTextureObject introduced in CUDA 5.0 and requiring sm3+ hardware.
+
+  The mipmap generation kernel uses cudaSurfaceObject and cudaTextureObject
+  passed as kernel arguments to compute the higher mip map level based on
+  the lower.
+*/
+
+#ifndef _BINDLESSTEXTURE_KERNEL_CU_
+#define _BINDLESSTEXTURE_KERNEL_CU_
+
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <math.h>
+
+#include <vector>
+
+#include "helper_cuda_hipified.h"
+#include <helper_math.h>
+
+#include "bindlessTexture.h"
+
+// set this to just see the mipmap chain of first image
+//#define SHOW_MIPMAPS
+
+// local references to resources
+
+Image atlasImage;
+std::vector<Image> contentImages;
+float highestLod = 1.0f;
+
+#ifndef MAX
+#define MAX(a, b) ((a > b) ? a : b)
+#endif
+
+//////////////////////////////////////////////////////////////////////////
+
+__host__ __device__ __inline__ uint2 encodeTextureObject(
+    hipTextureObject_t obj) {
+  return make_uint2((uint)(obj & 0xFFFFFFFF), (uint)(obj >> 32));
+}
+
+__host__ __device__ __inline__ hipTextureObject_t decodeTextureObject(
+    uint2 obj) {
+  return (((hipTextureObject_t)obj.x) | ((hipTextureObject_t)obj.y) << 32);
+}
+
+__device__ __inline__ float4 to_float4(uchar4 vec) {
+  return make_float4(vec.x, vec.y, vec.z, vec.w);
+}
+
+__device__ __inline__ uchar4 to_uchar4(float4 vec) {
+  return make_uchar4((uchar)vec.x, (uchar)vec.y, (uchar)vec.z, (uchar)vec.w);
+}
+
+//////////////////////////////////////////////////////////////////////////
+// Rendering
+
+// the atlas texture stores the 64 bit cudaTextureObjects
+// we use it for "virtual" texturing
+
+__global__ void d_render(uchar4 *d_output, uint imageW, uint imageH, float lod,
+                         hipTextureObject_t atlasTexture) {
+  uint x = blockIdx.x * blockDim.x + threadIdx.x;
+  uint y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  float u = x / (float)imageW;
+  float v = y / (float)imageH;
+
+  if ((x < imageW) && (y < imageH)) {
+    // read from 2D atlas texture and decode texture object
+    uint2 texCoded = tex2D<uint2>(atlasTexture, u, v);
+    hipTextureObject_t tex = decodeTextureObject(texCoded);
+
+    // read from cuda texture object, use template to specify what data will be
+    // returned. tex2DLod allows us to pass the lod (mip map level) directly.
+    // There is other functions with CUDA 5, e.g. tex2DGrad, that allow you
+    // to pass derivatives to perform automatic mipmap/anisotropic filtering.
+    float4 color = tex2DLod<float4>(tex, u, 1 - v, lod);
+    // In our sample tex is always valid, but for something like your own
+    // sparse texturing you would need to make sure to handle the zero case.
+
+    // write output color
+    uint i = y * imageW + x;
+    d_output[i] = to_uchar4(color * 255.0);
+  }
+}
+
+extern "C" void renderAtlasImage(dim3 gridSize, dim3 blockSize,
+                                 uchar4 *d_output, uint imageW, uint imageH,
+                                 float lod) {
+  // psuedo animate lod
+  lod = fmodf(lod, highestLod * 2);
+  lod = highestLod - fabs(lod - highestLod);
+
+#ifdef SHOW_MIPMAPS
+  lod = 0.0f;
+#endif
+
+  d_render<<<gridSize, blockSize>>>(d_output, imageW, imageH, lod,
+                                    atlasImage.textureObject);
+
+  HIPCHECK(hipGetLastError());
+}
+
+//////////////////////////////////////////////////////////////////////////
+// MipMap Generation
+
+//  A key benefit of using the new surface objects is that we don't need any
+//  global binding points anymore. We can directly pass them as function
+//  arguments.
+
+__global__ void d_mipmap(hipSurfaceObject_t mipOutput,
+                         hipTextureObject_t mipInput, uint imageW,
+                         uint imageH) {
+  uint x = blockIdx.x * blockDim.x + threadIdx.x;
+  uint y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  float px = 1.0 / float(imageW);
+  float py = 1.0 / float(imageH);
+
+  if ((x < imageW) && (y < imageH)) {
+    // take the average of 4 samples
+
+    // we are using the normalized access to make sure non-power-of-two textures
+    // behave well when downsized.
+    float4 color = (tex2D<float4>(mipInput, (x + 0) * px, (y + 0) * py)) +
+                   (tex2D<float4>(mipInput, (x + 1) * px, (y + 0) * py)) +
+                   (tex2D<float4>(mipInput, (x + 1) * px, (y + 1) * py)) +
+                   (tex2D<float4>(mipInput, (x + 0) * px, (y + 1) * py));
+
+    color /= 4.0;
+    color *= 255.0;
+    color = fminf(color, make_float4(255.0));
+
+    surf2Dwrite(to_uchar4(color), mipOutput, x * sizeof(uchar4), y);
+  }
+}
+
+void generateMipMaps(hipMipmappedArray_t mipmapArray, hipExtent size) {
+  size_t width = size.width;
+  size_t height = size.height;
+
+#ifdef SHOW_MIPMAPS
+  hipArray_t levelFirst;
+  HIPCHECK(hipGetMipmappedArrayLevel(&levelFirst, mipmapArray, 0));
+#endif
+
+  uint level = 0;
+
+  while (width != 1 || height != 1) {
+    width /= 2;
+    width = MAX((size_t)1, width);
+    height /= 2;
+    height = MAX((size_t)1, height);
+
+    hipArray_t levelFrom;
+    HIPCHECK(hipGetMipmappedArrayLevel(&levelFrom, mipmapArray, level));
+    hipArray_t levelTo;
+    HIPCHECK(
+        hipGetMipmappedArrayLevel(&levelTo, mipmapArray, level + 1));
+
+    hipExtent levelToSize;
+    HIPCHECK(cudaArrayGetInfo(NULL, &levelToSize, NULL, levelTo));
+    checkHost(levelToSize.width == width);
+    checkHost(levelToSize.height == height);
+    checkHost(levelToSize.depth == 0);
+
+    // generate texture object for reading
+    hipTextureObject_t texInput;
+    hipResourceDesc texRes;
+    memset(&texRes, 0, sizeof(hipResourceDesc));
+
+    texRes.resType = hipResourceTypeArray;
+    texRes.res.array.array = levelFrom;
+
+    hipTextureDesc texDescr;
+    memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+    texDescr.normalizedCoords = 1;
+    texDescr.filterMode = hipFilterModeLinear;
+
+    texDescr.addressMode[0] = hipAddressModeClamp;
+    texDescr.addressMode[1] = hipAddressModeClamp;
+    texDescr.addressMode[2] = hipAddressModeClamp;
+
+    texDescr.readMode = hipReadModeNormalizedFloat;
+
+    HIPCHECK(
+        hipCreateTextureObject(&texInput, &texRes, &texDescr, NULL));
+
+    // generate surface object for writing
+
+    hipSurfaceObject_t surfOutput;
+    hipResourceDesc surfRes;
+    memset(&surfRes, 0, sizeof(hipResourceDesc));
+    surfRes.resType = hipResourceTypeArray;
+    surfRes.res.array.array = levelTo;
+
+    HIPCHECK(hipCreateSurfaceObject(&surfOutput, &surfRes));
+
+    // run mipmap kernel
+    dim3 blockSize(16, 16, 1);
+    dim3 gridSize(((uint)width + blockSize.x - 1) / blockSize.x,
+                  ((uint)height + blockSize.y - 1) / blockSize.y, 1);
+
+    d_mipmap<<<gridSize, blockSize>>>(surfOutput, texInput, (uint)width,
+                                      (uint)height);
+
+    HIPCHECK(hipDeviceSynchronize());
+    HIPCHECK(hipGetLastError());
+
+    HIPCHECK(hipDestroySurfaceObject(surfOutput));
+
+    HIPCHECK(hipDestroyTextureObject(texInput));
+
+#ifdef SHOW_MIPMAPS
+    // we blit the current mipmap back into first level
+    hipMemcpy3DParms copyParams = {0};
+    copyParams.dstArray = levelFirst;
+    copyParams.srcArray = levelTo;
+    copyParams.extent = make_hipExtent(width, height, 1);
+    copyParams.kind = hipMemcpyDeviceToDevice;
+    HIPCHECK(hipMemcpy3D(&copyParams));
+#endif
+
+    level++;
+  }
+}
+
+uint getMipMapLevels(hipExtent size) {
+  size_t sz = MAX(MAX(size.width, size.height), size.depth);
+
+  uint levels = 0;
+
+  while (sz) {
+    sz /= 2;
+    levels++;
+  }
+
+  return levels;
+}
+
+//////////////////////////////////////////////////////////////////////////
+// Initalization
+
+extern "C" void randomizeAtlas() {
+  uint2 *h_data = (uint2 *)atlasImage.h_data;
+
+  // assign random texture object handles to our atlas image tiles
+  for (size_t i = 0; i < atlasImage.size.width * atlasImage.size.height; i++) {
+#ifdef SHOW_MIPMAPS
+    h_data[i] = encodeTextureObject(contentImages[0].textureObject);
+#else
+    h_data[i] = encodeTextureObject(
+        contentImages[rand() % contentImages.size()].textureObject);
+#endif
+  }
+
+  // copy data to atlas array
+  hipMemcpy3DParms copyParams = {0};
+  copyParams.srcPtr = make_hipPitchedPtr(
+      atlasImage.h_data, atlasImage.size.width * sizeof(uint2),
+      atlasImage.size.width, atlasImage.size.height);
+  copyParams.dstArray = atlasImage.dataArray;
+  copyParams.extent = atlasImage.size;
+  copyParams.extent.depth = 1;
+  copyParams.kind = hipMemcpyHostToDevice;
+  HIPCHECK(hipMemcpy3D(&copyParams));
+};
+
+extern "C" void deinitAtlasAndImages() {
+  for (size_t i = 0; i < contentImages.size(); i++) {
+    Image &image = contentImages[i];
+
+    if (image.h_data) {
+      free(image.h_data);
+    }
+
+    if (image.textureObject) {
+      HIPCHECK(hipDestroyTextureObject(image.textureObject));
+    }
+
+    if (image.mipmapArray) {
+      HIPCHECK(hipFreeMipmappedArray(image.mipmapArray));
+    }
+  }
+
+  if (atlasImage.h_data) {
+    free(atlasImage.h_data);
+  }
+
+  if (atlasImage.textureObject) {
+    HIPCHECK(hipDestroyTextureObject(atlasImage.textureObject));
+  }
+
+  if (atlasImage.dataArray) {
+    HIPCHECK(hipFreeArray(atlasImage.dataArray));
+  }
+}
+
+extern "C" void initAtlasAndImages(const Image *images, size_t numImages,
+                                   hipExtent atlasSize) {
+  // create individual textures
+  contentImages.resize(numImages);
+
+  for (size_t i = 0; i < numImages; i++) {
+    Image &image = contentImages[i];
+    image.size = images[i].size;
+    image.size.depth = 0;
+    image.type = hipResourceTypeMipmappedArray;
+
+    // how many mipmaps we need
+    uint levels = getMipMapLevels(image.size);
+    highestLod = MAX(highestLod, (float)levels - 1);
+
+    hipChannelFormatDesc desc = hipCreateChannelDesc<uchar4>();
+    HIPCHECK(hipMallocMipmappedArray(&image.mipmapArray, &desc,
+                                             image.size, levels));
+
+    // upload level 0
+    hipArray_t level0;
+    HIPCHECK(hipGetMipmappedArrayLevel(&level0, image.mipmapArray, 0));
+
+    hipMemcpy3DParms copyParams = {0};
+    copyParams.srcPtr =
+        make_hipPitchedPtr(images[i].h_data, image.size.width * sizeof(uchar4),
+                            image.size.width, image.size.height);
+    copyParams.dstArray = level0;
+    copyParams.extent = image.size;
+    copyParams.extent.depth = 1;
+    copyParams.kind = hipMemcpyHostToDevice;
+    HIPCHECK(hipMemcpy3D(&copyParams));
+
+    // compute rest of mipmaps based on level 0
+    generateMipMaps(image.mipmapArray, image.size);
+
+    // generate bindless texture object
+
+    hipResourceDesc resDescr;
+    memset(&resDescr, 0, sizeof(hipResourceDesc));
+
+    resDescr.resType = hipResourceTypeMipmappedArray;
+    resDescr.res.mipmap.mipmap = image.mipmapArray;
+
+    hipTextureDesc texDescr;
+    memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+    texDescr.normalizedCoords = 1;
+    texDescr.filterMode = hipFilterModeLinear;
+    texDescr.mipmapFilterMode = hipFilterModeLinear;
+
+    texDescr.addressMode[0] = hipAddressModeClamp;
+    texDescr.addressMode[1] = hipAddressModeClamp;
+    texDescr.addressMode[2] = hipAddressModeClamp;
+
+    texDescr.maxMipmapLevelClamp = float(levels - 1);
+
+    texDescr.readMode = hipReadModeNormalizedFloat;
+
+    HIPCHECK(hipCreateTextureObject(&image.textureObject, &resDescr,
+                                            &texDescr, NULL));
+  }
+
+  // create atlas array
+  hipChannelFormatDesc channelDesc = hipCreateChannelDesc<uint2>();
+  HIPCHECK(hipMallocArray(&atlasImage.dataArray, &channelDesc,
+                                  atlasSize.width, atlasSize.height));
+  atlasImage.h_data =
+      malloc(atlasSize.width * atlasSize.height * sizeof(uint2));
+  atlasImage.type = hipResourceTypeArray;
+  atlasImage.size = atlasSize;
+
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = atlasImage.dataArray;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = true;
+  texDescr.filterMode = hipFilterModePoint;
+  texDescr.addressMode[0] = hipAddressModeClamp;
+  texDescr.addressMode[1] = hipAddressModeClamp;
+  texDescr.addressMode[1] = hipAddressModeClamp;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(hipCreateTextureObject(&atlasImage.textureObject, &texRes,
+                                          &texDescr, NULL));
+
+  randomizeAtlas();
+}
+
+#endif  // #ifndef _SIMPLETEXTURE3D_KERNEL_CU_
+mage.textureObject, &texRes,
+                                          &texDescr, NULL));
+
+  randomizeAtlas();
+}
+
+#endif  // #ifndef _SIMPLETEXTURE3D_KERNEL_CU_
diff --git a/src/samples/Samples/3_CUDA_Features/cdpAdvancedQuicksort/cdpAdvancedQuicksort.cu.hip b/src/samples/Samples/3_CUDA_Features/cdpAdvancedQuicksort/cdpAdvancedQuicksort.cu.hip
index e69de29..6a55b0a 100755
--- a/src/samples/Samples/3_CUDA_Features/cdpAdvancedQuicksort/cdpAdvancedQuicksort.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cdpAdvancedQuicksort/cdpAdvancedQuicksort.cu.hip
@@ -0,0 +1,591 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+////////////////////////////////////////////////////////////////////////////////
+//
+//  cdpAdvancedQuicksort.cu
+//
+//  Implementation of a parallel quicksort in CUDA. It comes in
+//  several parts:
+//
+//  1. A small-set insertion sort. We do this on any set with <=32 elements
+//  2. A partitioning kernel, which - given a pivot - separates an input
+//     array into elements <=pivot, and >pivot. Two quicksorts will then
+//     be launched to resolve each of these.
+//  3. A quicksort co-ordinator, which figures out what kernels to launch
+//     and when.
+//
+////////////////////////////////////////////////////////////////////////////////
+#include <thrust/random.h>
+#include <thrust/device_vector.h>
+#include <hip/hip_cooperative_groups.h>
+
+namespace cg = cooperative_groups;
+
+#include "helper_cuda_hipified.h"
+#include <helper_string.h>
+#include "cdpQuicksort.h"
+
+////////////////////////////////////////////////////////////////////////////////
+// Inline PTX call to return index of highest non-zero bit in a word
+////////////////////////////////////////////////////////////////////////////////
+static __device__ __forceinline__ unsigned int __qsflo(unsigned int word) {
+  unsigned int ret;
+  asm volatile("bfind.u32 %0, %1;" : "=r"(ret) : "r"(word));
+  return ret;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//
+//  ringbufAlloc
+//
+//  Allocates from a ringbuffer. Allows for not failing when we run out
+//  of stack for tracking the offset counts for each sort subsection.
+//
+//  We use the atomicMax trick to allow out-of-order retirement. If we
+//  hit the size limit on the ringbuffer, then we spin-wait for people
+//  to complete.
+//
+////////////////////////////////////////////////////////////////////////////////
+template <typename T>
+static __device__ T *ringbufAlloc(qsortRingbuf *ringbuf) {
+  // Wait for there to be space in the ring buffer. We'll retry only a fixed
+  // number of times and then fail, to avoid an out-of-memory deadlock.
+  unsigned int loop = 10000;
+
+  while (((ringbuf->head - ringbuf->tail) >= ringbuf->stacksize) &&
+         (loop-- > 0))
+    ;
+
+  if (loop == 0) return NULL;
+
+  // Note that the element includes a little index book-keeping, for freeing
+  // later.
+  unsigned int index = atomicAdd((unsigned int *)&ringbuf->head, 1);
+  T *ret = (T *)(ringbuf->stackbase) + (index & (ringbuf->stacksize - 1));
+  ret->index = index;
+
+  return ret;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//
+//  ringBufFree
+//
+//  Releases an element from the ring buffer. If every element is released
+//  up to and including this one, we can advance the tail to indicate that
+//  space is now available.
+//
+////////////////////////////////////////////////////////////////////////////////
+template <typename T>
+static __device__ void ringbufFree(qsortRingbuf *ringbuf, T *data) {
+  unsigned int index = data->index;  // Non-wrapped index to free
+  unsigned int count = atomicAdd((unsigned int *)&(ringbuf->count), 1) + 1;
+  unsigned int max = atomicMax((unsigned int *)&(ringbuf->max), index + 1);
+
+  // Update the tail if need be. Note we update "max" to be the new value in
+  // ringbuf->max
+  if (max < (index + 1)) max = index + 1;
+
+  if (max == count) atomicMax((unsigned int *)&(ringbuf->tail), count);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//
+//  qsort_warp
+//
+//  Simplest possible implementation, does a per-warp quicksort with no
+//  inter-warp
+//  communication. This has a high atomic issue rate, but the rest should
+//  actually
+//  be fairly quick because of low work per thread.
+//
+//  A warp finds its section of the data, then writes all data <pivot to one
+//  buffer and all data >pivot to the other. Atomics are used to get a unique
+//  section of the buffer.
+//
+//  Obvious optimisation: do multiple chunks per warp, to increase in-flight
+//  loads
+//  and cover the instruction overhead.
+//
+////////////////////////////////////////////////////////////////////////////////
+__global__ void qsort_warp(unsigned *indata, unsigned *outdata,
+                           unsigned int offset, unsigned int len,
+                           qsortAtomicData *atomicData,
+                           qsortRingbuf *atomicDataStack,
+                           unsigned int source_is_indata, unsigned int depth) {
+  // Handle to thread block group
+  cg::thread_block cta = cg::this_thread_block();
+  // Find my data offset, based on warp ID
+  unsigned int thread_id = threadIdx.x + (blockIdx.x << QSORT_BLOCKSIZE_SHIFT);
+  // unsigned int warp_id = threadIdx.x >> 5;   // Used for debug only
+  unsigned int lane_id = threadIdx.x & (warpSize - 1);
+
+  // Exit if I'm outside the range of sort to be done
+  if (thread_id >= len) return;
+
+  //
+  // First part of the algorithm. Each warp counts the number of elements that
+  // are
+  // greater/less than the pivot.
+  //
+  // When a warp knows its count, it updates an atomic counter.
+  //
+
+  // Read in the data and the pivot. Arbitrary pivot selection for now.
+  unsigned pivot = indata[offset + len / 2];
+  unsigned data = indata[offset + thread_id];
+
+  // Count how many are <= and how many are > pivot.
+  // If all are <= pivot then we adjust the comparison
+  // because otherwise the sort will move nothing and
+  // we'll iterate forever.
+  cg::coalesced_group active = cg::coalesced_threads();
+  unsigned int greater = (data > pivot);
+  unsigned int gt_mask = active.ballot(greater);
+
+  if (gt_mask == 0) {
+    greater = (data >= pivot);
+    gt_mask = active.ballot(greater);  // Must re-ballot for adjusted comparator
+  }
+
+  unsigned int lt_mask = active.ballot(!greater);
+  unsigned int gt_count = __popc(gt_mask);
+  unsigned int lt_count = __popc(lt_mask);
+
+  // Atomically adjust the lt_ and gt_offsets by this amount. Only one thread
+  // need do this. Share the result using shfl
+  unsigned int lt_offset, gt_offset;
+
+  if (lane_id == 0) {
+    if (lt_count > 0)
+      lt_offset = atomicAdd((unsigned int *)&atomicData->lt_offset, lt_count);
+
+    if (gt_count > 0)
+      gt_offset =
+          len - (atomicAdd((unsigned int *)&atomicData->gt_offset, gt_count) +
+                 gt_count);
+  }
+
+  lt_offset =
+      active.shfl((int)lt_offset, 0);  // Everyone pulls the offsets from lane 0
+  gt_offset = active.shfl((int)gt_offset, 0);
+
+  // Now compute my own personal offset within this. I need to know how many
+  // threads with a lane ID less than mine are going to write to the same buffer
+  // as me. We can use popc to implement a single-operation warp scan in this
+  // case.
+  unsigned lane_mask_lt;
+  asm("mov.u32 %0, %%lanemask_lt;" : "=r"(lane_mask_lt));
+  unsigned int my_mask = greater ? gt_mask : lt_mask;
+  unsigned int my_offset = __popc(my_mask & lane_mask_lt);
+
+  // Move data.
+  my_offset += greater ? gt_offset : lt_offset;
+  outdata[offset + my_offset] = data;
+
+  // Count up if we're the last warp in. If so, then Kepler will launch the next
+  // set of sorts directly from here.
+  if (lane_id == 0) {
+    // Count "elements written". If I wrote the last one, then trigger the next
+    // qsorts
+    unsigned int mycount = lt_count + gt_count;
+
+    if (atomicAdd((unsigned int *)&atomicData->sorted_count, mycount) +
+            mycount ==
+        len) {
+      // We're the last warp to do any sorting. Therefore it's up to us to
+      // launch the next stage.
+      unsigned int lt_len = atomicData->lt_offset;
+      unsigned int gt_len = atomicData->gt_offset;
+
+      hipStream_t lstream, rstream;
+      hipStreamCreateWithFlags(&lstream, hipStreamNonBlocking);
+      hipStreamCreateWithFlags(&rstream, hipStreamNonBlocking);
+
+      // Begin by freeing our atomicData storage. It's better for the ringbuffer
+      // algorithm
+      // if we free when we're done, rather than re-using (makes for less
+      // fragmentation).
+      ringbufFree<qsortAtomicData>(atomicDataStack, atomicData);
+
+      // Exceptional case: if "lt_len" is zero, then all values in the batch
+      // are equal. We are then done (may need to copy into correct buffer,
+      // though)
+      if (lt_len == 0) {
+        if (source_is_indata)
+          hipMemcpyAsync(indata + offset, outdata + offset,
+                          gt_len * sizeof(unsigned), hipMemcpyDeviceToDevice,
+                          lstream);
+
+        return;
+      }
+
+      // Start with lower half first
+      if (lt_len > BITONICSORT_LEN) {
+        // If we've exceeded maximum depth, fall through to backup
+        // big_bitonicsort
+        if (depth >= QSORT_MAXDEPTH) {
+          // The final bitonic stage sorts in-place in "outdata". We therefore
+          // re-use "indata" as the out-of-range tracking buffer. For (2^n)+1
+          // elements we need (2^(n+1)) bytes of oor buffer. The backup qsort
+          // buffer is at least this large when sizeof(QTYPE) >= 2.
+          big_bitonicsort<<<1, BITONICSORT_LEN, 0, lstream>>>(
+              outdata, source_is_indata ? indata : outdata, indata, offset,
+              lt_len);
+        } else {
+          // Launch another quicksort. We need to allocate more storage for the
+          // atomic data.
+          if ((atomicData = ringbufAlloc<qsortAtomicData>(atomicDataStack)) ==
+              NULL)
+            printf("Stack-allocation error. Failing left child launch.\n");
+          else {
+            atomicData->lt_offset = atomicData->gt_offset =
+                atomicData->sorted_count = 0;
+            unsigned int numblocks =
+                (unsigned int)(lt_len + (QSORT_BLOCKSIZE - 1)) >>
+                QSORT_BLOCKSIZE_SHIFT;
+            qsort_warp<<<numblocks, QSORT_BLOCKSIZE, 0, lstream>>>(
+                outdata, indata, offset, lt_len, atomicData, atomicDataStack,
+                !source_is_indata, depth + 1);
+          }
+        }
+      } else if (lt_len > 1) {
+        // Final stage uses a bitonic sort instead. It's important to
+        // make sure the final stage ends up in the correct (original) buffer.
+        // We launch the smallest power-of-2 number of threads that we can.
+        unsigned int bitonic_len = 1 << (__qsflo(lt_len - 1U) + 1);
+        bitonicsort<<<1, bitonic_len, 0, lstream>>>(
+            outdata, source_is_indata ? indata : outdata, offset, lt_len);
+      }
+      // Finally, if we sorted just one single element, we must still make
+      // sure that it winds up in the correct place.
+      else if (source_is_indata && (lt_len == 1))
+        indata[offset] = outdata[offset];
+
+      if (hipPeekAtLastError() != hipSuccess)
+        printf("Left-side launch fail: %s\n",
+               hipGetErrorString(hipGetLastError()));
+
+      // Now the upper half.
+      if (gt_len > BITONICSORT_LEN) {
+        // If we've exceeded maximum depth, fall through to backup
+        // big_bitonicsort
+        if (depth >= QSORT_MAXDEPTH)
+          big_bitonicsort<<<1, BITONICSORT_LEN, 0, rstream>>>(
+              outdata, source_is_indata ? indata : outdata, indata,
+              offset + lt_len, gt_len);
+        else {
+          // Allocate new atomic storage for this launch
+          if ((atomicData = ringbufAlloc<qsortAtomicData>(atomicDataStack)) ==
+              NULL)
+            printf("Stack allocation error! Failing right-side launch.\n");
+          else {
+            atomicData->lt_offset = atomicData->gt_offset =
+                atomicData->sorted_count = 0;
+            unsigned int numblocks =
+                (unsigned int)(gt_len + (QSORT_BLOCKSIZE - 1)) >>
+                QSORT_BLOCKSIZE_SHIFT;
+            qsort_warp<<<numblocks, QSORT_BLOCKSIZE, 0, rstream>>>(
+                outdata, indata, offset + lt_len, gt_len, atomicData,
+                atomicDataStack, !source_is_indata, depth + 1);
+          }
+        }
+      } else if (gt_len > 1) {
+        unsigned int bitonic_len = 1 << (__qsflo(gt_len - 1U) + 1);
+        bitonicsort<<<1, bitonic_len, 0, rstream>>>(
+            outdata, source_is_indata ? indata : outdata, offset + lt_len,
+            gt_len);
+      } else if (source_is_indata && (gt_len == 1))
+        indata[offset + lt_len] = outdata[offset + lt_len];
+
+      if (hipPeekAtLastError() != hipSuccess)
+        printf("Right-side launch fail: %s\n",
+               hipGetErrorString(hipGetLastError()));
+    }
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//
+//  run_quicksort
+//
+//  Host-side code to run the Kepler version of quicksort. It's pretty
+//  simple, because all launch control is handled on the device via CDP.
+//
+//  All parallel quicksorts require an equal-sized scratch buffer. This
+//  must be passed in ahead of time.
+//
+//  Returns the time elapsed for the sort.
+//
+////////////////////////////////////////////////////////////////////////////////
+float run_quicksort_cdp(unsigned *gpudata, unsigned *scratchdata,
+                        unsigned int count, hipStream_t stream) {
+  unsigned int stacksize = QSORT_STACK_ELEMS;
+
+  // This is the stack, for atomic tracking of each sort's status
+  qsortAtomicData *gpustack;
+  HIPCHECK(
+      hipMalloc((void **)&gpustack, stacksize * sizeof(qsortAtomicData)));
+  HIPCHECK(hipMemset(
+      gpustack, 0, sizeof(qsortAtomicData)));  // Only need set first entry to 0
+
+  // Create the memory ringbuffer used for handling the stack.
+  // Initialise everything to where it needs to be.
+  qsortRingbuf buf;
+  qsortRingbuf *ringbuf;
+  HIPCHECK(hipMalloc((void **)&ringbuf, sizeof(qsortRingbuf)));
+  buf.head = 1;  // We start with one allocation
+  buf.tail = 0;
+  buf.count = 0;
+  buf.max = 0;
+  buf.stacksize = stacksize;
+  buf.stackbase = gpustack;
+  HIPCHECK(
+      hipMemcpy(ringbuf, &buf, sizeof(buf), hipMemcpyHostToDevice));
+
+  // Timing events...
+  hipEvent_t ev1, ev2;
+  HIPCHECK(hipEventCreate(&ev1));
+  HIPCHECK(hipEventCreate(&ev2));
+  HIPCHECK(hipEventRecord(ev1));
+
+  // Now we trivially launch the qsort kernel
+  if (count > BITONICSORT_LEN) {
+    unsigned int numblocks =
+        (unsigned int)(count + (QSORT_BLOCKSIZE - 1)) >> QSORT_BLOCKSIZE_SHIFT;
+    qsort_warp<<<numblocks, QSORT_BLOCKSIZE, 0, stream>>>(
+        gpudata, scratchdata, 0U, count, gpustack, ringbuf, true, 0);
+  } else {
+    bitonicsort<<<1, BITONICSORT_LEN>>>(gpudata, gpudata, 0, count);
+  }
+
+  HIPCHECK(hipGetLastError());
+  HIPCHECK(hipEventRecord(ev2));
+  HIPCHECK(hipDeviceSynchronize());
+
+  float elapse = 0.0f;
+
+  if (hipPeekAtLastError() != hipSuccess)
+    printf("Launch failure: %s\n", hipGetErrorString(hipGetLastError()));
+  else
+    HIPCHECK(hipEventElapsedTime(&elapse, ev1, ev2));
+
+  // Sanity check that the stack allocator is doing the right thing
+  HIPCHECK(
+      hipMemcpy(&buf, ringbuf, sizeof(*ringbuf), hipMemcpyDeviceToHost));
+
+  if (count > BITONICSORT_LEN && buf.head != buf.tail) {
+    printf("Stack allocation error!\nRingbuf:\n");
+    printf("\t head = %u\n", buf.head);
+    printf("\t tail = %u\n", buf.tail);
+    printf("\tcount = %u\n", buf.count);
+    printf("\t  max = %u\n", buf.max);
+  }
+
+  // Release our stack data once we're done
+  HIPCHECK(hipFree(ringbuf));
+  HIPCHECK(hipFree(gpustack));
+
+  return elapse;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
+int run_qsort(unsigned int size, int seed, int debug, int loop, int verbose) {
+  if (seed > 0) srand(seed);
+
+  // Create and set up our test
+  unsigned *gpudata, *scratchdata;
+  HIPCHECK(hipMalloc((void **)&gpudata, size * sizeof(unsigned)));
+  HIPCHECK(hipMalloc((void **)&scratchdata, size * sizeof(unsigned)));
+
+  // Create CPU data.
+  unsigned *data = new unsigned[size];
+  unsigned int min = loop ? loop : size;
+  unsigned int max = size;
+  loop = (loop == 0) ? 1 : loop;
+
+  for (size = min; size <= max; size += loop) {
+    if (verbose) printf(" Input: ");
+
+    for (unsigned int i = 0; i < size; i++) {
+      // Build data 8 bits at a time
+      data[i] = 0;
+      char *ptr = (char *)&(data[i]);
+
+      for (unsigned j = 0; j < sizeof(unsigned); j++) {
+        // Easy-to-read data in debug mode
+        if (debug) {
+          *ptr++ = (char)(rand() % 10);
+          break;
+        }
+
+        *ptr++ = (char)(rand() & 255);
+      }
+
+      if (verbose) {
+        if (i && !(i % 32)) printf("\n        ");
+
+        printf("%u ", data[i]);
+      }
+    }
+
+    if (verbose) printf("\n");
+
+    HIPCHECK(hipMemcpy(gpudata, data, size * sizeof(unsigned),
+                               hipMemcpyHostToDevice));
+
+    // So we're now populated and ready to go! We size our launch as
+    // blocks of up to BLOCKSIZE threads, and appropriate grid size.
+    // One thread is launched per element.
+    float elapse;
+    elapse = run_quicksort_cdp(gpudata, scratchdata, size, NULL);
+
+    // run_bitonicsort<SORTTYPE>(gpudata, scratchdata, size, verbose);
+    HIPCHECK(hipDeviceSynchronize());
+
+    // Copy back the data and verify correct sort
+    HIPCHECK(hipMemcpy(data, gpudata, size * sizeof(unsigned),
+                               hipMemcpyDeviceToHost));
+
+    if (verbose) {
+      printf("Output: ");
+
+      for (unsigned int i = 0; i < size; i++) {
+        if (i && !(i % 32)) printf("\n        ");
+
+        printf("%u ", data[i]);
+      }
+
+      printf("\n");
+    }
+
+    unsigned int check;
+
+    for (check = 1; check < size; check++) {
+      if (data[check] < data[check - 1]) {
+        printf("FAILED at element: %d\n", check);
+        break;
+      }
+    }
+
+    if (check != size) {
+      printf("    cdpAdvancedQuicksort FAILED\n");
+      exit(EXIT_FAILURE);
+    } else
+      printf("    cdpAdvancedQuicksort PASSED\n");
+
+    // Display the time between event recordings
+    printf("Sorted %u elems in %.3f ms (%.3f Melems/sec)\n", size, elapse,
+           (float)size / (elapse * 1000.0f));
+    fflush(stdout);
+  }
+
+  // Release everything and we're done
+  HIPCHECK(hipFree(scratchdata));
+  HIPCHECK(hipFree(gpudata));
+  delete (data);
+  return 0;
+}
+
+static void usage() {
+  printf(
+      "Syntax: cdpAdvancedQuicksort [-size=<num>] [-seed=<num>] [-debug] "
+      "[-loop-step=<num>] [-verbose]\n");
+  printf(
+      "If loop_step is non-zero, will run from 1->array_len in steps of "
+      "loop_step\n");
+}
+
+// Host side entry
+int main(int argc, char *argv[]) {
+  int size = 1000000;
+  unsigned int seed = 0;
+  int debug = 0;
+  int loop = 0;
+  int verbose = 0;
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "help") ||
+      checkCmdLineFlag(argc, (const char **)argv, "h")) {
+    usage();
+    printf("&&&& cdpAdvancedQuicksort WAIVED\n");
+    exit(EXIT_WAIVED);
+  }
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "size")) {
+    size = getCmdLineArgumentInt(argc, (const char **)argv, "size");
+  }
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "seed")) {
+    seed = getCmdLineArgumentInt(argc, (const char **)argv, "seed");
+  }
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "loop-step")) {
+    loop = getCmdLineArgumentInt(argc, (const char **)argv, "loop-step");
+  }
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "debug")) {
+    debug = 1;
+  }
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "verbose")) {
+    verbose = 1;
+  }
+
+  // Get device properties
+  int cuda_device = findCudaDevice(argc, (const char **)argv);
+  hipDeviceProp_t properties;
+  HIPCHECK(hipGetDeviceProperties(&properties, cuda_device));
+  int cdpCapable =
+      (properties.major == 3 && properties.minor >= 5) || properties.major >= 4;
+
+  printf("GPU device %s has compute capabilities (SM %d.%d)\n", properties.name,
+         properties.major, properties.minor);
+
+  if (!cdpCapable) {
+    printf(
+        "cdpAdvancedQuicksort requires SM 3.5 or higher to use CUDA Dynamic "
+        "Parallelism.  Exiting...\n");
+    exit(EXIT_WAIVED);
+  }
+
+  printf("Running qsort on %d elements with seed %d, on %s\n", size, seed,
+         properties.name);
+
+  run_qsort(size, seed, debug, loop, verbose);
+
+  exit(EXIT_SUCCESS);
+}
+rt on %d elements with seed %d, on %s\n", size, seed,
+         properties.name);
+
+  run_qsort(size, seed, debug, loop, verbose);
+
+  exit(EXIT_SUCCESS);
+}
diff --git a/src/samples/Samples/3_CUDA_Features/cdpAdvancedQuicksort/cdpBitonicSort.cu.hip b/src/samples/Samples/3_CUDA_Features/cdpAdvancedQuicksort/cdpBitonicSort.cu.hip
index e69de29..65e9eb9 100755
--- a/src/samples/Samples/3_CUDA_Features/cdpAdvancedQuicksort/cdpBitonicSort.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cdpAdvancedQuicksort/cdpBitonicSort.cu.hip
@@ -0,0 +1,287 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// This is a basic, recursive bitonic sort taken from
+// http://www.iti.fh-flensburg.de/lang/algorithmen/sortieren/bitonic/oddn.htm
+//
+// The parallel code is based on:
+// http://www.tools-of-computing.com/tc/CS/Sorts/bitonic_sort.htm
+//
+// The multithread code is from me.
+
+#include <stdio.h>
+#include <hip/hip_cooperative_groups.h>
+
+namespace cg = cooperative_groups;
+
+#include "cdpQuicksort.h"
+
+// Inline PTX call to return index of highest non-zero bit in a word
+static __device__ __forceinline__ unsigned int __btflo(unsigned int word) {
+  unsigned int ret;
+  asm volatile("bfind.u32 %0, %1;" : "=r"(ret) : "r"(word));
+  return ret;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//
+//  qcompare
+//
+//  Comparison function. Note difference from libc standard in
+//  that we take by reference, not by pointer. I can't figure
+//  out how to get a template-to-pointer specialisation working.
+//  Perhaps it requires a class?
+//
+////////////////////////////////////////////////////////////////////////////////
+__device__ __forceinline__ int qcompare(unsigned &val1, unsigned &val2) {
+  return (val1 > val2) ? 1 : (val1 == val2) ? 0 : -1;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//
+//  Basic any-N bitonic sort. We sort "len" elements of "indata", starting
+//  from the "offset" elements into the input data array. Note that "outdata"
+//  can safely be the same as "indata" for an in-place sort (we stage through
+//  shared memory).
+//
+//  We handle non-power-of-2 sizes by padding out to the next largest power of
+//  2.
+//  This is the fully-generic version, for sorting arbitrary data which does not
+//  have a clear "maximum" value. We track "invalid" entries in a separate array
+//  to make sure that they always sorts as "max value" elements. A template
+//  parameter "OOR" allows specialisation to optimise away the invalid tracking.
+//
+//  We can do a more specialised version for int/longlong/flat/double, in which
+//  we pad out the array with max-value-of-type elements. That's another
+//  function.
+//
+//  The last step copies from indata -> outdata... the rest is done in-place.
+//  We use shared memory as temporary storage, which puts an upper limit on
+//  how much data we can sort per block.
+//
+////////////////////////////////////////////////////////////////////////////////
+static __device__ __forceinline__ void bitonicsort_kernel(
+    unsigned *indata, unsigned *outdata, unsigned int offset, unsigned int len,
+    cg::thread_block cta) {
+  __shared__ unsigned
+      sortbuf[1024];  // Max of 1024 elements - TODO: make this dynamic
+
+  // First copy data into shared memory.
+  unsigned int inside = (threadIdx.x < len);
+  sortbuf[threadIdx.x] = inside ? indata[threadIdx.x + offset] : 0xffffffffu;
+  cg::sync(cta);
+
+  // Now the sort loops
+  // Here, "k" is the sort level (remember bitonic does a multi-level butterfly
+  // style sort)
+  // and "j" is the partner element in the butterfly.
+  // Two threads each work on one butterfly, because the read/write needs to
+  // happen
+  // simultaneously
+  for (unsigned int k = 2; k <= blockDim.x;
+       k *= 2)  // Butterfly stride increments in powers of 2
+  {
+    for (unsigned int j = k >> 1; j > 0;
+         j >>= 1)  // Strides also in powers of to, up to <k
+    {
+      unsigned int swap_idx =
+          threadIdx.x ^ j;  // Index of element we're compare-and-swapping with
+      unsigned my_elem = sortbuf[threadIdx.x];
+      unsigned swap_elem = sortbuf[swap_idx];
+
+      cg::sync(cta);
+
+      // The k'th bit of my threadid (and hence my sort item ID)
+      // determines if we sort ascending or descending.
+      // However, since threads are reading from the top AND the bottom of
+      // the butterfly, if my ID is > swap_idx, then ascending means mine<swap.
+      // Finally, if either my_elem or swap_elem is out of range, then it
+      // ALWAYS acts like it's the largest number.
+      // Confusing? It saves us two writes though.
+      unsigned int ascend = k * (swap_idx < threadIdx.x);
+      unsigned int descend = k * (swap_idx > threadIdx.x);
+      bool swap = false;
+
+      if ((threadIdx.x & k) == ascend) {
+        if (my_elem > swap_elem) swap = true;
+      }
+
+      if ((threadIdx.x & k) == descend) {
+        if (my_elem < swap_elem) swap = true;
+      }
+
+      // If we had to swap, then write my data to the other element's position.
+      // Don't forget to track out-of-range status too!
+      if (swap) {
+        sortbuf[swap_idx] = my_elem;
+      }
+
+      cg::sync(cta);
+    }
+  }
+
+  // Copy the sorted data from shared memory back to the output buffer
+  if (threadIdx.x < len) outdata[threadIdx.x + offset] = sortbuf[threadIdx.x];
+}
+
+//////////////////////////////////////////////////////////////////////////////////
+//  This is an emergency-CTA sort, which sorts an arbitrary sized chunk
+//  using a single block. Useful for if qsort runs out of nesting depth.
+//
+//  Note that bitonic sort needs enough storage to pad up to the nearest
+//  power of 2. This means that the double-buffer is always large enough
+//  (when combined with the main buffer), but we do not get enough space
+//  to keep OOR information.
+//
+//  This in turn means that this sort does not work with a generic data
+//  type. It must be a directly-comparable (i.e. with max value) type.
+//
+////////////////////////////////////////////////////////////////////////////////
+static __device__ __forceinline__ void big_bitonicsort_kernel(
+    unsigned *indata, unsigned *outdata, unsigned *backbuf, unsigned int offset,
+    unsigned int len, cg::thread_block cta) {
+  unsigned int len2 =
+      1 << (__btflo(len - 1U) + 1);  // Round up len to nearest power-of-2
+
+  if (threadIdx.x >= len2)
+    return;  // Early out for case where more threads launched than there is
+             // data
+
+  // First, set up our unused values to be the max data type.
+  for (unsigned int i = len; i < len2; i += blockDim.x) {
+    unsigned int index = i + threadIdx.x;
+
+    if (index < len2) {
+      // Must split our index between two buffers
+      if (index < len)
+        indata[index + offset] = 0xffffffffu;
+      else
+        backbuf[index + offset - len] = 0xffffffffu;
+    }
+  }
+
+  cg::sync(cta);
+
+  // Now the sort loops
+  // Here, "k" is the sort level (remember bitonic does a multi-level butterfly
+  // style sort)
+  // and "j" is the partner element in the butterfly.
+  // Two threads each work on one butterfly, because the read/write needs to
+  // happen
+  // simultaneously
+  for (unsigned int k = 2; k <= len2;
+       k *= 2)  // Butterfly stride increments in powers of 2
+  {
+    for (unsigned int j = k >> 1; j > 0;
+         j >>= 1)  // Strides also in powers of to, up to <k
+    {
+      for (unsigned int i = 0; i < len2; i += blockDim.x) {
+        unsigned int index = threadIdx.x + i;
+        unsigned int swap_idx =
+            index ^ j;  // Index of element we're compare-and-swapping with
+
+        // Only do the swap for index<swap_idx (avoids collision between other
+        // threads)
+        if (swap_idx > index) {
+          unsigned my_elem, swap_elem;
+
+          if (index < len)
+            my_elem = indata[index + offset];
+          else
+            my_elem = backbuf[index + offset - len];
+
+          if (swap_idx < len)
+            swap_elem = indata[swap_idx + offset];
+          else
+            swap_elem = backbuf[swap_idx + offset - len];
+
+          // The k'th bit of my index (and hence my sort item ID)
+          // determines if we sort ascending or descending.
+          // Also, if either my_elem or swap_elem is out of range, then it
+          // ALWAYS acts like it's the largest number.
+          bool swap = false;
+
+          if ((index & k) == 0) {
+            if (my_elem > swap_elem) swap = true;
+          }
+
+          if ((index & k) == k) {
+            if (my_elem < swap_elem) swap = true;
+          }
+
+          // If we had to swap, then write my data to the other element's
+          // position.
+          if (swap) {
+            if (swap_idx < len)
+              indata[swap_idx + offset] = my_elem;
+            else
+              backbuf[swap_idx + offset - len] = my_elem;
+
+            if (index < len)
+              indata[index + offset] = swap_elem;
+            else
+              backbuf[index + offset - len] = swap_elem;
+          }
+        }
+      }
+
+      cg::sync(cta);  // Only need to sync for each "j" pass
+    }
+  }
+
+  // Copy the sorted data from the input to the output buffer, because we sort
+  // in-place
+  if (outdata != indata) {
+    for (unsigned int i = 0; i < len; i += blockDim.x) {
+      unsigned int index = i + threadIdx.x;
+
+      if (index < len) outdata[index + offset] = indata[index + offset];
+    }
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// KERNELS
+////////////////////////////////////////////////////////////////////////////////
+
+__global__ void bitonicsort(unsigned *indata, unsigned *outdata,
+                            unsigned int offset, unsigned int len) {
+  // Handle to thread block group
+  cg::thread_block cta = cg::this_thread_block();
+  bitonicsort_kernel(indata, outdata, offset, len, cta);
+}
+
+__global__ void big_bitonicsort(unsigned *indata, unsigned *outdata,
+                                unsigned *backbuf, unsigned int offset,
+                                unsigned int len) {
+  // Handle to thread block group
+  cg::thread_block cta = cg::this_thread_block();
+  big_bitonicsort_kernel(indata, outdata, backbuf, offset, len, cta);
+}
+
+////////////////////////////////////////////////////////////////////////////////
diff --git a/src/samples/Samples/3_CUDA_Features/cdpQuadtree/cdpQuadtree.cu.hip b/src/samples/Samples/3_CUDA_Features/cdpQuadtree/cdpQuadtree.cu.hip
index 5906937..d7e9315 100755
--- a/src/samples/Samples/3_CUDA_Features/cdpQuadtree/cdpQuadtree.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cdpQuadtree/cdpQuadtree.cu.hip
@@ -32,7 +32,7 @@
 #include <hip/hip_cooperative_groups.h>
 
 namespace cg = cooperative_groups;
-#include <helper_cuda.h>
+#include "helper_cuda_hipified.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 // A structure of 2D points (structure of arrays).
@@ -654,8 +654,8 @@ bool cdpQuadtree(int warp_size) {
 
   // Allocate memory to store points.
   Points *points;
-  checkCudaErrors(hipMalloc((void **)&points, 2 * sizeof(Points)));
-  checkCudaErrors(hipMemcpy(points, points_init, 2 * sizeof(Points),
+  HIPCHECK(hipMalloc((void **)&points, 2 * sizeof(Points)));
+  HIPCHECK(hipMemcpy(points, points_init, 2 * sizeof(Points),
                              hipMemcpyHostToDevice));
 
   // We could use a close form...
@@ -669,14 +669,11 @@ bool cdpQuadtree(int warp_size) {
   Quadtree_node root;
   root.set_range(0, num_points);
   Quadtree_node *nodes;
-  checkCudaErrors(
+  HIPCHECK(
       hipMalloc((void **)&nodes, max_nodes * sizeof(Quadtree_node)));
-  checkCudaErrors(
+  HIPCHECK(
       hipMemcpy(nodes, &root, sizeof(Quadtree_node), hipMemcpyHostToDevice));
 
-  // We set the recursion limit for CDP to max_depth.
-  hipDeviceSetLimit(cudaLimitDevRuntimeSyncDepth, max_depth);
-
   // Build the quadtree.
   Parameters params(max_depth, min_points_per_node);
   std::cout << "Launching CDP kernel to build the quadtree" << std::endl;
@@ -686,7 +683,7 @@ bool cdpQuadtree(int warp_size) {
   build_quadtree_kernel<
       NUM_THREADS_PER_BLOCK><<<1, NUM_THREADS_PER_BLOCK, smem_size>>>(
       nodes, points, params);
-  checkCudaErrors(hipGetLastError());
+  HIPCHECK(hipGetLastError());
 
   // Copy points to CPU.
   thrust::host_vector<float> x_h(x_d0);
@@ -697,7 +694,7 @@ bool cdpQuadtree(int warp_size) {
 
   // Copy nodes to CPU.
   Quadtree_node *host_nodes = new Quadtree_node[max_nodes];
-  checkCudaErrors(hipMemcpy(host_nodes, nodes,
+  HIPCHECK(hipMemcpy(host_nodes, nodes,
                              max_nodes * sizeof(Quadtree_node),
                              hipMemcpyDeviceToHost));
 
@@ -709,8 +706,8 @@ bool cdpQuadtree(int warp_size) {
   delete[] host_nodes;
 
   // Free memory.
-  checkCudaErrors(hipFree(nodes));
-  checkCudaErrors(hipFree(points));
+  HIPCHECK(hipFree(nodes));
+  HIPCHECK(hipFree(points));
 
   return ok;
 }
@@ -723,7 +720,7 @@ int main(int argc, char **argv) {
   // The test requires an architecture SM35 or greater (CDP capable).
   int cuda_device = findCudaDevice(argc, (const char **)argv);
   hipDeviceProp_t deviceProps;
-  checkCudaErrors(hipGetDeviceProperties(&deviceProps, cuda_device));
+  HIPCHECK(hipGetDeviceProperties(&deviceProps, cuda_device));
   int cdpCapable = (deviceProps.major == 3 && deviceProps.minor >= 5) ||
                    deviceProps.major >= 4;
 
@@ -741,3 +738,7 @@ int main(int argc, char **argv) {
 
   return (ok ? EXIT_SUCCESS : EXIT_FAILURE);
 }
+ops.warpSize);
+
+  return (ok ? EXIT_SUCCESS : EXIT_FAILURE);
+}
diff --git a/src/samples/Samples/3_CUDA_Features/cdpSimplePrint/cdpSimplePrint.cu.hip b/src/samples/Samples/3_CUDA_Features/cdpSimplePrint/cdpSimplePrint.cu.hip
index e69de29..afbc4dd 100755
--- a/src/samples/Samples/3_CUDA_Features/cdpSimplePrint/cdpSimplePrint.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cdpSimplePrint/cdpSimplePrint.cu.hip
@@ -0,0 +1,172 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+//#include "helper_cuda_hipified.h"
+//#include <helper_string.h>
+
+#include <cstdio>
+#include <cstdlib>
+#include <iostream>
+
+////////////////////////////////////////////////////////////////////////////////
+// Variable on the GPU used to generate unique identifiers of blocks.
+////////////////////////////////////////////////////////////////////////////////
+__device__ int g_uids = 0;
+
+////////////////////////////////////////////////////////////////////////////////
+// Print a simple message to signal the block which is currently executing.
+////////////////////////////////////////////////////////////////////////////////
+__device__ void print_info(int depth, int thread, int uid, int parent_uid) {
+  if (threadIdx.x == 0) {
+    if (depth == 0)
+      printf("BLOCK %d launched by the host\n", uid);
+    else {
+      char buffer[32];
+
+      for (int i = 0; i < depth; ++i) {
+        buffer[3 * i + 0] = '|';
+        buffer[3 * i + 1] = ' ';
+        buffer[3 * i + 2] = ' ';
+      }
+
+      buffer[3 * depth] = '\0';
+      printf("%sBLOCK %d launched by thread %d of block %d\n", buffer, uid,
+             thread, parent_uid);
+    }
+  }
+
+  __syncthreads();
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// The kernel using CUDA dynamic parallelism.
+//
+// It generates a unique identifier for each block. Prints the information
+// about that block. Finally, if the 'max_depth' has not been reached, the
+// block launches new blocks directly from the GPU.
+////////////////////////////////////////////////////////////////////////////////
+__global__ void cdp_kernel(int max_depth, int depth, int thread,
+                           int parent_uid) {
+  // We create a unique ID per block. Thread 0 does that and shares the value
+  // with the other threads.
+  __shared__ int s_uid;
+
+  if (threadIdx.x == 0) {
+    s_uid = atomicAdd(&g_uids, 1);
+  }
+
+  __syncthreads();
+
+  // We print the ID of the block and information about its parent.
+  print_info(depth, thread, s_uid, parent_uid);
+
+  // We launch new blocks if we haven't reached the max_depth yet.
+  if (++depth >= max_depth) {
+    return;
+  }
+
+  cdp_kernel<<<gridDim.x, blockDim.x>>>(max_depth, depth, threadIdx.x, s_uid);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Main entry point.
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv) {
+  printf("starting Simple Print (CUDA Dynamic Parallelism)\n");
+
+  // Parse a few command-line arguments.
+  int max_depth = 2;
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "help") ||
+      checkCmdLineFlag(argc, (const char **)argv, "h")) {
+    printf(
+        "Usage: %s depth=<max_depth>\t(where max_depth is a value between 1 "
+        "and 8).\n",
+        argv[0]);
+    exit(EXIT_SUCCESS);
+  }
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "depth")) {
+    max_depth = getCmdLineArgumentInt(argc, (const char **)argv, "depth");
+
+    if (max_depth < 1 || max_depth > 8) {
+      printf("depth parameter has to be between 1 and 8\n");
+      exit(EXIT_FAILURE);
+    }
+  }
+
+  // Find/set the device.
+  int device = -1;
+  hipDeviceProp_t deviceProp;
+  device = findCudaDevice(argc, (const char **)argv);
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, device));
+
+  if (!(deviceProp.major > 3 ||
+        (deviceProp.major == 3 && deviceProp.minor >= 5))) {
+    printf("GPU %d - %s  does not support CUDA Dynamic Parallelism\n Exiting.",
+           device, deviceProp.name);
+    exit(EXIT_WAIVED);
+  }
+
+  // Print a message describing what the sample does.
+  printf(
+      "*********************************************************************"
+      "******\n");
+  printf(
+      "The CPU launches 2 blocks of 2 threads each. On the device each thread "
+      "will\n");
+  printf(
+      "launch 2 blocks of 2 threads each. The GPU we will do that "
+      "recursively\n");
+  printf("until it reaches max_depth=%d\n\n", max_depth);
+  printf("In total 2");
+  int num_blocks = 2, sum = 2;
+
+  for (int i = 1; i < max_depth; ++i) {
+    num_blocks *= 4;
+    printf("+%d", num_blocks);
+    sum += num_blocks;
+  }
+
+  printf("=%d blocks are launched!!! (%d from the GPU)\n", sum, sum - 2);
+  printf(
+      "************************************************************************"
+      "***\n\n");
+
+  // Launch the kernel from the CPU.
+  printf("Launching cdp_kernel() with CUDA Dynamic Parallelism:\n\n");
+  cdp_kernel<<<2, 2>>>(max_depth, 0, 0, -1);
+  HIPCHECK(hipGetLastError());
+
+  // Finalize.
+  HIPCHECK(hipDeviceSynchronize());
+
+  exit(EXIT_SUCCESS);
+}
+xit(EXIT_SUCCESS);
+}
diff --git a/src/samples/Samples/3_CUDA_Features/cdpSimpleQuicksort/cdpSimpleQuicksort.cu.hip b/src/samples/Samples/3_CUDA_Features/cdpSimpleQuicksort/cdpSimpleQuicksort.cu.hip
index e69de29..e11adb9 100755
--- a/src/samples/Samples/3_CUDA_Features/cdpSimpleQuicksort/cdpSimpleQuicksort.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cdpSimpleQuicksort/cdpSimpleQuicksort.cu.hip
@@ -0,0 +1,250 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <iostream>
+#include <cstdio>
+#include "helper_cuda_hipified.h"
+#include <helper_string.h>
+
+#define MAX_DEPTH 16
+#define INSERTION_SORT 32
+
+////////////////////////////////////////////////////////////////////////////////
+// Selection sort used when depth gets too big or the number of elements drops
+// below a threshold.
+////////////////////////////////////////////////////////////////////////////////
+__device__ void selection_sort(unsigned int *data, int left, int right) {
+  for (int i = left; i <= right; ++i) {
+    unsigned min_val = data[i];
+    int min_idx = i;
+
+    // Find the smallest value in the range [left, right].
+    for (int j = i + 1; j <= right; ++j) {
+      unsigned val_j = data[j];
+
+      if (val_j < min_val) {
+        min_idx = j;
+        min_val = val_j;
+      }
+    }
+
+    // Swap the values.
+    if (i != min_idx) {
+      data[min_idx] = data[i];
+      data[i] = min_val;
+    }
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Very basic quicksort algorithm, recursively launching the next level.
+////////////////////////////////////////////////////////////////////////////////
+__global__ void cdp_simple_quicksort(unsigned int *data, int left, int right,
+                                     int depth) {
+  // If we're too deep or there are few elements left, we use an insertion
+  // sort...
+  if (depth >= MAX_DEPTH || right - left <= INSERTION_SORT) {
+    selection_sort(data, left, right);
+    return;
+  }
+
+  unsigned int *lptr = data + left;
+  unsigned int *rptr = data + right;
+  unsigned int pivot = data[(left + right) / 2];
+
+  // Do the partitioning.
+  while (lptr <= rptr) {
+    // Find the next left- and right-hand values to swap
+    unsigned int lval = *lptr;
+    unsigned int rval = *rptr;
+
+    // Move the left pointer as long as the pointed element is smaller than the
+    // pivot.
+    while (lval < pivot) {
+      lptr++;
+      lval = *lptr;
+    }
+
+    // Move the right pointer as long as the pointed element is larger than the
+    // pivot.
+    while (rval > pivot) {
+      rptr--;
+      rval = *rptr;
+    }
+
+    // If the swap points are valid, do the swap!
+    if (lptr <= rptr) {
+      *lptr++ = rval;
+      *rptr-- = lval;
+    }
+  }
+
+  // Now the recursive part
+  int nright = rptr - data;
+  int nleft = lptr - data;
+
+  // Launch a new block to sort the left part.
+  if (left < (rptr - data)) {
+    hipStream_t s;
+    hipStreamCreateWithFlags(&s, hipStreamNonBlocking);
+    cdp_simple_quicksort<<<1, 1, 0, s>>>(data, left, nright, depth + 1);
+    hipStreamDestroy(s);
+  }
+
+  // Launch a new block to sort the right part.
+  if ((lptr - data) < right) {
+    hipStream_t s1;
+    hipStreamCreateWithFlags(&s1, hipStreamNonBlocking);
+    cdp_simple_quicksort<<<1, 1, 0, s1>>>(data, nleft, right, depth + 1);
+    hipStreamDestroy(s1);
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Call the quicksort kernel from the host.
+////////////////////////////////////////////////////////////////////////////////
+void run_qsort(unsigned int *data, unsigned int nitems) {
+  // Prepare CDP for the max depth 'MAX_DEPTH'.
+
+  // Launch on device
+  int left = 0;
+  int right = nitems - 1;
+  std::cout << "Launching kernel on the GPU" << std::endl;
+  cdp_simple_quicksort<<<1, 1>>>(data, left, right, 0);
+  HIPCHECK(hipDeviceSynchronize());
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Initialize data on the host.
+////////////////////////////////////////////////////////////////////////////////
+void initialize_data(unsigned int *dst, unsigned int nitems) {
+  // Fixed seed for illustration
+  srand(2047);
+
+  // Fill dst with random values
+  for (unsigned i = 0; i < nitems; i++) dst[i] = rand() % nitems;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Verify the results.
+////////////////////////////////////////////////////////////////////////////////
+void check_results(int n, unsigned int *results_d) {
+  unsigned int *results_h = new unsigned[n];
+  HIPCHECK(hipMemcpy(results_h, results_d, n * sizeof(unsigned),
+                             hipMemcpyDeviceToHost));
+
+  for (int i = 1; i < n; ++i)
+    if (results_h[i - 1] > results_h[i]) {
+      std::cout << "Invalid item[" << i - 1 << "]: " << results_h[i - 1]
+                << " greater than " << results_h[i] << std::endl;
+      exit(EXIT_FAILURE);
+    }
+
+  std::cout << "OK" << std::endl;
+  delete[] results_h;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Main entry point.
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv) {
+  int num_items = 128;
+  bool verbose = false;
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "help") ||
+      checkCmdLineFlag(argc, (const char **)argv, "h")) {
+    std::cerr << "Usage: " << argv[0]
+              << " num_items=<num_items>\twhere num_items is the number of "
+                 "items to sort"
+              << std::endl;
+    exit(EXIT_SUCCESS);
+  }
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "v")) {
+    verbose = true;
+  }
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "num_items")) {
+    num_items = getCmdLineArgumentInt(argc, (const char **)argv, "num_items");
+
+    if (num_items < 1) {
+      std::cerr << "ERROR: num_items has to be greater than 1" << std::endl;
+      exit(EXIT_FAILURE);
+    }
+  }
+
+  // Find/set device and get device properties
+  int device = -1;
+  hipDeviceProp_t deviceProp;
+  device = findCudaDevice(argc, (const char **)argv);
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, device));
+
+  if (!(deviceProp.major > 3 ||
+        (deviceProp.major == 3 && deviceProp.minor >= 5))) {
+    printf("GPU %d - %s  does not support CUDA Dynamic Parallelism\n Exiting.",
+           device, deviceProp.name);
+    exit(EXIT_WAIVED);
+  }
+
+  // Create input data
+  unsigned int *h_data = 0;
+  unsigned int *d_data = 0;
+
+  // Allocate CPU memory and initialize data.
+  std::cout << "Initializing data:" << std::endl;
+  h_data = (unsigned int *)malloc(num_items * sizeof(unsigned int));
+  initialize_data(h_data, num_items);
+
+  if (verbose) {
+    for (int i = 0; i < num_items; i++)
+      std::cout << "Data [" << i << "]: " << h_data[i] << std::endl;
+  }
+
+  // Allocate GPU memory.
+  HIPCHECK(
+      hipMalloc((void **)&d_data, num_items * sizeof(unsigned int)));
+  HIPCHECK(hipMemcpy(d_data, h_data, num_items * sizeof(unsigned int),
+                             hipMemcpyHostToDevice));
+
+  // Execute
+  std::cout << "Running quicksort on " << num_items << " elements" << std::endl;
+  run_qsort(d_data, num_items);
+
+  // Check result
+  std::cout << "Validating results: ";
+  check_results(num_items, d_data);
+
+  free(h_data);
+  HIPCHECK(hipFree(d_data));
+
+  exit(EXIT_SUCCESS);
+}
+ipFree(d_data));
+
+  exit(EXIT_SUCCESS);
+}
diff --git a/src/samples/Samples/3_CUDA_Features/cudaCompressibleMemory/saxpy.cu.hip b/src/samples/Samples/3_CUDA_Features/cudaCompressibleMemory/saxpy.cu.hip
index e69de29..dddd613 100755
--- a/src/samples/Samples/3_CUDA_Features/cudaCompressibleMemory/saxpy.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cudaCompressibleMemory/saxpy.cu.hip
@@ -0,0 +1,204 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+//
+// This sample uses the compressible memory allocation if device supports it
+// and performs saxpy on it. 
+// Compressible memory may give better performance if the data is amenable to 
+// compression.
+
+#include <stdio.h>
+#include <hip/hip_runtime.h>
+#define CUDA_DRIVER_API
+#include "helper_cuda.h"
+#include "compMalloc.h"
+
+__global__ void saxpy(const float a, const float4 *x, const float4 *y, float4 *z, const size_t n)
+{
+    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < n; i += gridDim.x * blockDim.x)
+    {
+        const float4 x4 = x[i];
+        const float4 y4 = y[i];
+        z[i] = make_float4(a * x4.x + y4.x, a * x4.y + y4.y,
+                            a * x4.z + y4.z, a * x4.w + y4.w);
+    }
+}
+
+__global__ void init(float4 *x, float4 *y, const float val, const size_t n)
+{
+    const float4 val4 = make_float4(val, val, val, val);
+    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < n; i += gridDim.x * blockDim.x)
+    {
+        x[i] = y[i] = val4;
+    }
+}
+
+void launchSaxpy(const float a, float4 *x, float4 *y, float4 *z, const size_t n, const float init_val, const bool compressibleZbuf)
+{
+    hipEvent_t start, stop;
+    float ms;
+    int blockSize;
+    int minGridSize;
+    dim3 threads, blocks; 
+
+    if (!compressibleZbuf)
+    {
+        // We are on config where compressible buffer can only be initialized through hipMemcpy
+        // hence, x & y buffers are allocated as compressible and initialized via hipMemcpy
+        // whereas z buffer is allocated as non-compressible.
+        float4 *h_x = (float4 *) malloc(sizeof(float4) * n);
+        float4 *h_y = (float4 *) malloc(sizeof(float4) * n);
+        for (int i = 0; i < n; i++)
+        {
+            h_x[i].x = h_x[i].y = h_x[i].z = h_x[i].w = init_val;
+            h_y[i].x = h_y[i].y = h_y[i].z = h_y[i].w = init_val;
+        }
+        HIPCHECK(hipMemcpy(x, h_x, sizeof(float4) * n, hipMemcpyHostToDevice));
+        HIPCHECK(hipMemcpy(y, h_y, sizeof(float4) * n, hipMemcpyHostToDevice));
+        free(h_x);
+        free(h_y);
+    }
+    else
+    {
+        HIPCHECK(hipOccupancyMaxPotentialBlockSize(&minGridSize, &blockSize, (void*)init));
+        threads = dim3(blockSize, 1, 1);
+        blocks  = dim3(minGridSize, 1, 1);
+        init<<<blocks, threads>>>(x, y, init_val, n);
+    }
+
+    HIPCHECK(hipOccupancyMaxPotentialBlockSize(&minGridSize, &blockSize, (void*)saxpy));
+    threads = dim3(blockSize, 1, 1);
+    blocks  = dim3(minGridSize, 1, 1);
+
+    HIPCHECK(hipEventCreate(&start));
+    HIPCHECK(hipEventCreate(&stop));
+    HIPCHECK(hipEventRecord(start));
+    saxpy<<<blocks, threads>>>(a, x, y, z, n);
+    HIPCHECK(hipEventRecord(stop));
+    HIPCHECK(hipEventSynchronize(stop));
+    HIPCHECK(hipEventElapsedTime(&ms, start, stop));
+
+    const size_t size = n * sizeof(float4);
+    printf("Running saxpy with %d blocks x %d threads = %.3f ms %.3f TB/s\n", blocks.x, threads.x, ms, (size*3)/ms/1e9);
+}
+
+int main(int argc, char **argv)
+{
+    const size_t n = 10485760;
+
+    if (checkCmdLineFlag(argc, (const char **)argv, "help") ||
+            checkCmdLineFlag(argc, (const char **)argv, "?")) {
+        printf("Usage -device=n (n >= 0 for deviceID)\n");
+        exit(EXIT_SUCCESS);
+    }
+
+    findCudaDevice(argc, (const char**)argv);
+    hipDevice_t currentDevice;
+    HIPCHECK(hipCtxGetDevice(&currentDevice));
+
+    // Check that the selected device supports virtual memory management
+    int vmm_supported = -1;
+    HIPCHECK(hipDeviceGetAttribute(&vmm_supported,
+                          CU_DEVICE_ATTRIBUTE_VIRTUAL_ADDRESS_MANAGEMENT_SUPPORTED,
+                          currentDevice));
+    if (vmm_supported == 0) {
+        printf("Device %d doesn't support Virtual Memory Management, waiving the execution.\n", currentDevice);
+        exit(EXIT_WAIVED);
+    }
+
+    int isCompressionAvailable;
+    HIPCHECK(hipDeviceGetAttribute(&isCompressionAvailable,
+                             CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED,
+                             currentDevice));
+    if (isCompressionAvailable == 0)
+    {
+        printf("Device %d doesn't support Generic memory compression, waiving the execution.\n", currentDevice);
+        exit(EXIT_WAIVED);
+    }
+
+    printf("Generic memory compression support is available\n");
+
+    int major, minor;
+    HIPCHECK(hipDeviceGetAttribute(&major,
+                          hipDeviceAttributeComputeCapabilityMajor,
+                          currentDevice));
+    HIPCHECK(hipDeviceGetAttribute(&minor,
+                          hipDeviceAttributeComputeCapabilityMinor,
+                          currentDevice));
+    float4 *x, *y, *z;
+    const size_t size = n * sizeof(float4);
+
+    // Allocating compressible memory
+    HIPCHECK(allocateCompressible((void **)&x, size, true));
+    HIPCHECK(allocateCompressible((void **)&y, size, true));
+    bool compressibleZbuf = 0;
+    if ((major == 8 && minor == 0) || (major == 8 && minor == 6))
+    {
+        // On SM 8.0 and 8.6 GPUs compressible buffer can only be initialized 
+        // through hipMemcpy.
+        printf("allocating non-compressible Z buffer\n");
+        HIPCHECK(allocateCompressible((void **)&z, size, false));
+        compressibleZbuf = 0;
+    }
+    else
+    {
+        HIPCHECK(allocateCompressible((void **)&z, size, true));
+        compressibleZbuf = 1;
+    }
+
+    printf("Running saxpy on %zu bytes of Compressible memory\n", size);
+
+    const float a = 1.0f;
+    const float init_val = 1.0f;
+    launchSaxpy(a, x, y, z, n, init_val, compressibleZbuf);
+ 
+    HIPCHECK(freeCompressible(x, size, true));
+    HIPCHECK(freeCompressible(y, size, true));
+    HIPCHECK(freeCompressible(z, size, true));
+
+    printf("Running saxpy on %zu bytes of Non-Compressible memory\n", size);
+    // Allocating non-compressible memory
+    HIPCHECK(allocateCompressible((void **)&x, size, false));
+    HIPCHECK(allocateCompressible((void **)&y, size, false));
+    HIPCHECK(allocateCompressible((void **)&z, size, false));
+
+    launchSaxpy(a, x, y, z, n, init_val, compressibleZbuf);
+
+    HIPCHECK(freeCompressible(x, size, false));
+    HIPCHECK(freeCompressible(y, size, false));
+    HIPCHECK(freeCompressible(z, size, false));
+
+    printf("\nNOTE: The CUDA Samples are not meant for performance measurements. "
+      "Results may vary when GPU Boost is enabled.\n");
+    return EXIT_SUCCESS;
+}ompressible(z, size, false));
+
+    printf("\nNOTE: The CUDA Samples are not meant for performance measurements. "
+      "Results may vary when GPU Boost is enabled.\n");
+    return EXIT_SUCCESS;
+}
\ No newline at end of file
diff --git a/src/samples/Samples/3_CUDA_Features/cudaTensorCoreGemm/cudaTensorCoreGemm.cu.hip b/src/samples/Samples/3_CUDA_Features/cudaTensorCoreGemm/cudaTensorCoreGemm.cu.hip
index e69de29..69ee2f6 100755
--- a/src/samples/Samples/3_CUDA_Features/cudaTensorCoreGemm/cudaTensorCoreGemm.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cudaTensorCoreGemm/cudaTensorCoreGemm.cu.hip
@@ -0,0 +1,655 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// CUDA sample demonstrating a GEMM computation using the Warp Matrix Multiply
+// and Accumulate API introduced in CUDA 9.
+
+// In this program, the compute_gemm kernel computes the result of a matrix
+// multiplication and addition: D = alpha * A * B + beta * C. The dimensions of
+// both C and D matrices are M_GLOBAL x N_GLOBAL. The A matrix is M_GLOBAL x
+// K_GLOBAL (row-major), the B matrix is K_GLOBAL x N_GLOBAL (column-major). In
+// that kernel, each CTA computes one 128 x 128 tile of the resulting matrix per
+// iteration. When the tile is computed, the CTA stores it to the global memory
+// and begins a new iteration, selecting a new 128 x 128 tile to compute.
+// Each CTA consists of eight warps. For the 128 x 128 tile, each warp computes
+// eight 16 x 16 subtiles, organized in a 2 x 4 two-dimensional array. Warps
+// compute the 16 x 16 subtiles using nvcuda::wmma::mma_sync operations by
+// moving through the K_GLOBAL dimension of the A and B matrices and
+// accumulating the intermediate result in the local thread state.
+
+// There are a number of simple optimizations used in the algorithm:
+// - The CTA copies the 128 x 128 tile of the C matrix from the global memory to
+//   shared memory. After that is done, each warp loads the C matrix fragments
+//   from shared memory, thus avoiding a random global memory access.
+// - On each internal iteration, the CTA copies a portion of the A and B
+//   matrices from global memory to shared memory. After that, all warps in the
+//   CTA reuse the A and B data from shared memory, thus reducing the number of
+//   data copies from global memory.
+// - The portions of the A and B matrices are stored in shared memory with an
+//   additional padding (skew) to reduce the number of shared memory access bank
+//   conflicts.
+//   (See a detailed explanation near the SKEW_HALF macro definition.)
+// - When the CTA finishes computing the tiles of the resulting matrix, each
+//   warp stores its subtiles to shared memory. The CTA then copies the shared
+//   memory contents to global memory, again avoiding redundant random global
+//   memory  accesses.
+// - Note that the CTA tile size is chosen to maximize the GPU register
+//   utilization, but carefully enough to avoid local memory use.
+
+#include <assert.h>
+#include <hip/hip_runtime.h>
+#include <mma.h>
+#include <stdio.h>
+
+// helper functions and utilities to work with CUDA
+#include "helper_cuda_hipified.h"
+#include "helper_functions.h"
+
+// Externally configurable parameters.
+
+#ifndef CPU_DEBUG
+// Set this to 1 to verify the correctness of the GPU-computed matrix.
+#define CPU_DEBUG 0
+#endif
+
+#ifndef SHARED_MEMORY_LIMIT_64K
+// Set this to 0 to use more than 64 Kb of shared memory to cache data, to
+// improve the performance of the computations on GPU.
+// Note that you need a GPU that can have more than 64 Kb of shared memory
+// per multiprocessor.
+#define SHARED_MEMORY_LIMIT_64K 1
+#endif
+
+// GPU configuration.
+
+#define WARP_SIZE 32
+
+// MMA matrix tile dimensions.
+
+#define M 16
+#define N 16
+#define K 16
+
+#define WMMA_M 16
+#define WMMA_N 16
+#define WMMA_K 16
+
+// GEMM configuration.
+
+#define M_TILES 256
+#define N_TILES 256
+#define K_TILES 256
+
+#define M_GLOBAL (M * M_TILES)
+#define N_GLOBAL (N * N_TILES)
+#define K_GLOBAL (K * K_TILES)
+
+#define C_LAYOUT wmma::mem_row_major
+
+// Implementation constants.
+
+#define WARPS_PER_BLOCK 8
+#define THREADS_PER_BLOCK (WARP_SIZE * WARPS_PER_BLOCK)
+
+#if SHARED_MEMORY_LIMIT_64K
+// With only 64 Kb shared memory available, we can fit two 8-tile chunks of
+// the A and B matrix data, that are 16 * 16 * 8 * 8 * 2 = 32 Kb each
+// (i.e. two 8x8 arrays of tiles of 16x16 half-typed elements per CTA).
+// But we cannot account the 8 Kb total skew overhead, without which the
+// performance would be severely impacted. So we choose to reduce the chunk size
+// in half, i.e. the amount of A and B matrix data we cache in shared memory.
+// Accordingly, this doubles the number of outer iterations across the global K
+// dimension, which only slightly impacts the performance.
+#define CHUNK_K 4
+#else
+#define CHUNK_K 8
+#endif
+
+#define CHUNK_LINE_BYTES (CHUNK_K * K * sizeof(half))
+#define WARP_COPY_BYTES (WARP_SIZE * sizeof(int4))
+#define CHUNK_COPY_LINES_PER_WARP (WARP_COPY_BYTES / CHUNK_LINE_BYTES)
+#define CHUNK_COPY_LINE_LANES (WARP_SIZE / CHUNK_COPY_LINES_PER_WARP)
+
+#define BLOCK_ROW_WARPS 2
+#define BLOCK_COL_WARPS 4
+
+#define WARP_ROW_TILES 4
+#define WARP_COL_TILES 2
+
+#define BLOCK_ROW_TILES (WARP_ROW_TILES * BLOCK_ROW_WARPS)
+#define BLOCK_COL_TILES (WARP_COL_TILES * BLOCK_COL_WARPS)
+
+#define GLOBAL_MEM_STRIDE N_GLOBAL
+
+#define SHMEM_STRIDE (N * BLOCK_ROW_TILES)
+#define SHMEM_OFFSET (N * WARP_ROW_TILES)
+
+// The macro below is used to shift rows of the A matrix and columns of the B matrix
+// in shared memory to minimize possible bank conflicts.
+// Before performing the nvcuda::wmma::mma_sync operation, the warp must load the matrix
+// data using the nvcuda::wmma::load_matrix_sync operation. Although the memory access pattern
+// is not specified for that function, each lane in the warp can read one or multiple matrix
+// elements from different matrix rows or columns.
+// For shared memory, such access can result in bank conflicts if different rows / columns
+// of the matrix map to the same bank. By shifting each row and column by a few bytes, we
+// make sure that they map to different banks, thus reducing the number of possible bank
+// conflicts.
+// The number of 16 two-byte "half" elements is chosen as the minimum possible shift because
+// we must keep each row and column 256-bit aligned, as required by nvcuda::wmma::load_matrix_sync.
+#define SKEW_HALF 16
+
+#define checkKernelErrors(expr)                             \
+  do {                                                      \
+    expr;                                                   \
+                                                            \
+    hipError_t __err = hipGetLastError();                 \
+    if (__err != hipSuccess) {                             \
+      printf("Line %d: '%s' failed: %s\n", __LINE__, #expr, \
+             hipGetErrorString(__err));                    \
+      abort();                                              \
+    }                                                       \
+  } while (0)
+
+using namespace nvcuda;
+
+__host__ void init_host_matrices(half *a, half *b, float *c) {
+  for (int i = 0; i < M_GLOBAL; i++) {
+    for (int j = 0; j < K_GLOBAL; j++) {
+      a[i * K_GLOBAL + j] = (half)(rand() % 3);
+    }
+  }
+
+  for (int i = 0; i < N_GLOBAL; i++) {
+    for (int j = 0; j < K_GLOBAL; j++) {
+      b[i * K_GLOBAL + j] = (half)(rand() % 3);
+    }
+  }
+
+  for (int t = 0; t < M_GLOBAL * N_GLOBAL; t++) {
+    c[t] = static_cast<float>(rand() % 3);
+  }
+}
+
+__global__ void compute_gemm(const half *A, const half *B, const float *C,
+                             float *D, float alpha, float beta) {
+  extern __shared__ half shmem[][CHUNK_K * K + SKEW_HALF];
+
+  // Warp and lane identification.
+  const unsigned int warpId = threadIdx.x / WARP_SIZE;
+  const unsigned int laneId = threadIdx.x % WARP_SIZE;
+
+  // Offset in shared memory from which the B matrix is stored.
+  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;
+
+  // This pointer is used to access the C and D matrix tiles this warp computes.
+  float *shmem_warp_tile_ptr = (float *)&shmem[0][0] +
+                               (warpId / 2) * SHMEM_STRIDE * K * 2 +
+                               (warpId % 2) * SHMEM_OFFSET;
+
+  // This pointer is used to stream the C and D matrices block-wide tile to and
+  // from shared memory.
+  float *shmem_warp_stream_ptr =
+      (float *)&shmem[0][0] + warpId * SHMEM_STRIDE * K;
+
+  // Adjust the beta scaler, as it'll be multiplied by alpha at the end of
+  // each tile computation. Technically this is not generally correct (may
+  // result in a loss of precision). Zero still needs to be specially handled
+  // though.
+  beta /= alpha;
+
+  // Each CTA slides along the 128 x 128 tiles from the top left corner of the
+  // matrix to the right and down, and selects the next tile to compute. Once
+  // there's no such tile, all warps in this CTA exit.
+  for (unsigned int block_pos = blockIdx.x;; block_pos += gridDim.x) {
+    const unsigned int block_tile_i =
+        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);
+    const unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % N_TILES;
+
+    // Stop when there are no more D matrix tiles to compute in this CTA.
+    if (block_tile_i >= M_TILES) {
+      break;
+    }
+
+    // This warp's pointer to the C matrix data to copy memory from to shared
+    // memory.
+    const size_t gmem_idx =
+        (block_tile_i + warpId) * M * GLOBAL_MEM_STRIDE + block_tile_j * N;
+    const float *src_gmem_warp_stream_ptr = &C[gmem_idx];
+
+    // Stream multiple C tiles to shared memory.
+#pragma unroll
+    for (int i = 0; i < K; i++) {
+      typedef int4 copy_t;
+
+      *((copy_t *)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId) =
+          *((copy_t *)(src_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) +
+            laneId);
+    }
+
+    __syncthreads();
+
+    // These fragments will accumulate the result of A and B matrix fragment
+    // multiplications along the K_GLOBAL dimension.
+    wmma::fragment<wmma::accumulator, M, N, K, float> c[WARP_COL_TILES]
+                                                       [WARP_ROW_TILES];
+
+    // Load the C matrix tiles into fragments from shared memory.
+#pragma unroll
+    for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+      for (int j = 0; j < WARP_ROW_TILES; j++) {
+        const float *tile_ptr =
+            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;
+
+        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, C_LAYOUT);
+      }
+    }
+
+    __syncthreads();
+
+    // Scale the C matrix.
+#pragma unroll
+    for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+      for (int j = 0; j < WARP_ROW_TILES; j++) {
+#pragma unroll
+        for (int t = 0; t < c[i][j].num_elements; t++) {
+          c[i][j].x[t] *= beta;
+        }
+      }
+    }
+
+    // Select what warp copies what matrix to shared memory.
+    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.
+    const half *warp_ptr = (warpId < 4) ? (&A[block_tile_i * M * K_GLOBAL] +
+                                           M * K_GLOBAL * (warpId % 4) * 2)
+                                        : (&B[block_tile_j * N * K_GLOBAL] +
+                                           N * K_GLOBAL * (warpId % 4) * 2);
+
+    // Go through the global K dimension by a fixed step at a time.
+#pragma unroll
+    for (int tile_k = 0; tile_k < K_TILES; tile_k += CHUNK_K) {
+      // Copy slices of the A and B matrices to shared memory.
+      // The first half of the warps in the CTA copy the A matrix, the rest copy
+      // the B matrix.
+      size_t shmem_idx =
+          warpId < (WARPS_PER_BLOCK / 2)
+              ? (M * (warpId % (WARPS_PER_BLOCK / 2)) * 2)
+              : (N * (warpId % (WARPS_PER_BLOCK / 2)) * 2 + shmem_idx_b_off);
+
+      // First half of the warp copies the first row / column of the matrix,
+      // the second half of the warp copies the next.
+      int4 *lane_ptr = (int4 *)(warp_ptr + tile_k * K +
+                                (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +
+                       (laneId % CHUNK_COPY_LINE_LANES);
+
+      // Shift the second half of the warp to the next row / column in the
+      // shared memory.
+      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;
+
+#pragma unroll
+      for (int i = 0; i < ((WARP_SIZE / 2) / CHUNK_COPY_LINES_PER_WARP) * 2;
+           i++) {
+        // Copy 16 bytes at once in each lane.
+        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =
+            *lane_ptr;
+
+        // Advance the global memory pointer and the shared memory index.
+        lane_ptr =
+            (int4 *)((half *)lane_ptr + K_GLOBAL * CHUNK_COPY_LINES_PER_WARP);
+        shmem_idx += CHUNK_COPY_LINES_PER_WARP;
+      }
+
+      __syncthreads();
+
+      // Compute a grid of C matrix tiles in each warp.
+#pragma unroll
+      for (int k_step = 0; k_step < CHUNK_K; k_step++) {
+        wmma::fragment<wmma::matrix_a, M, N, K, half, wmma::row_major>
+            a[WARP_COL_TILES];
+        wmma::fragment<wmma::matrix_b, M, N, K, half, wmma::col_major>
+            b[WARP_ROW_TILES];
+
+#pragma unroll
+        for (int i = 0; i < WARP_COL_TILES; i++) {
+          size_t shmem_idx_a = (warpId / 2) * M * 2 + (i * M);
+          const half *tile_ptr = &shmem[shmem_idx_a][k_step * K];
+
+          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_HALF);
+
+#pragma unroll
+          for (int j = 0; j < WARP_ROW_TILES; j++) {
+            if (i == 0) {
+              // Load the B matrix fragment once, because it is going to be
+              // reused against the other A matrix fragments.
+              size_t shmem_idx_b = shmem_idx_b_off +
+                                   (WARP_ROW_TILES * N) * (warpId % 2) +
+                                   (j * N);
+              const half *tile_ptr = &shmem[shmem_idx_b][k_step * K];
+
+              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_HALF);
+            }
+
+            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);
+          }
+        }
+      }
+
+      __syncthreads();
+    }
+
+      // Store the D fragments to shared memory.
+#pragma unroll
+    for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+      for (int j = 0; j < WARP_ROW_TILES; j++) {
+#pragma unroll
+        // Uniform, point-wise transformations of ALL fragment elements by ALL
+        // threads in the warp are well-defined even though element indices
+        // within fragment storage are not defined.
+        for (int t = 0; t < c[i][j].num_elements; t++) c[i][j].x[t] *= alpha;
+
+        float *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;
+
+        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);
+      }
+    }
+
+    __syncthreads();
+
+    // Now that shared memory contains all the D tiles, stream them to global
+    // memory.
+    float *dst_gmem_warp_stream_ptr = &D[gmem_idx];
+
+#pragma unroll
+    for (int i = 0; i < K; i++) {
+      *((int4 *)(dst_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) + laneId) =
+          *((int4 *)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId);
+    }
+
+    __syncthreads();
+  }
+}
+
+// Performs an MxNxK GEMM (C=alpha*A*B + beta*C) assuming:
+//  1) Matrices are packed in memory.
+//  2) M, N and K are multiples of 16.
+//  3) Neither A nor B are transposed.
+// Note: This is a less performant version of the compute_gemm kernel. It is
+// designed for
+//       demonstration purposes only to show the CUDA WMMA API use without
+//       relying on availability of the shared memory.
+__global__ void simple_wmma_gemm(half *a, half *b, float *c, float *d, int m_ld,
+                                 int n_ld, int k_ld, float alpha, float beta) {
+  // Leading dimensions. Packed with no transpositions.
+  int lda = k_ld;
+  int ldb = k_ld;
+  int ldc = n_ld;
+
+  // Tile using a 2D grid
+  int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / warpSize;
+  int warpN = (blockIdx.y * blockDim.y + threadIdx.y);
+
+  // Declare the fragments
+  wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major>
+      a_frag;
+  wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major>
+      b_frag;
+  wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> acc_frag;
+  wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> c_frag;
+
+  wmma::fill_fragment(acc_frag, 0.0f);
+
+  // Loop over k
+  for (int i = 0; i < k_ld; i += WMMA_K) {
+    int aCol = i;
+    int aRow = warpM * WMMA_M;
+    int bCol = warpN * N;
+    int bRow = i;
+
+    // Bounds checking
+    if (aRow < m_ld && aCol < k_ld && bRow < k_ld && bCol < n_ld) {
+      // Load the inputs
+      wmma::load_matrix_sync(a_frag, a + aCol + aRow * lda, lda);
+      wmma::load_matrix_sync(b_frag, b + bRow + bCol * ldb, ldb);
+
+      // Perform the matrix multiplication
+      wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag);
+    }
+  }
+
+  // Load in the current value of c, scale it by beta, and add this our result
+  // scaled by alpha
+  int cCol = warpN * WMMA_N;
+  int cRow = warpM * WMMA_M;
+
+  if (cRow < m_ld && cCol < n_ld) {
+    wmma::load_matrix_sync(c_frag, c + cCol + cRow * ldc, ldc,
+                           wmma::mem_row_major);
+
+    for (int i = 0; i < c_frag.num_elements; i++) {
+      c_frag.x[i] = alpha * acc_frag.x[i] + beta * c_frag.x[i];
+    }
+
+    // Store the output
+    wmma::store_matrix_sync(d + cCol + cRow * ldc, c_frag, ldc,
+                            wmma::mem_row_major);
+  }
+}
+
+__host__ void matMultiplyOnHost(half *A, half *B, float *C, float alpha,
+                                float beta, int numARows, int numAColumns,
+                                int numBRows, int numBColumns, int numCRows,
+                                int numCColumns) {
+  for (int i = 0; i < numCRows; i++) {
+    for (int j = 0; j < numCColumns; j++) {
+      float temp = 0.0;
+
+      for (int k = 0; k < numAColumns; k++) {
+        temp += (float)A[i * numAColumns + k] * (float)B[j * numBRows + k];
+      }
+
+      C[i * numCColumns + j] = temp * alpha + beta * C[i * numCColumns + j];
+    }
+  }
+}
+
+int main(int argc, char **argv) {
+  printf("Initializing...\n");
+
+  int dev = findCudaDevice(argc, (const char **)argv);
+
+  hipDeviceProp_t deviceProp;
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, dev));
+
+  // Tensor cores require a GPU of Volta (SM7X) architecture or higher.
+  if (deviceProp.major < 7) {
+    printf(
+        "cudaTensorCoreGemm requires SM 7.0 or higher to use Tensor "
+        "Cores.  Exiting...\n");
+    exit(EXIT_WAIVED);
+  }
+
+  printf("M: %d (%d x %d)\n", M_GLOBAL, M, M_TILES);
+  printf("N: %d (%d x %d)\n", N_GLOBAL, N, N_TILES);
+  printf("K: %d (%d x %d)\n", K_GLOBAL, K, K_TILES);
+
+  half *A_h = NULL;
+  half *B_h = NULL;
+  float *C_h = NULL;
+#if CPU_DEBUG
+  float *result_hD = NULL;
+  float *result_host = NULL;
+#endif
+
+  A_h = (half *)malloc(sizeof(half) * M_GLOBAL * K_GLOBAL);
+  B_h = (half *)malloc(sizeof(half) * K_GLOBAL * N_GLOBAL);
+  C_h = (float *)malloc(sizeof(float) * M_GLOBAL * N_GLOBAL);
+#if CPU_DEBUG
+  result_hD = (float *)malloc(sizeof(float) * M_GLOBAL * N_GLOBAL);
+  result_host = (float *)malloc(sizeof(float) * M_GLOBAL * N_GLOBAL);
+#endif
+
+  half *A = NULL;
+  half *B = NULL;
+  float *C = NULL;
+  float *D = NULL;
+
+  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&A),
+                             sizeof(half) * M_GLOBAL * K_GLOBAL));
+  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&B),
+                             sizeof(half) * N_GLOBAL * K_GLOBAL));
+  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&C),
+                             sizeof(float) * M_GLOBAL * N_GLOBAL));
+  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&D),
+                             sizeof(float) * M_GLOBAL * N_GLOBAL));
+
+  assert(((unsigned long long)A) % 128 == 0);
+  assert(((unsigned long long)B) % 128 == 0);
+  assert(((unsigned long long)C) % 128 == 0);
+  assert(((unsigned long long)D) % 128 == 0);
+
+  init_host_matrices(A_h, B_h, C_h);
+
+  printf("Preparing data for GPU...\n");
+
+  HIPCHECK(hipMemcpy(A, A_h, sizeof(half) * M_GLOBAL * K_GLOBAL,
+                             hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(B, B_h, sizeof(half) * N_GLOBAL * K_GLOBAL,
+                             hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(C, C_h, sizeof(float) * M_GLOBAL * N_GLOBAL,
+                             hipMemcpyHostToDevice));
+  HIPCHECK(hipMemset(D, 0, sizeof(float) * M_GLOBAL * N_GLOBAL));
+
+  enum {
+    // Compute the right amount of shared memory to request.
+    // We need shared memory to hold per-CTA C and D matrix tiles, and to cache
+    // per-CTA chunks
+    // of the A and B matrices. Therefore, the right amount to request is the
+    // maximum of those
+    // two numbers.
+    SHMEM_SZ = MAX(
+        sizeof(half) * (BLOCK_COL_TILES * M) * (CHUNK_K * K + SKEW_HALF) * 2,
+        M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *
+            (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(float))
+  };
+
+  printf("Required shared memory size: %lu Kb\n", SHMEM_SZ / 1024UL);
+
+  const float alpha = 1.1f;
+  const float beta = 1.2f;
+
+  hipEvent_t start, stop;
+
+  HIPCHECK(hipEventCreate(&start));
+  HIPCHECK(hipEventCreate(&stop));
+  HIPCHECK(hipEventRecord(start));
+
+  // If enough shared memory available on the GPU use high performant kernel
+  if (deviceProp.sharedMemPerMultiprocessor >= SHMEM_SZ) {
+    printf("Computing... using high performance kernel compute_gemm \n");
+
+    HIPCHECK(hipFuncSetAttribute(
+        compute_gemm, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
+    checkKernelErrors(
+        (compute_gemm<<<deviceProp.multiProcessorCount, THREADS_PER_BLOCK,
+                        SHMEM_SZ>>>(A, B, C, D, alpha, beta)));
+#if CPU_DEBUG
+    HIPCHECK(hipMemcpy(result_hD, D,
+                               sizeof(float) * M_GLOBAL * N_GLOBAL,
+                               hipMemcpyDeviceToHost));
+#endif
+  } else {
+    dim3 gridDim;
+    dim3 blockDim;
+
+    // blockDim.x must be a multple of warpSize
+    // 128x4 means we have 16 warps and a block computes a 64x64 output tile
+    blockDim.x = 128;
+    blockDim.y = 4;
+
+    gridDim.x = (M_GLOBAL + (WMMA_M * blockDim.x / 32 - 1)) /
+                (WMMA_M * blockDim.x / 32);
+    gridDim.y = (N_GLOBAL + WMMA_N * blockDim.y - 1) / (WMMA_N * blockDim.y);
+
+    printf("Computing... using simple_wmma_gemm kernel\n");
+    simple_wmma_gemm<<<gridDim, blockDim>>>(A, B, C, D, M_GLOBAL, N_GLOBAL,
+                                            K_GLOBAL, alpha, beta);
+#if CPU_DEBUG
+    HIPCHECK(hipMemcpy(result_hD, D,
+                               sizeof(float) * M_GLOBAL * N_GLOBAL,
+                               hipMemcpyDeviceToHost));
+#endif
+  }
+
+  HIPCHECK(hipEventRecord(stop));
+  HIPCHECK(hipEventSynchronize(stop));
+
+#if CPU_DEBUG
+  printf("Verifying correctness of the computations...\n");
+
+  memcpy(result_host, C_h, sizeof(float) * M_GLOBAL * N_GLOBAL);
+
+  matMultiplyOnHost(A_h, B_h, result_host, alpha, beta, M_GLOBAL, K_GLOBAL,
+                    K_GLOBAL, N_GLOBAL, M_GLOBAL, N_GLOBAL);
+
+  for (int i = 0; i < N_GLOBAL * M_GLOBAL; i++) {
+    if (fabs(result_hD[i] - result_host[i]) > 0.1f)
+      printf("mismatch i=%d result_hD=%f result_host=%f\n", i, result_hD[i],
+             result_host[i]);
+  }
+  free(result_hD);
+  free(result_host);
+#endif
+
+  float milliseconds = 0;
+
+  HIPCHECK(hipEventElapsedTime(&milliseconds, start, stop));
+
+  printf("Time: %f ms\n", milliseconds);
+  printf("TFLOPS: %.2f\n", static_cast<double>((static_cast<double>(M_GLOBAL) *
+                                                N_GLOBAL * K_GLOBAL * 2) /
+                                               (milliseconds / 1000.)) /
+                               1e12);
+
+  free(A_h);
+  free(B_h);
+  free(C_h);
+  HIPCHECK(hipFree(reinterpret_cast<void *>(A)));
+  HIPCHECK(hipFree(reinterpret_cast<void *>(B)));
+  HIPCHECK(hipFree(reinterpret_cast<void *>(C)));
+  HIPCHECK(hipFree(reinterpret_cast<void *>(D)));
+
+  return 0;
+}
+rpret_cast<void *>(B)));
+  checkCudaErrors(hipFree(reinterpret_cast<void *>(C)));
+  checkCudaErrors(hipFree(reinterpret_cast<void *>(D)));
+
+  return 0;
+}
diff --git a/src/samples/Samples/3_CUDA_Features/dmmaTensorCoreGemm/dmmaTensorCoreGemm.cu.hip b/src/samples/Samples/3_CUDA_Features/dmmaTensorCoreGemm/dmmaTensorCoreGemm.cu.hip
index e69de29..8ebd122 100755
--- a/src/samples/Samples/3_CUDA_Features/dmmaTensorCoreGemm/dmmaTensorCoreGemm.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/dmmaTensorCoreGemm/dmmaTensorCoreGemm.cu.hip
@@ -0,0 +1,1028 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+
+// CUDA sample demonstrating a Double precision GEMM computation using the Warp
+//  Matrix Multiply and Accumulate API introduced in CUDA 11.0.
+
+// In this program, the compute_dgemm kernel computes the result of a matrix multiplication
+// and addition: D = alpha * A * B + beta * C. The dimensions of both C and D matrices
+// are M_GLOBAL x N_GLOBAL. The A matrix is M_GLOBAL x K_GLOBAL (row-major), the B matrix
+// is K_GLOBAL x N_GLOBAL (column-major).
+// In that kernel, each CTA computes one 64 x 64 tile of the resulting matrix
+// per iteration. When the tile is computed, the CTA stores it to the global memory
+// and begins a new iteration, selecting a new 64 x 64 tile to compute.
+// Each CTA consists of eight warps. For the 64 x 64 tile, each warp computes eight
+// 8 x 8 subtiles, organized in a 2 x 4 two-dimensional array.
+// Warps compute the 8 x 8 subtiles using nvcuda::wmma::mma_sync operations by
+// moving through the K_GLOBAL dimension of the A and B matrices and accumulating
+// the intermediate result in the local thread state.
+
+// There are a number of simple optimizations used in the algorithm:
+// - The CTA copies the 64 x 64 tile of the C matrix from the global memory to
+//   shared memory. After that is done, each warp loads the C matrix fragments from
+//   shared memory, thus avoiding a random global memory access.
+// - On each internal iteration, the CTA copies a portion of the A and B matrices from
+//   global memory to shared memory. After that, all warps in the CTA reuse the A and B
+//   data from shared memory, thus reducing the number of data copies from global memory.
+// - The portions of the A and B matrices are stored in shared memory with an additional
+//   padding (skew) to reduce the number of shared memory access bank conflicts.
+//   (See a detailed explanation near the SKEW_DOUBLE macro definition.)
+// - When the CTA finishes computing the tiles of the resulting matrix, each warp stores
+//   its subtiles to shared memory. The CTA then copies the shared memory contents to
+//   global memory, again avoiding redundant random global memory accesses.
+// - Note that the CTA tile size is chosen to maximize the GPU register utilization,
+//   but carefully enough to avoid local memory use.
+
+#include <assert.h>
+#include <stdio.h>
+#include <hip/hip_runtime.h>
+#include <mma.h>
+#include <hip/hip_cooperative_groups.h>
+#include <cooperative_groups/memcpy_async.h>
+#include <cuda/std/type_traits>
+#include <cuda/barrier>
+#include <cuda/pipeline>
+
+// helper functions and utilities to work with CUDA
+#include "helper_functions.h"
+#include "helper_cuda_hipified.h"
+
+// Externally configurable parameters.
+
+#ifndef CPU_DEBUG
+// Set this to 1 to verify the correctness of the GPU-computed matrix.
+#define CPU_DEBUG 0
+#endif
+
+#ifndef SHARED_MEMORY_LIMIT_64K
+// Set this to 0 to use more than 64 Kb of shared memory to cache data, to
+// improve the performance of the computations on GPU.
+// Note that you need a GPU that can have more than 64 Kb of shared memory
+// per multiprocessor.
+#define SHARED_MEMORY_LIMIT_64K 0
+#endif
+
+// GPU configuration.
+
+#define WARP_SIZE 32
+
+// MMA matrix tile dimensions.
+
+#define M 8
+#define N 8
+#define K 4
+
+// GEMM configuration.
+
+#define M_TILES 1024
+#define N_TILES 1024
+#define K_TILES 1024
+
+#define M_GLOBAL (M * M_TILES)
+#define N_GLOBAL (N * N_TILES)
+#define K_GLOBAL (K * K_TILES)
+
+#define C_LAYOUT wmma::mem_row_major
+
+// Implementation constants.
+
+#define WARPS_PER_BLOCK 8
+#define THREADS_PER_BLOCK (WARP_SIZE * WARPS_PER_BLOCK)
+
+#if SHARED_MEMORY_LIMIT_64K
+// With only 64 Kb shared memory available, we can fit 8x16-tile chunks of each
+// the A and B matrix data, that are (M = 8) * (K = 4) * 8 * (CHUNK_K = 16) * sizeof(double) = 32 Kb each
+// But we cannot account the 4 Kb total skew overhead, without which the performance
+// would be severely impacted. So we choose to reduce the chunk size in half,
+// i.e. the amount of A and B matrix data we cache in shared memory.
+// Accordingly, this doubles the number of outer iterations across the global K
+// dimension, which only slightly impacts the performance.
+#define CHUNK_K 8
+#else
+#define CHUNK_K 16
+#endif
+
+#define CHUNK_LINE_BYTES (CHUNK_K * K * sizeof(double))
+#define WARP_COPY_BYTES (WARP_SIZE * sizeof(int4))
+#define CHUNK_COPY_LINES_PER_WARP (WARP_COPY_BYTES / CHUNK_LINE_BYTES)
+#define CHUNK_COPY_LINE_LANES (WARP_SIZE / CHUNK_COPY_LINES_PER_WARP)
+
+#define BLOCK_ROW_WARPS 2
+#define BLOCK_COL_WARPS 4
+
+#define WARP_ROW_TILES 4
+#define WARP_COL_TILES 2
+
+#define BLOCK_ROW_TILES (WARP_ROW_TILES * BLOCK_ROW_WARPS)
+#define BLOCK_COL_TILES (WARP_COL_TILES * BLOCK_COL_WARPS)
+
+#define GLOBAL_MEM_STRIDE N_GLOBAL
+
+#define SHMEM_STRIDE (N * BLOCK_ROW_TILES)
+#define SHMEM_OFFSET (N * WARP_ROW_TILES)
+
+// The macro below is used to shift rows of the A matrix and columns of the B matrix
+// in shared memory to minimize possible bank conflicts.
+// Before performing the nvcuda::wmma::mma_sync operation, the warp must load the matrix
+// data using the nvcuda::wmma::load_matrix_sync operation. Although the memory access pattern
+// is not specified for that function, each lane in the warp can read one or multiple matrix
+// elements from different matrix rows or columns.
+// For shared memory, such access can result in bank conflicts if different rows / columns
+// of the matrix map to the same bank. By shifting each row and column by a few bytes, we
+// make sure that they map to different banks, thus reducing the number of possible bank
+// conflicts.
+// The number of 4 eight-byte "double" elements is chosen as the minimum possible shift because
+// we must keep each row and column 256-bit aligned, as required by nvcuda::wmma::load_matrix_sync.
+#define SKEW_DOUBLE 4
+
+#define checkKernelErrors(expr) do {                                                        \
+    expr;                                                                                   \
+                                                                                            \
+    hipError_t __err = hipGetLastError();                                                 \
+    if (__err != hipSuccess) {                                                             \
+        printf("Line %d: '%s' failed: %s\n", __LINE__, # expr, hipGetErrorString(__err));  \
+        abort();                                                                            \
+    }                                                                                       \
+} while(0)
+
+enum kernels
+{
+    dmma_shmem_gemm_async_copy      = 0, // DMMA shmem using kernel with async_copy
+    dmma_shmem_gemm_cg_async_copy   = 1, // DMMA shmem using kernel with cooperative groups async_copy
+    dmma_shmem_gemm                 = 2, // DMMA shmem using kernel normal copy (without async_copy).
+    simple_dmma_gemm                = 3  // DMMA non-shmem using simple kernel.
+};
+
+const char* kernelNames[] = {"compute_dgemm_async_copy", "compute_dgemm_cg_async_copy",
+                            "compute_dgemm", "simple_wmma_gemm"};
+
+using namespace nvcuda;
+namespace cg = cooperative_groups;
+
+__host__ void init_host_matrices(double *a, double *b, double *c)
+{
+    for (int i = 0; i < M_GLOBAL; i++) {
+        for (int j = 0; j < K_GLOBAL; j++) {
+            a[i*K_GLOBAL+j] = (double) (rand() % 3);
+        }
+    }
+
+    for (int i = 0; i < N_GLOBAL; i++) {
+        for (int j = 0; j < K_GLOBAL; j++) {
+            b[i*K_GLOBAL+j] = (double) (rand() % 3);
+        }
+    }
+
+    for (int t = 0; t < M_GLOBAL * N_GLOBAL; t++) {
+        c[t] =  (double) (rand() % 3);
+    }
+}
+
+__global__ void compute_dgemm(const double *A, const double *B, const double *C, double *D, double alpha, double beta)
+{
+#if __CUDA_ARCH__ >= 800
+    extern __shared__ double shmem[][CHUNK_K * K + SKEW_DOUBLE];
+
+    // Warp and lane identification.
+    const unsigned int warpId = threadIdx.x / WARP_SIZE;
+    const unsigned int laneId = threadIdx.x % WARP_SIZE;
+
+    // Offset in shared memory from which the B matrix is stored.
+    const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;
+
+
+    // This pointer is used to access the C and D matrix tiles this warp computes.
+    double *shmem_warp_tile_ptr = (double*)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * SHMEM_STRIDE * N * BLOCK_ROW_WARPS + (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;
+
+    // This pointer is used to stream the C and D matrices block-wide tile to and from shared memory.
+    double *shmem_warp_stream_ptr = (double*)&shmem[0][0] + warpId * SHMEM_STRIDE * N;
+
+    // Adjust the beta scaler, as it'll be multiplied by alpha at the end of
+    // each tile computation. Technically this is not generally correct (may result
+    // in a loss of precision). Zero still needs to be specially handled though.
+    beta /= alpha;
+
+    // Each CTA slides along the 64 x 64 tiles from the top left corner of the matrix to the
+    // right and down, and selects the next tile to compute. Once there's no such tile,
+    // all warps in this CTA exit.
+    for(unsigned int block_pos = blockIdx.x;; block_pos += gridDim.x) {
+        const unsigned int block_tile_i = ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);
+        const unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % N_TILES;
+
+        // Stop when there are no more D matrix tiles to compute in this CTA.
+        if (block_tile_i >= M_TILES) {
+            break;
+        }
+
+        // This warp's pointer to the C matrix data to copy memory from to shared memory.
+        const size_t gmem_idx = (block_tile_i + warpId) * M * GLOBAL_MEM_STRIDE + block_tile_j * N;
+        const double *src_gmem_warp_stream_ptr = &C[gmem_idx];
+
+        // Stream multiple C tiles to shared memory.
+#pragma unroll
+        for (int i = 0; i < N; i++) {
+            *((int4 *)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId) =
+                *((int4 *)(src_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) + laneId);
+        }
+
+        __syncthreads();
+
+        // These fragments will accumulate the result of A and B matrix fragment multiplications
+        // along the K_GLOBAL dimension.
+        wmma::fragment<wmma::accumulator, M, N, K, double> c[WARP_COL_TILES][WARP_ROW_TILES];
+
+        // Load the C matrix tiles into fragments from shared memory.
+#pragma unroll
+        for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+                const double *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * N + j * N;
+
+                wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, C_LAYOUT);
+            }
+        }
+
+        __syncthreads();
+
+        // Scale the C matrix.
+#pragma unroll
+       for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+#pragma unroll
+                for (int t = 0; t < c[i][j].num_elements; t++) {
+                    c[i][j].x[t] *= beta;
+                }
+            }
+        }
+
+        // Select what warp copies what matrix to shared memory.
+        // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.
+        const double *warp_ptr = (warpId < (WARPS_PER_BLOCK/2)) ? (&A[block_tile_i * M * K_GLOBAL] + M * K_GLOBAL * (warpId % (WARPS_PER_BLOCK/2)) * 2) :
+                                              (&B[block_tile_j * N * K_GLOBAL] + N * K_GLOBAL * (warpId % (WARPS_PER_BLOCK/2)) * 2);
+
+        // Go through the global K dimension by a fixed step at a time.
+#pragma unroll
+        for (int tile_k = 0; tile_k < K_TILES; tile_k += CHUNK_K) {
+            // Copy slices of the A and B matrices to shared memory.
+            // The first half of the warps in the CTA copy the A matrix, the rest copy the B matrix.
+            size_t shmem_idx = warpId < (WARPS_PER_BLOCK/2) ? (M * (warpId % (WARPS_PER_BLOCK/2)) * 2) :
+                                                              (N * (warpId % (WARPS_PER_BLOCK/2)) * 2 + shmem_idx_b_off);
+
+            // First half of the warp copies the first row / column of the matrix,
+            // the second half of the warp copies the next.
+            const double *lane_ptr = warp_ptr + tile_k * K + (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL;
+
+            // Shift the second half of the warp to the next row / column in the shared memory.
+            shmem_idx += laneId / CHUNK_COPY_LINE_LANES;
+
+#pragma unroll
+            for(int i = 0; i < ((WARP_SIZE/2) / CHUNK_COPY_LINES_PER_WARP); i++) {
+                 // Copy 16 bytes at once in each lane.
+                *((int4*)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) = *((int4*)lane_ptr +  (laneId % CHUNK_COPY_LINE_LANES));
+
+                // Advance the global memory pointer and the shared memory index.
+                lane_ptr = lane_ptr + K_GLOBAL * CHUNK_COPY_LINES_PER_WARP;
+                shmem_idx += CHUNK_COPY_LINES_PER_WARP;
+            }
+
+            __syncthreads();
+
+            // Compute a grid of C matrix tiles in each warp.
+#pragma unroll
+            for (int k_step = 0; k_step < CHUNK_K; k_step++) {
+                wmma::fragment<wmma::matrix_a, M, N, K, double, wmma::row_major> a[WARP_COL_TILES];
+                wmma::fragment<wmma::matrix_b, M, N, K, double, wmma::col_major> b[WARP_ROW_TILES];
+
+#pragma unroll
+                for (int i = 0; i < WARP_COL_TILES; i++) {
+                    size_t shmem_idx_a = (warpId/2) * M * 2 + (i * M);
+                    const double *tile_ptr = &shmem[shmem_idx_a][k_step * K];
+
+                    wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_DOUBLE);
+
+#pragma unroll
+                    for (int j = 0; j < WARP_ROW_TILES; j++) {
+                        if (i == 0) {
+                            // Load the B matrix fragment once, because it is going to be reused
+                            // against the other A matrix fragments.
+                            size_t shmem_idx_b = shmem_idx_b_off + (WARP_ROW_TILES * N) * (warpId%2) + (j * N);
+                            const double *tile_ptr = &shmem[shmem_idx_b][k_step * K];
+
+                            wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_DOUBLE);
+
+                        }
+
+                        wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);
+                    }
+                }
+            }
+
+            __syncthreads();
+        }
+
+        // Store the D fragments to shared memory.
+#pragma unroll
+        for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+                // Uniform, point-wise transformations of ALL fragment elements by ALL threads in the
+                // warp are well-defined even though element indices within fragment storage are not defined.
+#pragma unroll
+                for (int t = 0; t < c[i][j].num_elements; t++)
+                    c[i][j].x[t] *= alpha;
+
+                double *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * N + j * N;
+
+                wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);
+            }
+        }
+
+        __syncthreads();
+
+        // Now that shared memory contains all the D tiles, stream them to global memory.
+        double *dst_gmem_warp_stream_ptr = &D[gmem_idx];
+
+#pragma unroll
+        for (int i = 0; i < N; i++) {
+            *((int4*)(dst_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) + laneId) =
+                *((int4*)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId);
+        }
+
+        __syncthreads();
+    }
+#endif
+}
+
+__global__ void compute_dgemm_async_copy(const double *A, const double *B, const double *C, double *D, double alpha, double beta)
+{
+#if __CUDA_ARCH__ >= 800
+    extern __shared__ double shmem[][CHUNK_K * K + SKEW_DOUBLE];
+
+    // Warp and lane identification.
+    const unsigned int warpId = threadIdx.x / WARP_SIZE;
+    const unsigned int laneId = threadIdx.x % WARP_SIZE;
+
+    // Offset in shared memory from which the B matrix is stored.
+    constexpr size_t shmem_idx_b_off = BLOCK_COL_TILES * M;
+
+    // This pointer is used to access the C and D matrix tiles this warp computes.
+    double *shmem_warp_tile_ptr = &shmem[0][0] + (warpId/BLOCK_ROW_WARPS) * SHMEM_STRIDE * N * BLOCK_ROW_WARPS + (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;
+
+    // This pointer is used to stream the C and D matrices block-wide tile to and from shared memory.
+    double *shmem_warp_stream_ptr = &shmem[0][0] + warpId * SHMEM_STRIDE * N;
+
+    // Adjust the beta scaler, as it'll be multiplied by alpha at the end of
+    // each tile computation. Technically this is not generally correct (may result
+    // in a loss of precision). Zero still needs to be specially handled though.
+    beta /= alpha;
+
+    cuda::pipeline<cuda::thread_scope_thread> pipe = cuda::make_pipeline();
+
+    const auto shape2 = cuda::aligned_size_t<alignof(double2)>(sizeof(double2));
+    constexpr int loadStride = 1; // load 2 double, left-shift by 1.
+
+    // Each CTA slides along the 64 x 64 tiles from the top left corner of the matrix to the
+    // right and down, and selects the next tile to compute. Once there's no such tile,
+    // all warps in this CTA exit.
+    for(unsigned int block_pos = blockIdx.x;; block_pos += gridDim.x) {
+        const unsigned int block_tile_i = ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);
+        const unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % N_TILES;
+
+        // Stop when there are no more D matrix tiles to compute in this CTA.
+        if (block_tile_i >= M_TILES) {
+            break;
+        }
+
+        // This warp's pointer to the C matrix data to copy memory from to shared memory.
+        const size_t gmem_idx = (block_tile_i + warpId) * M * GLOBAL_MEM_STRIDE + block_tile_j * N;
+        const double *src_gmem_warp_stream_ptr = &C[gmem_idx];
+
+        // Stream multiple C tiles to shared memory.
+#pragma unroll
+        for (int i = 0; i < N; i++) {
+            pipe.producer_acquire();
+            cuda::memcpy_async(&shmem_warp_stream_ptr[(SHMEM_STRIDE * i) + (laneId << loadStride)],
+                                &src_gmem_warp_stream_ptr[(GLOBAL_MEM_STRIDE * i) + (laneId << loadStride)],
+                                shape2, pipe);
+
+            pipe.producer_commit();
+        }
+        // Now wait for all the above issued 8 batches to complete.
+        cuda::pipeline_consumer_wait_prior<0>(pipe);
+        __syncthreads();
+
+        // These fragments will accumulate the result of A and B matrix fragment multiplications
+        // along the K_GLOBAL dimension.
+        wmma::fragment<wmma::accumulator, M, N, K, double> c[WARP_COL_TILES][WARP_ROW_TILES];
+
+        // Load the C matrix tiles into fragments from shared memory.
+#pragma unroll
+        for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+                const double *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * N + j * N;
+
+                wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, C_LAYOUT);
+                // Scale the C matrix.
+#pragma unroll
+                for (int t = 0; t < c[i][j].num_elements; t++) {
+                    c[i][j].x[t] *= beta;
+                }
+            }
+        }
+
+        pipe.consumer_release();
+        // sync here so that shared memory can then be used for loading A & B matrices.
+        __syncthreads();
+
+        // Select what warp copies what matrix to shared memory.
+        // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.
+        const double *warp_ptr = (warpId < (WARPS_PER_BLOCK/2)) ? (&A[block_tile_i * M * K_GLOBAL] + M * K_GLOBAL * (warpId % (WARPS_PER_BLOCK/2)) * 2) :
+                                              (&B[block_tile_j * N * K_GLOBAL] + N * K_GLOBAL * (warpId % (WARPS_PER_BLOCK/2)) * 2);
+
+        const int stridePerLaneCopy = (laneId / CHUNK_COPY_LINE_LANES);
+        constexpr int chunksPerLane = ((WARP_SIZE/2) / CHUNK_COPY_LINES_PER_WARP);
+        const int laneLoadElem = (laneId % CHUNK_COPY_LINE_LANES) << loadStride;
+
+        // Go through the global K dimension by a fixed step at a time.
+#pragma unroll
+        for (int tile_k = 0; tile_k < K_TILES; tile_k += CHUNK_K) {
+            // Copy slices of the A and B matrices to shared memory.
+            // The first half of the warps in the CTA copy the A matrix, the rest copy the B matrix.
+            // As for DMMA  M == N we use M for warp 4-7 + shmem_idx_b_off.
+            size_t shmem_idx = (M * (warpId % (WARPS_PER_BLOCK/2)) * 2) + (shmem_idx_b_off * (warpId/(WARPS_PER_BLOCK/2)));
+
+            // First half of the warp copies the first row / column of the matrix,
+            // the second half of the warp copies the next.
+            const double *lane_ptr = warp_ptr + tile_k * K + stridePerLaneCopy * K_GLOBAL + laneLoadElem;
+
+            // Shift the second half of the warp to the next row / column in the shared memory.
+            shmem_idx += stridePerLaneCopy;
+#pragma unroll
+            for(int i = 0; i < chunksPerLane; i++) {
+                 // Copy 16 bytes at once in each lane.
+                pipe.producer_acquire();
+
+                cuda::memcpy_async(&shmem[shmem_idx][laneLoadElem], lane_ptr, shape2, pipe);
+
+                pipe.producer_commit();
+
+                // Advance the global memory pointer and the shared memory index.
+                lane_ptr = lane_ptr + K_GLOBAL * CHUNK_COPY_LINES_PER_WARP;
+                shmem_idx += CHUNK_COPY_LINES_PER_WARP;
+            }
+
+            cuda::pipeline_consumer_wait_prior<0>(pipe);
+            __syncthreads();
+
+            // Compute a grid of C matrix tiles in each warp.
+#pragma unroll
+            for (int k_step = 0; k_step < CHUNK_K; k_step++) {
+                wmma::fragment<wmma::matrix_a, M, N, K, double, wmma::row_major> a[WARP_COL_TILES];
+                wmma::fragment<wmma::matrix_b, M, N, K, double, wmma::col_major> b[WARP_ROW_TILES];
+#pragma unroll
+                for (int i = 0; i < WARP_COL_TILES; i++) {
+                    size_t shmem_idx_a = (warpId/2) * M * 2 + (i * M);
+                    const double *tile_ptr = &shmem[shmem_idx_a][k_step * K];
+
+                    wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_DOUBLE);
+#pragma unroll
+                    for (int j = 0; j < WARP_ROW_TILES; j++) {
+                        if (i == 0) {
+                            // Load the B matrix fragment once, because it is going to be reused
+                            // against the other A matrix fragments.
+                            size_t shmem_idx_b = shmem_idx_b_off + (WARP_ROW_TILES * N) * (warpId%2) + (j * N);
+                            const double *tile_ptr = &shmem[shmem_idx_b][k_step * K];
+
+                            wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_DOUBLE);
+                        }
+                        wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);
+                    }
+                }
+            }
+            pipe.consumer_release();
+            __syncthreads();
+        }
+
+        // Store the D fragments to shared memory.
+#pragma unroll
+        for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+                // Uniform, point-wise transformations of ALL fragment elements by ALL threads in the
+                // warp are well-defined even though element indices within fragment storage are not defined.
+#pragma unroll
+                for (int t = 0; t < c[i][j].num_elements; t++)
+                    c[i][j].x[t] *= alpha;
+
+                double *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * N + j * N;
+
+                wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);
+            }
+        }
+
+        __syncthreads();
+
+        // Now that shared memory contains all the D tiles, stream them to global memory.
+        double *dst_gmem_warp_stream_ptr = &D[gmem_idx];
+
+#pragma unroll
+        for (int i = 0; i < N; i++) {
+            *((int4*)(dst_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) + laneId) =
+                *((int4*)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId);
+        }
+
+        __syncthreads();
+    }
+#endif
+}
+
+ __global__ void compute_dgemm_cg_async_copy(const double *A, const double *B, const double *C, double *D, double alpha, double beta)
+{
+#if __CUDA_ARCH__ >= 800
+    extern __shared__ double shmem[][CHUNK_K * K + SKEW_DOUBLE];
+    auto cta = cg::this_thread_block();
+    auto tile32 = cg::tiled_partition<32>(cta);
+
+    constexpr int tileChunkCopySize = WARP_SIZE / CHUNK_COPY_LINES_PER_WARP;
+    auto tileChunkCopy = cg::tiled_partition<tileChunkCopySize>(cta);
+
+    // Warp and lane identification.
+    const unsigned int warpId = threadIdx.x / WARP_SIZE;
+    const unsigned int laneId = threadIdx.x % WARP_SIZE;
+
+    // Offset in shared memory from which the B matrix is stored.
+    constexpr size_t shmem_idx_b_off = BLOCK_COL_TILES * M;
+
+    // This pointer is used to access the C and D matrix tiles this warp computes.
+    double *shmem_warp_tile_ptr = (double*)&shmem[0][0] + (warpId/2) * SHMEM_STRIDE * N * 2 + (warpId%2) * SHMEM_OFFSET;
+
+    // This pointer is used to stream the C and D matrices block-wide tile to and from shared memory.
+    double *shmem_warp_stream_ptr = (double*)&shmem[0][0] + warpId * SHMEM_STRIDE * N;
+
+    // Adjust the beta scaler, as it'll be multiplied by alpha at the end of
+    // each tile computation. Technically this is not generally correct (may result
+    // in a loss of precision). Zero still needs to be specially handled though.
+    beta /= alpha;
+
+    // Each CTA slides along the 64 x 64 tiles from the top left corner of the matrix to the
+    // right and down, and selects the next tile to compute. Once there's no such tile,
+    // all warps in this CTA exit.
+    for(unsigned int block_pos = blockIdx.x;; block_pos += gridDim.x) {
+        const unsigned int block_tile_i = ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);
+        const unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % N_TILES;
+
+        // Stop when there are no more D matrix tiles to compute in this CTA.
+        if (block_tile_i >= M_TILES) {
+            break;
+        }
+
+        // This warp's pointer to the C matrix data to copy memory from to shared memory.
+        const size_t gmem_idx = (block_tile_i + warpId) * M * GLOBAL_MEM_STRIDE + block_tile_j * N;
+        const double *src_gmem_warp_stream_ptr = &C[gmem_idx];
+
+        // Stream multiple C tiles to shared memory.
+#pragma unroll
+        for (int i = 0; i < N; i++) {
+            auto dst_ptr = &shmem_warp_stream_ptr[(SHMEM_STRIDE * i)];
+            auto src_ptr = &src_gmem_warp_stream_ptr[(GLOBAL_MEM_STRIDE * i)];
+            cg::memcpy_async(tile32, dst_ptr, src_ptr, cuda::aligned_size_t<alignof(double2)>{tile32.size() * sizeof(double2)});
+        }
+
+        cg::wait(cta);
+
+        // These fragments will accumulate the result of A and B matrix fragment multiplications
+        // along the K_GLOBAL dimension.
+        wmma::fragment<wmma::accumulator, M, N, K, double> c[WARP_COL_TILES][WARP_ROW_TILES];
+
+        // Load the C matrix tiles into fragments from shared memory.
+#pragma unroll
+        for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+                const double *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * N + j * N;
+                wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, C_LAYOUT);
+            }
+        }
+
+        // Scale the C matrix.
+#pragma unroll
+        for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+#pragma unroll
+                for (int t = 0; t < c[i][j].num_elements; t++) {
+                    c[i][j].x[t] *= beta;
+                }
+            }
+        }
+
+        // sync here so that shared memory can then be used for loading A & B matrices.
+        cg::wait(cta);
+        // Select what warp copies what matrix to shared memory.
+        // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.
+        const double *warp_ptr = (warpId < 4) ? (&A[block_tile_i * M * K_GLOBAL] + M * K_GLOBAL * (warpId % (WARPS_PER_BLOCK/2)) * 2) :
+            (&B[block_tile_j * N * K_GLOBAL] + N * K_GLOBAL * (warpId % (WARPS_PER_BLOCK/2)) * 2);
+
+        const int stridePerLaneCopy = (laneId / CHUNK_COPY_LINE_LANES);
+        constexpr int chunksPerLane = ((WARP_SIZE/2) / CHUNK_COPY_LINES_PER_WARP);
+        // Go through the global K dimension by a fixed step at a time.
+#pragma unroll
+        for (int tile_k = 0; tile_k < K_TILES; tile_k += CHUNK_K) {
+            // Copy slices of the A and B matrices to shared memory.
+            // The first half of the warps in the CTA copy the A matrix, the rest copy the B matrix.
+            // As for DMMA  M == N we use M for warp 4-7 + shmem_idx_b_off.
+            size_t shmem_idx = (M * (warpId % (WARPS_PER_BLOCK/2)) * 2) + (shmem_idx_b_off * (warpId/(WARPS_PER_BLOCK/2)));
+
+            // First half of the warp copies the first row / column of the matrix,
+            // the second half of the warp copies the next.
+            auto lane_ptr = warp_ptr + tile_k * K + stridePerLaneCopy * K_GLOBAL;
+
+            // Shift the second half of the warp to the next row / column in the shared memory.
+            shmem_idx += stridePerLaneCopy;
+
+#pragma unroll
+            for(int i = 0; i < chunksPerLane; i++) {
+                // Copy 16 bytes at once in each lane.
+                auto dst_ptr = &shmem[shmem_idx][0];
+                auto src_ptr = lane_ptr;
+
+                cg::memcpy_async(tileChunkCopy, dst_ptr, src_ptr, 
+                                cuda::aligned_size_t<alignof(double2)>{tileChunkCopySize * sizeof(double2)});
+
+                // Advance the global memory pointer and the shared memory index.
+                lane_ptr = lane_ptr + K_GLOBAL * CHUNK_COPY_LINES_PER_WARP;
+                shmem_idx += CHUNK_COPY_LINES_PER_WARP;
+            }
+            cg::wait(cta);
+
+            // Compute a grid of C matrix tiles in each warp.
+#pragma unroll
+            for (int k_step = 0; k_step < CHUNK_K; k_step++) {
+                wmma::fragment<wmma::matrix_a, M, N, K, double, wmma::row_major> a[WARP_COL_TILES];
+                wmma::fragment<wmma::matrix_b, M, N, K, double, wmma::col_major> b[WARP_ROW_TILES];
+
+#pragma unroll
+                for (int i = 0; i < WARP_COL_TILES; i++) {
+                    size_t shmem_idx_a = (warpId/2) * M * 2 + (i * M);
+                    const double *tile_ptr = &shmem[shmem_idx_a][k_step * K];
+
+                    wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_DOUBLE);
+
+#pragma unroll
+                    for (int j = 0; j < WARP_ROW_TILES; j++) {
+                        if (i == 0) {
+                            // Load the B matrix fragment once, because it is going to be reused
+                            // against the other A matrix fragments.
+                            size_t shmem_idx_b = shmem_idx_b_off + (WARP_ROW_TILES * N) * (warpId%2) + (j * N);
+                            const double *tile_ptr = &shmem[shmem_idx_b][k_step * K];
+
+                            wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_DOUBLE);
+
+                        }
+
+                        wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);
+                    }
+                }
+            }
+            cg::sync(cta);
+        }
+
+        // Store the D fragments to shared memory.
+#pragma unroll
+        for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+                // Uniform, point-wise transformations of ALL fragment elements by ALL threads in the
+                // warp are well-defined even though element indices within fragment storage are not defined.
+#pragma unroll
+                for (int t = 0; t < c[i][j].num_elements; t++)
+                    c[i][j].x[t] *= alpha;
+
+                double *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * N + j * N;
+
+                wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);
+            }
+        }
+
+        cg::sync(cta);
+
+        // Now that shared memory contains all the D tiles, stream them to global memory.
+        double *dst_gmem_warp_stream_ptr = &D[gmem_idx];
+
+#pragma unroll
+        for (int i = 0; i < N; i++) {
+            *((int4*)(dst_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) + laneId) =
+                *((int4*)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId);
+        }
+        cg::sync(cta);
+    }
+#endif
+}
+
+// Performs an MxNxK DGEMM (C=alpha*A*B + beta*C) assuming:
+//  1) Matrices are packed in memory.
+//  2) M, N and K are multiples of 8, 8 and 4 respectively. 
+//  3) A is row major, B is column major matrix.
+// Note: This is a less performant version of the compute_dgemm kernel. It is designed for
+//       demonstration purposes only to show the CUDA WMMA API use without relying on
+//       availability of the shared memory.
+__global__ void simple_wmma_gemm(double *a, double *b, double *c, double *d, int m_ld, int n_ld, int k_ld, double alpha, double beta)
+{
+#if __CUDA_ARCH__ >= 800
+    // Leading dimensions. Packed with no transpositions.
+    int lda = k_ld;
+    int ldb = k_ld;
+    int ldc = n_ld;
+
+    // Tile using a 2D grid
+    int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / warpSize;
+    int warpN = (blockIdx.y * blockDim.y + threadIdx.y);
+
+    // Declare the fragments
+    wmma::fragment<wmma::matrix_a, M, N, K, double, wmma::row_major> a_frag;
+    wmma::fragment<wmma::matrix_b, M, N, K, double, wmma::col_major> b_frag;
+    wmma::fragment<wmma::accumulator, M, N, K, double> acc_frag;
+    wmma::fragment<wmma::accumulator, M, N, K, double> c_frag;
+
+    wmma::fill_fragment(acc_frag, 0.0f);
+
+    // Loop over k
+    for (int i = 0; i < k_ld; i += K) {
+        int aCol = i;
+        int aRow = warpM * M;
+
+        int bCol = warpN * N;
+        int bRow = i;
+
+        // Bounds checking
+        if (aRow < m_ld && aCol < k_ld && bRow < k_ld && bCol < n_ld) {
+            // Load the inputs
+            wmma::load_matrix_sync(a_frag, a + aCol + aRow * lda, lda);
+            wmma::load_matrix_sync(b_frag, b + bRow + bCol * ldb, ldb);
+
+            // Perform the matrix multiplication
+            wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag);
+        }
+    }
+
+    // Load in the current value of c, scale it by beta, and add this our result scaled by alpha
+    int cCol = warpN * N;
+    int cRow = warpM * M;
+
+    if (cRow < m_ld && cCol < n_ld) {
+        wmma::load_matrix_sync(c_frag, c + cCol + cRow * ldc, ldc, wmma::mem_row_major);
+
+        for(int i=0; i < c_frag.num_elements; i++) {
+            c_frag.x[i] = alpha * acc_frag.x[i] + beta * c_frag.x[i];
+        }
+
+        // Store the output
+        wmma::store_matrix_sync(d + cCol + cRow * ldc, c_frag, ldc, wmma::mem_row_major);
+    }
+#endif
+}
+
+__host__ void matMultiplyOnHost(double *A, double *B, double *C,
+                                float alpha, float beta,
+                                int numARows, int numAColumns,
+                                int numBRows, int numBColumns,
+                                int numCRows, int numCColumns)
+{
+    for (int i = 0; i < numCRows; i++) {
+        for (int j = 0; j < numCColumns; j++) {
+            double temp = 0.0;
+
+            for (int k = 0; k < numAColumns; k++) {
+                // B matrix is column major. A matrix is row major.
+                temp += A[i * numAColumns + k] * B[j * numBRows + k];
+            }
+
+            C[i*numCColumns + j] = temp * alpha + beta * C[i * numCColumns + j];
+        }
+    }
+}
+
+int main(int argc, char **argv)
+{
+    printf("Initializing...\n");
+
+    int dev = findCudaDevice(argc, (const char **)argv);
+
+    hipDeviceProp_t deviceProp;
+    HIPCHECK(hipGetDeviceProperties(&deviceProp, dev));
+
+    // Double precision Tensor cores require a GPU of Ampere (SM8X) architecture or higher.
+    if (deviceProp.major < 8) {
+        printf("dmmaTensorCoreGemm requires SM 8.0 or higher.  Exiting...\n");
+        exit(EXIT_WAIVED);
+    }
+
+    printf("M: %d (%d x %d)\n", M_GLOBAL, M, M_TILES);
+    printf("N: %d (%d x %d)\n", N_GLOBAL, N, N_TILES);
+    printf("K: %d (%d x %d)\n", K_GLOBAL, K, K_TILES);
+
+    double *A_h = NULL;
+    double *B_h = NULL;
+    double *C_h = NULL;
+#if CPU_DEBUG
+    double *result_hD = NULL;
+    double *result_host = NULL;
+#endif
+
+    A_h = (double*) malloc(sizeof(double) * M_GLOBAL * K_GLOBAL);
+    B_h = (double*) malloc(sizeof(double) * K_GLOBAL * N_GLOBAL);
+    C_h = (double*) malloc(sizeof(double) * M_GLOBAL * N_GLOBAL);
+#if CPU_DEBUG
+    result_hD   = (double*) malloc(sizeof(double) * M_GLOBAL * N_GLOBAL);
+    result_host = (double*) malloc(sizeof(double) * M_GLOBAL * N_GLOBAL);
+#endif
+
+    double *A = NULL;
+    double *B = NULL;
+    double *C = NULL;
+    double *D = NULL;
+
+    HIPCHECK(hipMalloc((void**)&A, sizeof(double) * M_GLOBAL * K_GLOBAL));
+    HIPCHECK(hipMalloc((void**)&B, sizeof(double) * N_GLOBAL * K_GLOBAL));
+    HIPCHECK(hipMalloc((void**)&C, sizeof(double) * M_GLOBAL * N_GLOBAL));
+    HIPCHECK(hipMalloc((void**)&D, sizeof(double) * M_GLOBAL * N_GLOBAL));
+
+    assert(((unsigned long long)A) % 128 == 0);
+    assert(((unsigned long long)B) % 128 == 0);
+    assert(((unsigned long long)C) % 128 == 0);
+    assert(((unsigned long long)D) % 128 == 0);
+
+    init_host_matrices(A_h, B_h, C_h);
+
+    printf("Preparing data for GPU...\n");
+
+    HIPCHECK(hipMemcpy(A, A_h, sizeof(double) * M_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
+    HIPCHECK(hipMemcpy(B, B_h, sizeof(double) * N_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
+    HIPCHECK(hipMemcpy(C, C_h, sizeof(double) * M_GLOBAL * N_GLOBAL, hipMemcpyHostToDevice));
+    HIPCHECK(hipMemset(D, 0, sizeof(double) * M_GLOBAL * N_GLOBAL));
+
+    enum {
+        // Compute the right amount of shared memory to request.
+        // We need shared memory to hold per-CTA C and D matrix tiles, and to cache per-CTA chunks
+        // of the A and B matrices. Therefore, the right amount to request is the maximum of those
+        // two numbers.
+        SHMEM_SZ = MAX(sizeof(double) * (BLOCK_COL_TILES * M) * (CHUNK_K * K + SKEW_DOUBLE) * 2,
+                       M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N * (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(double))
+    };
+
+    printf("Required shared memory size: %lu Kb\n", SHMEM_SZ / 1024UL);
+
+    const double alpha = 1.1f;
+    const double beta = 1.2f;
+
+    hipEvent_t start, stop;
+
+    HIPCHECK(hipEventCreate(&start));
+    HIPCHECK(hipEventCreate(&stop));
+    HIPCHECK(hipEventRecord(start));
+
+    kernels selected_kernel = dmma_shmem_gemm_async_copy;
+
+    // kernel to run - default (dmma_shmem_gemm_async_copy == 0)
+    if (checkCmdLineFlag(argc, (const char **)argv, "kernel")) {
+        int kernel_number = getCmdLineArgumentInt(argc, (const char **)argv, "kernel");
+        if (kernel_number < 4)
+        {
+            selected_kernel = (kernels)kernel_number;
+        }
+        else
+        {
+            printf("Error: kernel number should be between 0 to 3, you have entered %d\n", kernel_number);
+            exit(EXIT_FAILURE);
+        }
+    }
+
+    // If enough shared memory available on the GPU use high performant kernel
+    if ((deviceProp.sharedMemPerMultiprocessor >= SHMEM_SZ) && (selected_kernel != simple_dmma_gemm))
+    {
+        printf("Computing using high performance kernel = %d - %s\n", selected_kernel, kernelNames[selected_kernel]);
+
+        switch (selected_kernel)
+        {
+            case dmma_shmem_gemm_async_copy :
+            default:
+                HIPCHECK(hipFuncSetAttribute(compute_dgemm_async_copy, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
+                checkKernelErrors((compute_dgemm_async_copy<<<deviceProp.multiProcessorCount*3, THREADS_PER_BLOCK, SHMEM_SZ>>>(A, B, C, D, alpha, beta)));
+                break;
+            case dmma_shmem_gemm_cg_async_copy :
+                HIPCHECK(hipFuncSetAttribute(compute_dgemm_cg_async_copy, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
+                checkKernelErrors((compute_dgemm_cg_async_copy<<<deviceProp.multiProcessorCount*3, THREADS_PER_BLOCK, SHMEM_SZ>>>(A, B, C, D, alpha, beta)));
+                break;
+            case dmma_shmem_gemm :
+                HIPCHECK(hipFuncSetAttribute(compute_dgemm, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
+                checkKernelErrors((compute_dgemm<<<deviceProp.multiProcessorCount*2, THREADS_PER_BLOCK, SHMEM_SZ>>>(A, B, C, D, alpha, beta)));
+                break;
+        }
+
+#if CPU_DEBUG
+        HIPCHECK(hipMemcpy(result_hD, D, sizeof(double)*M_GLOBAL*N_GLOBAL, hipMemcpyDeviceToHost));
+#endif
+    }
+    else
+    {
+        dim3 gridDim;
+        dim3 blockDim;
+
+        // blockDim.x must be a multple of warpSize
+        // 128x4 means we have 16 warps and a block computes a 64x64 output tile
+        blockDim.x = 128;
+        blockDim.y = 4;
+
+        gridDim.x = (M_GLOBAL + (M * blockDim.x / 32 - 1)) / (M * blockDim.x / 32);
+        gridDim.y = (N_GLOBAL + N * blockDim.y - 1) / (N * blockDim.y);
+
+        printf("Computing... using simple_wmma_gemm kernel\n");
+        simple_wmma_gemm<<<gridDim, blockDim>>>(A, B, C, D, M_GLOBAL, N_GLOBAL, K_GLOBAL, alpha, beta);
+#if CPU_DEBUG
+        HIPCHECK(hipMemcpy(result_hD, D, sizeof(double) * M_GLOBAL * N_GLOBAL, hipMemcpyDeviceToHost));
+#endif
+    }
+
+    HIPCHECK(hipEventRecord(stop));
+    HIPCHECK(hipEventSynchronize(stop));
+
+#if CPU_DEBUG
+    printf("Verifying correctness of the computations...\n");
+
+    memcpy(result_host, C_h, sizeof(double) * M_GLOBAL * N_GLOBAL);
+
+    matMultiplyOnHost(A_h, B_h, result_host,
+                      alpha, beta,
+                      M_GLOBAL, K_GLOBAL,
+                      K_GLOBAL, N_GLOBAL,
+                      M_GLOBAL, N_GLOBAL);
+
+    size_t number_of_matches = 0;
+    for (int i = 0; i < N_GLOBAL*M_GLOBAL; i++) {
+        if  (fabs(result_hD[i] - result_host[i]) > 0.1f)
+        {
+            printf("mismatch i=%d result_hD=%f result_host=%f\n", i, result_hD[i], result_host[i]);
+            break;
+        }
+        else
+        {
+            number_of_matches++;
+        }
+    }
+    printf("number_of_matches = %zu out of = %d \n", number_of_matches, N_GLOBAL*M_GLOBAL);
+    free(result_hD);
+    free(result_host);
+#endif
+
+    float milliseconds = 0;
+
+    HIPCHECK(hipEventElapsedTime(&milliseconds, start, stop));
+
+    printf("Time: %f ms\n", milliseconds);
+    printf("FP64 TFLOPS: %.2f\n", (((double)M_GLOBAL * N_GLOBAL * K_GLOBAL * 2)/(milliseconds/1000.)) / 1e12);
+
+    free(A_h);
+    free(B_h);
+    free(C_h);
+    HIPCHECK(hipFree((void*)A));
+    HIPCHECK(hipFree((void*)B));
+    HIPCHECK(hipFree((void*)C));
+    HIPCHECK(hipFree((void*)D));
+
+    return 0;
+}
+CudaErrors(hipFree((void*)A));
+    checkCudaErrors(hipFree((void*)B));
+    checkCudaErrors(hipFree((void*)C));
+    checkCudaErrors(hipFree((void*)D));
+
+    return 0;
+}
diff --git a/src/samples/Samples/3_CUDA_Features/globalToShmemAsyncCopy/globalToShmemAsyncCopy.cu.hip b/src/samples/Samples/3_CUDA_Features/globalToShmemAsyncCopy/globalToShmemAsyncCopy.cu.hip
index bfc7c82..103a347 100755
--- a/src/samples/Samples/3_CUDA_Features/globalToShmemAsyncCopy/globalToShmemAsyncCopy.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/globalToShmemAsyncCopy/globalToShmemAsyncCopy.cu.hip
@@ -55,8 +55,8 @@
 namespace cg = cooperative_groups;
 
 // Helper functions and utilities to work with CUDA
-#include <helper_functions.h>
-#include <helper_cuda.h>
+#include "helper_functions.h"
+#include "helper_cuda_hipified.h"
 
 enum kernels {
   AsyncCopyMultiStageLargeChunk = 0,
@@ -762,11 +762,11 @@ int MatrixMultiply(int argc, char **argv, const dim3 &dimsA, const dim3 &dimsB,
   unsigned int size_A = dimsA.x * dimsA.y;
   unsigned int mem_size_A = sizeof(float) * size_A;
   float *h_A;
-  checkCudaErrors(hipHostMalloc(&h_A, mem_size_A));
+  HIPCHECK(hipHostMalloc(&h_A, mem_size_A));
   unsigned int size_B = dimsB.x * dimsB.y;
   unsigned int mem_size_B = sizeof(float) * size_B;
   float *h_B;
-  checkCudaErrors(hipHostMalloc(&h_B, mem_size_B));
+  HIPCHECK(hipHostMalloc(&h_B, mem_size_B));
   hipStream_t stream;
 
   // Initialize host memory
@@ -781,29 +781,29 @@ int MatrixMultiply(int argc, char **argv, const dim3 &dimsA, const dim3 &dimsB,
   dim3 dimsC(dimsB.x, dimsA.y, 1);
   unsigned int mem_size_C = dimsC.x * dimsC.y * sizeof(float);
   float *h_C;
-  checkCudaErrors(hipHostMalloc(&h_C, mem_size_C));
+  HIPCHECK(hipHostMalloc(&h_C, mem_size_C));
 
   if (h_C == NULL) {
     fprintf(stderr, "Failed to allocate host matrix C!\n");
     exit(EXIT_FAILURE);
   }
 
-  checkCudaErrors(hipMalloc(reinterpret_cast<void **>(&d_A), mem_size_A));
-  checkCudaErrors(hipMalloc(reinterpret_cast<void **>(&d_B), mem_size_B));
-  checkCudaErrors(hipMalloc(reinterpret_cast<void **>(&d_C), mem_size_C));
+  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_A), mem_size_A));
+  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_B), mem_size_B));
+  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_C), mem_size_C));
   // Allocate CUDA events that we'll use for timing
   hipEvent_t start, stop;
-  checkCudaErrors(hipEventCreate(&start));
-  checkCudaErrors(hipEventCreate(&stop));
+  HIPCHECK(hipEventCreate(&start));
+  HIPCHECK(hipEventCreate(&stop));
 
-  checkCudaErrors(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
+  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
 
   // copy host memory to device
-  checkCudaErrors(
+  HIPCHECK(
       hipMemcpyAsync(d_A, h_A, mem_size_A, hipMemcpyHostToDevice, stream));
-  checkCudaErrors(
+  HIPCHECK(
       hipMemcpyAsync(d_B, h_B, mem_size_B, hipMemcpyHostToDevice, stream));
-  checkCudaErrors(hipMemsetAsync(d_C, 0, mem_size_C, stream));
+  HIPCHECK(hipMemsetAsync(d_C, 0, mem_size_C, stream));
 
   // Setup execution parameters
   dim3 threads(blockSize, blockSize);
@@ -861,13 +861,13 @@ int MatrixMultiply(int argc, char **argv, const dim3 &dimsA, const dim3 &dimsB,
   }
 
   printf("done\n");
-  checkCudaErrors(hipStreamSynchronize(stream));
+  HIPCHECK(hipStreamSynchronize(stream));
 
   // Execute the kernel
   int nIter = 100;
 
   // Record the start event
-  checkCudaErrors(hipEventRecord(start, stream));
+  HIPCHECK(hipEventRecord(start, stream));
 
   for (int j = 0; j < nIter; j++) {
     switch (kernel_number) {
@@ -911,13 +911,13 @@ int MatrixMultiply(int argc, char **argv, const dim3 &dimsA, const dim3 &dimsB,
   }
 
   // Record the stop event
-  checkCudaErrors(hipEventRecord(stop, stream));
+  HIPCHECK(hipEventRecord(stop, stream));
 
   // Wait for the stop event to complete
-  checkCudaErrors(hipEventSynchronize(stop));
+  HIPCHECK(hipEventSynchronize(stop));
 
   float msecTotal = 0.0f;
-  checkCudaErrors(hipEventElapsedTime(&msecTotal, start, stop));
+  HIPCHECK(hipEventElapsedTime(&msecTotal, start, stop));
 
   // Compute and print the performance
   float msecPerMatrixMul = msecTotal / nIter;
@@ -932,9 +932,9 @@ int MatrixMultiply(int argc, char **argv, const dim3 &dimsA, const dim3 &dimsB,
       gigaFlops, msecPerMatrixMul, flopsPerMatrixMul, threads.x * threads.y);
 
   // Copy result from device to host
-  checkCudaErrors(
+  HIPCHECK(
       hipMemcpyAsync(h_C, d_C, mem_size_C, hipMemcpyDeviceToHost, stream));
-  checkCudaErrors(hipStreamSynchronize(stream));
+  HIPCHECK(hipStreamSynchronize(stream));
 
   printf("Checking computed result for correctness: ");
   bool correct = true;
@@ -959,14 +959,14 @@ int MatrixMultiply(int argc, char **argv, const dim3 &dimsA, const dim3 &dimsB,
   printf("%s\n", correct ? "Result = PASS" : "Result = FAIL");
 
   // Clean up memory
-  checkCudaErrors(hipHostFree(h_A));
-  checkCudaErrors(hipHostFree(h_B));
-  checkCudaErrors(hipHostFree(h_C));
-  checkCudaErrors(hipFree(d_A));
-  checkCudaErrors(hipFree(d_B));
-  checkCudaErrors(hipFree(d_C));
-  checkCudaErrors(hipEventDestroy(start));
-  checkCudaErrors(hipEventDestroy(stop));
+  HIPCHECK(hipHostFree(h_A));
+  HIPCHECK(hipHostFree(h_B));
+  HIPCHECK(hipHostFree(h_C));
+  HIPCHECK(hipFree(d_A));
+  HIPCHECK(hipFree(d_B));
+  HIPCHECK(hipFree(d_C));
+  HIPCHECK(hipEventDestroy(start));
+  HIPCHECK(hipEventDestroy(stop));
   printf(
       "\nNOTE: The CUDA Samples are not meant for performance "
       "measurements. Results may vary when GPU Boost is enabled.\n");
@@ -1056,7 +1056,7 @@ int main(int argc, char **argv) {
   }
 
   int major = 0;
-  checkCudaErrors(
+  HIPCHECK(
       hipDeviceGetAttribute(&major, hipDeviceAttributeComputeCapabilityMajor, dev));
   if (major < 7) {
     printf("globalToShmemAsyncCopy requires SM 7.0 or higher.  Exiting...\n");
@@ -1070,3 +1070,10 @@ int main(int argc, char **argv) {
 
   exit(matrix_result);
 }
+rintf("MatrixA(%d,%d), MatrixB(%d,%d)\n", dimsA.x, dimsA.y, dimsB.x,
+         dimsB.y);
+
+  int matrix_result = MatrixMultiply(argc, argv, dimsA, dimsB, selected_kernel);
+
+  exit(matrix_result);
+}
diff --git a/src/samples/Samples/3_CUDA_Features/graphMemoryFootprint/graphMemoryFootprint.cu.hip b/src/samples/Samples/3_CUDA_Features/graphMemoryFootprint/graphMemoryFootprint.cu.hip
index e69de29..df280a5 100755
--- a/src/samples/Samples/3_CUDA_Features/graphMemoryFootprint/graphMemoryFootprint.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/graphMemoryFootprint/graphMemoryFootprint.cu.hip
@@ -0,0 +1,423 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// System includes
+#include <assert.h>
+#include <stdio.h>
+
+// helper functions and utilities to work with CUDA
+#include "helper_cuda_hipified.h"
+#include "helper_functions.h"
+
+#define NUM_GRAPHS 8
+#define THREADS_PER_BLOCK 512
+
+void printMemoryFootprint(int device) {
+  size_t footprint;
+  HIPCHECK(hipDeviceGetGraphMemAttribute(
+      device, (hipGraphMemAttributeType)0, &footprint));
+  printf("    FOOTPRINT: %lu bytes\n", footprint);
+}
+
+void prepareAllocParams(cudaMemAllocNodeParams *allocParams, size_t bytes,
+                        int device) {
+  memset(allocParams, 0, sizeof(*allocParams));
+
+  allocParams->bytesize = bytes;
+  allocParams->poolProps.allocType = hipMemAllocationTypePinned;
+  allocParams->poolProps.location.id = device;
+  allocParams->poolProps.location.type = hipMemLocationTypeDevice;
+}
+
+void createVirtAddrReuseGraph(hipGraphExec_t *graphExec, size_t bytes,
+                              int device) {
+  hipGraph_t graph;
+  hipGraphNode_t allocNodeA, allocNodeB, freeNodeA, freeNodeB;
+  cudaMemAllocNodeParams allocParams;
+  float *d_a, *d_b;
+
+  HIPCHECK(hipGraphCreate(&graph, 0));
+  prepareAllocParams(&allocParams, bytes, device);
+
+  HIPCHECK(
+      cudaGraphAddMemAllocNode(&allocNodeA, graph, NULL, 0, &allocParams));
+  d_a = (float *)allocParams.dptr;
+  HIPCHECK(
+      cudaGraphAddMemFreeNode(&freeNodeA, graph, &allocNodeA, 1, (void *)d_a));
+
+  // The dependency between the allocation of d_b and the free of d_a allows d_b
+  // to reuse the same VA.
+  HIPCHECK(cudaGraphAddMemAllocNode(&allocNodeB, graph, &freeNodeA, 1,
+                                           &allocParams));
+  d_b = (float *)allocParams.dptr;
+
+  if (d_a == d_b) {
+    printf("Check confirms that d_a and d_b share a virtual address.\n");
+  } else {
+    printf("Check shows that d_a and d_b DO NOT share a virtual address.\n");
+  }
+
+  HIPCHECK(
+      cudaGraphAddMemFreeNode(&freeNodeB, graph, &allocNodeB, 1, (void *)d_b));
+
+  HIPCHECK(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
+  HIPCHECK(hipGraphDestroy(graph));
+}
+
+void virtualAddressReuseSingleGraph(size_t bytes, int device) {
+  hipStream_t stream;
+  hipGraphExec_t graphExec;
+
+  printf("================================\n");
+  printf("Running virtual address reuse example.\n");
+  printf(
+      "Sequential allocations & frees within a single graph enable CUDA to "
+      "reuse virtual addresses.\n\n");
+
+  createVirtAddrReuseGraph(&graphExec, bytes, device);
+  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
+
+  HIPCHECK(hipGraphLaunch(graphExec, stream));
+  HIPCHECK(hipStreamSynchronize(stream));
+  printMemoryFootprint(device);
+
+  HIPCHECK(hipGraphExecDestroy(graphExec));
+  HIPCHECK(hipStreamDestroy(stream));
+}
+
+// This is a kernel that does no real work but runs at least for a specified
+// number of clocks
+__global__ void clockBlock(clock_t clock_count) {
+  unsigned int start_clock = (unsigned int)clock();
+
+  clock_t clock_offset = 0;
+
+  while (clock_offset < clock_count) {
+    unsigned int end_clock = (unsigned int)clock();
+
+    // The code below should work like
+    // this (thanks to modular arithmetics):
+    //
+    // clock_offset = (clock_t) (end_clock > start_clock ?
+    //                           end_clock - start_clock :
+    //                           end_clock + (0xffffffffu - start_clock));
+    //
+    // Indeed, let m = 2^32 then
+    // end - start = end + m - start (mod m).
+
+    clock_offset = (clock_t)(end_clock - start_clock);
+  }
+}
+
+// A pointer to the allocated device buffer is returned in dPtr so the caller
+// can compare virtual addresses. The kernel node is added to increase the
+// graph's runtime.
+void createSimpleAllocFreeGraph(hipGraphExec_t *graphExec, float **dPtr,
+                                size_t bytes, int device) {
+  hipGraph_t graph;
+  hipGraphNode_t allocNodeA, freeNodeA, blockDeviceNode;
+  cudaMemAllocNodeParams allocParams;
+  hipKernelNodeParams blockDeviceNodeParams = {0};
+  int numElements = bytes / sizeof(float);
+  float kernelTime = 5;  // time for each thread to run in microseconds
+
+  HIPCHECK(hipGraphCreate(&graph, 0));
+  prepareAllocParams(&allocParams, bytes, device);
+
+  HIPCHECK(
+      cudaGraphAddMemAllocNode(&allocNodeA, graph, NULL, 0, &allocParams));
+  *dPtr = (float *)allocParams.dptr;
+
+  hipDeviceProp_t deviceProp;
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, device));
+  clock_t time_clocks = (clock_t)((kernelTime / 1000.0) * deviceProp.clockRate);
+
+  void *blockDeviceArgs[1] = {(void *)&time_clocks};
+
+  size_t numBlocks = numElements / (size_t)THREADS_PER_BLOCK;
+  blockDeviceNodeParams.gridDim = dim3(numBlocks, 1, 1);
+  blockDeviceNodeParams.blockDim = dim3(THREADS_PER_BLOCK, 1, 1);
+  blockDeviceNodeParams.sharedMemBytes = 0;
+  blockDeviceNodeParams.extra = NULL;
+  blockDeviceNodeParams.func = (void *)clockBlock;
+  blockDeviceNodeParams.kernelParams = (void **)blockDeviceArgs;
+  HIPCHECK(hipGraphAddKernelNode(&blockDeviceNode, graph, &allocNodeA,
+                                         1, &blockDeviceNodeParams));
+
+  HIPCHECK(cudaGraphAddMemFreeNode(&freeNodeA, graph, &blockDeviceNode,
+                                          1, (void *)*dPtr));
+
+  HIPCHECK(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
+  HIPCHECK(hipGraphDestroy(graph));
+}
+
+void physicalMemoryReuseSingleStream(size_t bytes, int device) {
+  hipStream_t stream;
+  hipGraphExec_t graphExecs[NUM_GRAPHS];
+  float *dPtrs[NUM_GRAPHS];
+  bool virtualAddrDiffer = true;
+
+  printf("================================\n");
+  printf("Running physical memory reuse example.\n");
+  printf(
+      "CUDA reuses the same physical memory for allocations from separate "
+      "graphs when the allocation lifetimes don't overlap.\n\n");
+
+  for (int i = 0; i < NUM_GRAPHS; i++) {
+    createSimpleAllocFreeGraph(&graphExecs[i], &dPtrs[i], bytes, device);
+  }
+
+  printf("Creating the graph execs does not reserve any physical memory.\n");
+  printMemoryFootprint(device);
+
+  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
+
+  HIPCHECK(hipGraphLaunch(graphExecs[0], stream));
+  printf("\nThe first graph launched reserves the memory it needs.\n");
+  printMemoryFootprint(device);
+
+  HIPCHECK(hipGraphLaunch(graphExecs[0], stream));
+  printf(
+      "A subsequent launch of the same graph in the same stream reuses the "
+      "same physical memory. ");
+  printf("Thus the memory footprint does not grow here.\n");
+  printMemoryFootprint(device);
+
+  printf(
+      "\nSubsequent launches of other graphs in the same stream also reuse the "
+      "physical memory. ");
+  printf("Thus the memory footprint does not grow here.\n");
+  for (int i = 1; i < NUM_GRAPHS; i++) {
+    HIPCHECK(hipGraphLaunch(graphExecs[i], stream));
+    printf("%02d: ", i);
+    printMemoryFootprint(device);
+  }
+
+  HIPCHECK(hipStreamSynchronize(stream));
+
+  for (int i = 0; i < NUM_GRAPHS; i++) {
+    for (int j = i + 1; j < NUM_GRAPHS; j++) {
+      if (dPtrs[i] == dPtrs[j]) {
+        virtualAddrDiffer = false;
+        printf("Error: Graph exec %d and %d have the same virtual address!\n",
+               i - 1, i);
+      }
+    }
+    HIPCHECK(hipGraphExecDestroy(graphExecs[i]));
+  }
+  if (virtualAddrDiffer) {
+    printf("\nCheck confirms all graphs use a different virtual address.\n");
+  } else {
+    printf(
+        "\nAll graphs do NOT use different virtual addresses. Exiting test.\n");
+    exit(EXIT_FAILURE);
+  }
+
+  HIPCHECK(hipStreamDestroy(stream));
+}
+
+void simultaneousStreams(size_t bytes, int device) {
+  hipStream_t streams[NUM_GRAPHS];
+  hipGraphExec_t graphExecs[NUM_GRAPHS];
+  float *dPtrs[NUM_GRAPHS];
+
+  printf("================================\n");
+  printf("Running simultaneous streams example.\n");
+  printf("Graphs that can run concurrently need separate physical memory. ");
+  printf(
+      "In this example, each graph launched in a separate stream increases the "
+      "total memory footprint.\n\n");
+
+  printf(
+      "When launching a new graph, CUDA may reuse physical memory from a graph "
+      "whose execution has already ");
+  printf(
+      "finished -- even if the new graph is being launched in a different "
+      "stream from the completed graph. ");
+  printf(
+      "Therefore, a kernel node is added to the graphs to increase "
+      "runtime.\n\n");
+
+  for (int i = 0; i < NUM_GRAPHS; i++) {
+    createSimpleAllocFreeGraph(&graphExecs[i], &dPtrs[i], bytes, device);
+    HIPCHECK(
+        hipStreamCreateWithFlags(&streams[i], hipStreamNonBlocking));
+  }
+
+  printf("Initial footprint:\n");
+  printMemoryFootprint(device);
+
+  printf(
+      "\nEach graph launch in a seperate stream grows the memory footprint:\n");
+  for (int i = 1; i < NUM_GRAPHS; i++) {
+    HIPCHECK(hipGraphLaunch(graphExecs[i], streams[i]));
+    printf("%02d: ", i);
+    printMemoryFootprint(device);
+  }
+
+  for (int i = 0; i < NUM_GRAPHS; i++) {
+    HIPCHECK(hipStreamSynchronize(streams[i]));
+    HIPCHECK(hipGraphExecDestroy(graphExecs[i]));
+    HIPCHECK(hipStreamDestroy(streams[i]));
+  }
+}
+
+void createSimpleAllocNoFreeGraph(hipGraphExec_t *graphExec, float **dPtr,
+                                  size_t bytes, int device) {
+  hipGraph_t graph;
+  hipGraphNode_t allocNodeA;
+  cudaMemAllocNodeParams allocParams;
+
+  HIPCHECK(hipGraphCreate(&graph, 0));
+  prepareAllocParams(&allocParams, bytes, device);
+
+  HIPCHECK(
+      cudaGraphAddMemAllocNode(&allocNodeA, graph, NULL, 0, &allocParams));
+  *dPtr = (float *)allocParams.dptr;
+
+  HIPCHECK(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
+  HIPCHECK(hipGraphDestroy(graph));
+}
+
+void unfreedAllocations(size_t bytes, int device) {
+  hipStream_t stream;
+  hipGraphExec_t graphExecs[NUM_GRAPHS];
+  float *dPtrs[NUM_GRAPHS];
+
+  printf("================================\n");
+  printf("Running unfreed streams example.\n");
+  printf(
+      "CUDA cannot reuse phyiscal memory from graphs which do not free their "
+      "allocations.\n\n");
+
+  for (int i = 0; i < NUM_GRAPHS; i++) {
+    createSimpleAllocNoFreeGraph(&graphExecs[i], &dPtrs[i], bytes, device);
+  }
+
+  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
+
+  printf(
+      "Despite being launched in the same stream, each graph launch grows the "
+      "memory footprint. ");
+  printf(
+      "Since the allocation is not freed, CUDA keeps the memory valid for "
+      "use.\n");
+  for (int i = 0; i < NUM_GRAPHS; i++) {
+    HIPCHECK(hipGraphLaunch(graphExecs[i], stream));
+    printf("%02d: ", i);
+    printMemoryFootprint(device);
+  }
+
+  HIPCHECK(hipStreamSynchronize(stream));
+
+  HIPCHECK(hipDeviceGraphMemTrim(device));
+  printf(
+      "\nTrimming does not impact the memory footprint since the un-freed "
+      "allocations are still holding onto the memory.\n");
+  printMemoryFootprint(device);
+
+  for (int i = 0; i < NUM_GRAPHS; i++) {
+    HIPCHECK(hipFree(dPtrs[i]));
+  }
+  printf("\nFreeing the allocations does not shrink the footprint.\n");
+  printMemoryFootprint(device);
+
+  HIPCHECK(hipDeviceGraphMemTrim(device));
+  printf(
+      "\nSince the allocations are now freed, trimming does reduce the "
+      "footprint even when the graph execs are not yet destroyed.\n");
+  printMemoryFootprint(device);
+
+  for (int i = 0; i < NUM_GRAPHS; i++) {
+    HIPCHECK(hipGraphExecDestroy(graphExecs[i]));
+  }
+  HIPCHECK(hipStreamDestroy(stream));
+}
+
+void cleanupMemory(int device) {
+  HIPCHECK(hipDeviceGraphMemTrim(device));
+  printf("\nCleaning up example by trimming device memory.\n");
+  printMemoryFootprint(device);
+  printf("\n");
+}
+
+int main(int argc, char **argv) {
+  size_t bytes = 64 * 1024 * 1024;
+  int device = findCudaDevice(argc, (const char **)argv);
+
+  int driverVersion = 0;
+  int deviceSupportsMemoryPools = 0;
+
+  hipDriverGetVersion(&driverVersion);
+  printf("Driver version is: %d.%d\n", driverVersion / 1000,
+         (driverVersion % 100) / 10);
+
+  if (driverVersion < 11040) {
+    printf("Waiving execution as driver does not support Graph Memory Nodes\n");
+    exit(EXIT_WAIVED);
+  }
+
+  hipDeviceGetAttribute(&deviceSupportsMemoryPools,
+                         hipDeviceAttributeMemoryPoolsSupported, device);
+  if (!deviceSupportsMemoryPools) {
+    printf("Waiving execution as device does not support Memory Pools\n");
+    exit(EXIT_WAIVED);
+  } else {
+    printf("Running sample.\n");
+  }
+
+  virtualAddressReuseSingleGraph(bytes, device);
+  cleanupMemory(device);
+
+  physicalMemoryReuseSingleStream(bytes, device);
+  cleanupMemory(device);
+
+  simultaneousStreams(bytes, device);
+  cleanupMemory(device);
+
+  unfreedAllocations(bytes, device);
+  cleanupMemory(device);
+
+  printf("================================\n");
+  printf("Sample complete.\n");
+}
+e);
+  cleanupMemory(device);
+
+  physicalMemoryReuseSingleStream(bytes, device);
+  cleanupMemory(device);
+
+  simultaneousStreams(bytes, device);
+  cleanupMemory(device);
+
+  unfreedAllocations(bytes, device);
+  cleanupMemory(device);
+
+  printf("================================\n");
+  printf("Sample complete.\n");
+}
diff --git a/src/samples/Samples/3_CUDA_Features/graphMemoryNodes/graphMemoryNodes.cu.hip b/src/samples/Samples/3_CUDA_Features/graphMemoryNodes/graphMemoryNodes.cu.hip
index e69de29..d226ff8 100755
--- a/src/samples/Samples/3_CUDA_Features/graphMemoryNodes/graphMemoryNodes.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/graphMemoryNodes/graphMemoryNodes.cu.hip
@@ -0,0 +1,568 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// System includes
+#include <assert.h>
+#include <stdio.h>
+
+#include <climits>
+#include <vector>
+
+// CUDA runtime
+#include <hip/hip_runtime.h>
+
+// helper functions and utilities to work with CUDA
+#include "helper_cuda_hipified.h"
+#include "helper_functions.h"
+
+#define THREADS_PER_BLOCK 512
+#define ALLOWABLE_VARIANCE 1.e-6f
+#define NUM_ELEMENTS 8000000
+
+// Stores the square of each input element in output array
+__global__ void squareArray(const float *input, float *output,
+                            int numElements) {
+  int idx = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (idx < numElements) {
+    output[idx] = input[idx] * input[idx];
+  }
+}
+
+// Stores the negative of each input element in output array
+__global__ void negateArray(const float *input, float *output,
+                            int numElements) {
+  int idx = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (idx < numElements) {
+    output[idx] = input[idx] * -1;
+  }
+}
+
+struct negSquareArrays {
+  float *input;
+  float *square;
+  float *negSquare;
+  int numElements;
+  size_t bytes;
+  size_t numBlocks;
+};
+
+void fillRandomly(float *array, int numElements) {
+  for (int n = 0; n < numElements; n++) {
+    array[n] = rand() / (float)RAND_MAX;
+  }
+}
+
+void resetOutputArrays(negSquareArrays *hostArrays) {
+  fillRandomly(hostArrays->square, hostArrays->numElements);
+  fillRandomly(hostArrays->negSquare, hostArrays->numElements);
+}
+
+void prepareHostArrays(negSquareArrays *hostArrays) {
+  hostArrays->numElements = NUM_ELEMENTS;
+  size_t bytes = hostArrays->numElements * sizeof(float);
+
+  size_t numBlocks = hostArrays->numElements / (size_t)THREADS_PER_BLOCK;
+  if ((numBlocks % (size_t)THREADS_PER_BLOCK) != 0) {
+    numBlocks++;
+  }
+
+  hostArrays->input = (float *)malloc(bytes);
+  hostArrays->square = (float *)malloc(bytes);
+  hostArrays->negSquare = (float *)malloc(bytes);
+  hostArrays->bytes = bytes;
+  hostArrays->numBlocks = numBlocks;
+
+  fillRandomly(hostArrays->input, hostArrays->numElements);
+  fillRandomly(hostArrays->square, hostArrays->numElements);
+  fillRandomly(hostArrays->negSquare, hostArrays->numElements);
+}
+
+void createFreeGraph(hipGraphExec_t *graphExec, float *dPtr) {
+  hipGraph_t graph;
+  hipGraphNode_t freeNode;
+
+  HIPCHECK(hipGraphCreate(&graph, 0));
+
+  HIPCHECK(
+      cudaGraphAddMemFreeNode(&freeNode, graph, NULL, 0, (void *)dPtr));
+
+  HIPCHECK(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
+  HIPCHECK(hipGraphDestroy(graph));
+}
+
+/**
+ * Demonstrates explicitly creating a CUDA graph including memory nodes.
+ * createNegateSquaresGraphWithStreamCapture constructs an equivalent graph
+ * using stream capture.
+ *
+ * If d_negSquare_out is non null, then:
+ * 1) d_negSquare will not be freed;
+ * 2) the value of d_negSquare_out will be set to d_negSquare.
+ *
+ * Diagram of the graph constructed by createNegateSquaresGraphExplicitly:
+ *
+ * alloc d_input
+ *       |
+ * alloc d_square
+ *       |
+ * Memcpy a to device
+ *       |
+ * launch kernel squareArray ------->---- Memcpy d_square to host
+ *       |                                      |
+ * free d_input                                 |
+ *       |                                      |
+ * allocate d_negSquare                         |
+ *       |                                      |
+ * launch kernel negateArray -------->--- free d_square
+ *       |
+ * Memcpy d_negSquare to host
+ *       |
+ * free d_negSquare
+ */
+void createNegateSquaresGraphExplicitly(hipGraphExec_t *graphExec, int device,
+                                        negSquareArrays *hostArrays,
+                                        float **d_negSquare_out = NULL) {
+  // Array buffers on device
+  float *d_input, *d_square, *d_negSquare;
+
+  // Memory allocation parameters
+  cudaMemAllocNodeParams allocParams;
+  memset(&allocParams, 0, sizeof(allocParams));
+  allocParams.bytesize = hostArrays->bytes;
+  allocParams.poolProps.allocType = hipMemAllocationTypePinned;
+  allocParams.poolProps.location.id = device;
+  allocParams.poolProps.location.type = hipMemLocationTypeDevice;
+
+  // Kernel launch parameters
+  hipKernelNodeParams kernelNodeParams = {0};
+  kernelNodeParams.gridDim = dim3(hostArrays->numBlocks, 1, 1);
+  kernelNodeParams.blockDim = dim3(THREADS_PER_BLOCK, 1, 1);
+  kernelNodeParams.sharedMemBytes = 0;
+  kernelNodeParams.extra = NULL;
+
+  hipGraph_t graph;
+  hipGraphNode_t allocNodeInput, allocNodeSquare, allocNodeNegSquare;
+  hipGraphNode_t copyNodeInput, copyNodeSquare, copyNodeNegSquare;
+  hipGraphNode_t squareKernelNode, negateKernelNode;
+  hipGraphNode_t freeNodeInput, freeNodeSquare;
+
+  // Buffer for storing graph node dependencies
+  std::vector<hipGraphNode_t> nodeDependencies;
+
+  HIPCHECK(hipGraphCreate(&graph, 0));
+
+  HIPCHECK(
+      cudaGraphAddMemAllocNode(&allocNodeInput, graph, NULL, 0, &allocParams));
+  d_input = (float *)allocParams.dptr;
+
+  // To keep the graph structure simple (fewer branching dependencies),
+  // allocNodeSquare should depend on allocNodeInput
+  HIPCHECK(cudaGraphAddMemAllocNode(&allocNodeSquare, graph,
+                                           &allocNodeInput, 1, &allocParams));
+  d_square = (float *)allocParams.dptr;
+
+  // copyNodeInput needs to depend on allocNodeInput because copyNodeInput
+  // writes to d_input. It does so here indirectly through allocNodeSquare.
+  HIPCHECK(hipGraphAddMemcpyNode1D(
+      &copyNodeInput, graph, &allocNodeSquare, 1, d_input, hostArrays->input,
+      hostArrays->bytes, hipMemcpyHostToDevice));
+
+  void *squareKernelArgs[3] = {(void *)&d_input, (void *)&d_square,
+                               (void *)&(hostArrays->numElements)};
+  kernelNodeParams.func = (void *)squareArray;
+  kernelNodeParams.kernelParams = (void **)squareKernelArgs;
+
+  // Square kernel depends on copyNodeInput to ensure all data is on the device
+  // before kernel launch.
+  HIPCHECK(hipGraphAddKernelNode(&squareKernelNode, graph,
+                                         &copyNodeInput, 1, &kernelNodeParams));
+
+  HIPCHECK(hipGraphAddMemcpyNode1D(
+      &copyNodeSquare, graph, &squareKernelNode, 1, hostArrays->square,
+      d_square, hostArrays->bytes, hipMemcpyDeviceToHost));
+
+  // Free of d_input depends on the square kernel to ensure that d_input is not
+  // freed while being read by the kernel. It also depends on the alloc of
+  // d_input via squareKernelNode > copyNodeInput > allocNodeSquare >
+  // allocNodeInput.
+  HIPCHECK(cudaGraphAddMemFreeNode(&freeNodeInput, graph,
+                                          &squareKernelNode, 1, d_input));
+
+  // Allocation of C depends on free of A so CUDA can reuse the virtual address.
+  HIPCHECK(cudaGraphAddMemAllocNode(&allocNodeNegSquare, graph,
+                                           &freeNodeInput, 1, &allocParams));
+  d_negSquare = (float *)allocParams.dptr;
+
+  if (d_negSquare == d_input) {
+    printf(
+        "Check verified that d_negSquare and d_input share a virtual "
+        "address.\n");
+  }
+
+  void *negateKernelArgs[3] = {(void *)&d_square, (void *)&d_negSquare,
+                               (void *)&(hostArrays->numElements)};
+  kernelNodeParams.func = (void *)negateArray;
+  kernelNodeParams.kernelParams = (void **)negateKernelArgs;
+
+  HIPCHECK(hipGraphAddKernelNode(
+      &negateKernelNode, graph, &allocNodeNegSquare, 1, &kernelNodeParams));
+
+  nodeDependencies.push_back(copyNodeSquare);
+  nodeDependencies.push_back(negateKernelNode);
+  HIPCHECK(cudaGraphAddMemFreeNode(&freeNodeSquare, graph,
+                                          nodeDependencies.data(),
+                                          nodeDependencies.size(), d_square));
+  nodeDependencies.clear();
+
+  HIPCHECK(hipGraphAddMemcpyNode1D(
+      &copyNodeNegSquare, graph, &negateKernelNode, 1, hostArrays->negSquare,
+      d_negSquare, hostArrays->bytes, hipMemcpyDeviceToHost));
+
+  if (d_negSquare_out == NULL) {
+    hipGraphNode_t freeNodeNegSquare;
+    HIPCHECK(cudaGraphAddMemFreeNode(
+        &freeNodeNegSquare, graph, &copyNodeNegSquare, 1, d_negSquare));
+  } else {
+    *d_negSquare_out = d_negSquare;
+  }
+
+  HIPCHECK(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
+  HIPCHECK(hipGraphDestroy(graph));
+}
+
+/**
+ * Adds work to a CUDA stream which negates the square of values in the input
+ * array.
+ *
+ * If d_negSquare_out is non null, then:
+ * 1) d_negSquare will not be freed;
+ * 2) the value of d_negSquare_out will be set to d_negSquare.
+ *
+ * Diagram of the stream operations in doNegateSquaresInStream
+ * ---------------------------------------------------------------------
+ * | STREAM                             | STREAM2                      |
+ * ---------------------------------------------------------------------
+ *
+ * alloc d_input
+ *       |
+ * alloc d_square
+ *       |
+ * Memcpy a to device
+ *       |
+ * launch kernel squareArray
+ *       |
+ * record squareKernelCompleteEvent -->-- wait squareKernelCompleteEvent
+ *       |                                      |
+ * free d_input                                 |
+ *       |                                      |
+ * allocate d_negSquare                   Memcpy d_square to host
+ *       |                                      |
+ * launch kernel negateArray                    |
+ *       |                                      |
+ * record negateKernelCompleteEvent -->-- wait negateKernelCompleteEvent
+ *       |                                      |
+ * Memcpy d_negSquare to host                   |
+ *       |                                free d_square
+ * free d_negSquare                             |
+ *       |                                      |
+ * wait squareFreeEvent --------------<---- record squareFreeEvent
+ */
+void doNegateSquaresInStream(hipStream_t stream1, negSquareArrays *hostArrays,
+                             float **d_negSquare_out = NULL) {
+  float *d_input, *d_square, *d_negSquare;
+  hipStream_t stream2;
+  hipEvent_t squareKernelCompleteEvent, negateKernelCompleteEvent,
+      squareFreeEvent;
+
+  HIPCHECK(hipStreamCreateWithFlags(&stream2, hipStreamNonBlocking));
+
+  HIPCHECK(hipEventCreate(&squareKernelCompleteEvent));
+  HIPCHECK(hipEventCreate(&negateKernelCompleteEvent));
+  HIPCHECK(hipEventCreate(&squareFreeEvent));
+
+  // Virtual addresses are assigned synchronously when hipMallocAsync is
+  // called, thus there is no performace benefit gained by separating the
+  // allocations into two streams.
+  HIPCHECK(hipMallocAsync(&d_input, hostArrays->bytes, stream1));
+  HIPCHECK(hipMallocAsync(&d_square, hostArrays->bytes, stream1));
+
+  HIPCHECK(hipMemcpyAsync(d_input, hostArrays->input, hostArrays->bytes,
+                                  hipMemcpyHostToDevice, stream1));
+  squareArray<<<hostArrays->numBlocks, THREADS_PER_BLOCK, 0, stream1>>>(
+      d_input, d_square, hostArrays->numElements);
+  HIPCHECK(hipEventRecord(squareKernelCompleteEvent, stream1));
+
+  HIPCHECK(hipStreamWaitEvent(stream2, squareKernelCompleteEvent, 0));
+  HIPCHECK(hipMemcpyAsync(hostArrays->square, d_square,
+                                  hostArrays->bytes, hipMemcpyDeviceToHost,
+                                  stream2));
+
+  HIPCHECK(hipFreeAsync(d_input, stream1));
+  HIPCHECK(hipMallocAsync(&d_negSquare, hostArrays->bytes, stream1));
+  negateArray<<<hostArrays->numBlocks, THREADS_PER_BLOCK, 0, stream1>>>(
+      d_square, d_negSquare, hostArrays->numElements);
+  HIPCHECK(hipEventRecord(negateKernelCompleteEvent, stream1));
+  HIPCHECK(hipMemcpyAsync(hostArrays->negSquare, d_negSquare,
+                                  hostArrays->bytes, hipMemcpyDeviceToHost,
+                                  stream1));
+  if (d_negSquare_out == NULL) {
+    HIPCHECK(hipFreeAsync(d_negSquare, stream1));
+  } else {
+    *d_negSquare_out = d_negSquare;
+  }
+
+  HIPCHECK(hipStreamWaitEvent(stream2, negateKernelCompleteEvent, 0));
+  HIPCHECK(hipFreeAsync(d_square, stream2));
+  HIPCHECK(hipEventRecord(squareFreeEvent, stream2));
+
+  HIPCHECK(hipStreamWaitEvent(stream1, squareFreeEvent, 0));
+
+  HIPCHECK(hipStreamDestroy(stream2));
+  HIPCHECK(hipEventDestroy(squareKernelCompleteEvent));
+  HIPCHECK(hipEventDestroy(negateKernelCompleteEvent));
+  HIPCHECK(hipEventDestroy(squareFreeEvent));
+}
+
+/**
+ * Demonstrates creating a CUDA graph including memory nodes using stream
+ * capture. createNegateSquaresGraphExplicitly constructs an equivalent graph
+ * without stream capture.
+ */
+void createNegateSquaresGraphWithStreamCapture(hipGraphExec_t *graphExec,
+                                               negSquareArrays *hostArrays,
+                                               float **d_negSquare_out = NULL) {
+  hipGraph_t graph;
+  hipStream_t stream;
+
+  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
+
+  HIPCHECK(hipStreamBeginCapture(stream, hipStreamCaptureModeGlobal));
+  doNegateSquaresInStream(stream, hostArrays, d_negSquare_out);
+  HIPCHECK(hipStreamEndCapture(stream, &graph));
+
+  HIPCHECK(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
+  HIPCHECK(hipStreamDestroy(stream));
+  HIPCHECK(hipGraphDestroy(graph));
+}
+
+void prepareRefArrays(negSquareArrays *hostArrays,
+                      negSquareArrays *deviceRefArrays,
+                      bool **foundValidationFailure) {
+  deviceRefArrays->bytes = hostArrays->bytes;
+  deviceRefArrays->numElements = hostArrays->numElements;
+
+  for (int i = 0; i < hostArrays->numElements; i++) {
+    hostArrays->square[i] = hostArrays->input[i] * hostArrays->input[i];
+    hostArrays->negSquare[i] = hostArrays->square[i] * -1;
+  }
+
+  HIPCHECK(
+      hipMalloc((void **)&deviceRefArrays->negSquare, deviceRefArrays->bytes));
+  HIPCHECK(hipMemcpy(deviceRefArrays->negSquare, hostArrays->negSquare,
+                             hostArrays->bytes, hipMemcpyHostToDevice));
+
+  HIPCHECK(
+      hipMallocManaged((void **)foundValidationFailure, sizeof(bool)));
+}
+
+int checkValidationFailure(bool *foundValidationFailure) {
+  if (*foundValidationFailure) {
+    printf("Validation FAILURE!\n\n");
+    *foundValidationFailure = false;
+    return EXIT_FAILURE;
+  } else {
+    printf("Validation PASSED!\n\n");
+    return EXIT_SUCCESS;
+  }
+}
+
+__global__ void validateGPU(float *d_negSquare, negSquareArrays devRefArrays,
+                            bool *foundValidationFailure) {
+  int idx = blockIdx.x * blockDim.x + threadIdx.x;
+  float ref, diff;
+
+  if (idx < devRefArrays.numElements) {
+    ref = devRefArrays.negSquare[idx];
+    diff = d_negSquare[idx] - ref;
+    diff *= diff;
+    ref *= ref;
+    if (diff / ref > ALLOWABLE_VARIANCE) {
+      *foundValidationFailure = true;
+    }
+  }
+}
+
+void validateHost(negSquareArrays *hostArrays, bool *foundValidationFailure) {
+  float ref, diff;
+
+  for (int i = 0; i < hostArrays->numElements; i++) {
+    ref = hostArrays->input[i] * hostArrays->input[i] * -1;
+    diff = hostArrays->negSquare[i] - ref;
+    diff *= diff;
+    ref *= ref;
+    if (diff / ref > ALLOWABLE_VARIANCE) {
+      *foundValidationFailure = true;
+    }
+  }
+}
+
+int main(int argc, char **argv) {
+  negSquareArrays hostArrays, deviceRefArrays;
+  hipStream_t stream;
+  hipGraphExec_t graphExec, graphExecFreeC;
+
+  // Declare pointers for GPU buffers
+  float *d_negSquare = NULL;
+  bool *foundValidationFailure = NULL;
+
+  srand(time(0));
+  int device = findCudaDevice(argc, (const char **)argv);
+
+  int driverVersion = 0;
+  int deviceSupportsMemoryPools = 0;
+
+  hipDriverGetVersion(&driverVersion);
+  printf("Driver version is: %d.%d\n", driverVersion / 1000,
+         (driverVersion % 100) / 10);
+
+  if (driverVersion < 11040) {
+    printf("Waiving execution as driver does not support Graph Memory Nodes\n");
+    exit(EXIT_WAIVED);
+  }
+
+  hipDeviceGetAttribute(&deviceSupportsMemoryPools,
+                         hipDeviceAttributeMemoryPoolsSupported, device);
+  if (!deviceSupportsMemoryPools) {
+    printf("Waiving execution as device does not support Memory Pools\n");
+    exit(EXIT_WAIVED);
+  } else {
+    printf("Setting up sample.\n");
+  }
+
+  prepareHostArrays(&hostArrays);
+  prepareRefArrays(&hostArrays, &deviceRefArrays, &foundValidationFailure);
+  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
+  printf("Setup complete.\n\n");
+
+  printf("Running negateSquares in a stream.\n");
+  doNegateSquaresInStream(stream, &hostArrays);
+  HIPCHECK(hipStreamSynchronize(stream));
+  printf("Validating negateSquares in a stream...\n");
+  validateHost(&hostArrays, foundValidationFailure);
+  checkValidationFailure(foundValidationFailure);
+  resetOutputArrays(&hostArrays);
+
+  printf("Running negateSquares in a stream-captured graph.\n");
+  createNegateSquaresGraphWithStreamCapture(&graphExec, &hostArrays);
+  HIPCHECK(hipGraphLaunch(graphExec, stream));
+  HIPCHECK(hipStreamSynchronize(stream));
+  printf("Validating negateSquares in a stream-captured graph...\n");
+  validateHost(&hostArrays, foundValidationFailure);
+  checkValidationFailure(foundValidationFailure);
+  resetOutputArrays(&hostArrays);
+
+  printf("Running negateSquares in an explicitly constructed graph.\n");
+  createNegateSquaresGraphExplicitly(&graphExec, device, &hostArrays);
+  HIPCHECK(hipGraphLaunch(graphExec, stream));
+  HIPCHECK(hipStreamSynchronize(stream));
+  printf("Validating negateSquares in an explicitly constructed graph...\n");
+  validateHost(&hostArrays, foundValidationFailure);
+  checkValidationFailure(foundValidationFailure);
+  resetOutputArrays(&hostArrays);
+
+  // Each of the three examples below free d_negSquare outside the graph. As
+  // demonstrated by validateGPU, d_negSquare can be accessed by outside the
+  // graph before d_negSquare is freed.
+
+  printf("Running negateSquares with d_negSquare freed outside the stream.\n");
+  createNegateSquaresGraphExplicitly(&graphExec, device, &hostArrays,
+                                     &d_negSquare);
+  HIPCHECK(hipGraphLaunch(graphExec, stream));
+  validateGPU<<<hostArrays.numBlocks, THREADS_PER_BLOCK, 0, stream>>>(
+      d_negSquare, deviceRefArrays, foundValidationFailure);
+  // Since hipFree is synchronous, the stream must synchronize before freeing
+  // d_negSquare to ensure d_negSquare no longer being accessed.
+  HIPCHECK(hipStreamSynchronize(stream));
+  HIPCHECK(hipFree(d_negSquare));
+  printf(
+      "Validating negateSquares with d_negSquare freed outside the "
+      "stream...\n");
+  validateHost(&hostArrays, foundValidationFailure);
+  checkValidationFailure(foundValidationFailure);
+  resetOutputArrays(&hostArrays);
+
+  printf("Running negateSquares with d_negSquare freed outside the graph.\n");
+  HIPCHECK(hipGraphLaunch(graphExec, stream));
+  validateGPU<<<hostArrays.numBlocks, THREADS_PER_BLOCK, 0, stream>>>(
+      d_negSquare, deviceRefArrays, foundValidationFailure);
+  HIPCHECK(hipFreeAsync(d_negSquare, stream));
+  HIPCHECK(hipStreamSynchronize(stream));
+  printf(
+      "Validating negateSquares with d_negSquare freed outside the graph...\n");
+  checkValidationFailure(foundValidationFailure);
+  resetOutputArrays(&hostArrays);
+
+  printf(
+      "Running negateSquares with d_negSquare freed in a different graph.\n");
+  createFreeGraph(&graphExecFreeC, d_negSquare);
+  HIPCHECK(hipGraphLaunch(graphExec, stream));
+  validateGPU<<<hostArrays.numBlocks, THREADS_PER_BLOCK, 0, stream>>>(
+      d_negSquare, deviceRefArrays, foundValidationFailure);
+  HIPCHECK(hipGraphLaunch(graphExecFreeC, stream));
+  HIPCHECK(hipStreamSynchronize(stream));
+  printf(
+      "Validating negateSquares with d_negSquare freed in a different "
+      "graph...\n");
+  checkValidationFailure(foundValidationFailure);
+
+  printf("Cleaning up sample.\n");
+  HIPCHECK(hipGraphExecDestroy(graphExec));
+  HIPCHECK(hipGraphExecDestroy(graphExecFreeC));
+  HIPCHECK(hipStreamDestroy(stream));
+  HIPCHECK(hipFree(foundValidationFailure));
+  HIPCHECK(hipFree(deviceRefArrays.negSquare));
+  free(hostArrays.input);
+  free(hostArrays.square);
+  free(hostArrays.negSquare);
+  printf("Cleanup complete. Exiting sample.\n");
+}aph...\n");
+  checkValidationFailure(foundValidationFailure);
+
+  printf("Cleaning up sample.\n");
+  checkCudaErrors(hipGraphExecDestroy(graphExec));
+  checkCudaErrors(hipGraphExecDestroy(graphExecFreeC));
+  checkCudaErrors(hipStreamDestroy(stream));
+  checkCudaErrors(hipFree(foundValidationFailure));
+  checkCudaErrors(hipFree(deviceRefArrays.negSquare));
+  free(hostArrays.input);
+  free(hostArrays.square);
+  free(hostArrays.negSquare);
+  printf("Cleanup complete. Exiting sample.\n");
+}
\ No newline at end of file
diff --git a/src/samples/Samples/3_CUDA_Features/immaTensorCoreGemm/immaTensorCoreGemm.cu.hip b/src/samples/Samples/3_CUDA_Features/immaTensorCoreGemm/immaTensorCoreGemm.cu.hip
index e69de29..0ded558 100755
--- a/src/samples/Samples/3_CUDA_Features/immaTensorCoreGemm/immaTensorCoreGemm.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/immaTensorCoreGemm/immaTensorCoreGemm.cu.hip
@@ -0,0 +1,662 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// CUDA sample demonstrating a integer GEMM computation using the Warp Matrix
+// Multiply and Accumulate API.
+
+// In this program, the compute_gemm kernel computes the result of a matrix
+// multiplication and addition: D = alpha * A * B + beta * C. The dimensions of
+// both C and D matrices are M_GLOBAL x N_GLOBAL. The A matrix is M_GLOBAL x
+// K_GLOBAL (row-major), the B matrix is K_GLOBAL x N_GLOBAL (column-major). In
+// that kernel, each CTA computes one 128 x 128 tile of the resulting matrix per
+// iteration. When the tile is computed, the CTA stores it to the global memory
+// and begins a new iteration, selecting a new 128 x 128 tile to compute.
+// Each CTA consists of eight warps. For the 128 x 128 tile, each warp computes
+// eight 16 x 16 subtiles, organized in a 2 x 4 two-dimensional array. Warps
+// compute the 16 x 16 subtiles using nvcuda::wmma::mma_sync operations by
+// moving through the K_GLOBAL dimension of the A and B matrices and
+// accumulating the intermediate result in the local thread state.
+
+// There are a number of simple optimizations used in the algorithm:
+// - The CTA copies the 128 x 128 tile of the C matrix from the global memory to
+//   shared memory. After that is done, each warp loads the C matrix fragments
+//   from shared memory, thus avoiding a random global memory access.
+// - On each internal iteration, the CTA copies a portion of the A and B
+// matrices from
+//   global memory to shared memory. After that, all warps in the CTA reuse the
+//   A and B data from shared memory, thus reducing the number of data copies
+//   from global memory.
+// - The portions of the A and B matrices are stored in shared memory with an
+// additional
+//   padding (skew) to reduce the number of shared memory access bank conflicts.
+//   (See a detailed explanation near the SKEW_HALF macro definition.)
+// - When the CTA finishes computing the tiles of the resulting matrix, each
+// warp stores
+//   its subtiles to shared memory. The CTA then copies the shared memory
+//   contents to global memory, again avoiding redundant random global memory
+//   accesses.
+// - Note that the CTA tile size is chosen to maximize the GPU register
+// utilization,
+//   but carefully enough to avoid local memory use.
+
+#include <assert.h>
+#include <hip/hip_runtime.h>
+#include <mma.h>
+#include <stdio.h>
+
+// helper functions and utilities to work with CUDA
+#include "helper_cuda_hipified.h"
+#include "helper_functions.h"
+
+// Externally configurable parameters.
+
+#ifndef CPU_DEBUG
+// Set this to 1 to verify the correctness of the GPU-computed matrix.
+#define CPU_DEBUG 0
+#endif
+
+#ifndef SHARED_MEMORY_LIMIT_64K
+// Set this to 0 to use more than 64 Kb of shared memory to cache data, to
+// improve the performance of the computations on GPU.
+// Note that you need a GPU that can have more than 64 Kb of shared memory
+// per multiprocessor.
+#define SHARED_MEMORY_LIMIT_64K 1
+#endif
+
+// GPU configuration.
+
+#define WARP_SIZE 32
+
+// MMA matrix tile dimensions.
+
+#define M 16
+#define N 16
+#define K 16
+
+#define WMMA_M 16
+#define WMMA_N 16
+#define WMMA_K 16
+
+// GEMM configuration.
+
+#define M_TILES 256
+#define N_TILES 256
+#define K_TILES 256
+
+#define M_GLOBAL (M * M_TILES)
+#define N_GLOBAL (N * N_TILES)
+#define K_GLOBAL (K * K_TILES)
+
+#define C_LAYOUT wmma::mem_row_major
+
+// Implementation constants.
+
+#define WARPS_PER_BLOCK 8
+#define THREADS_PER_BLOCK (WARP_SIZE * WARPS_PER_BLOCK)
+
+#if SHARED_MEMORY_LIMIT_64K
+// With only 64 Kb shared memory available, we can fit two 8-tile chunks of
+// the A and B matrix data, that are 16 * 16 * 8 * 8 * 2 = 32 Kb each
+// (i.e. two 8x8 arrays of tiles of 16x16 uint8_t-typed elements per CTA).
+// But we cannot account the 8 Kb total skew overhead, without which the
+// performance would be severely impacted. So we choose to reduce the chunk size
+// in half, i.e. the amount of A and B matrix data we cache in shared memory.
+// Accordingly, this doubles the number of outer iterations across the global K
+// dimension, which only slightly impacts the performance.
+#define CHUNK_K 8
+#else
+#define CHUNK_K 16
+#endif
+
+#define CHUNK_LINE_BYTES (CHUNK_K * K * sizeof(uint8_t))
+#define WARP_COPY_BYTES (WARP_SIZE * sizeof(int4))
+#define CHUNK_COPY_LINES_PER_WARP (WARP_COPY_BYTES / CHUNK_LINE_BYTES)
+#define CHUNK_COPY_LINE_LANES (WARP_SIZE / CHUNK_COPY_LINES_PER_WARP)
+
+#define BLOCK_ROW_WARPS 2
+#define BLOCK_COL_WARPS 4
+
+#define WARP_ROW_TILES 4
+#define WARP_COL_TILES 2
+
+#define BLOCK_ROW_TILES (WARP_ROW_TILES * BLOCK_ROW_WARPS)
+#define BLOCK_COL_TILES (WARP_COL_TILES * BLOCK_COL_WARPS)
+
+#define GLOBAL_MEM_STRIDE N_GLOBAL
+
+#define SHMEM_STRIDE (N * BLOCK_ROW_TILES)
+#define SHMEM_OFFSET (N * WARP_ROW_TILES)
+
+// The macro below is used to shift rows of the A matrix and columns of the B
+// matrix in shared memory to minimize possible bank conflicts. Before
+// performing the nvcuda::wmma::mma_sync operation, the warp must load the
+// matrix data using the nvcuda::wmma::load_matrix_sync operation. Although the
+// memory access pattern is not specified for that function, each lane in the
+// warp can read one or multiple matrix elements from different matrix rows or
+// columns. For shared memory, such access can result in bank conflicts if
+// different rows / columns of the matrix map to the same bank. By shifting each
+// row and column by a few bytes, we make sure that they map to different banks,
+// thus reducing the number of possible bank conflicts. The number of 32
+// one-byte "uint8_t" elements is chosen as the minimum possible shift because
+// we must keep each row and column 256-bit aligned, as required by
+// nvcuda::wmma::load_matrix_sync.
+#define SKEW_UINT8 32
+
+#define checkKernelErrors(expr)                             \
+  do {                                                      \
+    expr;                                                   \
+                                                            \
+    hipError_t __err = hipGetLastError();                 \
+    if (__err != hipSuccess) {                             \
+      printf("Line %d: '%s' failed: %s\n", __LINE__, #expr, \
+             hipGetErrorString(__err));                    \
+      abort();                                              \
+    }                                                       \
+  } while (0)
+
+using namespace nvcuda;
+
+__host__ void init_host_matrices(uint8_t *a, uint8_t *b, int *c) {
+  for (int i = 0; i < M_GLOBAL; i++) {
+    for (int j = 0; j < K_GLOBAL; j++) {
+      a[i * K_GLOBAL + j] = (uint8_t)(rand() % 3);
+    }
+  }
+
+  for (int i = 0; i < N_GLOBAL; i++) {
+    for (int j = 0; j < K_GLOBAL; j++) {
+      b[i * K_GLOBAL + j] = (uint8_t)(rand() % 3);
+    }
+  }
+
+  for (int t = 0; t < M_GLOBAL * N_GLOBAL; t++) {
+    c[t] = (rand() % 3);
+  }
+}
+
+__global__ void compute_gemm_imma(const uint8_t *A, const uint8_t *B,
+                                  const int *C, int *D, int alpha, int beta) {
+  extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];
+
+  // Warp and lane identification.
+  const unsigned int warpId = threadIdx.x / WARP_SIZE;
+  const unsigned int laneId = threadIdx.x % WARP_SIZE;
+
+  // Offset in shared memory from which the B matrix is stored.
+  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;
+
+  // This pointer is used to access the C and D matrix tiles this warp computes.
+  int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +
+                             (warpId / 2) * SHMEM_STRIDE * K * 2 +
+                             (warpId % 2) * SHMEM_OFFSET;
+
+  // This pointer is used to stream the C and D matrices block-wide tile to and
+  // from shared memory.
+  int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + warpId * SHMEM_STRIDE * K;
+
+  // Adjust the beta scaler, as it'll be multiplied by alpha at the end of
+  // each tile computation. Technically this is not generally correct (may
+  // result in a loss of precision). Zero still needs to be specially handled
+  // though.
+  beta /= alpha;
+
+  // Each CTA slides along the 128 x 128 tiles from the top left corner of the
+  // matrix to the right and down, and selects the next tile to compute. Once
+  // there's no such tile, all warps in this CTA exit.
+  for (unsigned int block_pos = blockIdx.x;; block_pos += gridDim.x) {
+    const unsigned int block_tile_i =
+        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);
+    const unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % N_TILES;
+
+    // Stop when there are no more D matrix tiles to compute in this CTA.
+    if (block_tile_i >= M_TILES) {
+      break;
+    }
+
+    // This warp's pointer to the C matrix data to copy memory from to shared
+    // memory.
+    const size_t gmem_idx =
+        (block_tile_i + warpId) * M * GLOBAL_MEM_STRIDE + block_tile_j * N;
+    const int *src_gmem_warp_stream_ptr = &C[gmem_idx];
+
+    // Stream multiple C tiles to shared memory.
+#pragma unroll
+    for (int i = 0; i < K; i++) {
+      typedef int4 copy_t;
+
+      *((copy_t *)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId) =
+          *((copy_t *)(src_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) +
+            laneId);
+    }
+
+    __syncthreads();
+
+    // These fragments will accumulate the result of A and B matrix fragment
+    // multiplications along the K_GLOBAL dimension.
+    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]
+                                                     [WARP_ROW_TILES];
+
+    // Load the C matrix tiles into fragments from shared memory.
+#pragma unroll
+    for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+      for (int j = 0; j < WARP_ROW_TILES; j++) {
+        const int *tile_ptr =
+            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;
+
+        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, C_LAYOUT);
+      }
+    }
+
+    __syncthreads();
+
+    // Scale the C matrix.
+#pragma unroll
+    for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+      for (int j = 0; j < WARP_ROW_TILES; j++) {
+#pragma unroll
+        for (int t = 0; t < c[i][j].num_elements; t++) {
+          c[i][j].x[t] *= beta;
+        }
+      }
+    }
+
+    // Select what warp copies what matrix to shared memory.
+    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.
+    const uint8_t *warp_ptr = (warpId < 4) ? (&A[block_tile_i * M * K_GLOBAL] +
+                                              M * K_GLOBAL * (warpId % 4) * 2)
+                                           : (&B[block_tile_j * N * K_GLOBAL] +
+                                              N * K_GLOBAL * (warpId % 4) * 2);
+
+    // Go through the global K dimension by a fixed step at a time.
+#pragma unroll
+    for (int tile_k = 0; tile_k < K_TILES; tile_k += CHUNK_K) {
+      // Copy slices of the A and B matrices to shared memory.
+      // The first half of the warps in the CTA copy the A matrix, the rest copy
+      // the B matrix.
+      size_t shmem_idx =
+          warpId < (WARPS_PER_BLOCK / 2)
+              ? (M * (warpId % (WARPS_PER_BLOCK / 2)) * 2)
+              : (N * (warpId % (WARPS_PER_BLOCK / 2)) * 2 + shmem_idx_b_off);
+
+      // First half of the warp copies the first row / column of the matrix,
+      // the second half of the warp copies the next.
+      int4 *lane_ptr = (int4 *)(warp_ptr + tile_k * K +
+                                (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +
+                       (laneId % CHUNK_COPY_LINE_LANES);
+
+      // Shift the second half of the warp to the next row / column in the
+      // shared memory.
+      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;
+
+#pragma unroll
+      for (int i = 0; i < ((WARP_SIZE / 2) / CHUNK_COPY_LINES_PER_WARP) * 2;
+           i++) {
+        // Copy 16 bytes at once in each lane.
+        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =
+            *lane_ptr;
+
+        // Advance the global memory pointer and the shared memory index.
+        lane_ptr = (int4 *)((uint8_t *)lane_ptr +
+                            K_GLOBAL * CHUNK_COPY_LINES_PER_WARP);
+        shmem_idx += CHUNK_COPY_LINES_PER_WARP;
+      }
+
+      __syncthreads();
+
+      // Compute a grid of C matrix tiles in each warp.
+#pragma unroll
+      for (int k_step = 0; k_step < CHUNK_K; k_step++) {
+        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>
+            a[WARP_COL_TILES];
+        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>
+            b[WARP_ROW_TILES];
+
+#pragma unroll
+        for (int i = 0; i < WARP_COL_TILES; i++) {
+          size_t shmem_idx_a = (warpId / 2) * M * 2 + (i * M);
+          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];
+
+          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);
+
+#pragma unroll
+          for (int j = 0; j < WARP_ROW_TILES; j++) {
+            if (i == 0) {
+              // Load the B matrix fragment once, because it is going to be
+              // reused against the other A matrix fragments.
+              size_t shmem_idx_b = shmem_idx_b_off +
+                                   (WARP_ROW_TILES * N) * (warpId % 2) +
+                                   (j * N);
+              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];
+
+              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);
+            }
+
+            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);
+          }
+        }
+      }
+
+      __syncthreads();
+    }
+
+      // Store the D fragments to shared memory.
+#pragma unroll
+    for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+      for (int j = 0; j < WARP_ROW_TILES; j++) {
+#pragma unroll
+        // Uniform, point-wise transformations of ALL fragment elements by ALL
+        // threads in the warp are well-defined even though element indices
+        // within fragment storage are not defined.
+        for (int t = 0; t < c[i][j].num_elements; t++) c[i][j].x[t] *= alpha;
+
+        int *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;
+
+        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);
+      }
+    }
+
+    __syncthreads();
+
+    // Now that shared memory contains all the D tiles, stream them to global
+    // memory.
+    int *dst_gmem_warp_stream_ptr = &D[gmem_idx];
+
+#pragma unroll
+    for (int i = 0; i < K; i++) {
+      *((int4 *)(dst_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) + laneId) =
+          *((int4 *)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId);
+    }
+
+    __syncthreads();
+  }
+}
+
+// Performs an MxNxK GEMM (C=alpha*A*B + beta*C) assuming:
+//  1) Matrices are packed in memory.
+//  2) M, N and K are multiples of 16.
+//  3) Neither A nor B are transposed.
+// Note: This is a less performant version of the compute_gemm_imma kernel. It
+// is designed for
+//       demonstration purposes only to show the CUDA WMMA API use without
+//       relying on availability of the shared memory.
+__global__ void simple_wmma_gemm_imma(const uint8_t *a, const uint8_t *b,
+                                      const int *c, int *d, int m_ld, int n_ld,
+                                      int k_ld, int alpha, int beta) {
+  // Leading dimensions. Packed with no transpositions.
+  int lda = m_ld;
+  int ldb = k_ld;
+  int ldc = n_ld;
+
+  // Tile using a 2D grid
+  int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / warpSize;
+  int warpN = (blockIdx.y * blockDim.y + threadIdx.y);
+
+  // Declare the fragments
+  wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, uint8_t,
+                 wmma::row_major>
+      a_frag;
+  wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, uint8_t,
+                 wmma::col_major>
+      b_frag;
+  wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, int> acc_frag;
+  wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, int> c_frag;
+
+  wmma::fill_fragment(acc_frag, 0.0f);
+
+  // Loop over k
+  for (int i = 0; i < k_ld; i += WMMA_K) {
+    int aCol = i;
+    int aRow = warpM * WMMA_M;
+
+    int bCol = i;
+    int bRow = warpN * WMMA_N;
+
+    // Bounds checking
+    if (aRow < m_ld && aCol < k_ld && bRow < k_ld && bCol < n_ld) {
+      // Load the inputs
+      wmma::load_matrix_sync(a_frag, a + aCol + aRow * lda, lda);
+      wmma::load_matrix_sync(b_frag, b + bCol + bRow * ldb, ldb);
+
+      // Perform the matrix multiplication
+      wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag);
+    }
+  }
+
+  // Load in the current value of c, scale it by beta, and add this our result
+  // scaled by alpha
+  int cCol = warpN * WMMA_N;
+  int cRow = warpM * WMMA_M;
+
+  if (cRow < m_ld && cCol < n_ld) {
+    wmma::load_matrix_sync(c_frag, c + cCol + cRow * ldc, ldc,
+                           wmma::mem_row_major);
+
+    for (int i = 0; i < c_frag.num_elements; i++) {
+      c_frag.x[i] = alpha * acc_frag.x[i] + beta * c_frag.x[i];
+    }
+
+    // Store the output
+    wmma::store_matrix_sync(d + cCol + cRow * ldc, c_frag, ldc,
+                            wmma::mem_row_major);
+  }
+}
+
+__host__ void matMultiplyOnHost(uint8_t *A, uint8_t *B, int *C, int alpha,
+                                int beta, int numARows, int numAColumns,
+                                int numBRows, int numBColumns, int numCRows,
+                                int numCColumns) {
+  for (int i = 0; i < numCRows; i++) {
+    for (int j = 0; j < numCColumns; j++) {
+      int temp = 0;
+
+      for (int k = 0; k < numAColumns; k++) {
+        temp += A[i * numAColumns + k] * B[j * numBRows + k];
+      }
+
+      C[i * numCColumns + j] = temp * alpha + beta * C[i * numCColumns + j];
+    }
+  }
+}
+
+int main(int argc, char **argv) {
+  printf("Initializing...\n");
+
+  int dev = findCudaDevice(argc, (const char **)argv);
+
+  hipDeviceProp_t deviceProp;
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, dev));
+
+  // Tensor cores require a GPU of Volta (SM72) architecture or higher.
+  if (deviceProp.major < 7 || (deviceProp.major <= 7 && deviceProp.minor < 2)) {
+    printf(
+        "immaTensorCoreGemm requires SM 7.2 or higher to use Tensor Cores.  "
+        "Exiting...\n");
+    exit(EXIT_WAIVED);
+  }
+
+  printf("M: %d (%d x %d)\n", M_GLOBAL, M, M_TILES);
+  printf("N: %d (%d x %d)\n", N_GLOBAL, N, N_TILES);
+  printf("K: %d (%d x %d)\n", K_GLOBAL, K, K_TILES);
+
+  uint8_t *A_h = NULL;
+  uint8_t *B_h = NULL;
+  int *C_h = NULL;
+#if CPU_DEBUG
+  int *result_hD = NULL;
+  int *result_host = NULL;
+#endif
+
+  A_h = (uint8_t *)malloc(sizeof(uint8_t) * M_GLOBAL * K_GLOBAL);
+  B_h = (uint8_t *)malloc(sizeof(uint8_t) * K_GLOBAL * N_GLOBAL);
+  C_h = (int *)malloc(sizeof(int) * M_GLOBAL * N_GLOBAL);
+#if CPU_DEBUG
+  result_hD = (int *)malloc(sizeof(int) * M_GLOBAL * N_GLOBAL);
+  result_host = (int *)malloc(sizeof(int) * M_GLOBAL * N_GLOBAL);
+#endif
+
+  uint8_t *A = NULL;
+  uint8_t *B = NULL;
+  int *C = NULL;
+  int *D = NULL;
+
+  HIPCHECK(
+      hipMalloc(reinterpret_cast<void **>(&A), sizeof(uint8_t) * M_GLOBAL * K_GLOBAL));
+  HIPCHECK(
+      hipMalloc(reinterpret_cast<void **>(&B), sizeof(uint8_t) * N_GLOBAL * K_GLOBAL));
+  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&C), sizeof(int) * M_GLOBAL * N_GLOBAL));
+  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&D), sizeof(int) * M_GLOBAL * N_GLOBAL));
+
+  assert(((unsigned long long)A) % 128 == 0);
+  assert(((unsigned long long)B) % 128 == 0);
+  assert(((unsigned long long)C) % 128 == 0);
+  assert(((unsigned long long)D) % 128 == 0);
+
+  init_host_matrices(A_h, B_h, C_h);
+
+  HIPCHECK(hipMemcpy(A, A_h, sizeof(uint8_t) * M_GLOBAL * K_GLOBAL,
+                             hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(B, B_h, sizeof(uint8_t) * N_GLOBAL * K_GLOBAL,
+                             hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(C, C_h, sizeof(int) * M_GLOBAL * N_GLOBAL,
+                             hipMemcpyHostToDevice));
+  HIPCHECK(hipMemset(D, 0, sizeof(int) * M_GLOBAL * N_GLOBAL));
+
+  printf("Preparing data for GPU...\n");
+
+  assert(((unsigned long long)A) % 128 == 0);
+  assert(((unsigned long long)B) % 128 == 0);
+  assert(((unsigned long long)C) % 128 == 0);
+  assert(((unsigned long long)D) % 128 == 0);
+
+  enum {
+    // Compute the right amount of shared memory to request.
+    // We need shared memory to hold per-CTA C and D matrix tiles, and to cache
+    // per-CTA chunks
+    // of the A and B matrices. Therefore, the right amount to request is the
+    // maximum of those
+    // two numbers.
+    SHMEM_SZ = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M) *
+                       (CHUNK_K * K + SKEW_UINT8) * 2,
+                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *
+                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int))
+  };
+
+  printf("Required shared memory size: %lu Kb\n", SHMEM_SZ / 1024UL);
+
+  int alpha = 1;
+  int beta = 1;
+
+  hipEvent_t start, stop;
+
+  HIPCHECK(hipEventCreate(&start));
+  HIPCHECK(hipEventCreate(&stop));
+  HIPCHECK(hipEventRecord(start));
+
+  // If enough shared memory available on the GPU use high performant kernel
+  if (deviceProp.sharedMemPerMultiprocessor >= SHMEM_SZ) {
+    printf("Computing... using high performance kernel compute_gemm_imma \n");
+
+    HIPCHECK(hipFuncSetAttribute(
+        compute_gemm_imma, hipFuncAttributeMaxDynamicSharedMemorySize,
+        SHMEM_SZ));
+    checkKernelErrors(
+        (compute_gemm_imma<<<deviceProp.multiProcessorCount, THREADS_PER_BLOCK,
+                             SHMEM_SZ>>>(A, B, C, D, alpha, beta)));
+#if CPU_DEBUG
+    HIPCHECK(hipMemcpy(result_hD, D, sizeof(int) * M_GLOBAL * N_GLOBAL,
+                               hipMemcpyDeviceToHost));
+#endif
+  } else {
+    dim3 gridDim;
+    dim3 blockDim;
+
+    // blockDim.x must be a multiple of warpSize
+    // 128x4 means we have 16 warps and a block computes a 64x64 output tile
+    blockDim.x = 128;
+    blockDim.y = 4;
+
+    gridDim.x = (M_GLOBAL + (WMMA_M * blockDim.x / 32 - 1)) /
+                (WMMA_M * blockDim.x / 32);
+    gridDim.y = (N_GLOBAL + WMMA_N * blockDim.y - 1) / (WMMA_N * blockDim.y);
+
+    printf("Computing... using simple_wmma_gemm_imma kernel\n");
+    simple_wmma_gemm_imma<<<gridDim, blockDim>>>(A, B, C, D, M_GLOBAL, N_GLOBAL,
+                                                 K_GLOBAL, alpha, beta);
+#if CPU_DEBUG
+    HIPCHECK(hipMemcpy(result_hD, D, sizeof(int) * M_GLOBAL * N_GLOBAL,
+                               hipMemcpyDeviceToHost));
+#endif
+  }
+
+  HIPCHECK(hipEventRecord(stop));
+  HIPCHECK(hipEventSynchronize(stop));
+
+#if CPU_DEBUG
+  printf("Verifying correctness of the computations...\n");
+
+  memcpy(result_host, C_h, sizeof(int) * M_GLOBAL * N_GLOBAL);
+
+  matMultiplyOnHost(A_h, B_h, result_host, alpha, beta, M_GLOBAL, K_GLOBAL,
+                    K_GLOBAL, N_GLOBAL, M_GLOBAL, N_GLOBAL);
+
+  for (int i = 0; i < N_GLOBAL * M_GLOBAL; i++) {
+    if (abs(result_hD[i] - result_host[i]) > 0) {
+      printf("mismatch i=%d result_hD=%d result_host=%d\n", i, result_hD[i],
+             result_host[i]);
+    }
+  }
+  free(result_host);
+  free(result_hD);
+#endif
+
+  float milliseconds = 0;
+
+  HIPCHECK(hipEventElapsedTime(&milliseconds, start, stop));
+
+    printf("Time: %f ms\n", milliseconds);
+    printf("TOPS: %.2f\n", (((double)M_GLOBAL * N_GLOBAL * K_GLOBAL * 2)/(milliseconds/1000.)) / 1e12);
+
+  free(A_h);
+  free(B_h);
+  free(C_h);
+  HIPCHECK(hipFree(reinterpret_cast<void *>(A)));
+  HIPCHECK(hipFree(reinterpret_cast<void *>(B)));
+  HIPCHECK(hipFree(reinterpret_cast<void *>(C)));
+  HIPCHECK(hipFree(reinterpret_cast<void *>(D)));
+
+  return EXIT_SUCCESS;
+}
+void *>(B)));
+  checkCudaErrors(hipFree(reinterpret_cast<void *>(C)));
+  checkCudaErrors(hipFree(reinterpret_cast<void *>(D)));
+
+  return EXIT_SUCCESS;
+}
diff --git a/src/samples/Samples/3_CUDA_Features/memMapIPCDrv/memMapIpc_kernel.cu.hip b/src/samples/Samples/3_CUDA_Features/memMapIPCDrv/memMapIpc_kernel.cu.hip
index e69de29..41b1784 100755
--- a/src/samples/Samples/3_CUDA_Features/memMapIPCDrv/memMapIpc_kernel.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/memMapIPCDrv/memMapIpc_kernel.cu.hip
@@ -0,0 +1,38 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+
+// Device code
+extern "C" __global__ void memMapIpc_kernel(char *ptr, int sz, char val)
+{
+    // Dummy kernel
+    int idx = blockIdx.x * blockDim.x + threadIdx.x;
+    for (; idx < sz; idx += (gridDim.x * blockDim.x)) {
+        ptr[idx] = val;
+    }
+}
diff --git a/src/samples/Samples/3_CUDA_Features/ptxjit/ptxjit_kernel.cu.hip b/src/samples/Samples/3_CUDA_Features/ptxjit/ptxjit_kernel.cu.hip
index e69de29..566beb4 100755
--- a/src/samples/Samples/3_CUDA_Features/ptxjit/ptxjit_kernel.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/ptxjit/ptxjit_kernel.cu.hip
@@ -0,0 +1,36 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * Simple kernel for ptxjit demonstration.
+ *
+ */
+extern "C" __global__ void myKernel(int *data) {
+  int tid = blockIdx.x * blockDim.x + threadIdx.x;
+  data[tid] = tid;
+}
diff --git a/src/samples/Samples/3_CUDA_Features/tf32TensorCoreGemm/tf32TensorCoreGemm.cu.hip b/src/samples/Samples/3_CUDA_Features/tf32TensorCoreGemm/tf32TensorCoreGemm.cu.hip
index e69de29..f145adf 100755
--- a/src/samples/Samples/3_CUDA_Features/tf32TensorCoreGemm/tf32TensorCoreGemm.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/tf32TensorCoreGemm/tf32TensorCoreGemm.cu.hip
@@ -0,0 +1,845 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// CUDA sample demonstrating a tf32 (E8M10) GEMM computation using the Warp Matrix Multiply
+// and Accumulate API introduced in CUDA 11.0.
+
+// In this program, the compute_gemm kernel computes the result of a matrix multiplication
+// and addition: D = alpha * A * B + beta * C. The dimensions of both C and D matrices
+// are M_GLOBAL x N_GLOBAL. The A matrix is M_GLOBAL x K_GLOBAL (row-major), the B matrix
+// is K_GLOBAL x N_GLOBAL (column-major).
+// In that kernel, each CTA computes one 128 x 128 tile of the resulting matrix
+// per iteration. When the tile is computed, the CTA stores it to the global memory
+// and begins a new iteration, selecting a new 128 x 128 tile to compute.
+// Each CTA consists of eight warps. For the 128 x 128 tile, each warp computes eight
+// 16 x 16 subtiles, organized in a 2 x 4 two-dimensional array.
+// Warps compute the 16 x 16 subtiles using nvcuda::wmma::mma_sync operations by
+// moving through the K_GLOBAL dimension of the A and B matrices and accumulating
+// the intermediate result in the local thread state.
+
+// There are a number of simple optimizations used in the algorithm:
+// - The CTA copies the 128 x 128 tile of the C matrix from the global memory to
+//   shared memory. After that is done, each warp loads the C matrix fragments from
+//   shared memory, thus avoiding a random global memory access.
+// - On each internal iteration, the CTA copies a portion of the A and B matrices from
+//   global memory to shared memory. After that, all warps in the CTA reuse the A and B
+//   data from shared memory, thus reducing the number of data copies from global memory.
+// - The portions of the A and B matrices are stored in shared memory with an additional
+//   padding (skew) to reduce the number of shared memory access bank conflicts.
+//   (See a detailed explanation near the SKEW_FLOAT macro definition.)
+// - When the CTA finishes computing the tiles of the resulting matrix, each warp stores
+//   its subtiles to shared memory. The CTA then copies the shared memory contents to
+//   global memory, again avoiding redundant random global memory accesses.
+// - Note that the CTA tile size is chosen to maximize the GPU register utilization,
+//   but carefully enough to avoid local memory use.
+
+#include <assert.h>
+#include <stdio.h>
+#include <hip/hip_runtime.h>
+#include <mma.h>
+#include <cuda/pipeline>
+
+// helper functions and utilities to work with CUDA
+#include "helper_functions.h"
+#include "helper_cuda_hipified.h"
+
+// Externally configurable parameters.
+
+#ifndef CPU_DEBUG
+// Set this to 1 to verify the correctness of the GPU-computed matrix.
+#define CPU_DEBUG 0
+#endif
+
+#ifndef SHARED_MEMORY_LIMIT_64K
+// Set this to 0 to use more than 64 Kb of shared memory to cache data, to
+// improve the performance of the computations on GPU.
+// Note that you need a GPU that can have more than 64 Kb of shared memory
+// per multiprocessor.
+#define SHARED_MEMORY_LIMIT_64K 0
+#endif
+
+// GPU configuration.
+
+#define WARP_SIZE 32
+
+// MMA matrix tile dimensions.
+
+#define M 16
+#define N 16
+#define K 8
+
+// GEMM configuration.
+
+#define M_TILES 512
+#define N_TILES 512
+#define K_TILES 512
+
+#define M_GLOBAL (M * M_TILES)
+#define N_GLOBAL (N * N_TILES)
+#define K_GLOBAL (K * K_TILES)
+
+#define C_LAYOUT wmma::mem_row_major
+
+// Implementation constants.
+
+#define WARPS_PER_BLOCK 8
+#define THREADS_PER_BLOCK (WARP_SIZE * WARPS_PER_BLOCK)
+
+#if SHARED_MEMORY_LIMIT_64K
+// With only 64 Kb shared memory available, we can fit two 8-tile chunks of
+// the A and B matrix data, that is (M = 16) * (K = 8) * 8 * (CHUNK_K = 8)
+// * sizeof(float) = 32 Kb each.
+// (i.e. two 8x8 arrays of tiles of 16x8 float-typed elements per CTA).
+// But we cannot account the 8 Kb total skew overhead, without which the performance
+// would be severely impacted. So we choose to reduce the chunk size in half,
+// i.e. the amount of A and B matrix data we cache in shared memory.
+// Accordingly, this doubles the number of outer iterations across the global K
+// dimension, which only slightly impacts the performance.
+#define CHUNK_K 4
+#else
+#define CHUNK_K 8
+#endif
+
+#define CHUNK_LINE_BYTES (CHUNK_K * K * sizeof(float))
+#define WARP_COPY_BYTES (WARP_SIZE * sizeof(int4))
+#define CHUNK_COPY_LINES_PER_WARP (WARP_COPY_BYTES / CHUNK_LINE_BYTES)
+#define CHUNK_COPY_LINE_LANES (WARP_SIZE / CHUNK_COPY_LINES_PER_WARP)
+
+#define BLOCK_ROW_WARPS 2
+#define BLOCK_COL_WARPS 4
+
+#define WARP_ROW_TILES 4
+#define WARP_COL_TILES 2
+
+#define BLOCK_ROW_TILES (WARP_ROW_TILES * BLOCK_ROW_WARPS)
+#define BLOCK_COL_TILES (WARP_COL_TILES * BLOCK_COL_WARPS)
+
+#define GLOBAL_MEM_STRIDE N_GLOBAL
+
+#define SHMEM_STRIDE (N * BLOCK_ROW_TILES)
+#define SHMEM_OFFSET (N * WARP_ROW_TILES)
+
+// The macro below is used to shift rows of the A matrix and columns of the B matrix
+// in shared memory to minimize possible bank conflicts.
+// Before performing the nvcuda::wmma::mma_sync operation, the warp must load the matrix
+// data using the nvcuda::wmma::load_matrix_sync operation. Although the memory access pattern
+// is not specified for that function, each lane in the warp can read one or multiple matrix
+// elements from different matrix rows or columns.
+// For shared memory, such access can result in bank conflicts if different rows / columns
+// of the matrix map to the same bank. By shifting each row and column by a few bytes, we
+// make sure that they map to different banks, thus reducing the number of possible bank
+// conflicts.
+// The number of 8 four-byte "float" elements is chosen as the minimum possible shift because
+// we must keep each row and column 256-bit aligned, as required by nvcuda::wmma::load_matrix_sync.
+#define SKEW_FLOAT 8
+
+#define checkKernelErrors(expr) do {                                                        \
+    expr;                                                                                   \
+                                                                                            \
+    hipError_t __err = hipGetLastError();                                                 \
+    if (__err != hipSuccess) {                                                             \
+        printf("Line %d: '%s' failed: %s\n", __LINE__, # expr, hipGetErrorString(__err));  \
+        abort();                                                                            \
+    }                                                                                       \
+} while(0)
+
+enum kernels
+{
+    tf32mma_shmem_gemm_async_copy  = 0, // tf32 MMA shmem using kernel with async_copy 
+    tf32mma_shmem_gemm             = 1, // tf32 MMA shmem using kernel normal copy (without async_copy).
+    simple_tf32mma_gemm            = 2  // tf32 MMA non-shmem using simple kernel.
+};
+
+const char* kernelNames[] = {"compute_tf32gemm_async_copy", "compute_tf32gemm", 
+                            "simple_wmma_tf32gemm"};
+
+using namespace nvcuda;
+
+__host__ void init_host_matrices(float *a, float *b, float *c)
+{
+    for (int i = 0; i < M_GLOBAL; i++) {
+        for (int j = 0; j < K_GLOBAL; j++) {
+            a[i*K_GLOBAL+j] = (float)(rand() % 3);
+        }
+    }
+
+    for (int i = 0; i < N_GLOBAL; i++) {
+        for (int j = 0; j < K_GLOBAL; j++) {
+            b[i*K_GLOBAL+j] = (float)(rand() % 3);
+        }
+    }
+
+    for (int t = 0; t < M_GLOBAL * N_GLOBAL; t++) {
+        c[t] =  (float)(rand() % 3);
+    }
+}
+
+__global__ void compute_tf32gemm(const float *A, const float *B, const float *C, float *D, float alpha, float beta)
+{
+#if __CUDA_ARCH__ >= 800
+    extern __shared__ float shmem[][CHUNK_K * K + SKEW_FLOAT];
+
+    // Warp and lane identification.
+    const unsigned int warpId = threadIdx.x / WARP_SIZE;
+    const unsigned int laneId = threadIdx.x % WARP_SIZE;
+
+    // Offset in shared memory from which the B matrix is stored.
+    const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;
+
+    // This pointer is used to access the C and D matrix tiles this warp computes.
+    float *shmem_warp_tile_ptr = (float*)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * SHMEM_STRIDE * N * BLOCK_ROW_WARPS + (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;
+
+    // This pointer is used to stream the C and D matrices block-wide tile to and from shared memory.
+    float *shmem_warp_stream_ptr = (float*)&shmem[0][0] + warpId * SHMEM_STRIDE * N;
+
+    // Adjust the beta scaler, as it'll be multiplied by alpha at the end of
+    // each tile computation. Technically this is not generally correct (may result
+    // in a loss of precision). Zero still needs to be specially handled though.
+    beta /= alpha;
+
+    // Each CTA slides along the 128 x 128 tiles from the top left corner of the matrix to the
+    // right and down, and selects the next tile to compute. Once there's no such tile,
+    // all warps in this CTA exit.
+    for(unsigned int block_pos = blockIdx.x;; block_pos += gridDim.x) {
+        const unsigned int block_tile_i = ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);
+        const unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % N_TILES;
+
+        // Stop when there are no more D matrix tiles to compute in this CTA.
+        if (block_tile_i >= M_TILES) {
+            break;
+        }
+
+        // This warp's pointer to the C matrix data to copy memory from to shared memory.
+        const size_t gmem_idx = (block_tile_i + warpId) * M * GLOBAL_MEM_STRIDE + block_tile_j * N;
+        const float *src_gmem_warp_stream_ptr = &C[gmem_idx];
+
+        // Stream multiple C tiles to shared memory.
+#pragma unroll
+        for (int i = 0; i < N; i++) {
+            *((int4*)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId) = 
+                *((int4*)(src_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) + laneId);
+        }
+
+        __syncthreads();
+
+        // These fragments will accumulate the result of A and B matrix fragment multiplications
+        // along the K_GLOBAL dimension.
+        wmma::fragment<wmma::accumulator, M, N, K, float> c[WARP_COL_TILES][WARP_ROW_TILES];
+
+        // Load the C matrix tiles into fragments from shared memory.
+#pragma unroll
+        for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+                const float *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * N + j * N;
+
+                wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, C_LAYOUT);
+            }
+        }
+
+        __syncthreads();
+
+        // Scale the C matrix.
+#pragma unroll
+       for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+#pragma unroll
+                for (int t = 0; t < c[i][j].num_elements; t++) {
+                    c[i][j].x[t] *= beta;
+                }
+            }
+        }
+
+        // Select what warp copies what matrix to shared memory.
+        // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.
+        const float *warp_ptr = (warpId < (WARPS_PER_BLOCK/2)) ? (&A[block_tile_i * M * K_GLOBAL] + M * K_GLOBAL * (warpId % (WARPS_PER_BLOCK/2)) * 2) :
+                                              (&B[block_tile_j * N * K_GLOBAL] + N * K_GLOBAL * (warpId % (WARPS_PER_BLOCK/2)) * 2);
+
+        // Go through the global K dimension by a fixed step at a time.
+#pragma unroll
+        for (int tile_k = 0; tile_k < K_TILES; tile_k += CHUNK_K) {
+            // Copy slices of the A and B matrices to shared memory.
+            // The first half of the warps in the CTA copy the A matrix, the rest copy the B matrix.
+            size_t shmem_idx = warpId < (WARPS_PER_BLOCK/2) ? (M * (warpId % (WARPS_PER_BLOCK/2)) * 2) : 
+                                                              (N * (warpId % (WARPS_PER_BLOCK/2)) * 2 + shmem_idx_b_off);
+
+            // First half of the warp copies the first row / column of the matrix,
+            // the second half of the warp copies the next.
+            const float *lane_ptr = (warp_ptr + tile_k * K + (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL);
+
+            // Shift the second half of the warp to the next row / column in the shared memory.
+            shmem_idx += laneId / CHUNK_COPY_LINE_LANES;
+
+#pragma unroll
+            for(int i = 0; i < ((WARP_SIZE/2) / CHUNK_COPY_LINES_PER_WARP) * 2; i++) {
+                // Copy 16 bytes at once in each lane.
+                *((int4*)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) = *((int4*)lane_ptr +  (laneId % CHUNK_COPY_LINE_LANES));
+
+                // Advance the global memory pointer and the shared memory index.
+                lane_ptr = lane_ptr + K_GLOBAL * CHUNK_COPY_LINES_PER_WARP;
+                shmem_idx += CHUNK_COPY_LINES_PER_WARP;
+            }
+
+            __syncthreads();
+
+            // Compute a grid of C matrix tiles in each warp.
+#pragma unroll
+            for (int k_step = 0; k_step < CHUNK_K; k_step++) {
+                wmma::fragment<wmma::matrix_a, M, N, K, wmma::precision::tf32, wmma::row_major> a[WARP_COL_TILES];
+                wmma::fragment<wmma::matrix_b, M, N, K, wmma::precision::tf32, wmma::col_major> b[WARP_ROW_TILES];
+
+#pragma unroll
+                for (int i = 0; i < WARP_COL_TILES; i++) {
+                    size_t shmem_idx_a = (warpId/BLOCK_ROW_WARPS) * M * BLOCK_ROW_WARPS + (i * M);
+                    const float *tile_ptr = &shmem[shmem_idx_a][k_step * K];
+
+                    wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_FLOAT);
+#pragma unroll
+                    for (int t = 0; t < a[i].num_elements; t++) {
+                        a[i].x[t] = wmma::__float_to_tf32(a[i].x[t]);
+                    }
+#pragma unroll
+                    for (int j = 0; j < WARP_ROW_TILES; j++) {
+                        if (i == 0) {
+                            // Load the B matrix fragment once, because it is going to be reused
+                            // against the other A matrix fragments.
+                            size_t shmem_idx_b = shmem_idx_b_off + (WARP_ROW_TILES * N) * (warpId%2) + (j * N);
+                            const float *tile_ptr = &shmem[shmem_idx_b][k_step * K];
+
+                            wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_FLOAT);
+#pragma unroll
+                            for (int t = 0; t < b[j].num_elements; t++) {
+                                b[j].x[t] = wmma::__float_to_tf32(b[j].x[t]);
+                            }
+                        }
+
+                        wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);
+                    }
+                }
+            }
+
+            __syncthreads();
+        }
+
+        // Store the D fragments to shared memory.
+#pragma unroll
+        for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+#pragma unroll
+                // Uniform, point-wise transformations of ALL fragment elements by ALL threads in the
+                // warp are well-defined even though element indices within fragment storage are not defined.
+                for (int t = 0; t < c[i][j].num_elements; t++)
+                    c[i][j].x[t] *= alpha;
+
+                float *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * N + j * N;
+
+                wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);
+            }
+        }
+
+        __syncthreads();
+
+        // Now that shared memory contains all the D tiles, stream them to global memory.
+        float *dst_gmem_warp_stream_ptr = &D[gmem_idx];
+
+#pragma unroll
+        for (int i = 0; i < N; i++) {
+            *((int4*)(dst_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) + laneId) =
+                *((int4*)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId);
+        }
+
+        __syncthreads();
+    }
+#endif
+}
+
+__global__ void compute_tf32gemm_async_copy(const float *A, const float *B, const float *C, float *D, const float alpha, float beta)
+{
+#if __CUDA_ARCH__ >= 800
+    extern __shared__ float shmem[][CHUNK_K * K + SKEW_FLOAT];
+
+    // Warp and lane identification.
+    const unsigned int warpId = threadIdx.x / WARP_SIZE;
+    const unsigned int laneId = threadIdx.x % WARP_SIZE;
+
+    // This pointer is used to access the C and D matrix tiles this warp computes.
+    float *shmem_warp_tile_ptr = (float*)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * SHMEM_STRIDE * N * BLOCK_ROW_WARPS + (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;
+
+    // This pointer is used to stream the C and D matrices block-wide tile to and from shared memory.
+    float *shmem_warp_stream_ptr = (float*)&shmem[0][0] + warpId * SHMEM_STRIDE * N;
+
+    // Offset in shared memory from which the B matrix is stored.
+    constexpr size_t shmem_idx_b_off = BLOCK_COL_TILES * M;
+
+    // Adjust the beta scaler, as it'll be multiplied by alpha at the end of
+    // each tile computation. Technically this is not generally correct (may result
+    // in a loss of precision). Zero still needs to be specially handled though.
+    beta /= alpha;
+
+    cuda::pipeline<cuda::thread_scope_thread> pipe = cuda::make_pipeline();
+    const auto shape4 = cuda::aligned_size_t<alignof(float4)>(sizeof(float4));
+    constexpr int loadStride = 2; // load 4 floats, so left-shift by 2.
+
+    // Each CTA slides along the 128 x 128 tiles from the top left corner of the matrix to the
+    // right and down, and selects the next tile to compute. Once there's no such tile,
+    // all warps in this CTA exit.
+    for(unsigned int block_pos = blockIdx.x;; block_pos += gridDim.x) {
+        const unsigned int block_tile_i = ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);
+        const unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % N_TILES;
+
+        // Stop when there are no more D matrix tiles to compute in this CTA.
+        if (block_tile_i >= M_TILES) {
+            break;
+        }
+
+        // This warp's pointer to the C matrix data to copy memory from to shared memory.
+        const size_t gmem_idx = (block_tile_i + warpId) * M * GLOBAL_MEM_STRIDE + block_tile_j * N;
+        const float *src_gmem_warp_stream_ptr = &C[gmem_idx];
+
+        // Stream multiple C tiles to shared memory.
+#pragma unroll
+        for (int i = 0; i < N; i++) {
+            pipe.producer_acquire();
+            cuda::memcpy_async(&shmem_warp_stream_ptr[(SHMEM_STRIDE * i) + (laneId << loadStride)],
+                                &src_gmem_warp_stream_ptr[(GLOBAL_MEM_STRIDE * i) + (laneId << loadStride)],
+                                shape4, pipe);
+            pipe.producer_commit();
+        }
+        // Now wait for all the above issued 8 batches to complete.
+        cuda::pipeline_consumer_wait_prior<0>(pipe);
+        __syncthreads();
+
+        // These fragments will accumulate the result of A and B matrix fragment multiplications
+        // along the K_GLOBAL dimension.
+        wmma::fragment<wmma::accumulator, M, N, K, float> c[WARP_COL_TILES][WARP_ROW_TILES];
+
+        // Load the C matrix tiles into fragments from shared memory.
+#pragma unroll
+        for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+                const float *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * N + j * N;
+
+                wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, C_LAYOUT);
+                // Scale the C matrix.
+#pragma unroll
+                for (int t = 0; t < c[i][j].num_elements; t++) {
+                    c[i][j].x[t] *= beta;
+                }
+            }
+        }
+        pipe.consumer_release();
+
+        // sync here so that shared memory can then be used for loading A & B matrices.
+        __syncthreads();
+
+        // Select what warp copies what matrix to shared memory.
+        // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.
+        const float *warp_ptr = (warpId < (WARPS_PER_BLOCK/2)) ? (&A[block_tile_i * M * K_GLOBAL] + M * K_GLOBAL * (warpId % (WARPS_PER_BLOCK/2)) * 2) :
+                                              (&B[block_tile_j * N * K_GLOBAL] + N * K_GLOBAL * (warpId % (WARPS_PER_BLOCK/2)) * 2);
+
+        constexpr int chunksPerLane = ((WARP_SIZE/2) / CHUNK_COPY_LINES_PER_WARP) * 2;
+        const int laneLoadElem = (laneId % CHUNK_COPY_LINE_LANES) << loadStride;
+        const int stridePerLaneCopy = (laneId / CHUNK_COPY_LINE_LANES);
+        // Go through the global K dimension by a fixed step at a time.
+#pragma unroll
+        for (int tile_k = 0; tile_k < K_TILES; tile_k += CHUNK_K) {
+            // Copy slices of the A and B matrices to shared memory.
+            // The first half of the warps in the CTA copy the A matrix, the rest copy the B matrix.
+            // As for tf32 MMA  M == N we use M for warp 4-7 + shmem_idx_b_off.
+            size_t shmem_idx =  (M * (warpId % (WARPS_PER_BLOCK/2)) * 2)  + ((warpId / (WARPS_PER_BLOCK/2)) * shmem_idx_b_off);
+            // First half of the warp copies the first row / column of the matrix,
+            // the second half of the warp copies the next.
+            const float *lane_ptr = (warp_ptr + tile_k * K + stridePerLaneCopy * K_GLOBAL + laneLoadElem);
+
+            // Shift the second half of the warp to the next row / column in the shared memory.
+            shmem_idx += stridePerLaneCopy;
+
+#pragma unroll
+            for(int i = 0; i < chunksPerLane; i++) {
+                // Copy 16 bytes at once in each lane.
+                pipe.producer_acquire();
+                cuda::memcpy_async(&shmem[shmem_idx][laneLoadElem], lane_ptr, shape4, pipe);
+                pipe.producer_commit();
+
+                // Advance the global memory pointer and the shared memory index.
+                lane_ptr = lane_ptr + K_GLOBAL * CHUNK_COPY_LINES_PER_WARP;
+                shmem_idx += CHUNK_COPY_LINES_PER_WARP;
+            }
+
+            cuda::pipeline_consumer_wait_prior<0>(pipe);
+            __syncthreads();
+
+            // Compute a grid of C matrix tiles in each warp.
+#pragma unroll
+            for (int k_step = 0; k_step < CHUNK_K; k_step++) {
+                wmma::fragment<wmma::matrix_a, M, N, K, wmma::precision::tf32, wmma::row_major> a[WARP_COL_TILES];
+                wmma::fragment<wmma::matrix_b, M, N, K, wmma::precision::tf32, wmma::col_major> b[WARP_ROW_TILES];
+
+#pragma unroll
+                for (int i = 0; i < WARP_COL_TILES; i++) {
+                    size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * BLOCK_ROW_WARPS + (i * M);
+                    const float *tile_ptr = &shmem[shmem_idx_a][k_step * K];
+
+                    wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_FLOAT);
+
+#pragma unroll
+                    for (int t = 0; t < a[i].num_elements; t++) {
+                        a[i].x[t] = wmma::__float_to_tf32(a[i].x[t]);
+                    }
+#pragma unroll
+                    for (int j = 0; j < WARP_ROW_TILES; j++) {
+                        if (i == 0) {
+                            // Load the B matrix fragment once, because it is going to be reused
+                            // against the other A matrix fragments.
+                            size_t shmem_idx_b = shmem_idx_b_off + (WARP_ROW_TILES * N) * (warpId%2) + (j * N);
+                            const float *tile_ptr = &shmem[shmem_idx_b][k_step * K];
+
+                            wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_FLOAT);
+#pragma unroll
+                            for (int t = 0; t < b[j].num_elements; t++) {
+                                b[j].x[t] =  wmma::__float_to_tf32(b[j].x[t]);
+                            }
+                        }
+
+                        wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);
+                    }
+                }
+            }
+            pipe.consumer_release();
+            __syncthreads();
+        }
+
+        // Store the D fragments to shared memory.
+#pragma unroll
+        for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+#pragma unroll
+                // Uniform, point-wise transformations of ALL fragment elements by ALL threads in the
+                // warp are well-defined even though element indices within fragment storage are not defined.
+                for (int t = 0; t < c[i][j].num_elements; t++)
+                    c[i][j].x[t] *= alpha;
+
+                float *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * N + j * N;
+
+                wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);
+            }
+        }
+
+        __syncthreads();
+
+        // Now that shared memory contains all the D tiles, stream them to global memory.
+        float *dst_gmem_warp_stream_ptr = &D[gmem_idx];
+
+#pragma unroll
+        for (int i = 0; i < N; i++) {
+            *((float4*)(dst_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) + laneId) =
+                *((float4*)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId);
+        }
+
+        __syncthreads();
+    }
+#endif
+}
+
+// Performs an MxNxK tf32 GEMM (C=alpha*A*B + beta*C) assuming:
+//  1) Matrices are packed in memory.
+//  2) M, N and K are multiples of 16, 16 and 8 respectively. 
+//  3) A is row major, B is column major matrix.
+// Note: This is a less performant version of the compute_tf32gemm kernel. It is designed for
+//       demonstration purposes only to show the CUDA WMMA API use without relying on
+//       availability of the shared memory.
+__global__ void simple_wmma_tf32gemm(float *a, float *b, float *c, float *d, int m_ld, int n_ld, int k_ld, float alpha, float beta)
+{
+#if __CUDA_ARCH__ >= 800
+   // Leading dimensions. Packed with no transpositions.
+    int lda = k_ld;
+    int ldb = k_ld;
+    int ldc = n_ld;
+
+   // Tile using a 2D grid
+   int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / warpSize;
+   int warpN = (blockIdx.y * blockDim.y + threadIdx.y);
+ 
+   // Declare the fragments
+   wmma::fragment<wmma::matrix_a, M, N, K, wmma::precision::tf32, wmma::row_major> a_frag;
+   wmma::fragment<wmma::matrix_b, M, N, K, wmma::precision::tf32, wmma::col_major> b_frag;
+   wmma::fragment<wmma::accumulator, M, N, K, float> acc_frag;
+   wmma::fragment<wmma::accumulator, M, N, K, float> c_frag;
+
+   wmma::fill_fragment(acc_frag, 0.0f);
+
+   // Loop over k
+   for (int i = 0; i < k_ld; i += K) {
+      int aCol = i; 
+      int aRow = warpM * M;
+
+      //int bCol = i;
+      //int bRow = warpN * N;
+      int bCol = warpN * N;
+      int bRow = i;
+
+      // Bounds checking
+      if (aRow < m_ld && aCol < k_ld && bRow < k_ld && bCol < n_ld) {
+         // Load the inputs
+         wmma::load_matrix_sync(a_frag, a + aCol + aRow * lda, lda);
+         wmma::load_matrix_sync(b_frag, b + bRow + bCol * ldb, ldb);
+ 
+ #pragma unroll
+        for (int t = 0; t < a_frag.num_elements; t++) {
+                a_frag.x[t] =  wmma::__float_to_tf32(a_frag.x[t]);
+        }
+
+ #pragma unroll
+        for (int t = 0; t < b_frag.num_elements; t++) {
+                b_frag.x[t] =  wmma::__float_to_tf32(b_frag.x[t]);
+        }
+         // Perform the matrix multiplication
+         wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag);
+
+      }
+   }
+
+   // Load in the current value of c, scale it by beta, and add this our result scaled by alpha
+   int cCol = warpN * N;
+   int cRow = warpM * M;
+
+   if (cRow < m_ld && cCol < n_ld) {
+      wmma::load_matrix_sync(c_frag, c + cCol + cRow * ldc, ldc, wmma::mem_row_major);
+
+      for(int i=0; i < c_frag.num_elements; i++) {
+         c_frag.x[i] = alpha * acc_frag.x[i] + beta * c_frag.x[i];
+      }
+
+      // Store the output
+      wmma::store_matrix_sync(d + cCol + cRow * ldc, c_frag, ldc, wmma::mem_row_major);
+   }
+#endif
+}
+
+__host__ void matMultiplyOnHost(float *A, float *B, float *C,
+                                float alpha, float beta,
+                                int numARows, int numAColumns,
+                                int numBRows, int numBColumns,
+                                int numCRows, int numCColumns)
+{
+    for (int i = 0; i < numCRows; i++) {
+        for (int j = 0; j < numCColumns; j++) {
+            float temp = 0.0;
+
+            for (int k = 0; k < numAColumns; k++) {
+                temp += A[i * numAColumns + k] * B[j * numBRows + k];
+            }
+
+            C[i*numCColumns + j] = temp * alpha + beta * C[i * numCColumns + j];
+        }
+    }
+}
+
+int main(int argc, char **argv)
+{
+    printf("Initializing...\n");
+
+    int dev = findCudaDevice(argc, (const char **)argv);
+
+    hipDeviceProp_t deviceProp;
+    HIPCHECK(hipGetDeviceProperties(&deviceProp, dev));
+
+    // Tensor cores require a GPU of Volta (SM8X) architecture or higher.
+    if (deviceProp.major < 8) {
+        printf("tf32TensorCoreGemm requires requires SM 8.0 or higher to use Tensor Cores.  Exiting...\n");
+        exit(EXIT_WAIVED);
+    }
+
+    printf("M: %d (%d x %d)\n", M_GLOBAL, M, M_TILES);
+    printf("N: %d (%d x %d)\n", N_GLOBAL, N, N_TILES);
+    printf("K: %d (%d x %d)\n", K_GLOBAL, K, K_TILES);
+
+    float *A_h = NULL;
+    float *B_h = NULL;
+    float *C_h = NULL;
+#if CPU_DEBUG
+    float *result_hD = NULL;
+    float *result_host = NULL;
+#endif
+
+    A_h = (float*) malloc(sizeof(float) * M_GLOBAL * K_GLOBAL);
+    B_h = (float*) malloc(sizeof(float) * K_GLOBAL * N_GLOBAL);
+    C_h = (float*) malloc(sizeof(float) * M_GLOBAL * N_GLOBAL);
+#if CPU_DEBUG
+    result_hD   = (float*) malloc(sizeof(float) * M_GLOBAL * N_GLOBAL);
+    result_host = (float*) malloc(sizeof(float) * M_GLOBAL * N_GLOBAL);
+#endif
+
+    float *A = NULL;
+    float *B = NULL;
+    float *C = NULL;
+    float *D = NULL;
+
+    HIPCHECK(hipMalloc((void**)&A, sizeof(float) * M_GLOBAL * K_GLOBAL));
+    HIPCHECK(hipMalloc((void**)&B, sizeof(float) * N_GLOBAL * K_GLOBAL));
+    HIPCHECK(hipMalloc((void**)&C, sizeof(float) * M_GLOBAL * N_GLOBAL));
+    HIPCHECK(hipMalloc((void**)&D, sizeof(float) * M_GLOBAL * N_GLOBAL));
+
+    assert(((unsigned long long)A) % 128 == 0);
+    assert(((unsigned long long)B) % 128 == 0);
+    assert(((unsigned long long)C) % 128 == 0);
+    assert(((unsigned long long)D) % 128 == 0);
+
+    init_host_matrices(A_h, B_h, C_h);
+
+    printf("Preparing data for GPU...\n");
+
+    HIPCHECK(hipMemcpy(A, A_h, sizeof(float) * M_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
+    HIPCHECK(hipMemcpy(B, B_h, sizeof(float) * N_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
+    HIPCHECK(hipMemcpy(C, C_h, sizeof(float) * M_GLOBAL * N_GLOBAL, hipMemcpyHostToDevice));
+    HIPCHECK(hipMemset(D, 0, sizeof(float) * M_GLOBAL * N_GLOBAL));
+
+    enum {
+        // Compute the right amount of shared memory to request.
+        // We need shared memory to hold per-CTA C and D matrix tiles, and to cache per-CTA chunks
+        // of the A and B matrices. Therefore, the right amount to request is the maximum of those
+        // two numbers.
+        SHMEM_SZ = MAX(sizeof(float) * (BLOCK_COL_TILES * M) * (CHUNK_K * K + SKEW_FLOAT) * 2,
+                       M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N * (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(float))
+    };
+
+    printf("Required shared memory size: %lu Kb\n", SHMEM_SZ / 1024UL);
+
+    const float alpha = 1.1f;
+    const float beta = 1.2f;
+
+    hipEvent_t start, stop;
+
+    HIPCHECK(hipEventCreate(&start));    
+    HIPCHECK(hipEventCreate(&stop));
+    HIPCHECK(hipEventRecord(start));
+
+    // kernel to run - default (tf32mma_shmem_gemm_async_copy == 0)
+    kernels selected_kernel = tf32mma_shmem_gemm_async_copy;
+
+    if (checkCmdLineFlag(argc, (const char **)argv, "kernel")) {
+        int kernel_number = getCmdLineArgumentInt(argc, (const char **)argv, "kernel");
+        if (kernel_number < 3) {
+            selected_kernel = (kernels)kernel_number;
+        }
+        else {
+            printf("Error: kernel number should be between 0 to 2, you have entered %d\n", kernel_number);
+            exit(EXIT_FAILURE);
+        }
+    }
+
+    // If enough shared memory available on the GPU use high performant kernel
+    if ((deviceProp.sharedMemPerMultiprocessor >= SHMEM_SZ) && (selected_kernel != simple_tf32mma_gemm)) {
+        printf("Computing using high performance kernel = %d - %s\n", selected_kernel, kernelNames[selected_kernel]);
+
+        switch (selected_kernel)
+        {
+            case tf32mma_shmem_gemm_async_copy :
+            default:
+                HIPCHECK(hipFuncSetAttribute(compute_tf32gemm_async_copy, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
+                checkKernelErrors((compute_tf32gemm_async_copy<<<deviceProp.multiProcessorCount*2, THREADS_PER_BLOCK, SHMEM_SZ>>>(A, B, C, D, alpha, beta)));
+                break;
+            case tf32mma_shmem_gemm :
+                HIPCHECK(hipFuncSetAttribute(compute_tf32gemm, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
+                checkKernelErrors((compute_tf32gemm<<<deviceProp.multiProcessorCount*2, THREADS_PER_BLOCK, SHMEM_SZ>>>(A, B, C, D, alpha, beta)));
+                break;
+        }
+#if CPU_DEBUG
+        HIPCHECK(hipMemcpy(result_hD, D, sizeof(float)*M_GLOBAL*N_GLOBAL, hipMemcpyDeviceToHost));
+#endif
+    }
+    else {
+        dim3 gridDim;
+        dim3 blockDim;
+     
+        // blockDim.x must be a multple of warpSize
+        // 128x4 means we have 16 warps and a block computes a 64x64 output tile
+        blockDim.x = 128;
+        blockDim.y = 4;
+
+        gridDim.x = (M_GLOBAL + (M * blockDim.x / 32 - 1)) / (M * blockDim.x / 32);
+        gridDim.y = (N_GLOBAL + N * blockDim.y - 1) / (N * blockDim.y);
+
+        printf("Computing... using simple_wmma_gemm kernel\n");
+        simple_wmma_tf32gemm<<<gridDim, blockDim>>>(A, B, C, D, M_GLOBAL, N_GLOBAL, K_GLOBAL, alpha, beta);
+#if CPU_DEBUG
+        HIPCHECK(hipMemcpy(result_hD, D, sizeof(float) * M_GLOBAL * N_GLOBAL, hipMemcpyDeviceToHost));
+#endif
+    }
+
+    HIPCHECK(hipEventRecord(stop));
+    HIPCHECK(hipEventSynchronize(stop));
+
+#if CPU_DEBUG
+    printf("Verifying correctness of the computations...\n");
+
+    memcpy(result_host, C_h, sizeof(float) * M_GLOBAL * N_GLOBAL);
+
+    matMultiplyOnHost(A_h, B_h, result_host,
+                      alpha, beta,
+                      M_GLOBAL, K_GLOBAL,
+                      K_GLOBAL, N_GLOBAL,
+                      M_GLOBAL, N_GLOBAL);
+
+    for (int i = 0; i < N_GLOBAL * M_GLOBAL; i++) {
+        if (fabs(result_hD[i] - result_host[i]) > 0.1f) {
+            printf("mismatch i=%d result_hD=%f result_host=%f\n", i, result_hD[i], result_host[i]);
+        }
+    }
+    free(result_hD);
+    free(result_host);
+#endif
+
+    float milliseconds = 0;
+
+    HIPCHECK(hipEventElapsedTime(&milliseconds, start, stop));
+
+    printf("Time: %f ms\n", milliseconds);
+    printf("TFLOPS: %.2f\n", (((double)M_GLOBAL * N_GLOBAL * K_GLOBAL * 2)/(milliseconds/1000.)) / 1e12);
+
+    free(A_h);
+    free(B_h);
+    free(C_h);
+    HIPCHECK(hipFree((void*)A));
+    HIPCHECK(hipFree((void*)B));
+    HIPCHECK(hipFree((void*)C));
+    HIPCHECK(hipFree((void*)D));
+
+    return 0;
+}
+ors(hipFree((void*)A));
+    checkCudaErrors(hipFree((void*)B));
+    checkCudaErrors(hipFree((void*)C));
+    checkCudaErrors(hipFree((void*)D));
+
+    return 0;
+}
diff --git a/src/samples/Samples/3_CUDA_Features/warpAggregatedAtomicsCG/warpAggregatedAtomicsCG.cu.hip b/src/samples/Samples/3_CUDA_Features/warpAggregatedAtomicsCG/warpAggregatedAtomicsCG.cu.hip
index aec13fa..c17a1ee 100755
--- a/src/samples/Samples/3_CUDA_Features/warpAggregatedAtomicsCG/warpAggregatedAtomicsCG.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/warpAggregatedAtomicsCG/warpAggregatedAtomicsCG.cu.hip
@@ -28,8 +28,8 @@
 
 #include <stdio.h>
 // includes, project
-#include <helper_cuda.h>
-#include <helper_functions.h>
+#include "helper_cuda_hipified.h"
+#include "helper_functions.h"
 
 #include <hip/hip_runtime.h>
 
@@ -118,17 +118,17 @@ int mapIndicesToBuckets(int *h_srcArr, int *d_srcArr, int numOfBuckets)
     h_bucketCounters[i] = i*NUM_ELEMS;
   }
 
-  checkCudaErrors(hipMalloc(&d_indicesBuckets, sizeof(int) * NUM_ELEMS * numOfBuckets));
-  checkCudaErrors(hipMalloc(&d_bucketCounters, sizeof(int) * numOfBuckets));
+  HIPCHECK(hipMalloc(&d_indicesBuckets, sizeof(int) * NUM_ELEMS * numOfBuckets));
+  HIPCHECK(hipMalloc(&d_bucketCounters, sizeof(int) * numOfBuckets));
 
-  checkCudaErrors(hipMemcpy(d_bucketCounters, h_bucketCounters, sizeof(int)*numOfBuckets, hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(d_bucketCounters, h_bucketCounters, sizeof(int)*numOfBuckets, hipMemcpyHostToDevice));
 
   dim3 dimBlock(NUM_THREADS_PER_BLOCK, 1, 1);
   dim3 dimGrid((NUM_ELEMS / NUM_THREADS_PER_BLOCK), 1, 1);
 
   mapToBuckets<<<dimGrid, dimBlock>>>(d_srcArr, d_indicesBuckets, d_bucketCounters, NUM_ELEMS, numOfBuckets);
 
-  checkCudaErrors(hipMemcpy(h_bucketCounters, d_bucketCounters, sizeof(int)*numOfBuckets, hipMemcpyDeviceToHost));
+  HIPCHECK(hipMemcpy(h_bucketCounters, d_bucketCounters, sizeof(int)*numOfBuckets, hipMemcpyDeviceToHost));
 
   for (int i=0; i < NUM_ELEMS; i++)
   {
@@ -197,25 +197,25 @@ int calculateMaxInBuckets(int *h_srcArr, int *d_srcArr, int numOfBuckets)
 
   memset(cpuBucketsMax, 0, sizeof(int) * numOfBuckets);
 
-  // Here we create values which is assumed to correspond to each
+  // Here we create values which is assumed to correspond to each 
   // buckets of srcArr at same array index.
   for (int i=0; i < NUM_ELEMS; i++)
   {
     h_valueInBuckets[i] = rand();
   }
 
-  checkCudaErrors(hipMalloc(&d_valueInBuckets, sizeof(int) * NUM_ELEMS));
-  checkCudaErrors(hipMalloc(&d_bucketsMax, sizeof(int) * numOfBuckets));
+  HIPCHECK(hipMalloc(&d_valueInBuckets, sizeof(int) * NUM_ELEMS));
+  HIPCHECK(hipMalloc(&d_bucketsMax, sizeof(int) * numOfBuckets));
 
-  checkCudaErrors(hipMemset(d_bucketsMax, 0, sizeof(int) * numOfBuckets));
-  checkCudaErrors(hipMemcpy(d_valueInBuckets, h_valueInBuckets, sizeof(int) * NUM_ELEMS, hipMemcpyHostToDevice));
+  HIPCHECK(hipMemset(d_bucketsMax, 0, sizeof(int) * numOfBuckets));
+  HIPCHECK(hipMemcpy(d_valueInBuckets, h_valueInBuckets, sizeof(int) * NUM_ELEMS, hipMemcpyHostToDevice));
 
   dim3 dimBlock(NUM_THREADS_PER_BLOCK, 1, 1);
   dim3 dimGrid((NUM_ELEMS / NUM_THREADS_PER_BLOCK), 1, 1);
 
   calculateMaxInEachBuckets<<<dimGrid, dimBlock>>>(d_srcArr, d_valueInBuckets, d_bucketsMax, NUM_ELEMS, numOfBuckets);
 
-  checkCudaErrors(hipMemcpy(h_bucketsMax, d_bucketsMax, sizeof(int) * numOfBuckets, hipMemcpyDeviceToHost));
+  HIPCHECK(hipMemcpy(h_bucketsMax, d_bucketsMax, sizeof(int) * numOfBuckets, hipMemcpyDeviceToHost));
 
   for (int i = 0; i < NUM_ELEMS; i++)
   {
@@ -238,14 +238,14 @@ int calculateMaxInBuckets(int *h_srcArr, int *d_srcArr, int numOfBuckets)
   }
   if (allMatch)
   {
-    printf("CPU max matches GPU max\n");
+    printf("CPU max matches GPU max\n"); 
   }
 
   delete[] h_valueInBuckets;
   delete[] cpuBucketsMax;
   delete[] h_bucketsMax;
-  checkCudaErrors(hipFree(d_valueInBuckets));
-  checkCudaErrors(hipFree(d_bucketsMax));
+  HIPCHECK(hipFree(d_valueInBuckets));
+  HIPCHECK(hipFree(d_bucketsMax));
 
   if (!allMatch && finalElems != NUM_ELEMS)
   {
@@ -270,13 +270,13 @@ int main(int argc, char **argv) {
 
   int devId = findCudaDevice(argc, (const char **)argv);
 
-  checkCudaErrors(hipMalloc(&d_data_to_filter, sizeof(int) * NUM_ELEMS));
-  checkCudaErrors(hipMalloc(&d_filtered_data, sizeof(int) * NUM_ELEMS));
-  checkCudaErrors(hipMalloc(&d_nres, sizeof(int)));
+  HIPCHECK(hipMalloc(&d_data_to_filter, sizeof(int) * NUM_ELEMS));
+  HIPCHECK(hipMalloc(&d_filtered_data, sizeof(int) * NUM_ELEMS));
+  HIPCHECK(hipMalloc(&d_nres, sizeof(int)));
 
-  checkCudaErrors(hipMemcpy(d_data_to_filter, data_to_filter,
+  HIPCHECK(hipMemcpy(d_data_to_filter, data_to_filter,
                              sizeof(int) * NUM_ELEMS, hipMemcpyHostToDevice));
-  checkCudaErrors(hipMemset(d_nres, 0, sizeof(int)));
+  HIPCHECK(hipMemset(d_nres, 0, sizeof(int)));
 
   dim3 dimBlock(NUM_THREADS_PER_BLOCK, 1, 1);
   dim3 dimGrid((NUM_ELEMS / NUM_THREADS_PER_BLOCK) + 1, 1, 1);
@@ -284,12 +284,12 @@ int main(int argc, char **argv) {
   filter_arr<<<dimGrid, dimBlock>>>(d_filtered_data, d_nres, d_data_to_filter,
                                     NUM_ELEMS);
 
-  checkCudaErrors(
+  HIPCHECK(
       hipMemcpy(&nres, d_nres, sizeof(int), hipMemcpyDeviceToHost));
 
   filtered_data = reinterpret_cast<int *>(malloc(sizeof(int) * nres));
 
-  checkCudaErrors(hipMemcpy(filtered_data, d_filtered_data, sizeof(int) * nres,
+  HIPCHECK(hipMemcpy(filtered_data, d_filtered_data, sizeof(int) * nres,
                              hipMemcpyDeviceToHost));
 
   int *host_filtered_data =
@@ -304,7 +304,7 @@ int main(int argc, char **argv) {
   }
 
   int major = 0;
-  checkCudaErrors(hipDeviceGetAttribute(&major, hipDeviceAttributeComputeCapabilityMajor, devId));
+  HIPCHECK(hipDeviceGetAttribute(&major, hipDeviceAttributeComputeCapabilityMajor, devId));
 
   int mapIndicesToBucketsStatus = EXIT_SUCCESS;
   int calculateMaxInBucketsStatus = EXIT_SUCCESS;
@@ -316,11 +316,17 @@ int main(int argc, char **argv) {
   }
 
   printf("\nWarp Aggregated Atomics %s \n",
-         (host_flt_count == nres) && (mapIndicesToBucketsStatus == EXIT_SUCCESS) &&
+         (host_flt_count == nres) && (mapIndicesToBucketsStatus == EXIT_SUCCESS) && 
          (calculateMaxInBucketsStatus == EXIT_SUCCESS) ? "PASSED" : "FAILED");
 
-  checkCudaErrors(hipFree(d_data_to_filter));
-  checkCudaErrors(hipFree(d_filtered_data));
+  HIPCHECK(hipFree(d_data_to_filter));
+  HIPCHECK(hipFree(d_filtered_data));
+  HIPCHECK(hipFree(d_nres));
+  free(data_to_filter);
+  free(filtered_data);
+  free(host_filtered_data);
+}
+eckCudaErrors(hipFree(d_filtered_data));
   checkCudaErrors(hipFree(d_nres));
   free(data_to_filter);
   free(filtered_data);
diff --git a/src/samples/Samples/4_CUDA_Libraries/conjugateGradientCudaGraphs/conjugateGradientCudaGraphs.cu.hip b/src/samples/Samples/4_CUDA_Libraries/conjugateGradientCudaGraphs/conjugateGradientCudaGraphs.cu.hip
index e69de29..167dd92 100755
--- a/src/samples/Samples/4_CUDA_Libraries/conjugateGradientCudaGraphs/conjugateGradientCudaGraphs.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/conjugateGradientCudaGraphs/conjugateGradientCudaGraphs.cu.hip
@@ -0,0 +1,453 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * This sample implements a conjugate gradient solver on GPU
+ * using CUBLAS and CUSPARSE with CUDA Graphs
+ *
+ */
+
+// includes, system
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+/* Using updated (v2) interfaces to cublas */
+#include <hipblas.h>
+#include <hip/hip_runtime.h>
+#include <hipsparse.h>
+
+// Utilities and system includes
+#include <helper_cuda.h>  // helper function CUDA error checking and initialization
+#include <helper_functions.h>  // helper for shared functions common to CUDA Samples
+
+const char *sSDKname = "conjugateGradientCudaGraphs";
+
+#ifndef WITH_GRAPH
+#define WITH_GRAPH 1
+#endif
+
+/* genTridiag: generate a random tridiagonal symmetric matrix */
+void genTridiag(int *I, int *J, float *val, int N, int nz) {
+  I[0] = 0, J[0] = 0, J[1] = 1;
+  val[0] = (float)rand() / RAND_MAX + 10.0f;
+  val[1] = (float)rand() / RAND_MAX;
+  int start;
+
+  for (int i = 1; i < N; i++) {
+    if (i > 1) {
+      I[i] = I[i - 1] + 3;
+    } else {
+      I[1] = 2;
+    }
+
+    start = (i - 1) * 3 + 2;
+    J[start] = i - 1;
+    J[start + 1] = i;
+
+    if (i < N - 1) {
+      J[start + 2] = i + 1;
+    }
+
+    val[start] = val[start - 1];
+    val[start + 1] = (float)rand() / RAND_MAX + 10.0f;
+
+    if (i < N - 1) {
+      val[start + 2] = (float)rand() / RAND_MAX;
+    }
+  }
+
+  I[N] = nz;
+}
+
+__global__ void initVectors(float *rhs, float *x, int N) {
+  size_t gid = blockIdx.x * blockDim.x + threadIdx.x;
+
+  for (size_t i = gid; i < N; i += gridDim.x * blockDim.x) {
+    rhs[i] = 1.0;
+    x[i] = 0.0;
+  }
+}
+
+__global__ void r1_div_x(float *r1, float *r0, float *b) {
+  int gid = blockIdx.x * blockDim.x + threadIdx.x;
+  if (gid == 0) {
+    b[0] = r1[0] / r0[0];
+  }
+}
+
+__global__ void a_minus(float *a, float *na) {
+  int gid = blockIdx.x * blockDim.x + threadIdx.x;
+  if (gid == 0) {
+    na[0] = -(a[0]);
+  }
+}
+
+int main(int argc, char **argv) {
+  int N = 0, nz = 0, *I = NULL, *J = NULL;
+  float *val = NULL;
+  const float tol = 1e-5f;
+  const int max_iter = 10000;
+  float *x;
+  float *rhs;
+  float r1;
+
+  int *d_col, *d_row;
+  float *d_val, *d_x;
+  float *d_r, *d_p, *d_Ax;
+  int k;
+  float alpha, beta, alpham1;
+
+  hipStream_t stream1, streamForGraph;
+
+  // This will pick the best possible CUDA capable device
+  hipDeviceProp_t deviceProp;
+  int devID = findCudaDevice(argc, (const char **)argv);
+
+  if (devID < 0) {
+    printf("exiting...\n");
+    exit(EXIT_SUCCESS);
+  }
+
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, devID));
+
+  // Statistics about the GPU device
+  printf(
+      "> GPU device has %d Multi-Processors, SM %d.%d compute capabilities\n\n",
+      deviceProp.multiProcessorCount, deviceProp.major, deviceProp.minor);
+
+  /* Generate a random tridiagonal symmetric matrix in CSR format */
+  N = 1048576;
+  nz = (N - 2) * 3 + 4;
+  HIPCHECK(hipHostMalloc(&I, sizeof(int) * (N + 1)));
+  HIPCHECK(hipHostMalloc(&J, sizeof(int) * nz));
+  HIPCHECK(hipHostMalloc(&val, sizeof(float) * nz));
+  genTridiag(I, J, val, N, nz);
+
+  HIPCHECK(hipHostMalloc(&x, sizeof(float) * N));
+  rhs = (float *)malloc(sizeof(float) * N);
+
+  for (int i = 0; i < N; i++) {
+    rhs[i] = 1.0;
+    x[i] = 0.0;
+  }
+
+  /* Get handle to the CUBLAS context */
+  hipblasHandle_t cublasHandle = 0;
+  hipblasStatus_t hipblasStatus_t;
+  hipblasStatus_t = hipblasCreate(&cublasHandle);
+
+  HIPCHECK(hipblasStatus_t);
+
+  /* Get handle to the CUSPARSE context */
+  hipsparseHandle_t cusparseHandle = 0;
+  hipsparseStatus_t cusparseStatus;
+  cusparseStatus = hipsparseCreate(&cusparseHandle);
+
+  HIPCHECK(cusparseStatus);
+
+  HIPCHECK(hipStreamCreate(&stream1));
+
+  HIPCHECK(hipMalloc((void **)&d_col, nz * sizeof(int)));
+  HIPCHECK(hipMalloc((void **)&d_row, (N + 1) * sizeof(int)));
+  HIPCHECK(hipMalloc((void **)&d_val, nz * sizeof(float)));
+  HIPCHECK(hipMalloc((void **)&d_x, N * sizeof(float)));
+  HIPCHECK(hipMalloc((void **)&d_r, N * sizeof(float)));
+  HIPCHECK(hipMalloc((void **)&d_p, N * sizeof(float)));
+  HIPCHECK(hipMalloc((void **)&d_Ax, N * sizeof(float)));
+
+  float *d_r1, *d_r0, *d_dot, *d_a, *d_na, *d_b;
+  HIPCHECK(hipMalloc((void **)&d_r1, sizeof(float)));
+  HIPCHECK(hipMalloc((void **)&d_r0, sizeof(float)));
+  HIPCHECK(hipMalloc((void **)&d_dot, sizeof(float)));
+  HIPCHECK(hipMalloc((void **)&d_a, sizeof(float)));
+  HIPCHECK(hipMalloc((void **)&d_na, sizeof(float)));
+  HIPCHECK(hipMalloc((void **)&d_b, sizeof(float)));
+
+  /* Wrap raw data into cuSPARSE generic API objects */
+  hipsparseSpMatDescr_t matA = NULL;
+  HIPCHECK(hipsparseCreateCsr(&matA, N, N, nz, d_row, d_col, d_val,
+                                    HIPSPARSE_INDEX_32I, HIPSPARSE_INDEX_32I,
+                                    HIPSPARSE_INDEX_BASE_ZERO, HIPBLAS_R_32F));
+  hipsparseDnVecDescr_t vecx = NULL;
+  HIPCHECK(hipsparseCreateDnVec(&vecx, N, d_x, HIPBLAS_R_32F));
+  hipsparseDnVecDescr_t vecp = NULL;
+  HIPCHECK(hipsparseCreateDnVec(&vecp, N, d_p, HIPBLAS_R_32F));
+  hipsparseDnVecDescr_t vecAx = NULL;
+  HIPCHECK(hipsparseCreateDnVec(&vecAx, N, d_Ax, HIPBLAS_R_32F));
+
+  /* Allocate workspace for cuSPARSE */
+  size_t bufferSize = 0;
+  HIPCHECK(hipsparseSpMV_bufferSize(
+      cusparseHandle, HIPSPARSE_OPERATION_NON_TRANSPOSE, &alpha, matA, vecx,
+      &beta, vecAx, HIPBLAS_R_32F, HIPSPARSE_SPMV_ALG_DEFAULT, &bufferSize));
+  void *buffer = NULL;
+  HIPCHECK(hipMalloc(&buffer, bufferSize));
+
+  hipsparseMatDescr_t descr = 0;
+  HIPCHECK(hipsparseCreateMatDescr(&descr));
+
+  HIPCHECK(hipsparseSetMatType(descr, HIPSPARSE_MATRIX_TYPE_GENERAL));
+  HIPCHECK(hipsparseSetMatIndexBase(descr, HIPSPARSE_INDEX_BASE_ZERO));
+
+  int numBlocks = 0, blockSize = 0;
+  HIPCHECK(
+      hipOccupancyMaxPotentialBlockSize(&numBlocks, &blockSize, initVectors));
+
+  HIPCHECK(hipMemcpyAsync(d_col, J, nz * sizeof(int),
+                                  hipMemcpyHostToDevice, stream1));
+  HIPCHECK(hipMemcpyAsync(d_row, I, (N + 1) * sizeof(int),
+                                  hipMemcpyHostToDevice, stream1));
+  HIPCHECK(hipMemcpyAsync(d_val, val, nz * sizeof(float),
+                                  hipMemcpyHostToDevice, stream1));
+
+  initVectors<<<numBlocks, blockSize, 0, stream1>>>(d_r, d_x, N);
+
+  alpha = 1.0;
+  alpham1 = -1.0;
+  beta = 0.0;
+
+  HIPCHECK(hipsparseSetStream(cusparseHandle, stream1));
+  HIPCHECK(hipsparseSpMV(cusparseHandle, HIPSPARSE_OPERATION_NON_TRANSPOSE,
+                               &alpha, matA, vecx, &beta, vecAx, HIPBLAS_R_32F,
+                               HIPSPARSE_SPMV_ALG_DEFAULT, buffer));
+
+  HIPCHECK(hipblasSetStream(cublasHandle, stream1));
+  HIPCHECK(hipblasSaxpy(cublasHandle, N, &alpham1, d_Ax, 1, d_r, 1));
+
+  HIPCHECK(
+      hipblasSetPointerMode(cublasHandle, HIPBLAS_POINTER_MODE_DEVICE));
+  HIPCHECK(hipblasSdot(cublasHandle, N, d_r, 1, d_r, 1, d_r1));
+
+  k = 1;
+  // First Iteration when k=1 starts
+  HIPCHECK(hipblasScopy(cublasHandle, N, d_r, 1, d_p, 1));
+  HIPCHECK(hipsparseSpMV(cusparseHandle, HIPSPARSE_OPERATION_NON_TRANSPOSE,
+                               &alpha, matA, vecp, &beta, vecAx, HIPBLAS_R_32F,
+                               HIPSPARSE_SPMV_ALG_DEFAULT, buffer));
+
+  HIPCHECK(hipblasSdot(cublasHandle, N, d_p, 1, d_Ax, 1, d_dot));
+
+  r1_div_x<<<1, 1, 0, stream1>>>(d_r1, d_dot, d_a);
+
+  HIPCHECK(hipblasSaxpy(cublasHandle, N, d_a, d_p, 1, d_x, 1));
+
+  a_minus<<<1, 1, 0, stream1>>>(d_a, d_na);
+
+  HIPCHECK(hipblasSaxpy(cublasHandle, N, d_na, d_Ax, 1, d_r, 1));
+
+  HIPCHECK(hipMemcpyAsync(d_r0, d_r1, sizeof(float),
+                                  hipMemcpyDeviceToDevice, stream1));
+
+  HIPCHECK(hipblasSdot(cublasHandle, N, d_r, 1, d_r, 1, d_r1));
+
+  HIPCHECK(hipMemcpyAsync(&r1, d_r1, sizeof(float),
+                                  hipMemcpyDeviceToHost, stream1));
+  HIPCHECK(hipStreamSynchronize(stream1));
+  printf("iteration = %3d, residual = %e\n", k, sqrt(r1));
+  // First Iteration when k=1 ends
+  k++;
+
+#if WITH_GRAPH
+  hipGraph_t initGraph;
+  HIPCHECK(hipStreamCreate(&streamForGraph));
+  HIPCHECK(hipblasSetStream(cublasHandle, stream1));
+  HIPCHECK(hipsparseSetStream(cusparseHandle, stream1));
+  HIPCHECK(hipStreamBeginCapture(stream1, hipStreamCaptureModeGlobal));
+
+  r1_div_x<<<1, 1, 0, stream1>>>(d_r1, d_r0, d_b);
+  hipblasSetPointerMode(cublasHandle, HIPBLAS_POINTER_MODE_DEVICE);
+  HIPCHECK(hipblasSscal(cublasHandle, N, d_b, d_p, 1));
+  hipblasSetPointerMode(cublasHandle, HIPBLAS_POINTER_MODE_HOST);
+  HIPCHECK(hipblasSaxpy(cublasHandle, N, &alpha, d_r, 1, d_p, 1));
+  hipblasSetPointerMode(cublasHandle, HIPBLAS_POINTER_MODE_DEVICE);
+
+  HIPCHECK(
+      hipsparseSetPointerMode(cusparseHandle, HIPSPARSE_POINTER_MODE_HOST));
+  HIPCHECK(hipsparseSpMV(cusparseHandle, HIPSPARSE_OPERATION_NON_TRANSPOSE,
+                               &alpha, matA, vecp, &beta, vecAx, HIPBLAS_R_32F,
+                               HIPSPARSE_SPMV_ALG_DEFAULT, buffer));
+
+  HIPCHECK(hipMemsetAsync(d_dot, 0, sizeof(float), stream1));
+  HIPCHECK(hipblasSdot(cublasHandle, N, d_p, 1, d_Ax, 1, d_dot));
+
+  r1_div_x<<<1, 1, 0, stream1>>>(d_r1, d_dot, d_a);
+
+  HIPCHECK(hipblasSaxpy(cublasHandle, N, d_a, d_p, 1, d_x, 1));
+
+  a_minus<<<1, 1, 0, stream1>>>(d_a, d_na);
+
+  HIPCHECK(hipblasSaxpy(cublasHandle, N, d_na, d_Ax, 1, d_r, 1));
+
+  HIPCHECK(hipMemcpyAsync(d_r0, d_r1, sizeof(float),
+                                  hipMemcpyDeviceToDevice, stream1));
+  HIPCHECK(hipMemsetAsync(d_r1, 0, sizeof(float), stream1));
+
+  HIPCHECK(hipblasSdot(cublasHandle, N, d_r, 1, d_r, 1, d_r1));
+
+  HIPCHECK(hipMemcpyAsync((float *)&r1, d_r1, sizeof(float),
+                                  hipMemcpyDeviceToHost, stream1));
+
+  HIPCHECK(hipStreamEndCapture(stream1, &initGraph));
+  hipGraphExec_t graphExec;
+  HIPCHECK(hipGraphInstantiate(&graphExec, initGraph, NULL, NULL, 0));
+#endif
+
+  HIPCHECK(hipblasSetStream(cublasHandle, stream1));
+  HIPCHECK(hipsparseSetStream(cusparseHandle, stream1));
+
+  while (r1 > tol * tol && k <= max_iter) {
+#if WITH_GRAPH
+    HIPCHECK(hipGraphLaunch(graphExec, streamForGraph));
+    HIPCHECK(hipStreamSynchronize(streamForGraph));
+#else
+    r1_div_x<<<1, 1, 0, stream1>>>(d_r1, d_r0, d_b);
+    hipblasSetPointerMode(cublasHandle, HIPBLAS_POINTER_MODE_DEVICE);
+    HIPCHECK(hipblasSscal(cublasHandle, N, d_b, d_p, 1));
+
+    hipblasSetPointerMode(cublasHandle, HIPBLAS_POINTER_MODE_HOST);
+    HIPCHECK(hipblasSaxpy(cublasHandle, N, &alpha, d_r, 1, d_p, 1));
+
+    HIPCHECK(hipsparseSpMV(
+        cusparseHandle, HIPSPARSE_OPERATION_NON_TRANSPOSE, &alpha, matA, vecp,
+        &beta, vecAx, HIPBLAS_R_32F, HIPSPARSE_SPMV_ALG_DEFAULT, buffer));
+
+    hipblasSetPointerMode(cublasHandle, HIPBLAS_POINTER_MODE_DEVICE);
+    HIPCHECK(hipblasSdot(cublasHandle, N, d_p, 1, d_Ax, 1, d_dot));
+
+    r1_div_x<<<1, 1, 0, stream1>>>(d_r1, d_dot, d_a);
+
+    HIPCHECK(hipblasSaxpy(cublasHandle, N, d_a, d_p, 1, d_x, 1));
+
+    a_minus<<<1, 1, 0, stream1>>>(d_a, d_na);
+    HIPCHECK(hipblasSaxpy(cublasHandle, N, d_na, d_Ax, 1, d_r, 1));
+
+    HIPCHECK(hipMemcpyAsync(d_r0, d_r1, sizeof(float),
+                                    hipMemcpyDeviceToDevice, stream1));
+
+    HIPCHECK(hipblasSdot(cublasHandle, N, d_r, 1, d_r, 1, d_r1));
+    HIPCHECK(hipMemcpyAsync((float *)&r1, d_r1, sizeof(float),
+                                    hipMemcpyDeviceToHost, stream1));
+    HIPCHECK(hipStreamSynchronize(stream1));
+#endif
+    printf("iteration = %3d, residual = %e\n", k, sqrt(r1));
+    k++;
+  }
+
+#if WITH_GRAPH
+  HIPCHECK(hipMemcpyAsync(x, d_x, N * sizeof(float),
+                                  hipMemcpyDeviceToHost, streamForGraph));
+  HIPCHECK(hipStreamSynchronize(streamForGraph));
+#else
+  HIPCHECK(hipMemcpyAsync(x, d_x, N * sizeof(float),
+                                  hipMemcpyDeviceToHost, stream1));
+  HIPCHECK(hipStreamSynchronize(stream1));
+#endif
+
+  float rsum, diff, err = 0.0;
+
+  for (int i = 0; i < N; i++) {
+    rsum = 0.0;
+
+    for (int j = I[i]; j < I[i + 1]; j++) {
+      rsum += val[j] * x[J[j]];
+    }
+
+    diff = fabs(rsum - rhs[i]);
+
+    if (diff > err) {
+      err = diff;
+    }
+  }
+
+#if WITH_GRAPH
+  HIPCHECK(hipGraphExecDestroy(graphExec));
+  HIPCHECK(hipGraphDestroy(initGraph));
+  HIPCHECK(hipStreamDestroy(streamForGraph));
+#endif
+  HIPCHECK(hipStreamDestroy(stream1));
+  hipsparseDestroy(cusparseHandle);
+  hipblasDestroy(cublasHandle);
+
+  if (matA) {
+    HIPCHECK(hipsparseDestroySpMat(matA));
+  }
+  if (vecx) {
+    HIPCHECK(hipsparseDestroyDnVec(vecx));
+  }
+  if (vecAx) {
+    HIPCHECK(hipsparseDestroyDnVec(vecAx));
+  }
+  if (vecp) {
+    HIPCHECK(hipsparseDestroyDnVec(vecp));
+  }
+
+  HIPCHECK(hipHostFree(I));
+  HIPCHECK(hipHostFree(J));
+  HIPCHECK(hipHostFree(val));
+  HIPCHECK(hipHostFree(x));
+  free(rhs);
+  HIPCHECK(hipFree(d_col));
+  HIPCHECK(hipFree(d_row));
+  HIPCHECK(hipFree(d_val));
+  HIPCHECK(hipFree(d_x));
+  HIPCHECK(hipFree(d_r));
+  HIPCHECK(hipFree(d_p));
+  HIPCHECK(hipFree(d_Ax));
+
+  printf("Test Summary:  Error amount = %f\n", err);
+  exit((k <= max_iter) ? 0 : 1);
+}
+rors(hipsparseDestroySpMat(matA));
+  }
+  if (vecx) {
+    checkCudaErrors(hipsparseDestroyDnVec(vecx));
+  }
+  if (vecAx) {
+    checkCudaErrors(hipsparseDestroyDnVec(vecAx));
+  }
+  if (vecp) {
+    checkCudaErrors(hipsparseDestroyDnVec(vecp));
+  }
+
+  checkCudaErrors(hipHostFree(I));
+  checkCudaErrors(hipHostFree(J));
+  checkCudaErrors(hipHostFree(val));
+  checkCudaErrors(hipHostFree(x));
+  free(rhs);
+  checkCudaErrors(hipFree(d_col));
+  checkCudaErrors(hipFree(d_row));
+  checkCudaErrors(hipFree(d_val));
+  checkCudaErrors(hipFree(d_x));
+  checkCudaErrors(hipFree(d_r));
+  checkCudaErrors(hipFree(d_p));
+  checkCudaErrors(hipFree(d_Ax));
+
+  printf("Test Summary:  Error amount = %f\n", err);
+  exit((k <= max_iter) ? 0 : 1);
+}
diff --git a/src/samples/Samples/4_CUDA_Libraries/conjugateGradientMultiBlockCG/conjugateGradientMultiBlockCG.cu.hip b/src/samples/Samples/4_CUDA_Libraries/conjugateGradientMultiBlockCG/conjugateGradientMultiBlockCG.cu.hip
index e69de29..925c2a5 100755
--- a/src/samples/Samples/4_CUDA_Libraries/conjugateGradientMultiBlockCG/conjugateGradientMultiBlockCG.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/conjugateGradientMultiBlockCG/conjugateGradientMultiBlockCG.cu.hip
@@ -0,0 +1,497 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * This sample implements a conjugate gradient solver on GPU using
+ * Multi Block Cooperative Groups, also uses Unified Memory.
+ *
+ */
+
+// includes, system
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+#include <hip/hip_runtime.h>
+
+// Utilities and system includes
+#include <helper_cuda.h>  // helper function CUDA error checking and initialization
+#include <helper_functions.h>  // helper for shared functions common to CUDA Samples
+
+#include <hip/hip_cooperative_groups.h>
+#include <cooperative_groups/reduce.h>
+
+namespace cg = cooperative_groups;
+
+const char *sSDKname = "conjugateGradientMultiBlockCG";
+
+#define ENABLE_CPU_DEBUG_CODE 0
+#define THREADS_PER_BLOCK 512
+
+/* genTridiag: generate a random tridiagonal symmetric matrix */
+void genTridiag(int *I, int *J, float *val, int N, int nz) {
+  I[0] = 0, J[0] = 0, J[1] = 1;
+  val[0] = static_cast<float>(rand()) / RAND_MAX + 10.0f;
+  val[1] = static_cast<float>(rand()) / RAND_MAX;
+  int start;
+
+  for (int i = 1; i < N; i++) {
+    if (i > 1) {
+      I[i] = I[i - 1] + 3;
+    } else {
+      I[1] = 2;
+    }
+
+    start = (i - 1) * 3 + 2;
+    J[start] = i - 1;
+    J[start + 1] = i;
+
+    if (i < N - 1) {
+      J[start + 2] = i + 1;
+    }
+
+    val[start] = val[start - 1];
+    val[start + 1] = static_cast<float>(rand()) / RAND_MAX + 10.0f;
+
+    if (i < N - 1) {
+      val[start + 2] = static_cast<float>(rand()) / RAND_MAX;
+    }
+  }
+
+  I[N] = nz;
+}
+
+// I - contains location of the given non-zero element in the row of the matrix
+// J - contains location of the given non-zero element in the column of the
+// matrix val - contains values of the given non-zero elements of the matrix
+// inputVecX - input vector to be multiplied
+// outputVecY - resultant vector
+void cpuSpMV(int *I, int *J, float *val, int nnz, int num_rows, float alpha,
+             float *inputVecX, float *outputVecY) {
+  for (int i = 0; i < num_rows; i++) {
+    int num_elems_this_row = I[i + 1] - I[i];
+
+    float output = 0.0;
+    for (int j = 0; j < num_elems_this_row; j++) {
+      output += alpha * val[I[i] + j] * inputVecX[J[I[i] + j]];
+    }
+    outputVecY[i] = output;
+  }
+
+  return;
+}
+
+double dotProduct(float *vecA, float *vecB, int size) {
+  double result = 0.0;
+
+  for (int i = 0; i < size; i++) {
+    result = result + (vecA[i] * vecB[i]);
+  }
+
+  return result;
+}
+
+void scaleVector(float *vec, float alpha, int size) {
+  for (int i = 0; i < size; i++) {
+    vec[i] = alpha * vec[i];
+  }
+}
+
+void saxpy(float *x, float *y, float a, int size) {
+  for (int i = 0; i < size; i++) {
+    y[i] = a * x[i] + y[i];
+  }
+}
+
+void cpuConjugateGrad(int *I, int *J, float *val, float *x, float *Ax, float *p,
+                      float *r, int nnz, int N, float tol) {
+  int max_iter = 10000;
+
+  float alpha = 1.0;
+  float alpham1 = -1.0;
+  float r0 = 0.0, b, a, na;
+
+  cpuSpMV(I, J, val, nnz, N, alpha, x, Ax);
+  saxpy(Ax, r, alpham1, N);
+
+  float r1 = dotProduct(r, r, N);
+
+  int k = 1;
+
+  while (r1 > tol * tol && k <= max_iter) {
+    if (k > 1) {
+      b = r1 / r0;
+      scaleVector(p, b, N);
+
+      saxpy(r, p, alpha, N);
+    } else {
+      for (int i = 0; i < N; i++) p[i] = r[i];
+    }
+
+    cpuSpMV(I, J, val, nnz, N, alpha, p, Ax);
+
+    float dot = dotProduct(p, Ax, N);
+    a = r1 / dot;
+
+    saxpy(p, x, a, N);
+    na = -a;
+    saxpy(Ax, r, na, N);
+
+    r0 = r1;
+    r1 = dotProduct(r, r, N);
+
+    printf("\nCPU code iteration = %3d, residual = %e\n", k, sqrt(r1));
+    k++;
+  }
+}
+
+__device__ void gpuSpMV(int *I, int *J, float *val, int nnz, int num_rows,
+                        float alpha, float *inputVecX, float *outputVecY,
+                        cg::thread_block &cta, const cg::grid_group &grid) {
+  for (int i = grid.thread_rank(); i < num_rows; i += grid.size()) {
+    int row_elem = I[i];
+    int next_row_elem = I[i + 1];
+    int num_elems_this_row = next_row_elem - row_elem;
+
+    float output = 0.0;
+    for (int j = 0; j < num_elems_this_row; j++) {
+      // I or J or val arrays - can be put in shared memory
+      // as the access is random and reused in next calls of gpuSpMV function.
+      output += alpha * val[row_elem + j] * inputVecX[J[row_elem + j]];
+    }
+
+    outputVecY[i] = output;
+  }
+}
+
+__device__ void gpuSaxpy(float *x, float *y, float a, int size,
+                         const cg::grid_group &grid) {
+  for (int i = grid.thread_rank(); i < size; i += grid.size()) {
+    y[i] = a * x[i] + y[i];
+  }
+}
+
+__device__ void gpuDotProduct(float *vecA, float *vecB, double *result,
+                              int size, const cg::thread_block &cta,
+                              const cg::grid_group &grid) {
+  extern __shared__ double tmp[];
+
+  double temp_sum = 0.0;
+  for (int i = grid.thread_rank(); i < size; i += grid.size()) {
+    temp_sum += static_cast<double>(vecA[i] * vecB[i]);
+  }
+
+  cg::thread_block_tile<32> tile32 = cg::tiled_partition<32>(cta);
+
+  temp_sum = cg::reduce(tile32, temp_sum, cg::plus<double>());
+
+  if (tile32.thread_rank() == 0) {
+    tmp[tile32.meta_group_rank()] = temp_sum;
+  }
+
+  cg::sync(cta);
+
+  if (tile32.meta_group_rank() == 0) {
+     temp_sum = tile32.thread_rank() < tile32.meta_group_size() ? tmp[tile32.thread_rank()] : 0.0;
+     temp_sum = cg::reduce(tile32, temp_sum, cg::plus<double>());
+
+    if (tile32.thread_rank() == 0) {
+      atomicAdd(result, temp_sum);
+    }
+  }
+}
+
+__device__ void gpuCopyVector(float *srcA, float *destB, int size,
+                              const cg::grid_group &grid) {
+  for (int i = grid.thread_rank(); i < size; i += grid.size()) {
+    destB[i] = srcA[i];
+  }
+}
+
+__device__ void gpuScaleVectorAndSaxpy(const float *x, float *y, float a, float scale, int size,
+                         const cg::grid_group &grid) {
+  for (int i = grid.thread_rank(); i < size; i += grid.size()) {
+    y[i] = a * x[i] + scale * y[i];
+  }
+}
+
+extern "C" __global__ void gpuConjugateGradient(int *I, int *J, float *val,
+                                                float *x, float *Ax, float *p,
+                                                float *r, double *dot_result,
+                                                int nnz, int N, float tol) {
+  cg::thread_block cta = cg::this_thread_block();
+  cg::grid_group grid = cg::this_grid();
+
+  int max_iter = 10000;
+
+  float alpha = 1.0;
+  float alpham1 = -1.0;
+  float r0 = 0.0, r1, b, a, na;
+
+  gpuSpMV(I, J, val, nnz, N, alpha, x, Ax, cta, grid);
+
+  cg::sync(grid);
+
+  gpuSaxpy(Ax, r, alpham1, N, grid);
+
+  cg::sync(grid);
+
+  gpuDotProduct(r, r, dot_result, N, cta, grid);
+
+  cg::sync(grid);
+
+  r1 = *dot_result;
+
+  int k = 1;
+  while (r1 > tol * tol && k <= max_iter) {
+    if (k > 1) {
+      b = r1 / r0;
+      gpuScaleVectorAndSaxpy(r, p, alpha, b, N, grid);
+    } else {
+      gpuCopyVector(r, p, N, grid);
+    }
+
+    cg::sync(grid);
+
+    gpuSpMV(I, J, val, nnz, N, alpha, p, Ax, cta, grid);
+
+    if (threadIdx.x == 0 && blockIdx.x == 0) *dot_result = 0.0;
+
+    cg::sync(grid);
+
+    gpuDotProduct(p, Ax, dot_result, N, cta, grid);
+
+    cg::sync(grid);
+
+    a = r1 / *dot_result;
+
+    gpuSaxpy(p, x, a, N, grid);
+    na = -a;
+    gpuSaxpy(Ax, r, na, N, grid);
+
+    r0 = r1;
+
+    cg::sync(grid);
+    if (threadIdx.x == 0 && blockIdx.x == 0) *dot_result = 0.0;
+
+    cg::sync(grid);
+
+    gpuDotProduct(r, r, dot_result, N, cta, grid);
+
+    cg::sync(grid);
+
+    r1 = *dot_result;
+    k++;
+  }
+}
+
+bool areAlmostEqual(float a, float b, float maxRelDiff) {
+  float diff = fabsf(a - b);
+  float abs_a = fabsf(a);
+  float abs_b = fabsf(b);
+  float largest = abs_a > abs_b ? abs_a : abs_b;
+
+  if (diff <= largest * maxRelDiff) {
+    return true;
+  } else {
+    printf("maxRelDiff = %.8e\n", maxRelDiff);
+    printf(
+        "diff %.8e > largest * maxRelDiff %.8e therefore %.8e and %.8e are not "
+        "same\n",
+        diff, largest * maxRelDiff, a, b);
+    return false;
+  }
+}
+
+int main(int argc, char **argv) {
+  int N = 0, nz = 0, *I = NULL, *J = NULL;
+  float *val = NULL;
+  const float tol = 1e-5f;
+  float *x;
+  float *rhs;
+  float r1;
+  float *r, *p, *Ax;
+  hipEvent_t start, stop;
+
+  printf("Starting [%s]...\n", sSDKname);
+
+  // This will pick the best possible CUDA capable device
+  hipDeviceProp_t deviceProp;
+  int devID = findCudaDevice(argc, (const char **)argv);
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, devID));
+
+  if (!deviceProp.managedMemory) {
+    // This sample requires being run on a device that supports Unified Memory
+    fprintf(stderr, "Unified Memory not supported on this device\n");
+    exit(EXIT_WAIVED);
+  }
+
+  // This sample requires being run on a device that supports Cooperative Kernel
+  // Launch
+  if (!deviceProp.cooperativeLaunch) {
+    printf(
+        "\nSelected GPU (%d) does not support Cooperative Kernel Launch, "
+        "Waiving the run\n",
+        devID);
+    exit(EXIT_WAIVED);
+  }
+
+  // Statistics about the GPU device
+  printf(
+      "> GPU device has %d Multi-Processors, SM %d.%d compute capabilities\n\n",
+      deviceProp.multiProcessorCount, deviceProp.major, deviceProp.minor);
+
+  /* Generate a random tridiagonal symmetric matrix in CSR format */
+  N = 1048576;
+  nz = (N - 2) * 3 + 4;
+
+  hipMallocManaged(reinterpret_cast<void **>(&I), sizeof(int) * (N + 1));
+  hipMallocManaged(reinterpret_cast<void **>(&J), sizeof(int) * nz);
+  hipMallocManaged(reinterpret_cast<void **>(&val), sizeof(float) * nz);
+
+  genTridiag(I, J, val, N, nz);
+
+  hipMallocManaged(reinterpret_cast<void **>(&x), sizeof(float) * N);
+  hipMallocManaged(reinterpret_cast<void **>(&rhs), sizeof(float) * N);
+
+  double *dot_result;
+
+  hipMallocManaged(reinterpret_cast<void **>(&dot_result), sizeof(double));
+
+  *dot_result = 0.0;
+
+  // temp memory for CG
+  HIPCHECK(
+      hipMallocManaged(reinterpret_cast<void **>(&r), N * sizeof(float)));
+  HIPCHECK(
+      hipMallocManaged(reinterpret_cast<void **>(&p), N * sizeof(float)));
+  HIPCHECK(
+      hipMallocManaged(reinterpret_cast<void **>(&Ax), N * sizeof(float)));
+
+  hipDeviceSynchronize();
+
+  HIPCHECK(hipEventCreate(&start));
+  HIPCHECK(hipEventCreate(&stop));
+
+#if ENABLE_CPU_DEBUG_CODE
+  float *Ax_cpu = reinterpret_cast<float *>(malloc(sizeof(float) * N));
+  float *r_cpu = reinterpret_cast<float *>(malloc(sizeof(float) * N));
+  float *p_cpu = reinterpret_cast<float *>(malloc(sizeof(float) * N));
+  float *x_cpu = reinterpret_cast<float *>(malloc(sizeof(float) * N));
+
+  for (int i = 0; i < N; i++) {
+    r_cpu[i] = 1.0;
+    Ax_cpu[i] = x_cpu[i] = 0.0;
+  }
+
+#endif
+
+  for (int i = 0; i < N; i++) {
+    r[i] = rhs[i] = 1.0;
+    x[i] = 0.0;
+  }
+
+  void *kernelArgs[] = {
+      (void *)&I,  (void *)&J, (void *)&val, (void *)&x,
+      (void *)&Ax, (void *)&p, (void *)&r,   (void *)&dot_result,
+      (void *)&nz, (void *)&N, (void *)&tol,
+  };
+
+  int sMemSize = sizeof(double) * ((THREADS_PER_BLOCK/32) + 1);
+  int numBlocksPerSm = 0;
+  int numThreads = THREADS_PER_BLOCK;
+
+  HIPCHECK(hipOccupancyMaxActiveBlocksPerMultiprocessor(
+      &numBlocksPerSm, gpuConjugateGradient, numThreads, sMemSize));
+
+  int numSms = deviceProp.multiProcessorCount;
+  dim3 dimGrid(numSms * numBlocksPerSm, 1, 1),
+      dimBlock(THREADS_PER_BLOCK, 1, 1);
+  HIPCHECK(hipEventRecord(start, 0));
+  HIPCHECK(hipLaunchCooperativeKernel((void *)gpuConjugateGradient,
+                                              dimGrid, dimBlock, kernelArgs,
+                                              sMemSize, NULL));
+  HIPCHECK(hipEventRecord(stop, 0));
+  HIPCHECK(hipDeviceSynchronize());
+
+  float time;
+  HIPCHECK(hipEventElapsedTime(&time, start, stop));
+
+  r1 = *dot_result;
+
+  printf("GPU Final, residual = %e, kernel execution time = %f ms\n", sqrt(r1),
+         time);
+
+#if ENABLE_CPU_DEBUG_CODE
+  cpuConjugateGrad(I, J, val, x_cpu, Ax_cpu, p_cpu, r_cpu, nz, N, tol);
+#endif
+
+  float rsum, diff, err = 0.0;
+
+  for (int i = 0; i < N; i++) {
+    rsum = 0.0;
+
+    for (int j = I[i]; j < I[i + 1]; j++) {
+      rsum += val[j] * x[J[j]];
+    }
+
+    diff = fabs(rsum - rhs[i]);
+
+    if (diff > err) {
+      err = diff;
+    }
+  }
+
+  HIPCHECK(hipFree(I));
+  HIPCHECK(hipFree(J));
+  HIPCHECK(hipFree(val));
+  HIPCHECK(hipFree(x));
+  HIPCHECK(hipFree(rhs));
+  HIPCHECK(hipFree(r));
+  HIPCHECK(hipFree(p));
+  HIPCHECK(hipFree(Ax));
+  HIPCHECK(hipFree(dot_result));
+  HIPCHECK(hipEventDestroy(start));
+  HIPCHECK(hipEventDestroy(stop));
+
+#if ENABLE_CPU_DEBUG_CODE
+  free(Ax_cpu);
+  free(r_cpu);
+  free(p_cpu);
+  free(x_cpu);
+#endif
+
+  printf("Test Summary:  Error amount = %f \n", err);
+  fprintf(stdout, "&&&& conjugateGradientMultiBlockCG %s\n",
+          (sqrt(r1) < tol) ? "PASSED" : "FAILED");
+  exit((sqrt(r1) < tol) ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+(stdout, "&&&& conjugateGradientMultiBlockCG %s\n",
+          (sqrt(r1) < tol) ? "PASSED" : "FAILED");
+  exit((sqrt(r1) < tol) ? EXIT_SUCCESS : EXIT_FAILURE);
+}
diff --git a/src/samples/Samples/4_CUDA_Libraries/conjugateGradientMultiDeviceCG/conjugateGradientMultiDeviceCG.cu.hip b/src/samples/Samples/4_CUDA_Libraries/conjugateGradientMultiDeviceCG/conjugateGradientMultiDeviceCG.cu.hip
index 94deed1..551d266 100755
--- a/src/samples/Samples/4_CUDA_Libraries/conjugateGradientMultiDeviceCG/conjugateGradientMultiDeviceCG.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/conjugateGradientMultiDeviceCG/conjugateGradientMultiDeviceCG.cu.hip
@@ -432,13 +432,13 @@ extern "C" __global__ void multiGpuConjugateGradient(
 // Map of device version to device number
 std::multimap<std::pair<int, int>, int> getIdenticalGPUs() {
   int numGpus = 0;
-  checkCudaErrors(hipGetDeviceCount(&numGpus));
+  HIPCHECK(hipGetDeviceCount(&numGpus));
 
   std::multimap<std::pair<int, int>, int> identicalGpus;
 
   for (int i = 0; i < numGpus; i++) {
     hipDeviceProp_t deviceProp;
-    checkCudaErrors(hipGetDeviceProperties(&deviceProp, i));
+    HIPCHECK(hipGetDeviceProperties(&deviceProp, i));
 
     // Filter unsupported devices
     if (deviceProp.cooperativeLaunch && deviceProp.concurrentManagedAccess) {
@@ -497,7 +497,7 @@ int main(int argc, char **argv) {
   // access between participating GPUs gives better performance.
   for (auto itr = bestFit.first; itr != bestFit.second; itr++) {
     int deviceId = itr->second;
-    checkCudaErrors(hipSetDevice(deviceId));
+    HIPCHECK(hipSetDevice(deviceId));
 
     std::for_each(
         itr, bestFit.second,
@@ -505,7 +505,7 @@ int main(int argc, char **argv) {
          &kNumGpusRequired](decltype(*itr) mapPair) {
           if (deviceId != mapPair.second) {
             int access = 0;
-            checkCudaErrors(
+            HIPCHECK(
                 hipDeviceCanAccessPeer(&access, deviceId, mapPair.second));
             printf("Device=%d %s Access Peer Device=%d\n", deviceId,
                    access ? "CAN" : "CANNOT", mapPair.second);
@@ -551,12 +551,12 @@ int main(int argc, char **argv) {
     // participating devices.
     for (auto p1_itr = bestFitDeviceIds.begin();
          p1_itr != bestFitDeviceIds.end(); p1_itr++) {
-      checkCudaErrors(hipSetDevice(*p1_itr));
+      HIPCHECK(hipSetDevice(*p1_itr));
       for (auto p2_itr = bestFitDeviceIds.begin();
            p2_itr != bestFitDeviceIds.end(); p2_itr++) {
         if (*p1_itr != *p2_itr) {
-          checkCudaErrors(hipDeviceEnablePeerAccess(*p2_itr, 0));
-          checkCudaErrors(hipSetDevice(*p1_itr));
+          HIPCHECK(hipDeviceEnablePeerAccess(*p2_itr, 0));
+          HIPCHECK(hipSetDevice(*p1_itr));
         }
       }
     }
@@ -566,33 +566,33 @@ int main(int argc, char **argv) {
   N = 10485760 * 2;
   nz = (N - 2) * 3 + 4;
 
-  checkCudaErrors(hipMallocManaged((void **)&I, sizeof(int) * (N + 1)));
-  checkCudaErrors(hipMallocManaged((void **)&J, sizeof(int) * nz));
-  checkCudaErrors(hipMallocManaged((void **)&val, sizeof(float) * nz));
+  HIPCHECK(hipMallocManaged((void **)&I, sizeof(int) * (N + 1)));
+  HIPCHECK(hipMallocManaged((void **)&J, sizeof(int) * nz));
+  HIPCHECK(hipMallocManaged((void **)&val, sizeof(float) * nz));
 
   float *val_cpu = (float *)malloc(sizeof(float) * nz);
 
   genTridiag(I, J, val_cpu, N, nz);
 
   memcpy(val, val_cpu, sizeof(float) * nz);
-  checkCudaErrors(
+  HIPCHECK(
       hipMemAdvise(I, sizeof(int) * (N + 1), hipMemAdviseSetReadMostly, 0));
-  checkCudaErrors(
+  HIPCHECK(
       hipMemAdvise(J, sizeof(int) * nz, hipMemAdviseSetReadMostly, 0));
-  checkCudaErrors(
+  HIPCHECK(
       hipMemAdvise(val, sizeof(float) * nz, hipMemAdviseSetReadMostly, 0));
 
-  checkCudaErrors(hipMallocManaged((void **)&x, sizeof(float) * N));
+  HIPCHECK(hipMallocManaged((void **)&x, sizeof(float) * N));
 
   double *dot_result;
-  checkCudaErrors(hipMallocManaged((void **)&dot_result, sizeof(double)));
+  HIPCHECK(hipMallocManaged((void **)&dot_result, sizeof(double)));
 
-  checkCudaErrors(hipMemset(dot_result, 0, sizeof(double)));
+  HIPCHECK(hipMemset(dot_result, 0, sizeof(double)));
 
   // temp memory for ConjugateGradient
-  checkCudaErrors(hipMallocManaged((void **)&r, N * sizeof(float)));
-  checkCudaErrors(hipMallocManaged((void **)&p, N * sizeof(float)));
-  checkCudaErrors(hipMallocManaged((void **)&Ax, N * sizeof(float)));
+  HIPCHECK(hipMallocManaged((void **)&r, N * sizeof(float)));
+  HIPCHECK(hipMallocManaged((void **)&p, N * sizeof(float)));
+  HIPCHECK(hipMallocManaged((void **)&Ax, N * sizeof(float)));
 
   std::cout << "\nRunning on GPUs = " << kNumGpusRequired << std::endl;
   hipStream_t nStreams[kNumGpusRequired];
@@ -606,11 +606,11 @@ int main(int argc, char **argv) {
   // set numSms & numBlocksPerSm to be lowest of 2 devices
   while (deviceId != bestFitDeviceIds.end()) {
     hipDeviceProp_t deviceProp;
-    checkCudaErrors(hipSetDevice(*deviceId));
-    checkCudaErrors(hipGetDeviceProperties(&deviceProp, *deviceId));
+    HIPCHECK(hipSetDevice(*deviceId));
+    HIPCHECK(hipGetDeviceProperties(&deviceProp, *deviceId));
 
     int numBlocksPerSm_current = 0;
-    checkCudaErrors(hipOccupancyMaxActiveBlocksPerMultiprocessor(
+    HIPCHECK(hipOccupancyMaxActiveBlocksPerMultiprocessor(
         &numBlocksPerSm_current, multiGpuConjugateGradient, numThreads,
         sMemSize));
 
@@ -634,8 +634,8 @@ int main(int argc, char **argv) {
   int totalThreadsPerGPU = numSms * numBlocksPerSm * THREADS_PER_BLOCK;
   deviceId = bestFitDeviceIds.begin();
   while (deviceId != bestFitDeviceIds.end()) {
-    checkCudaErrors(hipSetDevice(*deviceId));
-    checkCudaErrors(hipStreamCreate(&nStreams[device_count]));
+    HIPCHECK(hipSetDevice(*deviceId));
+    HIPCHECK(hipStreamCreate(&nStreams[device_count]));
 
     int perGPUIter = N / (totalThreadsPerGPU * kNumGpusRequired);
     int offset_Ax = device_count * totalThreadsPerGPU;
@@ -643,11 +643,11 @@ int main(int argc, char **argv) {
     int offset_p = device_count * totalThreadsPerGPU;
     int offset_x = device_count * totalThreadsPerGPU;
 
-    checkCudaErrors(hipMemPrefetchAsync(I, sizeof(int) * N, *deviceId,
+    HIPCHECK(hipMemPrefetchAsync(I, sizeof(int) * N, *deviceId,
                                          nStreams[device_count]));
-    checkCudaErrors(hipMemPrefetchAsync(val, sizeof(float) * nz, *deviceId,
+    HIPCHECK(hipMemPrefetchAsync(val, sizeof(float) * nz, *deviceId,
                                          nStreams[device_count]));
-    checkCudaErrors(hipMemPrefetchAsync(J, sizeof(float) * nz, *deviceId,
+    HIPCHECK(hipMemPrefetchAsync(J, sizeof(float) * nz, *deviceId,
                                          nStreams[device_count]));
 
     if (offset_Ax <= N) {
@@ -704,7 +704,7 @@ int main(int argc, char **argv) {
 
   // Structure used for cross-grid synchronization.
   MultiDeviceData multi_device_data;
-  checkCudaErrors(hipHostAlloc(
+  HIPCHECK(hipHostAlloc(
       &multi_device_data.hostMemoryArrivedList,
       (kNumGpusRequired - 1) * sizeof(*multi_device_data.hostMemoryArrivedList),
       hipHostMallocPortable));
@@ -725,23 +725,23 @@ int main(int argc, char **argv) {
   deviceId = bestFitDeviceIds.begin();
   device_count = 0;
   while (deviceId != bestFitDeviceIds.end()) {
-    checkCudaErrors(hipSetDevice(*deviceId));
-    checkCudaErrors(hipLaunchCooperativeKernel(
+    HIPCHECK(hipSetDevice(*deviceId));
+    HIPCHECK(hipLaunchCooperativeKernel(
         (void *)multiGpuConjugateGradient, dimGrid, dimBlock, kernelArgs,
         sMemSize, nStreams[device_count++]));
     multi_device_data.deviceRank++;
     deviceId++;
   }
 
-  checkCudaErrors(hipMemPrefetchAsync(x, sizeof(float) * N, hipCpuDeviceId));
-  checkCudaErrors(
+  HIPCHECK(hipMemPrefetchAsync(x, sizeof(float) * N, hipCpuDeviceId));
+  HIPCHECK(
       hipMemPrefetchAsync(dot_result, sizeof(double), hipCpuDeviceId));
 
   deviceId = bestFitDeviceIds.begin();
   device_count = 0;
   while (deviceId != bestFitDeviceIds.end()) {
-    checkCudaErrors(hipSetDevice(*deviceId));
-    checkCudaErrors(hipStreamSynchronize(nStreams[device_count++]));
+    HIPCHECK(hipSetDevice(*deviceId));
+    HIPCHECK(hipStreamSynchronize(nStreams[device_count++]));
     deviceId++;
   }
 
@@ -769,15 +769,15 @@ int main(int argc, char **argv) {
     }
   }
 
-  checkCudaErrors(hipHostFree(multi_device_data.hostMemoryArrivedList));
-  checkCudaErrors(hipFree(I));
-  checkCudaErrors(hipFree(J));
-  checkCudaErrors(hipFree(val));
-  checkCudaErrors(hipFree(x));
-  checkCudaErrors(hipFree(r));
-  checkCudaErrors(hipFree(p));
-  checkCudaErrors(hipFree(Ax));
-  checkCudaErrors(hipFree(dot_result));
+  HIPCHECK(hipHostFree(multi_device_data.hostMemoryArrivedList));
+  HIPCHECK(hipFree(I));
+  HIPCHECK(hipFree(J));
+  HIPCHECK(hipFree(val));
+  HIPCHECK(hipFree(x));
+  HIPCHECK(hipFree(r));
+  HIPCHECK(hipFree(p));
+  HIPCHECK(hipFree(Ax));
+  HIPCHECK(hipFree(dot_result));
   free(val_cpu);
 
 #if ENABLE_CPU_DEBUG_CODE
@@ -792,3 +792,15 @@ int main(int argc, char **argv) {
           (sqrt(r1) < tol) ? "PASSED" : "FAILED");
   exit((sqrt(r1) < tol) ? EXIT_SUCCESS : EXIT_FAILURE);
 }
+G_CODE
+  free(Ax_cpu);
+  free(r_cpu);
+  free(p_cpu);
+  free(x_cpu);
+#endif
+
+  printf("Test Summary:  Error amount = %f \n", err);
+  fprintf(stdout, "&&&& conjugateGradientMultiDeviceCG %s\n",
+          (sqrt(r1) < tol) ? "PASSED" : "FAILED");
+  exit((sqrt(r1) < tol) ? EXIT_SUCCESS : EXIT_FAILURE);
+}
diff --git a/src/samples/Samples/4_CUDA_Libraries/cuDLAErrorReporting/main.cu.hip b/src/samples/Samples/4_CUDA_Libraries/cuDLAErrorReporting/main.cu.hip
index e69de29..c38223b 100755
--- a/src/samples/Samples/4_CUDA_Libraries/cuDLAErrorReporting/main.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/cuDLAErrorReporting/main.cu.hip
@@ -0,0 +1,431 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "cudla.h"
+#include "hip/hip_runtime.h"
+
+#include <cstdio>
+#include <cstdlib>
+#include <cstring>
+#include <sys/stat.h>
+#include <fstream>
+#include <sstream>
+
+#define DPRINTF(...) printf(__VA_ARGS__)
+
+static void printTensorDesc(cudlaModuleTensorDescriptor* tensorDesc) {
+  DPRINTF("\tTENSOR NAME : %s\n", tensorDesc->name);
+  DPRINTF("\tsize: %lu\n", tensorDesc->size);
+
+  DPRINTF("\tdims: [%lu, %lu, %lu, %lu]\n", tensorDesc->n, tensorDesc->c,
+          tensorDesc->h, tensorDesc->w);
+
+  DPRINTF("\tdata fmt: %d\n", tensorDesc->dataFormat);
+  DPRINTF("\tdata type: %d\n", tensorDesc->dataType);
+  DPRINTF("\tdata category: %d\n", tensorDesc->dataCategory);
+  DPRINTF("\tpixel fmt: %d\n", tensorDesc->pixelFormat);
+  DPRINTF("\tpixel mapping: %d\n", tensorDesc->pixelMapping);
+  DPRINTF("\tstride[0]: %d\n", tensorDesc->stride[0]);
+  DPRINTF("\tstride[1]: %d\n", tensorDesc->stride[1]);
+  DPRINTF("\tstride[2]: %d\n", tensorDesc->stride[2]);
+  DPRINTF("\tstride[3]: %d\n", tensorDesc->stride[3]);
+}
+
+typedef struct {
+  cudlaDevHandle devHandle;
+  cudlaModule moduleHandle;
+  unsigned char* loadableData;
+  hipStream_t stream;
+  unsigned char* inputBuffer;
+  unsigned char* outputBuffer;
+  void* inputBufferGPU;
+  void* outputBufferGPU;
+  cudlaModuleTensorDescriptor* inputTensorDesc;
+  cudlaModuleTensorDescriptor* outputTensorDesc;
+} ResourceList;
+
+void cleanUp(ResourceList* resourceList);
+
+void cleanUp(ResourceList* resourceList) {
+  if (resourceList->inputTensorDesc != NULL) {
+    free(resourceList->inputTensorDesc);
+    resourceList->inputTensorDesc = NULL;
+  }
+  if (resourceList->outputTensorDesc != NULL) {
+    free(resourceList->outputTensorDesc);
+    resourceList->outputTensorDesc = NULL;
+  }
+
+  if (resourceList->loadableData != NULL) {
+    free(resourceList->loadableData);
+    resourceList->loadableData = NULL;
+  }
+
+  if (resourceList->moduleHandle != NULL) {
+    cudlaModuleUnload(resourceList->moduleHandle, 0);
+    resourceList->moduleHandle = NULL;
+  }
+
+  if (resourceList->devHandle != NULL) {
+    cudlaDestroyDevice(resourceList->devHandle);
+    resourceList->devHandle = NULL;
+  }
+
+  if (resourceList->inputBufferGPU != 0) {
+    hipFree(resourceList->inputBufferGPU);
+    resourceList->inputBufferGPU = 0;
+  }
+  if (resourceList->outputBufferGPU != 0) {
+    hipFree(resourceList->outputBufferGPU);
+    resourceList->outputBufferGPU = 0;
+  }
+
+  if (resourceList->inputBuffer != NULL) {
+    free(resourceList->inputBuffer);
+    resourceList->inputBuffer = NULL;
+  }
+  if (resourceList->outputBuffer != NULL) {
+    free(resourceList->outputBuffer);
+    resourceList->outputBuffer = NULL;
+  }
+
+  if (resourceList->stream != NULL) {
+    hipStreamDestroy(resourceList->stream);
+    resourceList->stream = NULL;
+  }
+}
+
+int main(int argc, char** argv) {
+  cudlaDevHandle devHandle;
+  cudlaModule moduleHandle;
+  cudlaStatus err;
+  FILE* fp = NULL;
+  struct stat st;
+  size_t file_size;
+  size_t actually_read = 0;
+  unsigned char* loadableData = NULL;
+
+  hipStream_t stream;
+  hipError_t result;
+  const char* errPtr = NULL;
+
+  ResourceList resourceList;
+
+  memset(&resourceList, 0x00, sizeof(ResourceList));
+
+  if (argc != 2) {
+    DPRINTF("Usage : ./cuDLAErrorReporting <loadable>\n");
+    return 1;
+  }
+
+  // Read loadable into buffer.
+  fp = fopen(argv[1], "rb");
+  if (fp == NULL) {
+    DPRINTF("Cannot open file %s\n", argv[1]);
+    return 1;
+  }
+
+  if (stat(argv[1], &st) != 0) {
+    DPRINTF("Cannot stat file\n");
+    return 1;
+  }
+
+  file_size = st.st_size;
+  DPRINTF("The file size = %ld\n", file_size);
+
+  loadableData = (unsigned char*)malloc(file_size);
+  if (loadableData == NULL) {
+    DPRINTF("Cannot Allocate memory for loadable\n");
+    return 1;
+  }
+
+  actually_read = fread(loadableData, 1, file_size, fp);
+  if (actually_read != file_size) {
+    free(loadableData);
+    DPRINTF("Read wrong size\n");
+    return 1;
+  }
+  fclose(fp);
+
+  resourceList.loadableData = loadableData;
+
+  // Initialize CUDA.
+  result = hipFree(0);
+  if (result != hipSuccess) {
+    errPtr = hipGetErrorName(result);
+    DPRINTF("Error in creating hipFree = %s\n", errPtr);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  result = hipSetDevice(0);
+  if (result != hipSuccess) {
+    errPtr = hipGetErrorName(result);
+    DPRINTF("Error in creating hipSetDevice = %s\n", errPtr);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  err = cudlaCreateDevice(0, &devHandle, CUDLA_CUDA_DLA);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in cuDLA create device = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  DPRINTF("Device created successfully\n");
+  resourceList.devHandle = devHandle;
+
+  err = cudlaModuleLoadFromMemory(devHandle, loadableData, file_size,
+                                  &moduleHandle, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in cudlaModuleLoadFromMemory = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  } else {
+    DPRINTF("Successfully loaded module\n");
+  }
+
+  resourceList.moduleHandle = moduleHandle;
+
+  // Create CUDA stream.
+  result = hipStreamCreateWithFlags(&stream, hipStreamNonBlocking);
+
+  if (result != hipSuccess) {
+    errPtr = hipGetErrorName(result);
+    DPRINTF("Error in creating cuda stream = %s\n", errPtr);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.stream = stream;
+
+  // Get tensor attributes.
+  uint32_t numInputTensors = 0;
+  uint32_t numOutputTensors = 0;
+  cudlaModuleAttribute attribute;
+
+  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_NUM_INPUT_TENSORS,
+                                 &attribute);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in getting numInputTensors = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  numInputTensors = attribute.numInputTensors;
+  DPRINTF("numInputTensors = %d\n", numInputTensors);
+
+  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_NUM_OUTPUT_TENSORS,
+                                 &attribute);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in getting numOutputTensors = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  numOutputTensors = attribute.numOutputTensors;
+  DPRINTF("numOutputTensors = %d\n", numOutputTensors);
+
+  cudlaModuleTensorDescriptor* inputTensorDesc =
+      (cudlaModuleTensorDescriptor*)malloc(sizeof(cudlaModuleTensorDescriptor) *
+                                           numInputTensors);
+  cudlaModuleTensorDescriptor* outputTensorDesc =
+      (cudlaModuleTensorDescriptor*)malloc(sizeof(cudlaModuleTensorDescriptor) *
+                                           numOutputTensors);
+
+  if ((inputTensorDesc == NULL) || (outputTensorDesc == NULL)) {
+    if (inputTensorDesc != NULL) {
+      free(inputTensorDesc);
+      inputTensorDesc = NULL;
+    }
+
+    if (outputTensorDesc != NULL) {
+      free(outputTensorDesc);
+      outputTensorDesc = NULL;
+    }
+
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.inputTensorDesc = inputTensorDesc;
+  resourceList.outputTensorDesc = outputTensorDesc;
+
+  attribute.inputTensorDesc = inputTensorDesc;
+  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_INPUT_TENSOR_DESCRIPTORS,
+                                 &attribute);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in getting input tensor descriptor = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("Printing input tensor descriptor\n");
+  printTensorDesc(inputTensorDesc);
+
+  attribute.outputTensorDesc = outputTensorDesc;
+  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_OUTPUT_TENSOR_DESCRIPTORS,
+                                 &attribute);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in getting output tensor descriptor = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("Printing output tensor descriptor\n");
+  printTensorDesc(outputTensorDesc);
+
+  // Setup the input and output buffers which will be used as an input to CUDA.
+  unsigned char* inputBuffer = (unsigned char*)malloc(inputTensorDesc[0].size);
+  if (inputBuffer == NULL) {
+    DPRINTF("Error in allocating input memory\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.inputBuffer = inputBuffer;
+
+  unsigned char* outputBuffer =
+      (unsigned char*)malloc(outputTensorDesc[0].size);
+  if (outputBuffer == NULL) {
+    DPRINTF("Error in allocating output memory\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.outputBuffer = outputBuffer;
+
+  memset(inputBuffer, 0x01, inputTensorDesc[0].size);
+  memset(outputBuffer, 0x00, outputTensorDesc[0].size);
+
+  // Allocate memory on GPU.
+  void* inputBufferGPU;
+  void* outputBufferGPU;
+  result = hipMalloc(&inputBufferGPU, inputTensorDesc[0].size);
+  if (result != hipSuccess) {
+    DPRINTF("Error in allocating input memory on GPU\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.inputBufferGPU = inputBufferGPU;
+
+  result = hipMalloc(&outputBufferGPU, outputTensorDesc[0].size);
+  if (result != hipSuccess) {
+    DPRINTF("Error in allocating output memory on GPU\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.outputBufferGPU = outputBufferGPU;
+
+  // Register the CUDA-allocated buffers.
+  uint64_t* inputBufferRegisteredPtr = NULL;
+  uint64_t* outputBufferRegisteredPtr = NULL;
+
+  err = cudlaMemRegister(devHandle, (uint64_t*)inputBufferGPU,
+                         inputTensorDesc[0].size, &inputBufferRegisteredPtr, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in registering input memory = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  err =
+      cudlaMemRegister(devHandle, (uint64_t*)outputBufferGPU,
+                       outputTensorDesc[0].size, &outputBufferRegisteredPtr, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in registering output memory = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("ALL MEMORY REGISTERED SUCCESSFULLY\n");
+
+  // Copy data from CPU buffers to GPU buffers.
+  result = hipMemcpyAsync(inputBufferGPU, inputBuffer, inputTensorDesc[0].size,
+                           hipMemcpyHostToDevice, stream);
+  if (result != hipSuccess) {
+    DPRINTF("Error in enqueueing memcpy for input\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+  result =
+      hipMemsetAsync(outputBufferGPU, 0, outputTensorDesc[0].size, stream);
+  if (result != hipSuccess) {
+    DPRINTF("Error in enqueueing memset for output\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  // Enqueue a cuDLA task.
+  cudlaTask task;
+  task.moduleHandle = moduleHandle;
+  task.outputTensor = &outputBufferRegisteredPtr;
+  task.numOutputTensors = 1;
+  task.numInputTensors = 1;
+  task.inputTensor = &inputBufferRegisteredPtr;
+  task.waitEvents = NULL;
+  task.signalEvents = NULL;
+  err = cudlaSubmitTask(devHandle, &task, 1, stream, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in submitting task\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("SUBMIT IS DONE !!!\n");
+
+  // Wait for stream operations to finish and bring output buffer to CPU.
+  result =
+      hipMemcpyAsync(outputBuffer, outputBufferGPU, outputTensorDesc[0].size,
+                      hipMemcpyDeviceToHost, stream);
+  if (result != hipSuccess) {
+    if (result != cudaErrorExternalDevice) {
+      DPRINTF("Error in bringing result back to CPU\n");
+      cleanUp(&resourceList);
+      return 1;
+    } else {
+      cudlaStatus hwStatus = cudlaGetLastError(devHandle);
+      if (hwStatus != cudlaSuccess) {
+        DPRINTF("Asynchronous error in HW = %u\n", hwStatus);
+      }
+    }
+  }
+
+  result = hipStreamSynchronize(stream);
+  if (result != hipSuccess) {
+    DPRINTF("Error in synchronizing stream = %s\n", hipGetErrorName(result));
+
+    if (result == cudaErrorExternalDevice) {
+      cudlaStatus hwStatus = cudlaGetLastError(devHandle);
+      if (hwStatus != cudlaSuccess) {
+        DPRINTF("Asynchronous error in HW = %u\n", hwStatus);
+      }
+    }
+  }
+
+  cleanUp(&resourceList);
+
+  DPRINTF("cuDLAErrorReporting DONE !!!\n");
+
+  return 0;
+}
diff --git a/src/samples/Samples/4_CUDA_Libraries/cuDLAHybridMode/main.cu.hip b/src/samples/Samples/4_CUDA_Libraries/cuDLAHybridMode/main.cu.hip
index e69de29..ecaf605 100755
--- a/src/samples/Samples/4_CUDA_Libraries/cuDLAHybridMode/main.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/cuDLAHybridMode/main.cu.hip
@@ -0,0 +1,496 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "cudla.h"
+#include "hip/hip_runtime.h"
+
+#include <cstdio>
+#include <cstdlib>
+#include <cstring>
+#include <sys/stat.h>
+#include <fstream>
+#include <sstream>
+
+#define DPRINTF(...) printf(__VA_ARGS__)
+
+static void printTensorDesc(cudlaModuleTensorDescriptor* tensorDesc) {
+  DPRINTF("\tTENSOR NAME : %s\n", tensorDesc->name);
+  DPRINTF("\tsize: %lu\n", tensorDesc->size);
+
+  DPRINTF("\tdims: [%lu, %lu, %lu, %lu]\n", tensorDesc->n, tensorDesc->c,
+          tensorDesc->h, tensorDesc->w);
+
+  DPRINTF("\tdata fmt: %d\n", tensorDesc->dataFormat);
+  DPRINTF("\tdata type: %d\n", tensorDesc->dataType);
+  DPRINTF("\tdata category: %d\n", tensorDesc->dataCategory);
+  DPRINTF("\tpixel fmt: %d\n", tensorDesc->pixelFormat);
+  DPRINTF("\tpixel mapping: %d\n", tensorDesc->pixelMapping);
+  DPRINTF("\tstride[0]: %d\n", tensorDesc->stride[0]);
+  DPRINTF("\tstride[1]: %d\n", tensorDesc->stride[1]);
+  DPRINTF("\tstride[2]: %d\n", tensorDesc->stride[2]);
+  DPRINTF("\tstride[3]: %d\n", tensorDesc->stride[3]);
+}
+
+static int initializeInputBuffers(char* filePath,
+                                  cudlaModuleTensorDescriptor* tensorDesc,
+                                  unsigned char* buf) {
+  // Read the file in filePath and fill up 'buf' according to format
+  // specified by the user.
+
+  return 0;
+}
+
+typedef struct {
+  cudlaDevHandle devHandle;
+  cudlaModule moduleHandle;
+  unsigned char* loadableData;
+  hipStream_t stream;
+  unsigned char* inputBuffer;
+  unsigned char* outputBuffer;
+  void* inputBufferGPU;
+  void* outputBufferGPU;
+  cudlaModuleTensorDescriptor* inputTensorDesc;
+  cudlaModuleTensorDescriptor* outputTensorDesc;
+} ResourceList;
+
+void cleanUp(ResourceList* resourceList);
+
+void cleanUp(ResourceList* resourceList) {
+  if (resourceList->inputTensorDesc != NULL) {
+    free(resourceList->inputTensorDesc);
+    resourceList->inputTensorDesc = NULL;
+  }
+  if (resourceList->outputTensorDesc != NULL) {
+    free(resourceList->outputTensorDesc);
+    resourceList->outputTensorDesc = NULL;
+  }
+
+  if (resourceList->loadableData != NULL) {
+    free(resourceList->loadableData);
+    resourceList->loadableData = NULL;
+  }
+
+  if (resourceList->moduleHandle != NULL) {
+    cudlaModuleUnload(resourceList->moduleHandle, 0);
+    resourceList->moduleHandle = NULL;
+  }
+
+  if (resourceList->devHandle != NULL) {
+    cudlaDestroyDevice(resourceList->devHandle);
+    resourceList->devHandle = NULL;
+  }
+
+  if (resourceList->inputBufferGPU != 0) {
+    hipFree(resourceList->inputBufferGPU);
+    resourceList->inputBufferGPU = 0;
+  }
+  if (resourceList->outputBufferGPU != 0) {
+    hipFree(resourceList->outputBufferGPU);
+    resourceList->outputBufferGPU = 0;
+  }
+
+  if (resourceList->inputBuffer != NULL) {
+    free(resourceList->inputBuffer);
+    resourceList->inputBuffer = NULL;
+  }
+  if (resourceList->outputBuffer != NULL) {
+    free(resourceList->outputBuffer);
+    resourceList->outputBuffer = NULL;
+  }
+
+  if (resourceList->stream != NULL) {
+    hipStreamDestroy(resourceList->stream);
+    resourceList->stream = NULL;
+  }
+}
+
+int main(int argc, char** argv) {
+  cudlaDevHandle devHandle;
+  cudlaModule moduleHandle;
+  cudlaStatus err;
+  FILE* fp = NULL;
+  struct stat st;
+  size_t file_size;
+  size_t actually_read = 0;
+  unsigned char* loadableData = NULL;
+
+  hipStream_t stream;
+  hipError_t result;
+  const char* errPtr = NULL;
+
+  ResourceList resourceList;
+
+  memset(&resourceList, 0x00, sizeof(ResourceList));
+
+  if (argc != 3) {
+    DPRINTF("Usage : ./cuDLAHybridMode <loadable> <imageFile>\n");
+    return 1;
+  }
+
+  // Read loadable into buffer.
+  fp = fopen(argv[1], "rb");
+  if (fp == NULL) {
+    DPRINTF("Cannot open file %s\n", argv[1]);
+    return 1;
+  }
+
+  if (stat(argv[1], &st) != 0) {
+    DPRINTF("Cannot stat file\n");
+    return 1;
+  }
+
+  file_size = st.st_size;
+  DPRINTF("The file size = %ld\n", file_size);
+
+  loadableData = (unsigned char*)malloc(file_size);
+  if (loadableData == NULL) {
+    DPRINTF("Cannot Allocate memory for loadable\n");
+    return 1;
+  }
+
+  actually_read = fread(loadableData, 1, file_size, fp);
+  if (actually_read != file_size) {
+    free(loadableData);
+    DPRINTF("Read wrong size\n");
+    return 1;
+  }
+  fclose(fp);
+
+  resourceList.loadableData = loadableData;
+
+  // Initialize CUDA.
+  result = hipFree(0);
+  if (result != hipSuccess) {
+    errPtr = hipGetErrorName(result);
+    DPRINTF("Error in creating hipFree = %s\n", errPtr);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  result = hipSetDevice(0);
+  if (result != hipSuccess) {
+    errPtr = hipGetErrorName(result);
+    DPRINTF("Error in creating hipSetDevice = %s\n", errPtr);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  err = cudlaCreateDevice(0, &devHandle, CUDLA_CUDA_DLA);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in cuDLA create device = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  DPRINTF("Device created successfully\n");
+  resourceList.devHandle = devHandle;
+
+  err = cudlaModuleLoadFromMemory(devHandle, loadableData, file_size,
+                                  &moduleHandle, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in cudlaModuleLoadFromMemory = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  } else {
+    DPRINTF("Successfully loaded module\n");
+  }
+
+  resourceList.moduleHandle = moduleHandle;
+
+  // Create CUDA stream.
+  result = hipStreamCreateWithFlags(&stream, hipStreamNonBlocking);
+
+  if (result != hipSuccess) {
+    errPtr = hipGetErrorName(result);
+    DPRINTF("Error in creating cuda stream = %s\n", errPtr);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.stream = stream;
+
+  // Get tensor attributes.
+  uint32_t numInputTensors = 0;
+  uint32_t numOutputTensors = 0;
+  cudlaModuleAttribute attribute;
+
+  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_NUM_INPUT_TENSORS,
+                                 &attribute);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in getting numInputTensors = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  numInputTensors = attribute.numInputTensors;
+  DPRINTF("numInputTensors = %d\n", numInputTensors);
+
+  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_NUM_OUTPUT_TENSORS,
+                                 &attribute);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in getting numOutputTensors = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  numOutputTensors = attribute.numOutputTensors;
+  DPRINTF("numOutputTensors = %d\n", numOutputTensors);
+
+  cudlaModuleTensorDescriptor* inputTensorDesc =
+      (cudlaModuleTensorDescriptor*)malloc(sizeof(cudlaModuleTensorDescriptor) *
+                                           numInputTensors);
+  cudlaModuleTensorDescriptor* outputTensorDesc =
+      (cudlaModuleTensorDescriptor*)malloc(sizeof(cudlaModuleTensorDescriptor) *
+                                           numOutputTensors);
+
+  if ((inputTensorDesc == NULL) || (outputTensorDesc == NULL)) {
+    if (inputTensorDesc != NULL) {
+      free(inputTensorDesc);
+      inputTensorDesc = NULL;
+    }
+
+    if (outputTensorDesc != NULL) {
+      free(outputTensorDesc);
+      outputTensorDesc = NULL;
+    }
+
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.inputTensorDesc = inputTensorDesc;
+  resourceList.outputTensorDesc = outputTensorDesc;
+
+  attribute.inputTensorDesc = inputTensorDesc;
+  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_INPUT_TENSOR_DESCRIPTORS,
+                                 &attribute);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in getting input tensor descriptor = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("Printing input tensor descriptor\n");
+  printTensorDesc(inputTensorDesc);
+
+  attribute.outputTensorDesc = outputTensorDesc;
+  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_OUTPUT_TENSOR_DESCRIPTORS,
+                                 &attribute);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in getting output tensor descriptor = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("Printing output tensor descriptor\n");
+  printTensorDesc(outputTensorDesc);
+
+  // Setup the input and output buffers which will be used as an input to CUDA.
+  unsigned char* inputBuffer = (unsigned char*)malloc(inputTensorDesc[0].size);
+  if (inputBuffer == NULL) {
+    DPRINTF("Error in allocating input memory\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.inputBuffer = inputBuffer;
+
+  unsigned char* outputBuffer =
+      (unsigned char*)malloc(outputTensorDesc[0].size);
+  if (outputBuffer == NULL) {
+    DPRINTF("Error in allocating output memory\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.outputBuffer = outputBuffer;
+
+  memset(inputBuffer, 0x00, inputTensorDesc[0].size);
+  memset(outputBuffer, 0x00, outputTensorDesc[0].size);
+
+  // Fill up the buffers with data.
+  if (initializeInputBuffers(argv[2], inputTensorDesc, inputBuffer) != 0) {
+    DPRINTF("Error in initializing input buffer\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  // Allocate memory on GPU.
+  void* inputBufferGPU;
+  void* outputBufferGPU;
+  result = hipMalloc(&inputBufferGPU, inputTensorDesc[0].size);
+  if (result != hipSuccess) {
+    DPRINTF("Error in allocating input memory on GPU\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.inputBufferGPU = inputBufferGPU;
+
+  result = hipMalloc(&outputBufferGPU, outputTensorDesc[0].size);
+  if (result != hipSuccess) {
+    DPRINTF("Error in allocating output memory on GPU\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.outputBufferGPU = outputBufferGPU;
+
+  // Register the CUDA-allocated buffers.
+  uint64_t* inputBufferRegisteredPtr = NULL;
+  uint64_t* outputBufferRegisteredPtr = NULL;
+
+  err = cudlaMemRegister(devHandle, (uint64_t*)inputBufferGPU,
+                         inputTensorDesc[0].size, &inputBufferRegisteredPtr, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in registering input memory = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  err =
+      cudlaMemRegister(devHandle, (uint64_t*)outputBufferGPU,
+                       outputTensorDesc[0].size, &outputBufferRegisteredPtr, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in registering output memory = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("ALL MEMORY REGISTERED SUCCESSFULLY\n");
+
+  // Copy data from CPU buffers to GPU buffers.
+  result = hipMemcpyAsync(inputBufferGPU, inputBuffer, inputTensorDesc[0].size,
+                           hipMemcpyHostToDevice, stream);
+  if (result != hipSuccess) {
+    DPRINTF("Error in enqueueing memcpy for input\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+  result =
+      hipMemsetAsync(outputBufferGPU, 0, outputTensorDesc[0].size, stream);
+  if (result != hipSuccess) {
+    DPRINTF("Error in enqueueing memset for output\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  // Enqueue a cuDLA task.
+  cudlaTask task;
+  task.moduleHandle = moduleHandle;
+  task.outputTensor = &outputBufferRegisteredPtr;
+  task.numOutputTensors = 1;
+  task.numInputTensors = 1;
+  task.inputTensor = &inputBufferRegisteredPtr;
+  task.waitEvents = NULL;
+  task.signalEvents = NULL;
+  err = cudlaSubmitTask(devHandle, &task, 1, stream, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in submitting task\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("SUBMIT IS DONE !!!\n");
+
+  // Wait for stream operations to finish and bring output buffer to CPU.
+  result =
+      hipMemcpyAsync(outputBuffer, outputBufferGPU, outputTensorDesc[0].size,
+                      hipMemcpyDeviceToHost, stream);
+  if (result != hipSuccess) {
+    DPRINTF("Error in bringing result back to CPU\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+  result = hipStreamSynchronize(stream);
+  if (result != hipSuccess) {
+    DPRINTF("Error in synchronizing stream\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  // Output is available in outputBuffer.
+
+  // Teardown.
+  err = cudlaMemUnregister(devHandle, inputBufferRegisteredPtr);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in unregistering input memory = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  err = cudlaMemUnregister(devHandle, outputBufferRegisteredPtr);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in registering output memory = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("ALL MEMORY UNREGISTERED SUCCESSFULLY\n");
+
+  free(inputTensorDesc);
+  free(outputTensorDesc);
+  free(loadableData);
+  free(inputBuffer);
+  free(outputBuffer);
+  hipFree(inputBufferGPU);
+  hipFree(outputBufferGPU);
+
+  resourceList.inputTensorDesc = NULL;
+  resourceList.outputTensorDesc = NULL;
+  resourceList.loadableData = NULL;
+  resourceList.inputBuffer = NULL;
+  resourceList.outputBuffer = NULL;
+  resourceList.inputBufferGPU = 0;
+  resourceList.outputBufferGPU = 0;
+
+  result = hipStreamDestroy(stream);
+  if (result != hipSuccess) {
+    errPtr = hipGetErrorName(result);
+    DPRINTF("Error in destroying cuda stream = %s\n", errPtr);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.stream = NULL;
+
+  err = cudlaModuleUnload(moduleHandle, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in cudlaModuleUnload = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  } else {
+    DPRINTF("Successfully unloaded module\n");
+  }
+
+  resourceList.moduleHandle = NULL;
+
+  err = cudlaDestroyDevice(devHandle);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in cuDLA destroy device = %d\n", err);
+    return 1;
+  }
+  DPRINTF("Device destroyed successfully\n");
+
+  resourceList.devHandle = NULL;
+
+  DPRINTF("cuDLAHybridMode DONE !!!\n");
+
+  return 0;
+}
diff --git a/src/samples/Samples/4_CUDA_Libraries/cudaNvSci/imageKernels.cu.hip b/src/samples/Samples/4_CUDA_Libraries/cudaNvSci/imageKernels.cu.hip
index e69de29..8d24339 100755
--- a/src/samples/Samples/4_CUDA_Libraries/cudaNvSci/imageKernels.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/cudaNvSci/imageKernels.cu.hip
@@ -0,0 +1,123 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <hip/hip_runtime.h>
+#include "helper_cuda_hipified.h"
+#include <helper_image.h>
+
+// convert floating point rgba color to 32-bit integer
+__device__ unsigned int rgbaFloatToInt(float4 rgba) {
+  rgba.x = __saturatef(rgba.x);  // clamp to [0.0, 1.0]
+  rgba.y = __saturatef(rgba.y);
+  rgba.z = __saturatef(rgba.z);
+  rgba.w = __saturatef(rgba.w);
+  return ((unsigned int)(rgba.w * 255.0f) << 24) |
+         ((unsigned int)(rgba.z * 255.0f) << 16) |
+         ((unsigned int)(rgba.y * 255.0f) << 8) |
+         ((unsigned int)(rgba.x * 255.0f));
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Rotate an image using texture lookups
+//! @param outputData  output data in global memory
+////////////////////////////////////////////////////////////////////////////////
+static __global__ void transformKernel(unsigned int *outputData, int width,
+                                       int height, float theta,
+                                       hipTextureObject_t tex) {
+  // calculate normalized texture coordinates
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  float u = (float)x - (float)width / 2;
+  float v = (float)y - (float)height / 2;
+  float tu = u * cosf(theta) - v * sinf(theta);
+  float tv = v * cosf(theta) + u * sinf(theta);
+
+  tu /= (float)width;
+  tv /= (float)height;
+
+  // read from texture and write to global memory
+  float4 pix = tex2D<float4>(tex, tu + 0.5f, tv + 0.5f);
+  unsigned int pixelInt = rgbaFloatToInt(pix);
+  outputData[y * width + x] = pixelInt;
+}
+
+static __global__ void rgbToGrayscaleKernel(unsigned int *rgbaImage,
+                                            size_t imageWidth,
+                                            size_t imageHeight) {
+  size_t gidX = blockDim.x * blockIdx.x + threadIdx.x;
+
+  uchar4 *pixArray = (uchar4 *)rgbaImage;
+
+  for (int pixId = gidX; pixId < imageWidth * imageHeight;
+       pixId += gridDim.x * blockDim.x) {
+    uchar4 dataA = pixArray[pixId];
+    unsigned char grayscale =
+        (unsigned char)(dataA.x * 0.3 + dataA.y * 0.59 + dataA.z * 0.11);
+    uchar4 dataB = make_uchar4(grayscale, grayscale, grayscale, 0);
+    pixArray[pixId] = dataB;
+  }
+}
+
+void launchGrayScaleKernel(unsigned int *d_rgbaImage,
+                           std::string image_filename, size_t imageWidth,
+                           size_t imageHeight, hipStream_t stream) {
+  int numThreadsPerBlock = 1024;
+  int numOfBlocks = (imageWidth * imageHeight) / numThreadsPerBlock;
+
+  rgbToGrayscaleKernel<<<numOfBlocks, numThreadsPerBlock, 0, stream>>>(
+      d_rgbaImage, imageWidth, imageHeight);
+
+  unsigned int *outputData;
+  HIPCHECK(hipHostMalloc((void**)&outputData, sizeof(unsigned int) * imageWidth * imageHeight));
+  HIPCHECK(hipMemcpyAsync(
+      outputData, d_rgbaImage, sizeof(unsigned int) * imageWidth * imageHeight,
+      hipMemcpyDeviceToHost, stream));
+  HIPCHECK(hipStreamSynchronize(stream));
+
+  char outputFilename[1024];
+  strcpy(outputFilename, image_filename.c_str());
+  strcpy(outputFilename + image_filename.length() - 4, "_out.ppm");
+  sdkSavePPM4ub(outputFilename, (unsigned char *)outputData, imageWidth,
+                imageHeight);
+  printf("Wrote '%s'\n", outputFilename);
+
+  HIPCHECK(hipHostFree(outputData));
+}
+
+void rotateKernel(hipTextureObject_t &texObj, const float angle,
+                  unsigned int *d_outputData, const int imageWidth,
+                  const int imageHeight, hipStream_t stream) {
+  dim3 dimBlock(8, 8, 1);
+  dim3 dimGrid(imageWidth / dimBlock.x, imageHeight / dimBlock.y, 1);
+
+  transformKernel<<<dimGrid, dimBlock, 0, stream>>>(d_outputData, imageWidth,
+                                                    imageHeight, angle, texObj);
+}
+geHeight, angle, texObj);
+}
diff --git a/src/samples/Samples/4_CUDA_Libraries/cudaNvSciNvMedia/cuda_consumer.cu.hip b/src/samples/Samples/4_CUDA_Libraries/cudaNvSciNvMedia/cuda_consumer.cu.hip
index e69de29..f464b2a 100755
--- a/src/samples/Samples/4_CUDA_Libraries/cudaNvSciNvMedia/cuda_consumer.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/cudaNvSciNvMedia/cuda_consumer.cu.hip
@@ -0,0 +1,436 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <iostream>
+
+#include <hip/hip_runtime.h>
+#include "cuda_consumer.h"
+#include <helper_image.h>
+#include "nvmedia_image_nvscibuf.h"
+#include "nvmedia_utils/cmdline.h"
+
+// Enable this to 1 if require cuda processed output to ppm file.
+#define WRITE_OUTPUT_IMAGE 0
+
+#define checkNvSciErrors(call)                              \
+  do {                                                      \
+    NvSciError _status = call;                              \
+    if (NvSciError_Success != _status) {                    \
+      printf(                                               \
+          "NVSCI call in file '%s' in line %i returned"     \
+          " %d, expected %d\n",                             \
+          __FILE__, __LINE__, _status, NvSciError_Success); \
+      fflush(stdout);                                       \
+      exit(EXIT_FAILURE);                                   \
+    }                                                       \
+  } while (0)
+
+__global__ static void yuvToGrayscale(hipSurfaceObject_t surfaceObject,
+                                      unsigned int *dstImage,
+                                      int32_t imageWidth, int32_t imageHeight) {
+  size_t x = blockIdx.x * blockDim.x + threadIdx.x;
+  size_t y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  uchar4 *dstImageUchar4 = (uchar4 *)dstImage;
+  for (; x < imageWidth && y < imageHeight;
+       x += gridDim.x * blockDim.x, y += gridDim.y * blockDim.y) {
+    int colInBytes = x * sizeof(unsigned char);
+    unsigned char luma =
+        surf2Dread<unsigned char>(surfaceObject, colInBytes, y);
+    uchar4 grayscalePix = make_uchar4(luma, luma, luma, 0);
+
+    dstImageUchar4[y * imageWidth + x] = grayscalePix;
+  }
+}
+
+static void cudaImportNvSciSync(hipExternalSemaphore_t &extSem,
+                                NvSciSyncObj &syncObj) {
+  hipExternalSemaphoreHandleDesc extSemDesc;
+  memset(&extSemDesc, 0, sizeof(extSemDesc));
+  extSemDesc.type = cudaExternalSemaphoreHandleTypeNvSciSync;
+  extSemDesc.handle.nvSciSyncObj = (void *)syncObj;
+
+  HIPCHECK(hipImportExternalSemaphore(&extSem, &extSemDesc));
+}
+
+static void waitExternalSemaphore(hipExternalSemaphore_t &waitSem,
+                                  NvSciSyncFence *fence, hipStream_t stream) {
+  hipExternalSemaphoreWaitParams waitParams;
+  memset(&waitParams, 0, sizeof(waitParams));
+  // For cross-process signaler-waiter applications need to use NvSciIpc
+  // and NvSciSync[Export|Import] utilities to share the NvSciSyncFence
+  // across process. This step is optional in single-process.
+  waitParams.params.nvSciSync.fence = (void *)fence;
+  waitParams.flags = 0;
+
+  HIPCHECK(
+      hipWaitExternalSemaphoresAsync(&waitSem, &waitParams, 1, stream));
+}
+
+static void signalExternalSemaphore(hipExternalSemaphore_t &signalSem,
+                                    NvSciSyncFence *fence,
+                                    hipStream_t stream) {
+  hipExternalSemaphoreSignalParams signalParams;
+  memset(&signalParams, 0, sizeof(signalParams));
+  // For cross-process signaler-waiter applications need to use NvSciIpc
+  // and NvSciSync[Export|Import] utilities to share the NvSciSyncFence
+  // across process. This step is optional in single-process.
+  signalParams.params.nvSciSync.fence = (void *)fence;
+  signalParams.flags = 0;
+
+  HIPCHECK(
+      hipSignalExternalSemaphoresAsync(&signalSem, &signalParams, 1, stream));
+}
+
+static void yuvToGrayscaleCudaKernel(cudaExternalResInterop &cudaExtResObj,
+                                     int32_t imageWidth, int32_t imageHeight) {
+#if WRITE_OUTPUT_IMAGE
+  unsigned int *h_dstImage;
+  HIPCHECK(hipHostMalloc(
+      &h_dstImage, sizeof(unsigned int) * imageHeight * imageWidth));
+#endif
+  dim3 block(16, 16, 1);
+  dim3 grid((imageWidth / block.x) + 1, (imageHeight / block.y) + 1, 1);
+
+  yuvToGrayscale<<<grid, block, 0, cudaExtResObj.stream>>>(
+      cudaExtResObj.cudaSurfaceNvmediaBuf[0], cudaExtResObj.d_outputImage,
+      imageWidth, imageHeight);
+
+#if WRITE_OUTPUT_IMAGE
+  HIPCHECK(
+      hipMemcpyAsync(h_dstImage, cudaExtResObj.d_outputImage,
+                      sizeof(unsigned int) * imageHeight * imageWidth,
+                      hipMemcpyDeviceToHost, cudaExtResObj.stream));
+  HIPCHECK(hipStreamSynchronize(cudaExtResObj.stream));
+  char outputFilename[1024];
+  std::string image_filename = "Grayscale";
+  strcpy(outputFilename, image_filename.c_str());
+  strcpy(outputFilename + image_filename.length(), "_nvsci_out.ppm");
+  sdkSavePPM4ub(outputFilename, (unsigned char *)h_dstImage, imageWidth,
+                imageHeight);
+  printf("Wrote '%s'\n", outputFilename);
+  HIPCHECK(hipHostFree(h_dstImage));
+#endif
+}
+
+static void cudaImportNvSciImage(cudaExternalResInterop &cudaExtResObj,
+                                 NvSciBufObj &inputBufObj) {
+  NvSciBufModule module = NULL;
+  NvSciBufAttrList attrlist = NULL;
+  NvSciBufAttrKeyValuePair pairArrayOut[10];
+
+  checkNvSciErrors(NvSciBufModuleOpen(&module));
+  checkNvSciErrors(NvSciBufAttrListCreate(module, &attrlist));
+  checkNvSciErrors(NvSciBufObjGetAttrList(inputBufObj, &attrlist));
+
+  memset(pairArrayOut, 0, sizeof(NvSciBufAttrKeyValuePair) * 10);
+
+  int numAttrs = 0;
+  pairArrayOut[numAttrs++].key = NvSciBufImageAttrKey_Size;
+  pairArrayOut[numAttrs++].key = NvSciBufImageAttrKey_PlaneChannelCount;
+  pairArrayOut[numAttrs++].key = NvSciBufImageAttrKey_PlaneCount;
+  pairArrayOut[numAttrs++].key = NvSciBufImageAttrKey_PlaneWidth;
+  pairArrayOut[numAttrs++].key = NvSciBufImageAttrKey_PlaneHeight;
+  pairArrayOut[numAttrs++].key = NvSciBufImageAttrKey_Layout;
+  pairArrayOut[numAttrs++].key = NvSciBufImageAttrKey_PlaneBitsPerPixel;
+  pairArrayOut[numAttrs++].key = NvSciBufImageAttrKey_PlaneOffset;
+
+  checkNvSciErrors(NvSciBufAttrListGetAttrs(attrlist, pairArrayOut, numAttrs));
+
+  uint64_t size = *(uint64_t *)pairArrayOut[0].value;
+  uint8_t channelCount = *(uint8_t *)pairArrayOut[1].value;
+  cudaExtResObj.planeCount = *(int32_t *)pairArrayOut[2].value;
+  cudaExtResObj.imageWidth =
+      (int32_t *)malloc(sizeof(int32_t) * cudaExtResObj.planeCount);
+  cudaExtResObj.imageHeight =
+      (int32_t *)malloc(sizeof(int32_t) * cudaExtResObj.planeCount);
+  cudaExtResObj.planeOffset =
+      (uint64_t *)malloc(sizeof(uint64_t) * cudaExtResObj.planeCount);
+
+  memcpy(cudaExtResObj.imageWidth, (int32_t *)pairArrayOut[3].value,
+         cudaExtResObj.planeCount * sizeof(int32_t));
+  memcpy(cudaExtResObj.imageHeight, (int32_t *)pairArrayOut[4].value,
+         cudaExtResObj.planeCount * sizeof(int32_t));
+  memcpy(cudaExtResObj.planeOffset, (uint64_t *)pairArrayOut[7].value,
+         cudaExtResObj.planeCount * sizeof(uint64_t));
+
+  NvSciBufAttrValImageLayoutType layout =
+      *(NvSciBufAttrValImageLayoutType *)pairArrayOut[5].value;
+  uint32_t bitsPerPixel = *(uint32_t *)pairArrayOut[6].value;
+
+  if (layout != NvSciBufImage_BlockLinearType) {
+    printf("Image layout is not block linear.. waiving execution\n");
+    exit(EXIT_WAIVED);
+  }
+
+  hipExternalMemoryHandleDesc memHandleDesc;
+  memset(&memHandleDesc, 0, sizeof(memHandleDesc));
+  memHandleDesc.type = cudaExternalMemoryHandleTypeNvSciBuf;
+  memHandleDesc.handle.nvSciBufObject = inputBufObj;
+  memHandleDesc.size = size;
+  HIPCHECK(
+      hipImportExternalMemory(&cudaExtResObj.extMemImageBuf, &memHandleDesc));
+
+  cudaExtResObj.d_mipmapArray = (hipMipmappedArray_t *)malloc(
+      sizeof(hipMipmappedArray_t) * cudaExtResObj.planeCount);
+
+  for (int i = 0; i < cudaExtResObj.planeCount; i++) {
+    hipExtent extent = {};
+    memset(&extent, 0, sizeof(extent));
+    extent.width = cudaExtResObj.imageWidth[i];
+    extent.height = cudaExtResObj.imageHeight[i];
+    extent.depth = 0;
+    hipChannelFormatDesc desc;
+    switch (channelCount) {
+      case 1:
+      default:
+        desc = hipCreateChannelDesc(bitsPerPixel, 0, 0, 0,
+                                     hipChannelFormatKindUnsigned);
+        break;
+      case 2:
+        desc = hipCreateChannelDesc(bitsPerPixel, bitsPerPixel, 0, 0,
+                                     hipChannelFormatKindUnsigned);
+        break;
+      case 3:
+        desc = hipCreateChannelDesc(bitsPerPixel, bitsPerPixel, bitsPerPixel,
+                                     0, hipChannelFormatKindUnsigned);
+        break;
+      case 4:
+        desc =
+            hipCreateChannelDesc(bitsPerPixel, bitsPerPixel, bitsPerPixel,
+                                  bitsPerPixel, hipChannelFormatKindUnsigned);
+        break;
+    }
+
+    cudaExternalMemoryMipmappedArrayDesc mipmapDesc = {0};
+    mipmapDesc.offset = cudaExtResObj.planeOffset[i];
+    mipmapDesc.formatDesc = desc;
+    mipmapDesc.extent = extent;
+    mipmapDesc.flags = 0;
+    mipmapDesc.numLevels = 1;
+    HIPCHECK(cudaExternalMemoryGetMappedMipmappedArray(
+        &cudaExtResObj.d_mipmapArray[i], cudaExtResObj.extMemImageBuf,
+        &mipmapDesc));
+  }
+}
+
+static hipSurfaceObject_t createCudaSurface(hipArray_t &d_mipLevelArray) {
+  hipResourceDesc resourceDesc;
+  memset(&resourceDesc, 0, sizeof(resourceDesc));
+  resourceDesc.resType = hipResourceTypeArray;
+  resourceDesc.res.array.array = d_mipLevelArray;
+
+  hipSurfaceObject_t surfaceObject;
+  HIPCHECK(hipCreateSurfaceObject(&surfaceObject, &resourceDesc));
+  return surfaceObject;
+}
+
+static hipStream_t createCudaStream(int deviceId) {
+  HIPCHECK(hipSetDevice(deviceId));
+  hipStream_t stream;
+  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
+  return stream;
+}
+
+// CUDA setup buffers/synchronization objects for interop via NvSci API.
+void setupCuda(cudaExternalResInterop &cudaExtResObj, NvSciBufObj &inputBufObj,
+               NvSciSyncObj &syncObj, NvSciSyncObj &cudaSignalerSyncObj,
+               int deviceId) {
+  HIPCHECK(hipSetDevice(deviceId));
+  cudaImportNvSciSync(cudaExtResObj.waitSem, syncObj);
+  cudaImportNvSciSync(cudaExtResObj.signalSem, cudaSignalerSyncObj);
+
+  cudaImportNvSciImage(cudaExtResObj, inputBufObj);
+  cudaExtResObj.d_mipLevelArray =
+      (hipArray_t *)malloc(sizeof(hipArray_t) * cudaExtResObj.planeCount);
+  cudaExtResObj.cudaSurfaceNvmediaBuf = (hipSurfaceObject_t *)malloc(
+      sizeof(hipSurfaceObject_t) * cudaExtResObj.planeCount);
+
+  for (int i = 0; i < cudaExtResObj.planeCount; ++i) {
+    uint32_t mipLevelId = 0;
+    HIPCHECK(
+        hipGetMipmappedArrayLevel(&cudaExtResObj.d_mipLevelArray[i],
+                                   cudaExtResObj.d_mipmapArray[i], mipLevelId));
+    cudaExtResObj.cudaSurfaceNvmediaBuf[i] =
+        createCudaSurface(cudaExtResObj.d_mipLevelArray[i]);
+  }
+
+  cudaExtResObj.stream = createCudaStream(deviceId);
+  HIPCHECK(hipMalloc(&cudaExtResObj.d_outputImage,
+                             sizeof(unsigned int) *
+                                 cudaExtResObj.imageWidth[0] *
+                                 cudaExtResObj.imageHeight[0]));
+}
+
+// CUDA clean up buffers used **with** NvSci API.
+void cleanupCuda(cudaExternalResInterop &cudaExtResObj) {
+  for (int i = 0; i < cudaExtResObj.planeCount; i++) {
+    HIPCHECK(
+        hipDestroySurfaceObject(cudaExtResObj.cudaSurfaceNvmediaBuf[i]));
+    HIPCHECK(hipFreeMipmappedArray(cudaExtResObj.d_mipmapArray[i]));
+  }
+  free(cudaExtResObj.d_mipmapArray);
+  free(cudaExtResObj.d_mipLevelArray);
+  free(cudaExtResObj.cudaSurfaceNvmediaBuf);
+  free(cudaExtResObj.imageWidth);
+  free(cudaExtResObj.imageHeight);
+  HIPCHECK(hipDestroyExternalSemaphore(cudaExtResObj.waitSem));
+  HIPCHECK(hipDestroyExternalSemaphore(cudaExtResObj.signalSem));
+  HIPCHECK(hipDestroyExternalMemory(cudaExtResObj.extMemImageBuf));
+  HIPCHECK(hipStreamDestroy(cudaExtResObj.stream));
+  HIPCHECK(hipFree(cudaExtResObj.d_outputImage));
+}
+
+void runCudaOperation(cudaExternalResInterop &cudaExtResObj,
+                      NvSciSyncFence *cudaWaitFence,
+                      NvSciSyncFence *cudaSignalFence, int deviceId,
+                      int iterations) {
+  HIPCHECK(hipSetDevice(deviceId));
+  static int64_t launch = 0;
+
+  waitExternalSemaphore(cudaExtResObj.waitSem, cudaWaitFence,
+                        cudaExtResObj.stream);
+
+  // run cuda kernel over surface object of the LUMA surface part to extract
+  // grayscale.
+  yuvToGrayscaleCudaKernel(cudaExtResObj, cudaExtResObj.imageWidth[0],
+                           cudaExtResObj.imageHeight[0]);
+
+  // signal fence till the second last iterations for NvMedia2DBlit to wait for
+  // cuda signal and for final iteration as there is no corresponding NvMedia
+  // operation pending therefore we end with hipStreamSynchronize()
+  if (launch < iterations - 1) {
+    signalExternalSemaphore(cudaExtResObj.signalSem, cudaSignalFence,
+                            cudaExtResObj.stream);
+  } else {
+    HIPCHECK(hipStreamSynchronize(cudaExtResObj.stream));
+  }
+  launch++;
+}
+
+// CUDA imports and operates on NvSci buffer/synchronization objects
+void setupCuda(Blit2DTest *ctx, cudaResources &cudaResObj, int deviceId) {
+  HIPCHECK(hipSetDevice(deviceId));
+  cudaResObj.d_yuvArray =
+      (hipArray_t *)malloc(sizeof(hipArray_t) * ctx->numSurfaces);
+  cudaResObj.cudaSurfaceNvmediaBuf = (hipSurfaceObject_t *)malloc(
+      sizeof(hipSurfaceObject_t) * ctx->numSurfaces);
+  hipChannelFormatDesc channelDesc;
+  switch (ctx->bytesPerPixel) {
+    case 1:
+    default:
+      channelDesc =
+          hipCreateChannelDesc(8, 0, 0, 0, hipChannelFormatKindUnsigned);
+      break;
+  }
+
+  for (int k = 0; k < ctx->numSurfaces; k++) {
+    HIPCHECK(hipMallocArray(
+        &cudaResObj.d_yuvArray[k], &channelDesc,
+        ctx->widthSurface * ctx->xScalePtr[k] * ctx->bytesPerPixel,
+        ctx->heightSurface * ctx->yScalePtr[k]));
+    cudaResObj.cudaSurfaceNvmediaBuf[k] =
+        createCudaSurface(cudaResObj.d_yuvArray[k]);
+  }
+  HIPCHECK(hipMalloc(
+      &cudaResObj.d_outputImage,
+      sizeof(unsigned int) * ctx->widthSurface * ctx->heightSurface));
+
+  cudaResObj.stream = createCudaStream(deviceId);
+}
+
+// CUDA clean up buffers used **without** NvSci API.
+void cleanupCuda(Blit2DTest *ctx, cudaResources &cudaResObj) {
+  for (int k = 0; k < ctx->numSurfaces; k++) {
+    HIPCHECK(
+        hipDestroySurfaceObject(cudaResObj.cudaSurfaceNvmediaBuf[k]));
+    HIPCHECK(hipFreeArray(cudaResObj.d_yuvArray[k]));
+  }
+
+  free(cudaResObj.cudaSurfaceNvmediaBuf);
+
+  HIPCHECK(hipStreamDestroy(cudaResObj.stream));
+  HIPCHECK(hipFree(cudaResObj.d_outputImage));
+}
+
+static void yuvToGrayscaleCudaKernelNonNvSci(cudaResources &cudaResObj,
+                                             int deviceId, int32_t imageWidth,
+                                             int32_t imageHeight) {
+#if WRITE_OUTPUT_IMAGE
+  unsigned int *h_dstImage;
+  HIPCHECK(hipHostMalloc(
+      &h_dstImage, sizeof(unsigned int) * imageHeight * imageWidth));
+#endif
+  dim3 block(16, 16, 1);
+  dim3 grid((imageWidth / block.x) + 1, (imageHeight / block.y) + 1, 1);
+
+  yuvToGrayscale<<<grid, block, 0, cudaResObj.stream>>>(
+      cudaResObj.cudaSurfaceNvmediaBuf[0], cudaResObj.d_outputImage, imageWidth,
+      imageHeight);
+
+#if WRITE_OUTPUT_IMAGE
+  HIPCHECK(
+      hipMemcpyAsync(h_dstImage, cudaResObj.d_outputImage,
+                      sizeof(unsigned int) * imageHeight * imageWidth,
+                      hipMemcpyDeviceToHost, cudaResObj.stream));
+  HIPCHECK(hipStreamSynchronize(cudaResObj.stream));
+  char outputFilename[1024];
+  std::string image_filename = "Grayscale";
+  strcpy(outputFilename, image_filename.c_str());
+  strcpy(outputFilename + image_filename.length(), "_non-nvsci_out.ppm");
+  sdkSavePPM4ub(outputFilename, (unsigned char *)h_dstImage, imageWidth,
+                imageHeight);
+  printf("Wrote '%s'\n", outputFilename);
+  HIPCHECK(hipHostFree(h_dstImage));
+#else
+  HIPCHECK(hipStreamSynchronize(cudaResObj.stream));
+#endif
+}
+
+// CUDA operates **without** NvSci APIs buffer/synchronization objects.
+void runCudaOperation(Blit2DTest *ctx, cudaResources &cudaResObj,
+                      int deviceId) {
+  for (int k = 0; k < ctx->numSurfaces; k++) {
+    HIPCHECK(hipMemcpy2DToArray(
+        cudaResObj.d_yuvArray[k], 0, 0, ctx->dstBuff[k],
+        ctx->widthSurface * ctx->xScalePtr[k] * ctx->bytesPerPixel,
+        ctx->widthSurface * ctx->xScalePtr[k] * ctx->bytesPerPixel,
+        ctx->heightSurface * ctx->yScalePtr[k], hipMemcpyHostToDevice));
+  }
+  // run cuda kernel over surface object of the LUMA surface part to extract
+  // grayscale.
+  yuvToGrayscaleCudaKernelNonNvSci(cudaResObj, deviceId, ctx->widthSurface,
+                                   ctx->heightSurface);
+}
+emcpyHostToDevice));
+  }
+  // run cuda kernel over surface object of the LUMA surface part to extract
+  // grayscale.
+  yuvToGrayscaleCudaKernelNonNvSci(cudaResObj, deviceId, ctx->widthSurface,
+                                   ctx->heightSurface);
+}
diff --git a/src/samples/Samples/4_CUDA_Libraries/lineOfSight/lineOfSight.cu.hip b/src/samples/Samples/4_CUDA_Libraries/lineOfSight/lineOfSight.cu.hip
index b9d0a43..7edb61c 100755
--- a/src/samples/Samples/4_CUDA_Libraries/lineOfSight/lineOfSight.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/lineOfSight/lineOfSight.cu.hip
@@ -152,11 +152,11 @@ int runTest(int argc, char **argv) {
   hipChannelFormatDesc channelDesc =
       hipCreateChannelDesc(32, 0, 0, 0, hipChannelFormatKindFloat);
   hipArray *heightFieldArray;
-  checkCudaErrors(
+  HIPCHECK(
       hipMallocArray(&heightFieldArray, &channelDesc, dim.x, dim.y));
 
   // Initialize device memory
-  checkCudaErrors(hipMemcpy2DToArray(
+  HIPCHECK(hipMemcpy2DToArray(
       heightFieldArray, 0, 0, heightField.height, dim.x * sizeof(float),
       dim.x * sizeof(float), dim.y, hipMemcpyHostToDevice));
 
@@ -175,7 +175,7 @@ int runTest(int argc, char **argv) {
   texDescr.addressMode[1] = hipAddressModeClamp;
   texDescr.readMode = hipReadModeElementType;
 
-  checkCudaErrors(
+  HIPCHECK(
       hipCreateTextureObject(&heightFieldTex, &texRes, &texDescr, NULL));
 
   //////////////////////////////////////////////////////////////////////////////
@@ -256,7 +256,7 @@ int runTest(int argc, char **argv) {
   sdkResetTimer(&timer);
 
   // Cleanup memory
-  checkCudaErrors(hipFreeArray(heightFieldArray));
+  HIPCHECK(hipFreeArray(heightFieldArray));
   return res;
 }
 
@@ -344,3 +344,5 @@ __device__ __host__ float getAngle(const Ray ray, float2 location,
   float2 dir = location - make_float2(ray.origin.x, ray.origin.y);
   return atanf((height - ray.origin.z) / length(dir));
 }
+origin.z) / length(dir));
+}
diff --git a/src/samples/Samples/4_CUDA_Libraries/oceanFFT/oceanFFT_kernel.cu.hip b/src/samples/Samples/4_CUDA_Libraries/oceanFFT/oceanFFT_kernel.cu.hip
index e69de29..16d309d 100755
--- a/src/samples/Samples/4_CUDA_Libraries/oceanFFT/oceanFFT_kernel.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/oceanFFT/oceanFFT_kernel.cu.hip
@@ -0,0 +1,160 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+///////////////////////////////////////////////////////////////////////////////
+#include <hipfft.h>
+#include <math_constants.h>
+
+// Round a / b to nearest higher integer value
+int cuda_iDivUp(int a, int b) { return (a + (b - 1)) / b; }
+
+// complex math functions
+__device__ float2 conjugate(float2 arg) { return make_float2(arg.x, -arg.y); }
+
+__device__ float2 complex_exp(float arg) {
+  return make_float2(cosf(arg), sinf(arg));
+}
+
+__device__ float2 complex_add(float2 a, float2 b) {
+  return make_float2(a.x + b.x, a.y + b.y);
+}
+
+__device__ float2 complex_mult(float2 ab, float2 cd) {
+  return make_float2(ab.x * cd.x - ab.y * cd.y, ab.x * cd.y + ab.y * cd.x);
+}
+
+// generate wave heightfield at time t based on initial heightfield and
+// dispersion relationship
+__global__ void generateSpectrumKernel(float2 *h0, float2 *ht,
+                                       unsigned int in_width,
+                                       unsigned int out_width,
+                                       unsigned int out_height, float t,
+                                       float patchSize) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+  unsigned int in_index = y * in_width + x;
+  unsigned int in_mindex =
+      (out_height - y) * in_width + (out_width - x);  // mirrored
+  unsigned int out_index = y * out_width + x;
+
+  // calculate wave vector
+  float2 k;
+  k.x = (-(int)out_width / 2.0f + x) * (2.0f * CUDART_PI_F / patchSize);
+  k.y = (-(int)out_width / 2.0f + y) * (2.0f * CUDART_PI_F / patchSize);
+
+  // calculate dispersion w(k)
+  float k_len = sqrtf(k.x * k.x + k.y * k.y);
+  float w = sqrtf(9.81f * k_len);
+
+  if ((x < out_width) && (y < out_height)) {
+    float2 h0_k = h0[in_index];
+    float2 h0_mk = h0[in_mindex];
+
+    // output frequency-space complex values
+    ht[out_index] =
+        complex_add(complex_mult(h0_k, complex_exp(w * t)),
+                    complex_mult(conjugate(h0_mk), complex_exp(-w * t)));
+    // ht[out_index] = h0_k;
+  }
+}
+
+// update height map values based on output of FFT
+__global__ void updateHeightmapKernel(float *heightMap, float2 *ht,
+                                      unsigned int width) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+  unsigned int i = y * width + x;
+
+  // cos(pi * (m1 + m2))
+  float sign_correction = ((x + y) & 0x01) ? -1.0f : 1.0f;
+
+  heightMap[i] = ht[i].x * sign_correction;
+}
+
+// update height map values based on output of FFT
+__global__ void updateHeightmapKernel_y(float *heightMap, float2 *ht,
+                                        unsigned int width) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+  unsigned int i = y * width + x;
+
+  // cos(pi * (m1 + m2))
+  float sign_correction = ((x + y) & 0x01) ? -1.0f : 1.0f;
+
+  heightMap[i] = ht[i].y * sign_correction;
+}
+
+// generate slope by partial differences in spatial domain
+__global__ void calculateSlopeKernel(float *h, float2 *slopeOut,
+                                     unsigned int width, unsigned int height) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+  unsigned int i = y * width + x;
+
+  float2 slope = make_float2(0.0f, 0.0f);
+
+  if ((x > 0) && (y > 0) && (x < width - 1) && (y < height - 1)) {
+    slope.x = h[i + 1] - h[i - 1];
+    slope.y = h[i + width] - h[i - width];
+  }
+
+  slopeOut[i] = slope;
+}
+
+// wrapper functions
+extern "C" void cudaGenerateSpectrumKernel(float2 *d_h0, float2 *d_ht,
+                                           unsigned int in_width,
+                                           unsigned int out_width,
+                                           unsigned int out_height,
+                                           float animTime, float patchSize) {
+  dim3 block(8, 8, 1);
+  dim3 grid(cuda_iDivUp(out_width, block.x), cuda_iDivUp(out_height, block.y),
+            1);
+  generateSpectrumKernel<<<grid, block>>>(d_h0, d_ht, in_width, out_width,
+                                          out_height, animTime, patchSize);
+}
+
+extern "C" void cudaUpdateHeightmapKernel(float *d_heightMap, float2 *d_ht,
+                                          unsigned int width,
+                                          unsigned int height, bool autoTest) {
+  dim3 block(8, 8, 1);
+  dim3 grid(cuda_iDivUp(width, block.x), cuda_iDivUp(height, block.y), 1);
+  if (autoTest) {
+    updateHeightmapKernel_y<<<grid, block>>>(d_heightMap, d_ht, width);
+  } else {
+    updateHeightmapKernel<<<grid, block>>>(d_heightMap, d_ht, width);
+  }
+}
+
+extern "C" void cudaCalculateSlopeKernel(float *hptr, float2 *slopeOut,
+                                         unsigned int width,
+                                         unsigned int height) {
+  dim3 block(8, 8, 1);
+  dim3 grid2(cuda_iDivUp(width, block.x), cuda_iDivUp(height, block.y), 1);
+  calculateSlopeKernel<<<grid2, block>>>(hptr, slopeOut, width, height);
+}
diff --git a/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT/simpleCUFFT.cu.hip b/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT/simpleCUFFT.cu.hip
index e69de29..9b500a1 100755
--- a/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT/simpleCUFFT.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT/simpleCUFFT.cu.hip
@@ -0,0 +1,291 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* Example showing the use of CUFFT for fast 1D-convolution using FFT. */
+
+// includes, system
+#include <math.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+// includes, project
+#include <hip/hip_runtime.h>
+#include <hipfft.h>
+#include <hipfftXt.h>
+#include "helper_cuda_hipified.h"
+#include "helper_functions.h"
+
+// Complex data type
+typedef float2 Complex;
+static __device__ __host__ inline Complex ComplexAdd(Complex, Complex);
+static __device__ __host__ inline Complex ComplexScale(Complex, float);
+static __device__ __host__ inline Complex ComplexMul(Complex, Complex);
+static __global__ void ComplexPointwiseMulAndScale(Complex *, const Complex *,
+                                                   int, float);
+
+// Filtering functions
+void Convolve(const Complex *, int, const Complex *, int, Complex *);
+
+// Padding functions
+int PadData(const Complex *, Complex **, int, const Complex *, Complex **, int);
+
+////////////////////////////////////////////////////////////////////////////////
+// declaration, forward
+void runTest(int argc, char **argv);
+
+// The filter size is assumed to be a number smaller than the signal size
+#define SIGNAL_SIZE 50
+#define FILTER_KERNEL_SIZE 11
+
+////////////////////////////////////////////////////////////////////////////////
+// Program main
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv) { runTest(argc, argv); }
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run a simple test for CUDA
+////////////////////////////////////////////////////////////////////////////////
+void runTest(int argc, char **argv) {
+  printf("[simpleCUFFT] is starting...\n");
+
+  findCudaDevice(argc, (const char **)argv);
+
+  // Allocate host memory for the signal
+  Complex *h_signal =
+      reinterpret_cast<Complex *>(malloc(sizeof(Complex) * SIGNAL_SIZE));
+
+  // Initialize the memory for the signal
+  for (unsigned int i = 0; i < SIGNAL_SIZE; ++i) {
+    h_signal[i].x = rand() / static_cast<float>(RAND_MAX);
+    h_signal[i].y = 0;
+  }
+
+  // Allocate host memory for the filter
+  Complex *h_filter_kernel =
+      reinterpret_cast<Complex *>(malloc(sizeof(Complex) * FILTER_KERNEL_SIZE));
+
+  // Initialize the memory for the filter
+  for (unsigned int i = 0; i < FILTER_KERNEL_SIZE; ++i) {
+    h_filter_kernel[i].x = rand() / static_cast<float>(RAND_MAX);
+    h_filter_kernel[i].y = 0;
+  }
+
+  // Pad signal and filter kernel
+  Complex *h_padded_signal;
+  Complex *h_padded_filter_kernel;
+  int new_size =
+      PadData(h_signal, &h_padded_signal, SIGNAL_SIZE, h_filter_kernel,
+              &h_padded_filter_kernel, FILTER_KERNEL_SIZE);
+  int mem_size = sizeof(Complex) * new_size;
+
+  // Allocate device memory for signal
+  Complex *d_signal;
+  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_signal), mem_size));
+  // Copy host memory to device
+  HIPCHECK(
+      hipMemcpy(d_signal, h_padded_signal, mem_size, hipMemcpyHostToDevice));
+
+  // Allocate device memory for filter kernel
+  Complex *d_filter_kernel;
+  HIPCHECK(
+      hipMalloc(reinterpret_cast<void **>(&d_filter_kernel), mem_size));
+
+  // Copy host memory to device
+  HIPCHECK(hipMemcpy(d_filter_kernel, h_padded_filter_kernel, mem_size,
+                             hipMemcpyHostToDevice));
+
+  // CUFFT plan simple API
+  hipfftHandle plan;
+  HIPCHECK(hipfftPlan1d(&plan, new_size, HIPFFT_C2C, 1));
+
+  // CUFFT plan advanced API
+  hipfftHandle plan_adv;
+  size_t workSize;
+  long long int new_size_long = new_size;
+
+  HIPCHECK(hipfftCreate(&plan_adv));
+  HIPCHECK(cufftXtMakePlanMany(plan_adv, 1, &new_size_long, NULL, 1, 1,
+                                      HIPBLAS_C_32F, NULL, 1, 1, HIPBLAS_C_32F, 1,
+                                      &workSize, HIPBLAS_C_32F));
+  printf("Temporary buffer size %li bytes\n", workSize);
+
+  // Transform signal and kernel
+  printf("Transforming signal hipfftExecC2C\n");
+  HIPCHECK(hipfftExecC2C(plan, reinterpret_cast<hipfftComplex *>(d_signal),
+                               reinterpret_cast<hipfftComplex *>(d_signal),
+                               HIPFFT_FORWARD));
+  HIPCHECK(hipfftExecC2C(
+      plan_adv, reinterpret_cast<hipfftComplex *>(d_filter_kernel),
+      reinterpret_cast<hipfftComplex *>(d_filter_kernel), HIPFFT_FORWARD));
+
+  // Multiply the coefficients together and normalize the result
+  printf("Launching ComplexPointwiseMulAndScale<<< >>>\n");
+  ComplexPointwiseMulAndScale<<<32, 256>>>(d_signal, d_filter_kernel, new_size,
+                                           1.0f / new_size);
+
+  // Check if kernel execution generated and error
+  getLastCudaError("Kernel execution failed [ ComplexPointwiseMulAndScale ]");
+
+  // Transform signal back
+  printf("Transforming signal back hipfftExecC2C\n");
+  HIPCHECK(hipfftExecC2C(plan, reinterpret_cast<hipfftComplex *>(d_signal),
+                               reinterpret_cast<hipfftComplex *>(d_signal),
+                               HIPFFT_BACKWARD));
+
+  // Copy device memory to host
+  Complex *h_convolved_signal = h_padded_signal;
+  HIPCHECK(hipMemcpy(h_convolved_signal, d_signal, mem_size,
+                             hipMemcpyDeviceToHost));
+
+  // Allocate host memory for the convolution result
+  Complex *h_convolved_signal_ref =
+      reinterpret_cast<Complex *>(malloc(sizeof(Complex) * SIGNAL_SIZE));
+
+  // Convolve on the host
+  Convolve(h_signal, SIGNAL_SIZE, h_filter_kernel, FILTER_KERNEL_SIZE,
+           h_convolved_signal_ref);
+
+  // check result
+  bool bTestResult = sdkCompareL2fe(
+      reinterpret_cast<float *>(h_convolved_signal_ref),
+      reinterpret_cast<float *>(h_convolved_signal), 2 * SIGNAL_SIZE, 1e-5f);
+
+  // Destroy CUFFT context
+  HIPCHECK(hipfftDestroy(plan));
+  HIPCHECK(hipfftDestroy(plan_adv));
+
+  // cleanup memory
+  free(h_signal);
+  free(h_filter_kernel);
+  free(h_padded_signal);
+  free(h_padded_filter_kernel);
+  free(h_convolved_signal_ref);
+  HIPCHECK(hipFree(d_signal));
+  HIPCHECK(hipFree(d_filter_kernel));
+
+  exit(bTestResult ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+
+// Pad data
+int PadData(const Complex *signal, Complex **padded_signal, int signal_size,
+            const Complex *filter_kernel, Complex **padded_filter_kernel,
+            int filter_kernel_size) {
+  int minRadius = filter_kernel_size / 2;
+  int maxRadius = filter_kernel_size - minRadius;
+  int new_size = signal_size + maxRadius;
+
+  // Pad signal
+  Complex *new_data =
+      reinterpret_cast<Complex *>(malloc(sizeof(Complex) * new_size));
+  memcpy(new_data + 0, signal, signal_size * sizeof(Complex));
+  memset(new_data + signal_size, 0, (new_size - signal_size) * sizeof(Complex));
+  *padded_signal = new_data;
+
+  // Pad filter
+  new_data = reinterpret_cast<Complex *>(malloc(sizeof(Complex) * new_size));
+  memcpy(new_data + 0, filter_kernel + minRadius, maxRadius * sizeof(Complex));
+  memset(new_data + maxRadius, 0,
+         (new_size - filter_kernel_size) * sizeof(Complex));
+  memcpy(new_data + new_size - minRadius, filter_kernel,
+         minRadius * sizeof(Complex));
+  *padded_filter_kernel = new_data;
+
+  return new_size;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Filtering operations
+////////////////////////////////////////////////////////////////////////////////
+
+// Computes convolution on the host
+void Convolve(const Complex *signal, int signal_size,
+              const Complex *filter_kernel, int filter_kernel_size,
+              Complex *filtered_signal) {
+  int minRadius = filter_kernel_size / 2;
+  int maxRadius = filter_kernel_size - minRadius;
+
+  // Loop over output element indices
+  for (int i = 0; i < signal_size; ++i) {
+    filtered_signal[i].x = filtered_signal[i].y = 0;
+
+    // Loop over convolution indices
+    for (int j = -maxRadius + 1; j <= minRadius; ++j) {
+      int k = i + j;
+
+      if (k >= 0 && k < signal_size) {
+        filtered_signal[i] =
+            ComplexAdd(filtered_signal[i],
+                       ComplexMul(signal[k], filter_kernel[minRadius - j]));
+      }
+    }
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Complex operations
+////////////////////////////////////////////////////////////////////////////////
+
+// Complex addition
+static __device__ __host__ inline Complex ComplexAdd(Complex a, Complex b) {
+  Complex c;
+  c.x = a.x + b.x;
+  c.y = a.y + b.y;
+  return c;
+}
+
+// Complex scale
+static __device__ __host__ inline Complex ComplexScale(Complex a, float s) {
+  Complex c;
+  c.x = s * a.x;
+  c.y = s * a.y;
+  return c;
+}
+
+// Complex multiplication
+static __device__ __host__ inline Complex ComplexMul(Complex a, Complex b) {
+  Complex c;
+  c.x = a.x * b.x - a.y * b.y;
+  c.y = a.x * b.y + a.y * b.x;
+  return c;
+}
+
+// Complex pointwise multiplication
+static __global__ void ComplexPointwiseMulAndScale(Complex *a, const Complex *b,
+                                                   int size, float scale) {
+  const int numThreads = blockDim.x * gridDim.x;
+  const int threadID = blockIdx.x * blockDim.x + threadIdx.x;
+
+  for (int i = threadID; i < size; i += numThreads) {
+    a[i] = ComplexScale(ComplexMul(a[i], b[i]), scale);
+  }
+}
+i = threadID; i < size; i += numThreads) {
+    a[i] = ComplexScale(ComplexMul(a[i], b[i]), scale);
+  }
+}
diff --git a/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_2d_MGPU/simpleCUFFT_2d_MGPU.cu.hip b/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_2d_MGPU/simpleCUFFT_2d_MGPU.cu.hip
index e69de29..7d4124a 100755
--- a/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_2d_MGPU/simpleCUFFT_2d_MGPU.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_2d_MGPU/simpleCUFFT_2d_MGPU.cu.hip
@@ -0,0 +1,384 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+////////////////////////////////////////////////////////////////////////////////
+//
+//  simpleCUFFT_2d_MGPU.cu
+//
+//  This sample code demonstrate the use of CUFFT library for 2D data on multiple GPU.
+//  Example showing the use of CUFFT for solving 2D-POISSON equation using FFT on multiple GPU.
+//  For reference we have used the equation given in http://www.bu.edu/pasi/files/2011/07/
+//  Lecture83.pdf
+//
+////////////////////////////////////////////////////////////////////////////////
+
+
+// System includes
+#include <stdlib.h>
+#include <stdio.h>
+
+#include <string.h>
+#include <math.h>
+
+// CUDA runtime
+#include <hip/hip_runtime.h>
+
+//CUFFT Header file
+#include <hipfftXt.h>
+
+// helper functions and utilities to work with CUDA
+#include "helper_functions.h"
+#include "helper_cuda_hipified.h"
+
+// Complex data type
+typedef float2 Complex;
+
+// Data configuration
+const int GPU_COUNT = 2;
+const int BSZ_Y = 4;
+const int BSZ_X = 4;
+
+// Forward Declaration
+void solvePoissonEquation(cudaLibXtDesc *, cudaLibXtDesc *, float **, int, int);
+
+__global__ void solvePoisson(hipfftComplex *, hipfftComplex *, float *, int, int,
+                             int n_gpu);
+
+///////////////////////////////////////////////////////////////////////////////
+// Program main
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv) {
+  printf(
+      "\nPoisson equation using CUFFT library on Multiple GPUs is "
+      "starting...\n\n");
+
+  int GPU_N;
+  HIPCHECK(hipGetDeviceCount(&GPU_N));
+
+  if (GPU_N < GPU_COUNT) {
+    printf("No. of GPU on node %d\n", GPU_N);
+    printf("Two GPUs are required to run simpleCUFFT_2d_MGPU sample code\n");
+    exit(EXIT_WAIVED);
+  }
+
+  int *major_minor = (int *)malloc(sizeof(int) * GPU_N * 2);
+  int found2IdenticalGPUs = 0;
+  int nGPUs = 2;
+  int *whichGPUs;
+  whichGPUs = (int *)malloc(sizeof(int) * nGPUs);
+
+  for (int i = 0; i < GPU_N; i++) {
+    hipDeviceProp_t deviceProp;
+    HIPCHECK(hipGetDeviceProperties(&deviceProp, i));
+    major_minor[i * 2] = deviceProp.major;
+    major_minor[i * 2 + 1] = deviceProp.minor;
+    printf("GPU Device %d: \"%s\" with compute capability %d.%d\n", i,
+           deviceProp.name, deviceProp.major, deviceProp.minor);
+  }
+
+  for (int i = 0; i < GPU_N; i++) {
+    for (int j = i + 1; j < GPU_N; j++) {
+      if ((major_minor[i * 2] == major_minor[j * 2]) &&
+          (major_minor[i * 2 + 1] == major_minor[j * 2 + 1])) {
+        whichGPUs[0] = i;
+        whichGPUs[1] = j;
+        found2IdenticalGPUs = 1;
+        break;
+      }
+    }
+    if (found2IdenticalGPUs) {
+      break;
+    }
+  }
+
+  free(major_minor);
+  if (!found2IdenticalGPUs) {
+    printf(
+        "No Two GPUs with same architecture found\nWaiving simpleCUFFT_2d_MGPU "
+        "sample\n");
+    exit(EXIT_WAIVED);
+  }
+
+  int N = 64;
+  float xMAX = 1.0f, xMIN = 0.0f, yMIN = 0.0f, h = (xMAX - xMIN) / ((float)N),
+        s = 0.1f, s2 = s * s;
+  float *x, *y, *f, *u_a, r2;
+
+  x = (float *)malloc(sizeof(float) * N * N);
+  y = (float *)malloc(sizeof(float) * N * N);
+  f = (float *)malloc(sizeof(float) * N * N);
+  u_a = (float *)malloc(sizeof(float) * N * N);
+
+  for (int j = 0; j < N; j++)
+    for (int i = 0; i < N; i++) {
+      x[N * j + i] = xMIN + i * h;
+      y[N * j + i] = yMIN + j * h;
+      r2 = (x[N * j + i] - 0.5f) * (x[N * j + i] - 0.5f) +
+           (y[N * j + i] - 0.5f) * (y[N * j + i] - 0.5f);
+      f[N * j + i] = (r2 - 2 * s2) / (s2 * s2) * exp(-r2 / (2 * s2));
+      u_a[N * j + i] = exp(-r2 / (2 * s2));  // analytical solution
+    }
+
+  float *k, *d_k[GPU_COUNT];
+  k = (float *)malloc(sizeof(float) * N);
+  for (int i = 0; i <= N / 2; i++) {
+    k[i] = i * 2 * (float)M_PI;
+  }
+  for (int i = N / 2 + 1; i < N; i++) {
+    k[i] = (i - N) * 2 * (float)M_PI;
+  }
+
+  // Create a complex variable on host
+  Complex *h_f = (Complex *)malloc(sizeof(Complex) * N * N);
+
+  // Initialize the memory for the signal
+  for (int i = 0; i < (N * N); i++) {
+    h_f[i].x = f[i];
+    h_f[i].y = 0.0f;
+  }
+
+  // hipfftCreate() - Create an empty plan
+  hipfftResult result;
+  hipfftHandle planComplex;
+  result = hipfftCreate(&planComplex);
+  if (result != HIPFFT_SUCCESS) {
+    printf("hipfftCreate failed\n");
+    exit(EXIT_FAILURE);
+  }
+
+  // cufftXtSetGPUs() - Define which GPUs to use
+  result = cufftXtSetGPUs(planComplex, nGPUs, whichGPUs);
+
+  if (result == HIPFFT_INVALID_DEVICE) {
+    printf("This sample requires two GPUs on the same board.\n");
+    printf("No such board was found. Waiving sample.\n");
+    exit(EXIT_WAIVED);
+  } else if (result != HIPFFT_SUCCESS) {
+    printf("cufftXtSetGPUs failed\n");
+    exit(EXIT_FAILURE);
+  }
+
+  // Print the device information to run the code
+  printf("\nRunning on GPUs\n");
+  for (int i = 0; i < 2; i++) {
+    hipDeviceProp_t deviceProp;
+    HIPCHECK(hipGetDeviceProperties(&deviceProp, whichGPUs[i]));
+    printf("GPU Device %d: \"%s\" with compute capability %d.%d\n",
+           whichGPUs[i], deviceProp.name, deviceProp.major, deviceProp.minor);
+  }
+
+  size_t *worksize;
+  worksize = (size_t *)malloc(sizeof(size_t) * nGPUs);
+
+  // hipfftMakePlan2d() - Create the plan
+  result = hipfftMakePlan2d(planComplex, N, N, HIPFFT_C2C, worksize);
+  if (result != HIPFFT_SUCCESS) {
+    printf("*MakePlan* failed\n");
+    exit(EXIT_FAILURE);
+  }
+
+  for (int i = 0; i < nGPUs; i++) {
+    hipSetDevice(whichGPUs[i]);
+    hipMalloc((void **)&d_k[i], sizeof(float) * N);
+    hipMemcpy(d_k[i], k, sizeof(float) * N, hipMemcpyHostToDevice);
+  }
+
+  // Create a variable on device
+  // d_f - variable on device to store the input data
+  // d_d_f - variable that store the natural order of d_f data
+  // d_out - device output
+  cudaLibXtDesc *d_f, *d_d_f, *d_out;
+
+  // cufftXtMalloc() - Malloc data on multiple GPUs
+
+  result = cufftXtMalloc(planComplex, (cudaLibXtDesc **)&d_f,
+                         CUFFT_XT_FORMAT_INPLACE);
+  if (result != HIPFFT_SUCCESS) {
+    printf("*XtMalloc failed\n");
+    exit(EXIT_FAILURE);
+  }
+
+  result = cufftXtMalloc(planComplex, (cudaLibXtDesc **)&d_d_f,
+                         CUFFT_XT_FORMAT_INPLACE);
+  if (result != HIPFFT_SUCCESS) {
+    printf("*XtMalloc failed\n");
+    exit(EXIT_FAILURE);
+  }
+
+  result = cufftXtMalloc(planComplex, (cudaLibXtDesc **)&d_out,
+                         CUFFT_XT_FORMAT_INPLACE);
+  if (result != HIPFFT_SUCCESS) {
+    printf("*XtMalloc failed\n");
+    exit(EXIT_FAILURE);
+  }
+
+  // cufftXtMemcpy() - Copy the data from host to device
+  result = cufftXtMemcpy(planComplex, d_f, h_f, CUFFT_COPY_HOST_TO_DEVICE);
+  if (result != HIPFFT_SUCCESS) {
+    printf("*XtMemcpy failed\n");
+    exit(EXIT_FAILURE);
+  }
+
+  // cufftXtExecDescriptorC2C() - Execute FFT on data on multiple GPUs
+  printf("Forward 2d FFT on multiple GPUs\n");
+  result = cufftXtExecDescriptorC2C(planComplex, d_f, d_f, HIPFFT_FORWARD);
+  if (result != HIPFFT_SUCCESS) {
+    printf("*XtExecC2C  failed\n");
+    exit(EXIT_FAILURE);
+  }
+
+  // cufftXtMemcpy() - Copy the data to natural order on GPUs
+  result = cufftXtMemcpy(planComplex, d_d_f, d_f, CUFFT_COPY_DEVICE_TO_DEVICE);
+  if (result != HIPFFT_SUCCESS) {
+    printf("*XtMemcpy failed\n");
+    exit(EXIT_FAILURE);
+  }
+
+  printf("Solve Poisson Equation\n");
+  solvePoissonEquation(d_d_f, d_out, d_k, N, nGPUs);
+
+  printf("Inverse 2d FFT on multiple GPUs\n");
+  // cufftXtExecDescriptorC2C() - Execute inverse  FFT on data on multiple GPUs
+  result = cufftXtExecDescriptorC2C(planComplex, d_out, d_out, HIPFFT_BACKWARD);
+  if (result != HIPFFT_SUCCESS) {
+    printf("*XtExecC2C  failed\n");
+    exit(EXIT_FAILURE);
+  }
+
+  // Create a variable on host to copy the data from device
+  // h_d_out - variable store the output of device
+  Complex *h_d_out = (Complex *)malloc(sizeof(Complex) * N * N);
+
+  // cufftXtMemcpy() - Copy data from multiple GPUs to host
+  result =
+      cufftXtMemcpy(planComplex, h_d_out, d_out, CUFFT_COPY_DEVICE_TO_HOST);
+  if (result != HIPFFT_SUCCESS) {
+    printf("*XtMemcpy failed\n");
+    exit(EXIT_FAILURE);
+  }
+
+  float *out = (float *)malloc(sizeof(float) * N * N);
+  float constant = h_d_out[0].x / N * N;
+  for (int i = 0; i < N * N; i++) {
+    // subtract u[0] to force the arbitrary constant to be 0
+    out[i] = (h_d_out[i].x / (N * N)) - constant;
+  }
+
+  // cleanup memory
+
+  free(h_f);
+  free(k);
+  free(out);
+  free(h_d_out);
+  free(x);
+  free(whichGPUs);
+  free(y);
+  free(f);
+  free(u_a);
+  free(worksize);
+
+  // cudaXtFree() - Free GPU memory
+  for (int i = 0; i < GPU_COUNT; i++) {
+    hipFree(d_k[i]);
+  }
+  result = cufftXtFree(d_out);
+  if (result != HIPFFT_SUCCESS) {
+    printf("*XtFree failed\n");
+    exit(EXIT_FAILURE);
+  }
+  result = cufftXtFree(d_f);
+  if (result != HIPFFT_SUCCESS) {
+    printf("*XtFree failed\n");
+    exit(EXIT_FAILURE);
+  }
+  result = cufftXtFree(d_d_f);
+  if (result != HIPFFT_SUCCESS) {
+    printf("*XtFree failed\n");
+    exit(EXIT_FAILURE);
+  }
+
+  // hipfftDestroy() - Destroy FFT plan
+  result = hipfftDestroy(planComplex);
+  if (result != HIPFFT_SUCCESS) {
+    printf("hipfftDestroy failed: code %d\n", (int)result);
+    exit(EXIT_FAILURE);
+  }
+
+  exit(EXIT_SUCCESS);
+}
+
+////////////////////////////////////////////////////////////////////////////////////
+// Launch kernel on  multiple GPU
+///////////////////////////////////////////////////////////////////////////////////
+void solvePoissonEquation(cudaLibXtDesc *d_ft, cudaLibXtDesc *d_ft_k, float **k,
+                          int N, int nGPUs) {
+  int device;
+  dim3 dimGrid(int(N / BSZ_X), int((N / 2) / BSZ_Y));
+  dim3 dimBlock(BSZ_X, BSZ_Y);
+
+  for (int i = 0; i < nGPUs; i++) {
+    device = d_ft_k->descriptor->GPUs[i];
+    hipSetDevice(device);
+    solvePoisson<<<dimGrid, dimBlock>>>(
+        (hipfftComplex *)d_ft->descriptor->data[i],
+        (hipfftComplex *)d_ft_k->descriptor->data[i], k[i], N, i, nGPUs);
+  }
+
+  // Wait for device to finish all operation
+  for (int i = 0; i < nGPUs; i++) {
+    device = d_ft_k->descriptor->GPUs[i];
+    hipSetDevice(device);
+    hipDeviceSynchronize();
+
+    // Check if kernel execution generated and error
+    getLastCudaError("Kernel execution failed [ solvePoisson ]");
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Kernel for Solving Poisson equation on GPU
+////////////////////////////////////////////////////////////////////////////////
+__global__ void solvePoisson(hipfftComplex *ft, hipfftComplex *ft_k, float *k,
+                             int N, int gpu_id, int n_gpu) {
+  int i = threadIdx.x + blockIdx.x * blockDim.x;
+  int j = threadIdx.y + blockIdx.y * blockDim.y;
+  int index = j * N + i;
+  if (i < N && j < N / n_gpu) {
+    float k2 =
+        k[i] * k[i] + k[j + gpu_id * N / n_gpu] * k[j + gpu_id * N / n_gpu];
+    if (i == 0 && j == 0 && gpu_id == 0) {
+      k2 = 1.0f;
+    }
+
+    ft_k[index].x = -ft[index].x * 1 / k2;
+    ft_k[index].y = -ft[index].y * 1 / k2;
+  }
+}
+x].y * 1 / k2;
+  }
+}
diff --git a/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_MGPU/simpleCUFFT_MGPU.cu.hip b/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_MGPU/simpleCUFFT_MGPU.cu.hip
index e69de29..8813625 100755
--- a/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_MGPU/simpleCUFFT_MGPU.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_MGPU/simpleCUFFT_MGPU.cu.hip
@@ -0,0 +1,406 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* Example showing the use of CUFFT for fast 1D-convolution using FFT. */
+
+// System includes
+#include <stdlib.h>
+#include <stdio.h>
+
+#include <string.h>
+#include <math.h>
+
+// CUDA runtime
+#include <hip/hip_runtime.h>
+
+//CUFFT Header file
+#include <hipfftXt.h>
+
+// helper functions and utilities to work with CUDA
+#include "helper_functions.h"
+#include "helper_cuda_hipified.h"
+
+// Complex data type
+typedef float2 Complex;
+
+static __device__ __host__ inline Complex ComplexAdd(Complex, Complex);
+static __device__ __host__ inline Complex ComplexScale(Complex, float);
+static __device__ __host__ inline Complex ComplexMul(Complex, Complex);
+static __global__ void ComplexPointwiseMulAndScale(hipfftComplex *,
+                                                   hipfftComplex *, int, float);
+
+// Kernel for GPU
+void multiplyCoefficient(cudaLibXtDesc *, cudaLibXtDesc *, int, float, int);
+
+// Filtering functions
+void Convolve(const Complex *, int, const Complex *, int, Complex *);
+
+// Padding functions
+int PadData(const Complex *, Complex **, int, const Complex *, Complex **, int);
+
+////////////////////////////////////////////////////////////////////////////////
+// Data configuration
+// The filter size is assumed to be a number smaller than the signal size
+///////////////////////////////////////////////////////////////////////////////
+const int SIGNAL_SIZE = 1018;
+const int FILTER_KERNEL_SIZE = 11;
+const int GPU_COUNT = 2;
+
+////////////////////////////////////////////////////////////////////////////////
+// Program main
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv) {
+  printf("\n[simpleCUFFT_MGPU] is starting...\n\n");
+
+  int GPU_N;
+  HIPCHECK(hipGetDeviceCount(&GPU_N));
+
+  if (GPU_N < GPU_COUNT) {
+    printf("No. of GPU on node %d\n", GPU_N);
+    printf("Two GPUs are required to run simpleCUFFT_MGPU sample code\n");
+    exit(EXIT_WAIVED);
+  }
+
+  int *major_minor = (int *)malloc(sizeof(int) * GPU_N * 2);
+  int found2IdenticalGPUs = 0;
+  int nGPUs = 2;
+  int *whichGPUs;
+  whichGPUs = (int *)malloc(sizeof(int) * nGPUs);
+
+  for (int i = 0; i < GPU_N; i++) {
+    hipDeviceProp_t deviceProp;
+    HIPCHECK(hipGetDeviceProperties(&deviceProp, i));
+    major_minor[i * 2] = deviceProp.major;
+    major_minor[i * 2 + 1] = deviceProp.minor;
+    printf("GPU Device %d: \"%s\" with compute capability %d.%d\n", i,
+           deviceProp.name, deviceProp.major, deviceProp.minor);
+  }
+
+  for (int i = 0; i < GPU_N; i++) {
+    for (int j = i + 1; j < GPU_N; j++) {
+      if ((major_minor[i * 2] == major_minor[j * 2]) &&
+          (major_minor[i * 2 + 1] == major_minor[j * 2 + 1])) {
+        whichGPUs[0] = i;
+        whichGPUs[1] = j;
+        found2IdenticalGPUs = 1;
+        break;
+      }
+    }
+    if (found2IdenticalGPUs) {
+      break;
+    }
+  }
+
+  free(major_minor);
+  if (!found2IdenticalGPUs) {
+    printf(
+        "No Two GPUs with same architecture found\nWaiving simpleCUFFT_2d_MGPU "
+        "sample\n");
+    exit(EXIT_WAIVED);
+  }
+
+  // Allocate host memory for the signal
+  Complex *h_signal = (Complex *)malloc(sizeof(Complex) * SIGNAL_SIZE);
+
+  // Initialize the memory for the signal
+  for (int i = 0; i < SIGNAL_SIZE; ++i) {
+    h_signal[i].x = rand() / (float)RAND_MAX;
+    h_signal[i].y = 0;
+  }
+
+  // Allocate host memory for the filter
+  Complex *h_filter_kernel =
+      (Complex *)malloc(sizeof(Complex) * FILTER_KERNEL_SIZE);
+
+  // Initialize the memory for the filter
+  for (int i = 0; i < FILTER_KERNEL_SIZE; ++i) {
+    h_filter_kernel[i].x = rand() / (float)RAND_MAX;
+    h_filter_kernel[i].y = 0;
+  }
+
+  // Pad signal and filter kernel
+  Complex *h_padded_signal;
+  Complex *h_padded_filter_kernel;
+  int new_size =
+      PadData(h_signal, &h_padded_signal, SIGNAL_SIZE, h_filter_kernel,
+              &h_padded_filter_kernel, FILTER_KERNEL_SIZE);
+
+  // hipfftCreate() - Create an empty plan
+  hipfftResult result;
+  hipfftHandle plan_input;
+  HIPCHECK(hipfftCreate(&plan_input));
+
+  // cufftXtSetGPUs() - Define which GPUs to use
+  result = cufftXtSetGPUs(plan_input, nGPUs, whichGPUs);
+
+  if (result == HIPFFT_INVALID_DEVICE) {
+    printf("This sample requires two GPUs on the same board.\n");
+    printf("No such board was found. Waiving sample.\n");
+    exit(EXIT_WAIVED);
+  } else if (result != HIPFFT_SUCCESS) {
+    printf("cufftXtSetGPUs failed\n");
+    exit(EXIT_FAILURE);
+  }
+
+  // Print the device information to run the code
+  printf("\nRunning on GPUs\n");
+  for (int i = 0; i < nGPUs; i++) {
+    hipDeviceProp_t deviceProp;
+    HIPCHECK(hipGetDeviceProperties(&deviceProp, whichGPUs[i]));
+    printf("GPU Device %d: \"%s\" with compute capability %d.%d\n",
+           whichGPUs[i], deviceProp.name, deviceProp.major, deviceProp.minor);
+  }
+
+  size_t *worksize;
+  worksize = (size_t *)malloc(sizeof(size_t) * nGPUs);
+
+  // hipfftMakePlan1d() - Create the plan
+  HIPCHECK(
+      hipfftMakePlan1d(plan_input, new_size, HIPFFT_C2C, 1, worksize));
+
+  // cufftXtMalloc() - Malloc data on multiple GPUs
+  cudaLibXtDesc *d_signal;
+  HIPCHECK(cufftXtMalloc(plan_input, (cudaLibXtDesc **)&d_signal,
+                                CUFFT_XT_FORMAT_INPLACE));
+  cudaLibXtDesc *d_out_signal;
+  HIPCHECK(cufftXtMalloc(plan_input, (cudaLibXtDesc **)&d_out_signal,
+                                CUFFT_XT_FORMAT_INPLACE));
+  cudaLibXtDesc *d_filter_kernel;
+  HIPCHECK(cufftXtMalloc(plan_input, (cudaLibXtDesc **)&d_filter_kernel,
+                                CUFFT_XT_FORMAT_INPLACE));
+  cudaLibXtDesc *d_out_filter_kernel;
+  HIPCHECK(cufftXtMalloc(plan_input,
+                                (cudaLibXtDesc **)&d_out_filter_kernel,
+                                CUFFT_XT_FORMAT_INPLACE));
+
+  // cufftXtMemcpy() - Copy data from host to multiple GPUs
+  HIPCHECK(cufftXtMemcpy(plan_input, d_signal, h_padded_signal,
+                                CUFFT_COPY_HOST_TO_DEVICE));
+  HIPCHECK(cufftXtMemcpy(plan_input, d_filter_kernel,
+                                h_padded_filter_kernel,
+                                CUFFT_COPY_HOST_TO_DEVICE));
+
+  // cufftXtExecDescriptorC2C() - Execute FFT on data on multiple GPUs
+  HIPCHECK(
+      cufftXtExecDescriptorC2C(plan_input, d_signal, d_signal, HIPFFT_FORWARD));
+  HIPCHECK(cufftXtExecDescriptorC2C(plan_input, d_filter_kernel,
+                                           d_filter_kernel, HIPFFT_FORWARD));
+
+  // cufftXtMemcpy() - Copy the data to natural order on GPUs
+  HIPCHECK(cufftXtMemcpy(plan_input, d_out_signal, d_signal,
+                                CUFFT_COPY_DEVICE_TO_DEVICE));
+  HIPCHECK(cufftXtMemcpy(plan_input, d_out_filter_kernel,
+                                d_filter_kernel, CUFFT_COPY_DEVICE_TO_DEVICE));
+
+  printf("\n\nValue of Library Descriptor\n");
+  printf("Number of GPUs %d\n", d_out_signal->descriptor->nGPUs);
+  printf("Device id  %d %d\n", d_out_signal->descriptor->GPUs[0],
+         d_out_signal->descriptor->GPUs[1]);
+  printf("Data size on GPU %ld %ld\n",
+         (long)(d_out_signal->descriptor->size[0] / sizeof(hipfftComplex)),
+         (long)(d_out_signal->descriptor->size[1] / sizeof(hipfftComplex)));
+
+  // Multiply the coefficients together and normalize the result
+  printf("Launching ComplexPointwiseMulAndScale<<< >>>\n");
+  multiplyCoefficient(d_out_signal, d_out_filter_kernel, new_size,
+                      1.0f / new_size, nGPUs);
+
+  // cufftXtExecDescriptorC2C() - Execute inverse  FFT on data on multiple GPUs
+  printf("Transforming signal back hipfftExecC2C\n");
+  HIPCHECK(cufftXtExecDescriptorC2C(plan_input, d_out_signal,
+                                           d_out_signal, HIPFFT_BACKWARD));
+
+  // Create host pointer pointing to padded signal
+  Complex *h_convolved_signal = h_padded_signal;
+
+  // Allocate host memory for the convolution result
+  Complex *h_convolved_signal_ref =
+      (Complex *)malloc(sizeof(Complex) * SIGNAL_SIZE);
+
+  // cufftXtMemcpy() - Copy data from multiple GPUs to host
+  HIPCHECK(cufftXtMemcpy(plan_input, h_convolved_signal, d_out_signal,
+                                CUFFT_COPY_DEVICE_TO_HOST));
+
+  // Convolve on the host
+  Convolve(h_signal, SIGNAL_SIZE, h_filter_kernel, FILTER_KERNEL_SIZE,
+           h_convolved_signal_ref);
+
+  // Compare CPU and GPU result
+  bool bTestResult =
+      sdkCompareL2fe((float *)h_convolved_signal_ref,
+                     (float *)h_convolved_signal, 2 * SIGNAL_SIZE, 1e-5f);
+  printf("\nvalue of TestResult %d\n", bTestResult);
+
+  // Cleanup memory
+  free(whichGPUs);
+  free(worksize);
+  free(h_signal);
+  free(h_filter_kernel);
+  free(h_padded_signal);
+  free(h_padded_filter_kernel);
+  free(h_convolved_signal_ref);
+
+  // cudaXtFree() - Free GPU memory
+  HIPCHECK(cufftXtFree(d_signal));
+  HIPCHECK(cufftXtFree(d_filter_kernel));
+  HIPCHECK(cufftXtFree(d_out_signal));
+  HIPCHECK(cufftXtFree(d_out_filter_kernel));
+
+  // hipfftDestroy() - Destroy FFT plan
+  HIPCHECK(hipfftDestroy(plan_input));
+
+  exit(bTestResult ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+
+///////////////////////////////////////////////////////////////////////////////////
+// Function for padding original data
+//////////////////////////////////////////////////////////////////////////////////
+int PadData(const Complex *signal, Complex **padded_signal, int signal_size,
+            const Complex *filter_kernel, Complex **padded_filter_kernel,
+            int filter_kernel_size) {
+  int minRadius = filter_kernel_size / 2;
+  int maxRadius = filter_kernel_size - minRadius;
+  int new_size = signal_size + maxRadius;
+
+  // Pad signal
+  Complex *new_data = (Complex *)malloc(sizeof(Complex) * new_size);
+  memcpy(new_data + 0, signal, signal_size * sizeof(Complex));
+  memset(new_data + signal_size, 0, (new_size - signal_size) * sizeof(Complex));
+  *padded_signal = new_data;
+
+  // Pad filter
+  new_data = (Complex *)malloc(sizeof(Complex) * new_size);
+  memcpy(new_data + 0, filter_kernel + minRadius, maxRadius * sizeof(Complex));
+  memset(new_data + maxRadius, 0,
+         (new_size - filter_kernel_size) * sizeof(Complex));
+  memcpy(new_data + new_size - minRadius, filter_kernel,
+         minRadius * sizeof(Complex));
+  *padded_filter_kernel = new_data;
+
+  return new_size;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Filtering operations - Computing Convolution on the host
+////////////////////////////////////////////////////////////////////////////////
+void Convolve(const Complex *signal, int signal_size,
+              const Complex *filter_kernel, int filter_kernel_size,
+              Complex *filtered_signal) {
+  int minRadius = filter_kernel_size / 2;
+  int maxRadius = filter_kernel_size - minRadius;
+
+  // Loop over output element indices
+  for (int i = 0; i < signal_size; ++i) {
+    filtered_signal[i].x = filtered_signal[i].y = 0;
+
+    // Loop over convolution indices
+    for (int j = -maxRadius + 1; j <= minRadius; ++j) {
+      int k = i + j;
+
+      if (k >= 0 && k < signal_size) {
+        filtered_signal[i] =
+            ComplexAdd(filtered_signal[i],
+                       ComplexMul(signal[k], filter_kernel[minRadius - j]));
+      }
+    }
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//  Launch Kernel on multiple GPU
+////////////////////////////////////////////////////////////////////////////////
+void multiplyCoefficient(cudaLibXtDesc *d_signal,
+                         cudaLibXtDesc *d_filter_kernel, int new_size,
+                         float val, int nGPUs) {
+  int device;
+  // Launch the ComplexPointwiseMulAndScale<<< >>> kernel on multiple GPU
+  for (int i = 0; i < nGPUs; i++) {
+    device = d_signal->descriptor->GPUs[i];
+
+    // Set device
+    HIPCHECK(hipSetDevice(device));
+
+    // Perform GPU computations
+    ComplexPointwiseMulAndScale<<<32, 256>>>(
+        (hipfftComplex *)d_signal->descriptor->data[i],
+        (hipfftComplex *)d_filter_kernel->descriptor->data[i],
+        int(d_signal->descriptor->size[i] / sizeof(hipfftComplex)), val);
+  }
+
+  // Wait for device to finish all operation
+  for (int i = 0; i < nGPUs; i++) {
+    device = d_signal->descriptor->GPUs[i];
+    HIPCHECK(hipSetDevice(device));
+    hipDeviceSynchronize();
+    // Check if kernel execution generated and error
+    getLastCudaError("Kernel execution failed [ ComplexPointwiseMulAndScale ]");
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Complex operations
+////////////////////////////////////////////////////////////////////////////////
+
+// Complex addition
+static __device__ __host__ inline Complex ComplexAdd(Complex a, Complex b) {
+  Complex c;
+  c.x = a.x + b.x;
+  c.y = a.y + b.y;
+  return c;
+}
+
+// Complex scale
+static __device__ __host__ inline Complex ComplexScale(Complex a, float s) {
+  Complex c;
+  c.x = s * a.x;
+  c.y = s * a.y;
+  return c;
+}
+
+// Complex multiplication
+static __device__ __host__ inline Complex ComplexMul(Complex a, Complex b) {
+  Complex c;
+  c.x = a.x * b.x - a.y * b.y;
+  c.y = a.x * b.y + a.y * b.x;
+  return c;
+}
+// Complex pointwise multiplication
+static __global__ void ComplexPointwiseMulAndScale(hipfftComplex *a,
+                                                   hipfftComplex *b, int size,
+                                                   float scale) {
+  const int numThreads = blockDim.x * gridDim.x;
+  const int threadID = blockIdx.x * blockDim.x + threadIdx.x;
+  for (int i = threadID; i < size; i += numThreads) {
+    a[i] = ComplexScale(ComplexMul(a[i], b[i]), scale);
+  }
+}
+t threadID = blockIdx.x * blockDim.x + threadIdx.x;
+  for (int i = threadID; i < size; i += numThreads) {
+    a[i] = ComplexScale(ComplexMul(a[i], b[i]), scale);
+  }
+}
diff --git a/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_callback/simpleCUFFT_callback.cu.hip b/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_callback/simpleCUFFT_callback.cu.hip
index e69de29..85e1c9c 100755
--- a/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_callback/simpleCUFFT_callback.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_callback/simpleCUFFT_callback.cu.hip
@@ -0,0 +1,342 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+
+/* 
+ * Example showing the use of CUFFT for fast 1D-convolution using FFT. 
+ * This sample is the same as simpleCUFFT, except that it uses a callback
+ * function to perform the pointwise multiply and scale, on input to the
+ * inverse transform.
+ * 
+*/
+
+// includes, system
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <math.h>
+
+// includes, project
+#include <hip/hip_runtime.h>
+#include <hipfft.h>
+#include <hipfftXt.h>
+#include "helper_functions.h"
+#include "helper_cuda_hipified.h"
+
+// Complex data type
+typedef float2 Complex;
+static __device__ __host__ inline Complex ComplexAdd(Complex, Complex);
+static __device__ __host__ inline Complex ComplexScale(Complex, float);
+static __device__ __host__ inline Complex ComplexMul(Complex, Complex);
+
+// This is the callback routine prototype
+static __device__ hipfftComplex ComplexPointwiseMulAndScale(void *a,
+                                                           size_t index,
+                                                           void *cb_info,
+                                                           void *sharedmem);
+
+typedef struct _cb_params {
+  Complex *filter;
+  float scale;
+} cb_params;
+
+// This is the callback routine. It does complex pointwise multiplication with
+// scaling.
+static __device__ hipfftComplex ComplexPointwiseMulAndScale(void *a,
+                                                           size_t index,
+                                                           void *cb_info,
+                                                           void *sharedmem) {
+  cb_params *my_params = (cb_params *)cb_info;
+  return (hipfftComplex)ComplexScale(
+      ComplexMul(((Complex *)a)[index], (my_params->filter)[index]),
+      my_params->scale);
+}
+
+// Define the device pointer to the callback routine. The host code will fetch
+// this and pass it to CUFFT
+__device__ hipfftCallbackLoadC myOwnCallbackPtr = ComplexPointwiseMulAndScale;
+// Filtering functions
+void Convolve(const Complex *, int, const Complex *, int, Complex *);
+
+// Padding functions
+int PadData(const Complex *, Complex **, int, const Complex *, Complex **, int);
+
+////////////////////////////////////////////////////////////////////////////////
+// declaration, forward
+int runTest(int argc, char **argv);
+
+// The filter size is assumed to be a number smaller than the signal size
+#define SIGNAL_SIZE 50
+#define FILTER_KERNEL_SIZE 11
+
+////////////////////////////////////////////////////////////////////////////////
+// Program main
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv) {
+  struct hipDeviceProp_t properties;
+  int device;
+  HIPCHECK(hipGetDevice(&device));
+  HIPCHECK(hipGetDeviceProperties(&properties, device));
+  if (!(properties.major >= 2)) {
+    printf("simpleCUFFT_callback requires CUDA architecture SM2.0 or higher\n");
+    return EXIT_WAIVED;
+  }
+
+  return runTest(argc, argv);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run a simple test for CUFFT callbacks
+////////////////////////////////////////////////////////////////////////////////
+int runTest(int argc, char **argv) {
+  printf("[simpleCUFFT_callback] is starting...\n");
+
+  findCudaDevice(argc, (const char **)argv);
+
+  // Allocate host memory for the signal
+  Complex *h_signal = (Complex *)malloc(sizeof(Complex) * SIGNAL_SIZE);
+
+  // Initialize the memory for the signal
+  for (unsigned int i = 0; i < SIGNAL_SIZE; ++i) {
+    h_signal[i].x = rand() / (float)RAND_MAX;
+    h_signal[i].y = 0;
+  }
+
+  // Allocate host memory for the filter
+  Complex *h_filter_kernel =
+      (Complex *)malloc(sizeof(Complex) * FILTER_KERNEL_SIZE);
+
+  // Initialize the memory for the filter
+  for (unsigned int i = 0; i < FILTER_KERNEL_SIZE; ++i) {
+    h_filter_kernel[i].x = rand() / (float)RAND_MAX;
+    h_filter_kernel[i].y = 0;
+  }
+
+  // Pad signal and filter kernel
+  Complex *h_padded_signal;
+  Complex *h_padded_filter_kernel;
+  int new_size =
+      PadData(h_signal, &h_padded_signal, SIGNAL_SIZE, h_filter_kernel,
+              &h_padded_filter_kernel, FILTER_KERNEL_SIZE);
+  int mem_size = sizeof(Complex) * new_size;
+
+  // Allocate device memory for signal
+  Complex *d_signal;
+  HIPCHECK(hipMalloc((void **)&d_signal, mem_size));
+  // Copy host memory to device
+  HIPCHECK(
+      hipMemcpy(d_signal, h_padded_signal, mem_size, hipMemcpyHostToDevice));
+
+  // Allocate device memory for filter kernel
+  Complex *d_filter_kernel;
+  HIPCHECK(hipMalloc((void **)&d_filter_kernel, mem_size));
+
+  // Copy host memory to device
+  HIPCHECK(hipMemcpy(d_filter_kernel, h_padded_filter_kernel, mem_size,
+                             hipMemcpyHostToDevice));
+
+  // Create one CUFFT plan for the forward transforms, and one for the reverse
+  // transform with load callback.
+  hipfftHandle plan, cb_plan;
+  size_t work_size;
+
+  HIPCHECK(hipfftCreate(&plan));
+  HIPCHECK(hipfftCreate(&cb_plan));
+
+  HIPCHECK(hipfftMakePlan1d(plan, new_size, HIPFFT_C2C, 1, &work_size));
+  HIPCHECK(hipfftMakePlan1d(cb_plan, new_size, HIPFFT_C2C, 1, &work_size));
+
+  // Define a structure used to pass in the device address of the filter kernel,
+  // and the scale factor
+  cb_params h_params;
+
+  h_params.filter = d_filter_kernel;
+  h_params.scale = 1.0f / new_size;
+
+  // Allocate device memory for parameters
+  cb_params *d_params;
+  HIPCHECK(hipMalloc((void **)&d_params, sizeof(cb_params)));
+
+  // Copy host memory to device
+  HIPCHECK(hipMemcpy(d_params, &h_params, sizeof(cb_params),
+                             hipMemcpyHostToDevice));
+
+  // The host needs to get a copy of the device pointer to the callback
+  hipfftCallbackLoadC hostCopyOfCallbackPtr;
+
+  HIPCHECK(hipMemcpyFromSymbol(&hostCopyOfCallbackPtr, HIP_SYMBOL(myOwnCallbackPtr),
+                                       sizeof(hostCopyOfCallbackPtr)));
+
+  // Now associate the load callback with the plan.
+  hipfftResult status =
+      hipfftXtSetCallback(cb_plan, (void **)&hostCopyOfCallbackPtr,
+                         HIPFFT_CB_LD_COMPLEX, (void **)&d_params);
+  if (status == CUFFT_LICENSE_ERROR) {
+    printf("This sample requires a valid license file.\n");
+    printf(
+        "The file was either not found, out of date, or otherwise invalid.\n");
+    return EXIT_WAIVED;
+  }
+
+  HIPCHECK(hipfftXtSetCallback(cb_plan, (void **)&hostCopyOfCallbackPtr,
+                                     HIPFFT_CB_LD_COMPLEX, (void **)&d_params));
+
+  // Transform signal and kernel
+  printf("Transforming signal hipfftExecC2C\n");
+  HIPCHECK(hipfftExecC2C(plan, (hipfftComplex *)d_signal,
+                               (hipfftComplex *)d_signal, HIPFFT_FORWARD));
+  HIPCHECK(hipfftExecC2C(plan, (hipfftComplex *)d_filter_kernel,
+                               (hipfftComplex *)d_filter_kernel, HIPFFT_FORWARD));
+
+  // Transform signal back, using the callback to do the pointwise multiply on
+  // the way in.
+  printf("Transforming signal back hipfftExecC2C\n");
+  HIPCHECK(hipfftExecC2C(cb_plan, (hipfftComplex *)d_signal,
+                               (hipfftComplex *)d_signal, HIPFFT_BACKWARD));
+
+  // Copy device memory to host
+  Complex *h_convolved_signal = h_padded_signal;
+  HIPCHECK(hipMemcpy(h_convolved_signal, d_signal, mem_size,
+                             hipMemcpyDeviceToHost));
+
+  // Allocate host memory for the convolution result
+  Complex *h_convolved_signal_ref =
+      (Complex *)malloc(sizeof(Complex) * SIGNAL_SIZE);
+
+  // Convolve on the host
+  Convolve(h_signal, SIGNAL_SIZE, h_filter_kernel, FILTER_KERNEL_SIZE,
+           h_convolved_signal_ref);
+
+  // check result
+  bool bTestResult =
+      sdkCompareL2fe((float *)h_convolved_signal_ref,
+                     (float *)h_convolved_signal, 2 * SIGNAL_SIZE, 1e-5f);
+
+  // Destroy CUFFT context
+  HIPCHECK(hipfftDestroy(plan));
+  HIPCHECK(hipfftDestroy(cb_plan));
+
+  // cleanup memory
+  free(h_signal);
+  free(h_filter_kernel);
+  free(h_padded_signal);
+  free(h_padded_filter_kernel);
+  free(h_convolved_signal_ref);
+  HIPCHECK(hipFree(d_signal));
+  HIPCHECK(hipFree(d_filter_kernel));
+  HIPCHECK(hipFree(d_params));
+
+  return bTestResult ? EXIT_SUCCESS : EXIT_FAILURE;
+}
+
+// Pad data
+int PadData(const Complex *signal, Complex **padded_signal, int signal_size,
+            const Complex *filter_kernel, Complex **padded_filter_kernel,
+            int filter_kernel_size) {
+  int minRadius = filter_kernel_size / 2;
+  int maxRadius = filter_kernel_size - minRadius;
+  int new_size = signal_size + maxRadius;
+
+  // Pad signal
+  Complex *new_data = (Complex *)malloc(sizeof(Complex) * new_size);
+  memcpy(new_data + 0, signal, signal_size * sizeof(Complex));
+  memset(new_data + signal_size, 0, (new_size - signal_size) * sizeof(Complex));
+  *padded_signal = new_data;
+
+  // Pad filter
+  new_data = (Complex *)malloc(sizeof(Complex) * new_size);
+  memcpy(new_data + 0, filter_kernel + minRadius, maxRadius * sizeof(Complex));
+  memset(new_data + maxRadius, 0,
+         (new_size - filter_kernel_size) * sizeof(Complex));
+  memcpy(new_data + new_size - minRadius, filter_kernel,
+         minRadius * sizeof(Complex));
+  *padded_filter_kernel = new_data;
+
+  return new_size;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Filtering operations
+////////////////////////////////////////////////////////////////////////////////
+
+// Computes convolution on the host
+void Convolve(const Complex *signal, int signal_size,
+              const Complex *filter_kernel, int filter_kernel_size,
+              Complex *filtered_signal) {
+  int minRadius = filter_kernel_size / 2;
+  int maxRadius = filter_kernel_size - minRadius;
+
+  // Loop over output element indices
+  for (int i = 0; i < signal_size; ++i) {
+    filtered_signal[i].x = filtered_signal[i].y = 0;
+
+    // Loop over convolution indices
+    for (int j = -maxRadius + 1; j <= minRadius; ++j) {
+      int k = i + j;
+
+      if (k >= 0 && k < signal_size) {
+        filtered_signal[i] =
+            ComplexAdd(filtered_signal[i],
+                       ComplexMul(signal[k], filter_kernel[minRadius - j]));
+      }
+    }
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Complex operations
+////////////////////////////////////////////////////////////////////////////////
+
+// Complex addition
+static __device__ __host__ inline Complex ComplexAdd(Complex a, Complex b) {
+  Complex c;
+  c.x = a.x + b.x;
+  c.y = a.y + b.y;
+  return c;
+}
+
+// Complex scale
+static __device__ __host__ inline Complex ComplexScale(Complex a, float s) {
+  Complex c;
+  c.x = s * a.x;
+  c.y = s * a.y;
+  return c;
+}
+
+// Complex multiplication
+static __device__ __host__ inline Complex ComplexMul(Complex a, Complex b) {
+  Complex c;
+  c.x = a.x * b.x - a.y * b.y;
+  c.y = a.x * b.y + a.y * b.x;
+  return c;
+}
+c __device__ __host__ inline Complex ComplexMul(Complex a, Complex b) {
+  Complex c;
+  c.x = a.x * b.x - a.y * b.y;
+  c.y = a.x * b.y + a.y * b.x;
+  return c;
+}
diff --git a/src/samples/Samples/5_Domain_Specific/FDTD3d/src/FDTD3dGPU.cu.hip b/src/samples/Samples/5_Domain_Specific/FDTD3d/src/FDTD3dGPU.cu.hip
index e69de29..52f5f7b 100755
--- a/src/samples/Samples/5_Domain_Specific/FDTD3d/src/FDTD3dGPU.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/FDTD3d/src/FDTD3dGPU.cu.hip
@@ -0,0 +1,273 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "FDTD3dGPU.h"
+
+#include <iostream>
+#include <algorithm>
+#include "helper_functions.h"
+#include "helper_cuda_hipified.h"
+
+#include "FDTD3dGPUKernel.cuh"
+
+bool getTargetDeviceGlobalMemSize(memsize_t *result, const int argc,
+                                  const char **argv) {
+  int deviceCount = 0;
+  int targetDevice = 0;
+  size_t memsize = 0;
+
+  // Get the number of CUDA enabled GPU devices
+  printf(" hipGetDeviceCount\n");
+  HIPCHECK(hipGetDeviceCount(&deviceCount));
+
+  // Select target device (device 0 by default)
+  targetDevice = findCudaDevice(argc, (const char **)argv);
+
+  // Query target device for maximum memory allocation
+  printf(" hipGetDeviceProperties\n");
+  struct hipDeviceProp_t deviceProp;
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, targetDevice));
+
+  memsize = deviceProp.totalGlobalMem;
+
+  // Save the result
+  *result = (memsize_t)memsize;
+  return true;
+}
+
+bool fdtdGPU(float *output, const float *input, const float *coeff,
+             const int dimx, const int dimy, const int dimz, const int radius,
+             const int timesteps, const int argc, const char **argv) {
+  const int outerDimx = dimx + 2 * radius;
+  const int outerDimy = dimy + 2 * radius;
+  const int outerDimz = dimz + 2 * radius;
+  const size_t volumeSize = outerDimx * outerDimy * outerDimz;
+  int deviceCount = 0;
+  int targetDevice = 0;
+  float *bufferOut = 0;
+  float *bufferIn = 0;
+  dim3 dimBlock;
+  dim3 dimGrid;
+
+  // Ensure that the inner data starts on a 128B boundary
+  const int padding = (128 / sizeof(float)) - radius;
+  const size_t paddedVolumeSize = volumeSize + padding;
+
+#ifdef GPU_PROFILING
+  hipEvent_t profileStart = 0;
+  hipEvent_t profileEnd = 0;
+  const int profileTimesteps = timesteps - 1;
+
+  if (profileTimesteps < 1) {
+    printf(
+        " cannot profile with fewer than two timesteps (timesteps=%d), "
+        "profiling is disabled.\n",
+        timesteps);
+  }
+
+#endif
+
+  // Check the radius is valid
+  if (radius != RADIUS) {
+    printf("radius is invalid, must be %d - see kernel for details.\n", RADIUS);
+    exit(EXIT_FAILURE);
+  }
+
+  // Get the number of CUDA enabled GPU devices
+  HIPCHECK(hipGetDeviceCount(&deviceCount));
+
+  // Select target device (device 0 by default)
+  targetDevice = findCudaDevice(argc, (const char **)argv);
+
+  HIPCHECK(hipSetDevice(targetDevice));
+
+  // Allocate memory buffers
+  HIPCHECK(
+      hipMalloc((void **)&bufferOut, paddedVolumeSize * sizeof(float)));
+  HIPCHECK(
+      hipMalloc((void **)&bufferIn, paddedVolumeSize * sizeof(float)));
+
+  // Check for a command-line specified block size
+  int userBlockSize;
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "block-size")) {
+    userBlockSize = getCmdLineArgumentInt(argc, argv, "block-size");
+    // Constrain to a multiple of k_blockDimX
+    userBlockSize = (userBlockSize / k_blockDimX * k_blockDimX);
+
+    // Constrain within allowed bounds
+    userBlockSize = MIN(MAX(userBlockSize, k_blockSizeMin), k_blockSizeMax);
+  } else {
+    userBlockSize = k_blockSizeMax;
+  }
+
+  // Check the device limit on the number of threads
+  struct hipFuncAttributes funcAttrib;
+  HIPCHECK(hipFuncGetAttributes(&funcAttrib, FiniteDifferencesKernel));
+
+  userBlockSize = MIN(userBlockSize, funcAttrib.maxThreadsPerBlock);
+
+  // Set the block size
+  dimBlock.x = k_blockDimX;
+  // Visual Studio 2005 does not like std::min
+  //    dimBlock.y = std::min<size_t>(userBlockSize / k_blockDimX,
+  //    (size_t)k_blockDimMaxY);
+  dimBlock.y = ((userBlockSize / k_blockDimX) < (size_t)k_blockDimMaxY)
+                   ? (userBlockSize / k_blockDimX)
+                   : (size_t)k_blockDimMaxY;
+  dimGrid.x = (unsigned int)ceil((float)dimx / dimBlock.x);
+  dimGrid.y = (unsigned int)ceil((float)dimy / dimBlock.y);
+  printf(" set block size to %dx%d\n", dimBlock.x, dimBlock.y);
+  printf(" set grid size to %dx%d\n", dimGrid.x, dimGrid.y);
+
+  // Check the block size is valid
+  if (dimBlock.x < RADIUS || dimBlock.y < RADIUS) {
+    printf("invalid block size, x (%d) and y (%d) must be >= radius (%d).\n",
+           dimBlock.x, dimBlock.y, RADIUS);
+    exit(EXIT_FAILURE);
+  }
+
+  // Copy the input to the device input buffer
+  HIPCHECK(hipMemcpy(bufferIn + padding, input,
+                             volumeSize * sizeof(float),
+                             hipMemcpyHostToDevice));
+
+  // Copy the input to the device output buffer (actually only need the halo)
+  HIPCHECK(hipMemcpy(bufferOut + padding, input,
+                             volumeSize * sizeof(float),
+                             hipMemcpyHostToDevice));
+
+  // Copy the coefficients to the device coefficient buffer
+  HIPCHECK(
+      hipMemcpyToSymbol(HIP_SYMBOL(stencil), (void *)coeff, (radius + 1) * sizeof(float)));
+
+#ifdef GPU_PROFILING
+
+  // Create the events
+  HIPCHECK(hipEventCreate(&profileStart));
+  HIPCHECK(hipEventCreate(&profileEnd));
+
+#endif
+
+  // Execute the FDTD
+  float *bufferSrc = bufferIn + padding;
+  float *bufferDst = bufferOut + padding;
+  printf(" GPU FDTD loop\n");
+
+#ifdef GPU_PROFILING
+  // Enqueue start event
+  HIPCHECK(hipEventRecord(profileStart, 0));
+#endif
+
+  for (int it = 0; it < timesteps; it++) {
+    printf("\tt = %d ", it);
+
+    // Launch the kernel
+    printf("launch kernel\n");
+    FiniteDifferencesKernel<<<dimGrid, dimBlock>>>(bufferDst, bufferSrc, dimx,
+                                                   dimy, dimz);
+
+    // Toggle the buffers
+    // Visual Studio 2005 does not like std::swap
+    //    std::swap<float *>(bufferSrc, bufferDst);
+    float *tmp = bufferDst;
+    bufferDst = bufferSrc;
+    bufferSrc = tmp;
+  }
+
+  printf("\n");
+
+#ifdef GPU_PROFILING
+  // Enqueue end event
+  HIPCHECK(hipEventRecord(profileEnd, 0));
+#endif
+
+  // Wait for the kernel to complete
+  HIPCHECK(hipDeviceSynchronize());
+
+  // Read the result back, result is in bufferSrc (after final toggle)
+  HIPCHECK(hipMemcpy(output, bufferSrc, volumeSize * sizeof(float),
+                             hipMemcpyDeviceToHost));
+
+// Report time
+#ifdef GPU_PROFILING
+  float elapsedTimeMS = 0;
+
+  if (profileTimesteps > 0) {
+    HIPCHECK(
+        hipEventElapsedTime(&elapsedTimeMS, profileStart, profileEnd));
+  }
+
+  if (profileTimesteps > 0) {
+    // Convert milliseconds to seconds
+    double elapsedTime = elapsedTimeMS * 1.0e-3;
+    double avgElapsedTime = elapsedTime / (double)profileTimesteps;
+    // Determine number of computations per timestep
+    size_t pointsComputed = dimx * dimy * dimz;
+    // Determine throughput
+    double throughputM = 1.0e-6 * (double)pointsComputed / avgElapsedTime;
+    printf(
+        "FDTD3d, Throughput = %.4f MPoints/s, Time = %.5f s, Size = %u Points, "
+        "NumDevsUsed = %u, Blocksize = %u\n",
+        throughputM, avgElapsedTime, pointsComputed, 1,
+        dimBlock.x * dimBlock.y);
+  }
+
+#endif
+
+  // Cleanup
+  if (bufferIn) {
+    HIPCHECK(hipFree(bufferIn));
+  }
+
+  if (bufferOut) {
+    HIPCHECK(hipFree(bufferOut));
+  }
+
+#ifdef GPU_PROFILING
+
+  if (profileStart) {
+    HIPCHECK(hipEventDestroy(profileStart));
+  }
+
+  if (profileEnd) {
+    HIPCHECK(hipEventDestroy(profileEnd));
+  }
+
+#endif
+  return true;
+}
+CudaErrors(hipEventDestroy(profileStart));
+  }
+
+  if (profileEnd) {
+    checkCudaErrors(hipEventDestroy(profileEnd));
+  }
+
+#endif
+  return true;
+}
diff --git a/src/samples/Samples/5_Domain_Specific/Mandelbrot/Mandelbrot_cuda.cu.hip b/src/samples/Samples/5_Domain_Specific/Mandelbrot/Mandelbrot_cuda.cu.hip
index e69de29..d639356 100755
--- a/src/samples/Samples/5_Domain_Specific/Mandelbrot/Mandelbrot_cuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/Mandelbrot/Mandelbrot_cuda.cu.hip
@@ -0,0 +1,395 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <stdio.h>
+#include "helper_cuda.h"
+#include "Mandelbrot_kernel.h"
+#include "Mandelbrot_kernel.cuh"
+
+// The Mandelbrot CUDA GPU thread function
+
+template <class T>
+__global__ void Mandelbrot0(uchar4 *dst, const int imageW, const int imageH,
+                            const int crunch, const T xOff, const T yOff,
+                            const T xJP, const T yJP, const T scale,
+                            const uchar4 colors, const int frame,
+                            const int animationFrame, const int gridWidth,
+                            const int numBlocks, const bool isJ) {
+  // loop until all blocks completed
+  for (unsigned int blockIndex = blockIdx.x; blockIndex < numBlocks;
+       blockIndex += gridDim.x) {
+    unsigned int blockX = blockIndex % gridWidth;
+    unsigned int blockY = blockIndex / gridWidth;
+
+    // process this block
+    const int ix = blockDim.x * blockX + threadIdx.x;
+    const int iy = blockDim.y * blockY + threadIdx.y;
+
+    if ((ix < imageW) && (iy < imageH)) {
+      // Calculate the location
+      const T xPos = (T)ix * scale + xOff;
+      const T yPos = (T)iy * scale + yOff;
+
+      // Calculate the Mandelbrot index for the current location
+      int m = CalcMandelbrot<T>(xPos, yPos, xJP, yJP, crunch, isJ);
+      //            int m = blockIdx.x;         // uncomment to see scheduling
+      //            order
+      m = m > 0 ? crunch - m : 0;
+
+      // Convert the Mandelbrot index into a color
+      uchar4 color;
+
+      if (m) {
+        m += animationFrame;
+        color.x = m * colors.x;
+        color.y = m * colors.y;
+        color.z = m * colors.z;
+      } else {
+        color.x = 0;
+        color.y = 0;
+        color.z = 0;
+      }
+
+      // Output the pixel
+      int pixel = imageW * iy + ix;
+
+      if (frame == 0) {
+        color.w = 0;
+        dst[pixel] = color;
+      } else {
+        int frame1 = frame + 1;
+        int frame2 = frame1 / 2;
+        dst[pixel].x = (dst[pixel].x * frame + color.x + frame2) / frame1;
+        dst[pixel].y = (dst[pixel].y * frame + color.y + frame2) / frame1;
+        dst[pixel].z = (dst[pixel].z * frame + color.z + frame2) / frame1;
+      }
+    }
+  }
+
+}  // Mandelbrot0
+
+// The Mandelbrot CUDA GPU thread function (double single version)
+__global__ void MandelbrotDS0(uchar4 *dst, const int imageW, const int imageH,
+                              const int crunch, const float xOff0,
+                              const float xOff1, const float yOff0,
+                              const float yOff1, const float xJP,
+                              const float yJP, const float scale,
+                              const uchar4 colors, const int frame,
+                              const int animationFrame, const int gridWidth,
+                              const int numBlocks, const bool isJ) {
+  // loop until all blocks completed
+  for (unsigned int blockIndex = blockIdx.x; blockIndex < numBlocks;
+       blockIndex += gridDim.x) {
+    unsigned int blockX = blockIndex % gridWidth;
+    unsigned int blockY = blockIndex / gridWidth;
+
+    // process this block
+    const int ix = blockDim.x * blockX + threadIdx.x;
+    const int iy = blockDim.y * blockY + threadIdx.y;
+
+    if ((ix < imageW) && (iy < imageH)) {
+      // Calculate the location
+      float xPos0 = (float)ix * scale;
+      float xPos1 = 0.0f;
+      float yPos0 = (float)iy * scale;
+      float yPos1 = 0.0f;
+      dsadd(xPos0, xPos1, xPos0, xPos1, xOff0, xOff1);
+      dsadd(yPos0, yPos1, yPos0, yPos1, yOff0, yOff1);
+
+      // Calculate the Mandelbrot index for the current location
+      int m =
+          CalcMandelbrotDS(xPos0, xPos1, yPos0, yPos1, xJP, yJP, crunch, isJ);
+      m = m > 0 ? crunch - m : 0;
+
+      // Convert the Mandelbrot index into a color
+      uchar4 color;
+
+      if (m) {
+        m += animationFrame;
+        color.x = m * colors.x;
+        color.y = m * colors.y;
+        color.z = m * colors.z;
+      } else {
+        color.x = 0;
+        color.y = 0;
+        color.z = 0;
+      }
+
+      // Output the pixel
+      int pixel = imageW * iy + ix;
+
+      if (frame == 0) {
+        color.w = 0;
+        dst[pixel] = color;
+      } else {
+        int frame1 = frame + 1;
+        int frame2 = frame1 / 2;
+        dst[pixel].x = (dst[pixel].x * frame + color.x + frame2) / frame1;
+        dst[pixel].y = (dst[pixel].y * frame + color.y + frame2) / frame1;
+        dst[pixel].z = (dst[pixel].z * frame + color.z + frame2) / frame1;
+      }
+    }
+  }
+}  // MandelbrotDS0
+
+// The Mandelbrot secondary AA pass CUDA GPU thread function
+template <class T>
+__global__ void Mandelbrot1(uchar4 *dst, const int imageW, const int imageH,
+                            const int crunch, const T xOff, const T yOff,
+                            const T xJP, const T yJP, const T scale,
+                            const uchar4 colors, const int frame,
+                            const int animationFrame, const int gridWidth,
+                            const int numBlocks, const bool isJ) {
+  // loop until all blocks completed
+  for (unsigned int blockIndex = blockIdx.x; blockIndex < numBlocks;
+       blockIndex += gridDim.x) {
+    unsigned int blockX = blockIndex % gridWidth;
+    unsigned int blockY = blockIndex / gridWidth;
+
+    // process this block
+    const int ix = blockDim.x * blockX + threadIdx.x;
+    const int iy = blockDim.y * blockY + threadIdx.y;
+
+    if ((ix < imageW) && (iy < imageH)) {
+      // Get the current pixel color
+      int pixel = imageW * iy + ix;
+      uchar4 pixelColor = dst[pixel];
+      int count = 0;
+
+      // Search for pixels out of tolerance surrounding the current pixel
+      if (ix > 0) {
+        count += CheckColors(pixelColor, dst[pixel - 1]);
+      }
+
+      if (ix + 1 < imageW) {
+        count += CheckColors(pixelColor, dst[pixel + 1]);
+      }
+
+      if (iy > 0) {
+        count += CheckColors(pixelColor, dst[pixel - imageW]);
+      }
+
+      if (iy + 1 < imageH) {
+        count += CheckColors(pixelColor, dst[pixel + imageW]);
+      }
+
+      if (count) {
+        // Calculate the location
+        const T xPos = (T)ix * scale + xOff;
+        const T yPos = (T)iy * scale + yOff;
+
+        // Calculate the Mandelbrot index for the current location
+        int m = CalcMandelbrot(xPos, yPos, xJP, yJP, crunch, isJ);
+        m = m > 0 ? crunch - m : 0;
+
+        // Convert the Mandelbrot index into a color
+        uchar4 color;
+
+        if (m) {
+          m += animationFrame;
+          color.x = m * colors.x;
+          color.y = m * colors.y;
+          color.z = m * colors.z;
+        } else {
+          color.x = 0;
+          color.y = 0;
+          color.z = 0;
+        }
+
+        // Output the pixel
+        int frame1 = frame + 1;
+        int frame2 = frame1 / 2;
+        dst[pixel].x = (pixelColor.x * frame + color.x + frame2) / frame1;
+        dst[pixel].y = (pixelColor.y * frame + color.y + frame2) / frame1;
+        dst[pixel].z = (pixelColor.z * frame + color.z + frame2) / frame1;
+      }
+    }
+  }
+
+}  // Mandelbrot1
+
+// The Mandelbrot secondary AA pass CUDA GPU thread function (double single
+// version)
+__global__ void MandelbrotDS1(uchar4 *dst, const int imageW, const int imageH,
+                              const int crunch, const float xOff0,
+                              const float xOff1, const float yOff0,
+                              const float yOff1, const float xJP,
+                              const float yJP, const float scale,
+                              const uchar4 colors, const int frame,
+                              const int animationFrame, const int gridWidth,
+                              const int numBlocks, const bool isJ) {
+  // loop until all blocks completed
+  for (unsigned int blockIndex = blockIdx.x; blockIndex < numBlocks;
+       blockIndex += gridDim.x) {
+    unsigned int blockX = blockIndex % gridWidth;
+    unsigned int blockY = blockIndex / gridWidth;
+
+    // process this block
+    const int ix = blockDim.x * blockX + threadIdx.x;
+    const int iy = blockDim.y * blockY + threadIdx.y;
+
+    if ((ix < imageW) && (iy < imageH)) {
+      // Get the current pixel color
+      int pixel = imageW * iy + ix;
+      uchar4 pixelColor = dst[pixel];
+      int count = 0;
+
+      // Search for pixels out of tolerance surrounding the current pixel
+      if (ix > 0) {
+        count += CheckColors(pixelColor, dst[pixel - 1]);
+      }
+
+      if (ix + 1 < imageW) {
+        count += CheckColors(pixelColor, dst[pixel + 1]);
+      }
+
+      if (iy > 0) {
+        count += CheckColors(pixelColor, dst[pixel - imageW]);
+      }
+
+      if (iy + 1 < imageH) {
+        count += CheckColors(pixelColor, dst[pixel + imageW]);
+      }
+
+      if (count) {
+        // Calculate the location
+        float xPos0 = (float)ix * scale;
+        float xPos1 = 0.0f;
+        float yPos0 = (float)iy * scale;
+        float yPos1 = 0.0f;
+        dsadd(xPos0, xPos1, xPos0, xPos1, xOff0, xOff1);
+        dsadd(yPos0, yPos1, yPos0, yPos1, yOff0, yOff1);
+
+        // Calculate the Mandelbrot index for the current location
+        int m =
+            CalcMandelbrotDS(xPos0, xPos1, yPos0, yPos1, xJP, yJP, crunch, isJ);
+        m = m > 0 ? crunch - m : 0;
+
+        // Convert the Mandelbrot index into a color
+        uchar4 color;
+
+        if (m) {
+          m += animationFrame;
+          color.x = m * colors.x;
+          color.y = m * colors.y;
+          color.z = m * colors.z;
+        } else {
+          color.x = 0;
+          color.y = 0;
+          color.z = 0;
+        }
+
+        // Output the pixel
+        int frame1 = frame + 1;
+        int frame2 = frame1 / 2;
+        dst[pixel].x = (pixelColor.x * frame + color.x + frame2) / frame1;
+        dst[pixel].y = (pixelColor.y * frame + color.y + frame2) / frame1;
+        dst[pixel].z = (pixelColor.z * frame + color.z + frame2) / frame1;
+      }
+    }
+  }
+
+}  // MandelbrotDS1
+
+// The host CPU Mandelbrot thread spawner
+void RunMandelbrot0(uchar4 *dst, const int imageW, const int imageH,
+                    const int crunch, const double xOff, const double yOff,
+                    const double xjp, const double yjp, const double scale,
+                    const uchar4 colors, const int frame,
+                    const int animationFrame, const int mode, const int numSMs,
+                    const bool isJ, int version) {
+  dim3 threads(BLOCKDIM_X, BLOCKDIM_Y);
+  dim3 grid(iDivUp(imageW, BLOCKDIM_X), iDivUp(imageH, BLOCKDIM_Y));
+
+  int numWorkerBlocks = numSMs;
+
+  switch (mode) {
+    default:
+    case 0:
+      Mandelbrot0<float><<<numWorkerBlocks, threads>>>(
+          dst, imageW, imageH, crunch, (float)xOff, (float)yOff, (float)xjp,
+          (float)yjp, (float)scale, colors, frame, animationFrame, grid.x,
+          grid.x * grid.y, isJ);
+      break;
+    case 1:
+      float x0, x1, y0, y1;
+      dsdeq(x0, x1, xOff);
+      dsdeq(y0, y1, yOff);
+      MandelbrotDS0<<<numWorkerBlocks, threads>>>(
+          dst, imageW, imageH, crunch, x0, x1, y0, y1, (float)xjp, (float)yjp,
+          (float)scale, colors, frame, animationFrame, grid.x, grid.x * grid.y,
+          isJ);
+      break;
+    case 2:
+      Mandelbrot0<double><<<numWorkerBlocks, threads>>>(
+          dst, imageW, imageH, crunch, xOff, yOff, xjp, yjp, scale, colors,
+          frame, animationFrame, grid.x, grid.x * grid.y, isJ);
+      break;
+  }
+
+  getLastCudaError("Mandelbrot0 kernel execution failed.\n");
+}  // RunMandelbrot0
+
+// The host CPU Mandelbrot thread spawner
+void RunMandelbrot1(uchar4 *dst, const int imageW, const int imageH,
+                    const int crunch, const double xOff, const double yOff,
+                    const double xjp, const double yjp, const double scale,
+                    const uchar4 colors, const int frame,
+                    const int animationFrame, const int mode, const int numSMs,
+                    const bool isJ, int version) {
+  dim3 threads(BLOCKDIM_X, BLOCKDIM_Y);
+  dim3 grid(iDivUp(imageW, BLOCKDIM_X), iDivUp(imageH, BLOCKDIM_Y));
+
+  int numWorkerBlocks = numSMs;
+
+  switch (mode) {
+    default:
+    case 0:
+      Mandelbrot1<float><<<numWorkerBlocks, threads>>>(
+          dst, imageW, imageH, crunch, (float)xOff, (float)yOff, (float)xjp,
+          (float)yjp, (float)scale, colors, frame, animationFrame, grid.x,
+          grid.x * grid.y, isJ);
+      break;
+    case 1:
+      float x0, x1, y0, y1;
+      dsdeq(x0, x1, xOff);
+      dsdeq(y0, y1, yOff);
+      MandelbrotDS1<<<numWorkerBlocks, threads>>>(
+          dst, imageW, imageH, crunch, x0, x1, y0, y1, (float)xjp, (float)yjp,
+          (float)scale, colors, frame, animationFrame, grid.x, grid.x * grid.y,
+          isJ);
+      break;
+    case 2:
+      Mandelbrot1<double><<<numWorkerBlocks, threads>>>(
+          dst, imageW, imageH, crunch, xOff, yOff, xjp, yjp, scale, colors,
+          frame, animationFrame, grid.x, grid.x * grid.y, isJ);
+      break;
+  }
+
+  getLastCudaError("Mandelbrot1 kernel execution failed.\n");
+}  // RunMandelbrot1
diff --git a/src/samples/Samples/5_Domain_Specific/MonteCarloMultiGPU/MonteCarlo_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/MonteCarloMultiGPU/MonteCarlo_kernel.cu.hip
index e69de29..ca043df 100755
--- a/src/samples/Samples/5_Domain_Specific/MonteCarloMultiGPU/MonteCarlo_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/MonteCarloMultiGPU/MonteCarlo_kernel.cu.hip
@@ -0,0 +1,231 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+////////////////////////////////////////////////////////////////////////////////
+// Global types
+////////////////////////////////////////////////////////////////////////////////
+#include <stdlib.h>
+#include <stdio.h>
+#include <hip/hip_cooperative_groups.h>
+
+namespace cg = cooperative_groups;
+#include "helper_cuda_hipified.h"
+#include <hiprand_kernel.h>
+#include "MonteCarlo_common.h"
+
+////////////////////////////////////////////////////////////////////////////////
+// Helper reduction template
+// Please see the "reduction" CUDA Sample for more information
+////////////////////////////////////////////////////////////////////////////////
+#include "MonteCarlo_reduction.cuh"
+
+////////////////////////////////////////////////////////////////////////////////
+// Internal GPU-side data structures
+////////////////////////////////////////////////////////////////////////////////
+#define MAX_OPTIONS (1024 * 1024)
+
+// Preprocessed input option data
+typedef struct {
+  real S;
+  real X;
+  real MuByT;
+  real VBySqrtT;
+} __TOptionData;
+
+////////////////////////////////////////////////////////////////////////////////
+// Overloaded shortcut payoff functions for different precision modes
+////////////////////////////////////////////////////////////////////////////////
+__device__ inline float endCallValue(float S, float X, float r, float MuByT,
+                                     float VBySqrtT) {
+  float callValue = S * __expf(MuByT + VBySqrtT * r) - X;
+  return (callValue > 0.0F) ? callValue : 0.0F;
+}
+
+__device__ inline double endCallValue(double S, double X, double r,
+                                      double MuByT, double VBySqrtT) {
+  double callValue = S * exp(MuByT + VBySqrtT * r) - X;
+  return (callValue > 0.0) ? callValue : 0.0;
+}
+
+#define THREAD_N 256
+
+////////////////////////////////////////////////////////////////////////////////
+// This kernel computes the integral over all paths using a single thread block
+// per option. It is fastest when the number of thread blocks times the work per
+// block is high enough to keep the GPU busy.
+////////////////////////////////////////////////////////////////////////////////
+static __global__ void MonteCarloOneBlockPerOption(
+    hiprandState *__restrict rngStates,
+    const __TOptionData *__restrict d_OptionData,
+    __TOptionValue *__restrict d_CallValue, int pathN, int optionN) {
+  // Handle to thread block group
+  cg::thread_block cta = cg::this_thread_block();
+  cg::thread_block_tile<32> tile32 = cg::tiled_partition<32>(cta);
+
+  const int SUM_N = THREAD_N;
+  __shared__ real s_SumCall[SUM_N];
+  __shared__ real s_Sum2Call[SUM_N];
+
+  // determine global thread id
+  int tid = threadIdx.x + blockIdx.x * blockDim.x;
+
+  // Copy random number state to local memory for efficiency
+  hiprandState localState = rngStates[tid];
+  for (int optionIndex = blockIdx.x; optionIndex < optionN;
+       optionIndex += gridDim.x) {
+    const real S = d_OptionData[optionIndex].S;
+    const real X = d_OptionData[optionIndex].X;
+    const real MuByT = d_OptionData[optionIndex].MuByT;
+    const real VBySqrtT = d_OptionData[optionIndex].VBySqrtT;
+
+    // Cycle through the entire samples array:
+    // derive end stock price for each path
+    // accumulate partial integrals into intermediate shared memory buffer
+    for (int iSum = threadIdx.x; iSum < SUM_N; iSum += blockDim.x) {
+      __TOptionValue sumCall = {0, 0};
+
+#pragma unroll 8
+      for (int i = iSum; i < pathN; i += SUM_N) {
+        real r = hiprand_normal(&localState);
+        real callValue = endCallValue(S, X, r, MuByT, VBySqrtT);
+        sumCall.Expected += callValue;
+        sumCall.Confidence += callValue * callValue;
+      }
+
+      s_SumCall[iSum] = sumCall.Expected;
+      s_Sum2Call[iSum] = sumCall.Confidence;
+    }
+
+    // Reduce shared memory accumulators
+    // and write final result to global memory
+    cg::sync(cta);
+    sumReduce<real, SUM_N, THREAD_N>(s_SumCall, s_Sum2Call, cta, tile32,
+                                     &d_CallValue[optionIndex]);
+  }
+}
+
+static __global__ void rngSetupStates(hiprandState *rngState, int device_id) {
+  // determine global thread id
+  int tid = threadIdx.x + blockIdx.x * blockDim.x;
+  // Each threadblock gets different seed,
+  // Threads within a threadblock get different sequence numbers
+  hiprand_init(blockIdx.x + gridDim.x * device_id, threadIdx.x, 0,
+              &rngState[tid]);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Host-side interface to GPU Monte Carlo
+////////////////////////////////////////////////////////////////////////////////
+
+extern "C" void initMonteCarloGPU(TOptionPlan *plan) {
+  HIPCHECK(hipMalloc(&plan->d_OptionData,
+                             sizeof(__TOptionData) * (plan->optionCount)));
+  HIPCHECK(hipMalloc(&plan->d_CallValue,
+                             sizeof(__TOptionValue) * (plan->optionCount)));
+  HIPCHECK(hipHostMalloc(&plan->h_OptionData,
+                                 sizeof(__TOptionData) * (plan->optionCount)));
+  // Allocate internal device memory
+  HIPCHECK(hipHostMalloc(&plan->h_CallValue,
+                                 sizeof(__TOptionValue) * (plan->optionCount)));
+  // Allocate states for pseudo random number generators
+  HIPCHECK(hipMalloc((void **)&plan->rngStates,
+                             plan->gridSize * THREAD_N * sizeof(hiprandState)));
+  HIPCHECK(hipMemset(plan->rngStates, 0,
+                             plan->gridSize * THREAD_N * sizeof(hiprandState)));
+
+  // place each device pathN random numbers apart on the random number sequence
+  rngSetupStates<<<plan->gridSize, THREAD_N>>>(plan->rngStates, plan->device);
+  getLastCudaError("rngSetupStates kernel failed.\n");
+}
+
+// Compute statistics and deallocate internal device memory
+extern "C" void closeMonteCarloGPU(TOptionPlan *plan) {
+  for (int i = 0; i < plan->optionCount; i++) {
+    const double RT = plan->optionData[i].R * plan->optionData[i].T;
+    const double sum = plan->h_CallValue[i].Expected;
+    const double sum2 = plan->h_CallValue[i].Confidence;
+    const double pathN = plan->pathN;
+    // Derive average from the total sum and discount by riskfree rate
+    plan->callValue[i].Expected = (float)(exp(-RT) * sum / pathN);
+    // Standard deviation
+    double stdDev = sqrt((pathN * sum2 - sum * sum) / (pathN * (pathN - 1)));
+    // Confidence width; in 95% of all cases theoretical value lies within these
+    // borders
+    plan->callValue[i].Confidence =
+        (float)(exp(-RT) * 1.96 * stdDev / sqrt(pathN));
+  }
+
+  HIPCHECK(hipFree(plan->rngStates));
+  HIPCHECK(hipHostFree(plan->h_CallValue));
+  HIPCHECK(hipHostFree(plan->h_OptionData));
+  HIPCHECK(hipFree(plan->d_CallValue));
+  HIPCHECK(hipFree(plan->d_OptionData));
+}
+
+// Main computations
+extern "C" void MonteCarloGPU(TOptionPlan *plan, hipStream_t stream) {
+  __TOptionValue *h_CallValue = plan->h_CallValue;
+
+  if (plan->optionCount <= 0 || plan->optionCount > MAX_OPTIONS) {
+    printf("MonteCarloGPU(): bad option count.\n");
+    return;
+  }
+
+  __TOptionData *h_OptionData = (__TOptionData *)plan->h_OptionData;
+
+  for (int i = 0; i < plan->optionCount; i++) {
+    const double T = plan->optionData[i].T;
+    const double R = plan->optionData[i].R;
+    const double V = plan->optionData[i].V;
+    const double MuByT = (R - 0.5 * V * V) * T;
+    const double VBySqrtT = V * sqrt(T);
+    h_OptionData[i].S = (real)plan->optionData[i].S;
+    h_OptionData[i].X = (real)plan->optionData[i].X;
+    h_OptionData[i].MuByT = (real)MuByT;
+    h_OptionData[i].VBySqrtT = (real)VBySqrtT;
+  }
+
+  HIPCHECK(hipMemcpyAsync(plan->d_OptionData, h_OptionData,
+                                  plan->optionCount * sizeof(__TOptionData),
+                                  hipMemcpyHostToDevice, stream));
+
+  MonteCarloOneBlockPerOption<<<plan->gridSize, THREAD_N, 0, stream>>>(
+      plan->rngStates, (__TOptionData *)(plan->d_OptionData),
+      (__TOptionValue *)(plan->d_CallValue), plan->pathN, plan->optionCount);
+  getLastCudaError("MonteCarloOneBlockPerOption() execution failed\n");
+
+  HIPCHECK(hipMemcpyAsync(h_CallValue, plan->d_CallValue,
+                                  plan->optionCount * sizeof(__TOptionValue),
+                                  hipMemcpyDeviceToHost, stream));
+
+  // hipDeviceSynchronize();
+}
+                          hipMemcpyDeviceToHost, stream));
+
+  // hipDeviceSynchronize();
+}
diff --git a/src/samples/Samples/5_Domain_Specific/SLID3D10Texture/texture_2d.cu.hip b/src/samples/Samples/5_Domain_Specific/SLID3D10Texture/texture_2d.cu.hip
index e69de29..fcbe2d1 100755
--- a/src/samples/Samples/5_Domain_Specific/SLID3D10Texture/texture_2d.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/SLID3D10Texture/texture_2d.cu.hip
@@ -0,0 +1,89 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+//
+// Paint a 2D texture with a moving red/green hatch pattern on a
+// strobing blue background.  Note that this kernel reads to and
+// writes from the texture, hence why this texture was not mapped
+// as WriteDiscard.
+//
+__global__ void cuda_kernel_texture_2d(unsigned char *surface, int width,
+                                       int height, size_t pitch, float t) {
+  int x = blockIdx.x * blockDim.x + threadIdx.x;
+  int y = blockIdx.y * blockDim.y + threadIdx.y;
+  float *pixel;
+
+  // in the case where, due to quantization into grids, we have
+  // more threads than pixels, skip the threads which don't
+  // correspond to valid pixels
+  if (x >= width || y >= height) return;
+
+  // get a pointer to the pixel at (x,y)
+  pixel = (float *)(surface + y * pitch) + 4 * x;
+
+  // populate it
+  float value_x = 0.5f + 0.5f * cos(t + 10.0f * ((2.0f * x) / width - 1.0f));
+  float value_y = 0.5f + 0.5f * cos(t + 10.0f * ((2.0f * y) / height - 1.0f));
+  pixel[0] = value_x > 0.5 ? 1 : 0;
+  pixel[1] = value_y > 0.5 ? 1 : 0;
+  pixel[2] = 0.5f + 0.5f * cos(t);
+  pixel[3] = 1;  // alpha
+
+  for (int i = 0; i < 6; ++i) {
+    for (int j = 0; j < 4; ++j) {
+      pixel[j] = sqrt(pixel[j]);
+    }
+  }
+
+  for (int i = 0; i < 6; ++i) {
+    for (int j = 0; j < 4; ++j) {
+      pixel[j] *= pixel[j];
+    }
+  }
+}
+
+extern "C" void cuda_texture_2d(void *surface, int width, int height,
+                                size_t pitch, float t) {
+  hipError_t error = hipSuccess;
+
+  dim3 Db = dim3(16, 16);  // block dimensions are fixed to be 256 threads
+  dim3 Dg = dim3((width + Db.x - 1) / Db.x, (height + Db.y - 1) / Db.y);
+
+  cuda_kernel_texture_2d<<<Dg, Db>>>((unsigned char *)surface, width, height,
+                                     pitch, t);
+
+  error = hipGetLastError();
+
+  if (error != hipSuccess) {
+    printf("cuda_kernel_texture_2d() failed to launch error = %d\n", error);
+  }
+}
diff --git a/src/samples/Samples/5_Domain_Specific/SobelFilter/SobelFilter_kernels.cu.hip b/src/samples/Samples/5_Domain_Specific/SobelFilter/SobelFilter_kernels.cu.hip
index 19a887e..b0fcd54 100755
--- a/src/samples/Samples/5_Domain_Specific/SobelFilter/SobelFilter_kernels.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/SobelFilter/SobelFilter_kernels.cu.hip
@@ -27,8 +27,6 @@
  */
 
 #include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
 #include <stdlib.h>
 #include <hip/hip_runtime.h>
 #include <hip/hip_cooperative_groups.h>
@@ -296,3 +294,4 @@ extern "C" void sobelFilter(Pixel *odata, int iw, int ih,
          iw, ih, fScale, texObject);
     } break;
   }
+}
diff --git a/src/samples/Samples/5_Domain_Specific/VFlockingD3D10/VFlocking_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/VFlockingD3D10/VFlocking_kernel.cu.hip
index e69de29..1a9d7a5 100755
--- a/src/samples/Samples/5_Domain_Specific/VFlockingD3D10/VFlocking_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/VFlockingD3D10/VFlocking_kernel.cu.hip
@@ -0,0 +1,289 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+#include "helper_cuda_hipified.h"
+#include <helper_math.h>
+#include <VFlockingD3D10.h>
+
+#define PI 3.1415926536f
+
+typedef unsigned int uint;
+
+__device__ bool isInsideQuad_D(float2 pos0, float2 pos1, float width,
+                               float height) {
+  if (fabs(pos0.x - pos1.x) < 0.5f * width &&
+      fabs(pos0.y - pos1.y) < 0.5f * height) {
+    return true;
+  } else {
+    return false;
+  }
+}
+
+__device__ bool isInsideBird(float2 pixel, float2 pos, float width,
+                             float height, float radius) {
+  if (abs(pixel.x - pos.x) < 0.5f * width &&
+          abs(pixel.y - pos.y) < 0.5f * height ||
+      (pixel.x - pos.x) * (pixel.x - pos.x) +
+              (pixel.y - pos.y) * (pixel.y - pos.y) <
+          radius * radius) {
+    return true;
+  } else {
+    return false;
+  }
+}
+
+__global__ void cuda_kernel_update(float2 *newPos, float2 *curPos,
+                                   uint numBirds, bool *hasproxy,
+                                   bool *neighbors, bool *rightgoals,
+                                   bool *leftgoals, Params *params) {
+  uint i = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (i >= numBirds) {
+    return;
+  }
+
+  float minDist = 50000.f;
+  float2 dij = make_float2(0.f);
+
+  if (!hasproxy[i]) {
+    for (uint j = 0; j < numBirds; j++) {
+      if (j == i) {
+        continue;
+      }
+
+      if (leftgoals[i * numBirds + j]) {
+        dij = params->dX * normalize(curPos[j] - curPos[i]);
+        break;
+      }
+    }
+  } else {
+    bool collision = false;
+
+    for (uint j = 0; j < numBirds; j++) {
+      float d;
+
+      if (leftgoals[i * numBirds + j]) {
+        d = curPos[j].x - (params->wingspan + params->lambda) - curPos[i].x;
+
+        if (fabs(d) < fabs(minDist)) {
+          minDist = d;
+        }
+      }
+
+      if (rightgoals[i * numBirds + j]) {
+        d = curPos[j].x + (params->wingspan + params->lambda) - curPos[i].x;
+
+        if (fabs(d) < fabs(minDist)) {
+          minDist = d;
+        }
+      }
+
+      if (neighbors[i * numBirds + j] && !collision) {
+        if (curPos[j].y >= curPos[i].y &&
+            curPos[j].y < curPos[i].y + params->epsilon) {
+          dij.y = -params->dY;
+          collision = true;
+        }
+      }
+    }
+
+    if (fabs(minDist) <= params->dX) {
+      return;
+    }
+
+    dij.x = minDist > 0 ? params->dX : -params->dX;
+  }
+
+  newPos[i].x = curPos[i].x + dij.x;
+  newPos[i].y = curPos[i].y + dij.y;
+}
+
+__global__ void cuda_kernel_checktriples(float2 *pos, uint numBirds,
+                                         bool *hasproxy, bool *neighbors,
+                                         bool *rightgoals, bool *leftgoals,
+                                         uint3 *triples, Params *params) {
+  uint ith = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (ith >= numBirds * (numBirds - 1) * (numBirds - 2) / 6) {
+    return;
+  }
+
+  uint a[3];
+  a[0] = triples[ith].x;
+  a[1] = triples[ith].y;
+  a[2] = triples[ith].z;
+
+  uint i, j, x;
+
+  for (i = 0; i < 3; i++) {
+    for (j = 2; j > i; j--) {
+      if (pos[a[j - 1]].y > pos[a[j]].y) {
+        x = a[j - 1];
+        a[j - 1] = a[j];
+        a[j] = x;
+      }
+    }
+  }
+
+  if (hasproxy[a[0]]) {
+    float a2a1 = pos[a[2]].x - pos[a[1]].x;
+
+    if (fabs(a2a1) < 2.f * (params->wingspan + params->lambda))
+      if (a2a1 >= 0) {
+        if (leftgoals[a[0] * numBirds + a[2]]) {
+          leftgoals[a[0] * numBirds + a[2]] = false;
+        }
+
+        if (rightgoals[a[0] * numBirds + a[1]]) {
+          rightgoals[a[0] * numBirds + a[1]] = false;
+        }
+      } else {
+        if (leftgoals[a[0] * numBirds + a[1]]) {
+          leftgoals[a[0] * numBirds + a[1]] = false;
+        }
+
+        if (rightgoals[a[0] * numBirds + a[2]]) {
+          rightgoals[a[0] * numBirds + a[2]] = false;
+        }
+      }
+  } else {
+    if ((leftgoals[a[0] * numBirds + a[2]]) &&
+        (leftgoals[a[0] * numBirds + a[1]]))
+      if ((length(pos[a[1]] - pos[a[0]]) < length(pos[a[2]] - pos[a[0]]))) {
+        leftgoals[a[0] * numBirds + a[2]] = false;
+      } else {
+        leftgoals[a[0] * numBirds + a[1]] = false;
+      }
+  }
+}
+
+__global__ void cuda_kernel_checkpairs(float2 *pos, uint numBirds,
+                                       bool *hasproxy, bool *neighbors,
+                                       bool *rightgoals, bool *leftgoals,
+                                       uint2 *pairs, Params *params) {
+  uint i = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (i >= numBirds * (numBirds - 1) / 2) {
+    return;
+  }
+
+  uint front, back;
+
+  if (pos[pairs[i].y].y > pos[pairs[i].x].y) {
+    front = pairs[i].y;
+    back = pairs[i].x;
+  } else {
+    front = pairs[i].x;
+    back = pairs[i].y;
+  }
+
+  leftgoals[back * numBirds + front] = true;
+  rightgoals[back * numBirds + front] = true;
+
+  float2 stepback;
+  stepback.x = pos[front].x;
+  stepback.y = pos[front].y - 0.5f * params->upwashY;
+
+  if (isInsideQuad_D(
+          pos[back], stepback,
+          2.f * (params->wingspan + params->lambda + params->upwashX),
+          params->upwashY)) {
+    neighbors[back * numBirds + front] = true;
+
+    if (!hasproxy[back]) {
+      hasproxy[back] = true;
+    }
+  }
+}
+
+extern "C" void cuda_simulate(float2 *newPos, float2 *curPos, uint numBirds,
+                              bool *d_hasproxy, bool *d_neighbors,
+                              bool *d_leftgoals, bool *d_rightgoals,
+                              uint2 *d_pairs, uint3 *d_triples,
+                              Params *d_params) {
+  hipError_t error = hipSuccess;
+  float tempms;
+  static float ms = 0.f;
+  static uint step = 0;
+  int smallblockSize = 32, midblockSize = 128, bigblockSize = 32;
+
+  hipEvent_t e_start, e_stop;
+  hipEventCreate(&e_start);
+  hipEventCreate(&e_stop);
+  hipEventRecord(e_start, 0);
+
+  hipMemset(d_leftgoals, 0, numBirds * numBirds * sizeof(bool));
+  hipMemset(d_rightgoals, 0, numBirds * numBirds * sizeof(bool));
+  hipMemset(d_hasproxy, 0, numBirds * sizeof(bool));
+  hipMemset(d_neighbors, 0, numBirds * numBirds * sizeof(bool));
+
+  dim3 Db = dim3(bigblockSize);
+  dim3 Dg =
+      dim3((numBirds * (numBirds - 1) / 2 + bigblockSize - 1) / bigblockSize);
+  cuda_kernel_checkpairs<<<Dg, Db>>>(curPos, numBirds, d_hasproxy, d_neighbors,
+                                     d_rightgoals, d_leftgoals, d_pairs,
+                                     d_params);
+
+  Db = dim3(midblockSize);
+  Dg =
+      dim3((numBirds * (numBirds - 1) * (numBirds - 2) / 6 + bigblockSize - 1) /
+           bigblockSize);
+  cuda_kernel_checktriples<<<Dg, Db>>>(curPos, numBirds, d_hasproxy,
+                                       d_neighbors, d_rightgoals, d_leftgoals,
+                                       d_triples, d_params);
+
+  Db = dim3(smallblockSize);
+  Dg = dim3((numBirds + smallblockSize - 1) / smallblockSize);
+  cuda_kernel_update<<<Dg, Db>>>(newPos, curPos, numBirds, d_hasproxy,
+                                 d_neighbors, d_rightgoals, d_leftgoals,
+                                 d_params /*, d_pWingTips */);
+
+  hipDeviceSynchronize();
+
+  hipEventRecord(e_stop, 0);
+  hipEventSynchronize(e_stop);
+  hipEventElapsedTime(&tempms, e_start, e_stop);
+  ms += tempms;
+
+  if (!(step % 100) && step) {
+    printf("GPU, step %d \ntime per step %6.3f ms \n", step, ms / 100.f);
+    ms = 0.f;
+  }
+
+  step++;
+
+  error = hipGetLastError();
+
+  if (error != hipSuccess) {
+    printf("one of the cuda kernels failed to launch, error = %d\n", error);
+  }
+}
diff --git a/src/samples/Samples/5_Domain_Specific/bicubicTexture/bicubicTexture_cuda.cu.hip b/src/samples/Samples/5_Domain_Specific/bicubicTexture/bicubicTexture_cuda.cu.hip
index e69de29..32888a7 100755
--- a/src/samples/Samples/5_Domain_Specific/bicubicTexture/bicubicTexture_cuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/bicubicTexture/bicubicTexture_cuda.cu.hip
@@ -0,0 +1,134 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _BICUBICTEXTURE_CU_
+#define _BICUBICTEXTURE_CU_
+
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+
+#include <helper_math.h>
+
+// includes, cuda
+#include "helper_cuda_hipified.h"
+
+typedef unsigned int uint;
+typedef unsigned char uchar;
+
+#include "bicubicTexture_kernel.cuh"
+
+hipArray *d_imageArray = 0;
+
+extern "C" void initTexture(int imageWidth, int imageHeight, uchar *h_data) {
+  // allocate array and copy image data
+  hipChannelFormatDesc channelDesc =
+      hipCreateChannelDesc(8, 0, 0, 0, hipChannelFormatKindUnsigned);
+  HIPCHECK(
+      hipMallocArray(&d_imageArray, &channelDesc, imageWidth, imageHeight));
+  HIPCHECK(hipMemcpy2DToArray(
+      d_imageArray, 0, 0, h_data, imageWidth * sizeof(uchar),
+      imageWidth * sizeof(uchar), imageHeight, hipMemcpyHostToDevice));
+  free(h_data);
+
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = d_imageArray;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeClamp;
+  texDescr.addressMode[1] = hipAddressModeClamp;
+  texDescr.readMode = hipReadModeNormalizedFloat;
+
+  HIPCHECK(
+      hipCreateTextureObject(&texObjLinear, &texRes, &texDescr, NULL));
+
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModePoint;
+  texDescr.addressMode[0] = hipAddressModeClamp;
+  texDescr.addressMode[1] = hipAddressModeClamp;
+  texDescr.readMode = hipReadModeNormalizedFloat;
+
+  HIPCHECK(
+      hipCreateTextureObject(&texObjPoint, &texRes, &texDescr, NULL));
+}
+
+extern "C" void freeTexture() {
+  HIPCHECK(hipDestroyTextureObject(texObjPoint));
+  HIPCHECK(hipDestroyTextureObject(texObjLinear));
+  HIPCHECK(hipFreeArray(d_imageArray));
+}
+
+// render image using CUDA
+extern "C" void render(int width, int height, float tx, float ty, float scale,
+                       float cx, float cy, dim3 blockSize, dim3 gridSize,
+                       int filter_mode, uchar4 *output) {
+  // call CUDA kernel, writing results to PBO memory
+  switch (filter_mode) {
+    case MODE_NEAREST:
+      d_render<<<gridSize, blockSize>>>(output, width, height, tx, ty, scale,
+                                        cx, cy, texObjPoint);
+      break;
+
+    case MODE_BILINEAR:
+      d_render<<<gridSize, blockSize>>>(output, width, height, tx, ty, scale,
+                                        cx, cy, texObjLinear);
+      break;
+
+    case MODE_BICUBIC:
+      d_renderBicubic<<<gridSize, blockSize>>>(output, width, height, tx, ty,
+                                               scale, cx, cy, texObjPoint);
+      break;
+
+    case MODE_FAST_BICUBIC:
+      d_renderFastBicubic<<<gridSize, blockSize>>>(
+          output, width, height, tx, ty, scale, cx, cy, texObjLinear);
+      break;
+
+    case MODE_CATROM:
+      d_renderCatRom<<<gridSize, blockSize>>>(output, width, height, tx, ty,
+                                              scale, cx, cy, texObjPoint);
+      break;
+  }
+
+  getLastCudaError("kernel failed");
+}
+
+#endif
+
+
+  getLastCudaError("kernel failed");
+}
+
+#endif
diff --git a/src/samples/Samples/5_Domain_Specific/bilateralFilter/bilateral_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/bilateralFilter/bilateral_kernel.cu.hip
index e69de29..431adc9 100755
--- a/src/samples/Samples/5_Domain_Specific/bilateralFilter/bilateral_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/bilateralFilter/bilateral_kernel.cu.hip
@@ -0,0 +1,268 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <helper_math.h>
+#include "helper_functions.h"
+#include <helper_cuda.h>  // CUDA device initialization helper functions
+
+__constant__ float cGaussian[64];  // gaussian array in device side
+
+hipTextureObject_t rgbaTexdImage;
+hipTextureObject_t rgbaTexdTemp;
+
+uint *dImage = NULL;  // original image
+uint *dTemp = NULL;  // temp array for iterations
+size_t pitch;
+
+/*
+    Perform a simple bilateral filter.
+
+    Bilateral filter is a nonlinear filter that is a mixture of range
+    filter and domain filter, the previous one preserves crisp edges and
+    the latter one filters noise. The intensity value at each pixel in
+    an image is replaced by a weighted average of intensity values from
+    nearby pixels.
+
+    The weight factor is calculated by the product of domain filter
+    component(using the gaussian distribution as a spatial distance) as
+    well as range filter component(Euclidean distance between center pixel
+    and the current neighbor pixel). Because this process is nonlinear,
+    the sample just uses a simple pixel by pixel step.
+
+    Texture fetches automatically clamp to edge of image. 1D gaussian array
+    is mapped to a 1D texture instead of using shared memory, which may
+    cause severe bank conflict.
+
+    Threads are y-pass(column-pass), because the output is coalesced.
+
+    Parameters
+    od - pointer to output data in global memory
+    d_f - pointer to the 1D gaussian array
+    e_d - euclidean delta
+    w  - image width
+    h  - image height
+    r  - filter radius
+*/
+
+// Euclidean Distance (x, y, d) = exp((|x - y| / d)^2 / 2)
+__device__ float euclideanLen(float4 a, float4 b, float d) {
+  float mod = (b.x - a.x) * (b.x - a.x) + (b.y - a.y) * (b.y - a.y) +
+              (b.z - a.z) * (b.z - a.z);
+
+  return __expf(-mod / (2.f * d * d));
+}
+
+__device__ uint rgbaFloatToInt(float4 rgba) {
+  rgba.x = __saturatef(fabs(rgba.x));  // clamp to [0.0, 1.0]
+  rgba.y = __saturatef(fabs(rgba.y));
+  rgba.z = __saturatef(fabs(rgba.z));
+  rgba.w = __saturatef(fabs(rgba.w));
+  return (uint(rgba.w * 255.0f) << 24) | (uint(rgba.z * 255.0f) << 16) |
+         (uint(rgba.y * 255.0f) << 8) | uint(rgba.x * 255.0f);
+}
+
+__device__ float4 rgbaIntToFloat(uint c) {
+  float4 rgba;
+  rgba.x = (c & 0xff) * 0.003921568627f;          //  /255.0f;
+  rgba.y = ((c >> 8) & 0xff) * 0.003921568627f;   //  /255.0f;
+  rgba.z = ((c >> 16) & 0xff) * 0.003921568627f;  //  /255.0f;
+  rgba.w = ((c >> 24) & 0xff) * 0.003921568627f;  //  /255.0f;
+  return rgba;
+}
+
+// column pass using coalesced global memory reads
+__global__ void d_bilateral_filter(uint *od, int w, int h, float e_d, int r,
+                                   hipTextureObject_t rgbaTex) {
+  int x = blockIdx.x * blockDim.x + threadIdx.x;
+  int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  if (x >= w || y >= h) {
+    return;
+  }
+
+  float sum = 0.0f;
+  float factor;
+  float4 t = {0.f, 0.f, 0.f, 0.f};
+  float4 center = tex2D<float4>(rgbaTex, x, y);
+
+  for (int i = -r; i <= r; i++) {
+    for (int j = -r; j <= r; j++) {
+      float4 curPix = tex2D<float4>(rgbaTex, x + j, y + i);
+      factor = cGaussian[i + r] * cGaussian[j + r] *  // domain factor
+               euclideanLen(curPix, center, e_d);  // range factor
+
+      t += factor * curPix;
+      sum += factor;
+    }
+  }
+
+  od[y * w + x] = rgbaFloatToInt(t / sum);
+}
+
+extern "C" void initTexture(int width, int height, uint *hImage) {
+  // copy image data to array
+  HIPCHECK(
+      hipMallocPitch(&dImage, &pitch, sizeof(uint) * width, height));
+  HIPCHECK(
+      hipMallocPitch(&dTemp, &pitch, sizeof(uint) * width, height));
+  HIPCHECK(hipMemcpy2D(dImage, pitch, hImage, sizeof(uint) * width,
+                               sizeof(uint) * width, height,
+                               hipMemcpyHostToDevice));
+
+  // texture<uchar4, 2, hipReadModeNormalizedFloat> rgbaTex;
+  hipChannelFormatDesc desc = hipCreateChannelDesc<uchar4>();
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypePitch2D;
+  texRes.res.pitch2D.devPtr = dImage;
+  texRes.res.pitch2D.desc = desc;
+  texRes.res.pitch2D.width = width;
+  texRes.res.pitch2D.height = height;
+  texRes.res.pitch2D.pitchInBytes = pitch;
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModePoint;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.addressMode[1] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeNormalizedFloat;
+
+  HIPCHECK(
+      hipCreateTextureObject(&rgbaTexdImage, &texRes, &texDescr, NULL));
+
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypePitch2D;
+  texRes.res.pitch2D.devPtr = dTemp;
+  texRes.res.pitch2D.desc = desc;
+  texRes.res.pitch2D.width = width;
+  texRes.res.pitch2D.height = height;
+  texRes.res.pitch2D.pitchInBytes = pitch;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.addressMode[1] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeNormalizedFloat;
+
+  HIPCHECK(
+      hipCreateTextureObject(&rgbaTexdTemp, &texRes, &texDescr, NULL));
+}
+
+extern "C" void freeTextures() {
+  HIPCHECK(hipDestroyTextureObject(rgbaTexdImage));
+  HIPCHECK(hipDestroyTextureObject(rgbaTexdTemp));
+  HIPCHECK(hipFree(dImage));
+  HIPCHECK(hipFree(dTemp));
+}
+
+/*
+    Because a 2D gaussian mask is symmetry in row and column,
+    here only generate a 1D mask, and use the product by row
+    and column index later.
+
+    1D gaussian distribution :
+        g(x, d) -- C * exp(-x^2/d^2), C is a constant amplifier
+
+    parameters:
+    og - output gaussian array in global memory
+    delta - the 2nd parameter 'd' in the above function
+    radius - half of the filter size
+             (total filter size = 2 * radius + 1)
+*/
+extern "C" void updateGaussian(float delta, int radius) {
+  float fGaussian[64];
+
+  for (int i = 0; i < 2 * radius + 1; ++i) {
+    float x = (float)(i - radius);
+    fGaussian[i] = expf(-(x * x) / (2 * delta * delta));
+  }
+
+  HIPCHECK(hipMemcpyToSymbol(HIP_SYMBOL(cGaussian), fGaussian,
+                                     sizeof(float) * (2 * radius + 1)));
+}
+
+/*
+    Perform 2D bilateral filter on image using CUDA
+
+    Parameters:
+    d_dest - pointer to destination image in device memory
+    width  - image width
+    height - image height
+    e_d    - euclidean delta
+    radius - filter radius
+    iterations - number of iterations
+*/
+
+// RGBA version
+extern "C" double bilateralFilterRGBA(uint *dDest, int width, int height,
+                                      float e_d, int radius, int iterations,
+                                      StopWatchInterface *timer) {
+  // var for kernel computation timing
+  double dKernelTime;
+
+  for (int i = 0; i < iterations; i++) {
+    // sync host and start kernel computation timer
+    dKernelTime = 0.0;
+    HIPCHECK(hipDeviceSynchronize());
+    sdkResetTimer(&timer);
+
+    dim3 gridSize((width + 16 - 1) / 16, (height + 16 - 1) / 16);
+    dim3 blockSize(16, 16);
+
+    if (iterations > 1) {
+      d_bilateral_filter<<<gridSize, blockSize>>>(dDest, width, height, e_d,
+                                                  radius, rgbaTexdTemp);
+    } else {
+      d_bilateral_filter<<<gridSize, blockSize>>>(dDest, width, height, e_d,
+                                                  radius, rgbaTexdImage);
+    }
+
+    // sync host and stop computation timer
+    HIPCHECK(hipDeviceSynchronize());
+    dKernelTime += sdkGetTimerValue(&timer);
+
+    if (iterations > 1) {
+      // copy result back from global memory to array
+      HIPCHECK(hipMemcpy2D(dTemp, pitch, dDest, sizeof(int) * width,
+                                   sizeof(int) * width, height,
+                                   hipMemcpyDeviceToDevice));
+    }
+  }
+
+  return ((dKernelTime / 1000.) / (double)iterations);
+}
+emcpyDeviceToDevice));
+    }
+  }
+
+  return ((dKernelTime / 1000.) / (double)iterations);
+}
diff --git a/src/samples/Samples/5_Domain_Specific/binomialOptions_nvrtc/binomialOptions_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/binomialOptions_nvrtc/binomialOptions_kernel.cu.hip
index e69de29..24fd6b5 100755
--- a/src/samples/Samples/5_Domain_Specific/binomialOptions_nvrtc/binomialOptions_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/binomialOptions_nvrtc/binomialOptions_kernel.cu.hip
@@ -0,0 +1,108 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "common_gpu_header.h"
+#include "binomialOptions_common.h"
+#include "realtype.h"
+
+// Preprocessed input option data
+typedef struct {
+  real S;
+  real X;
+  real vDt;
+  real puByDf;
+  real pdByDf;
+} __TOptionData;
+static __constant__ __TOptionData d_OptionData[MAX_OPTIONS];
+__device__ real d_CallValue[MAX_OPTIONS];
+
+#define THREADBLOCK_SIZE 128
+#define ELEMS_PER_THREAD (NUM_STEPS / THREADBLOCK_SIZE)
+#if NUM_STEPS % THREADBLOCK_SIZE
+#error Bad constants
+#endif
+
+////////////////////////////////////////////////////////////////////////////////
+// Overloaded shortcut functions for different precision modes
+////////////////////////////////////////////////////////////////////////////////
+
+#ifndef DOUBLE_PRECISION
+__device__ inline float expiryCallValue(float S, float X, float vDt, int i) {
+  float d = S * __expf(vDt * (2.0f * i - NUM_STEPS)) - X;
+  return (d > 0.0F) ? d : 0.0F;
+}
+
+#else
+__device__ inline double expiryCallValue(double S, double X, double vDt,
+                                         int i) {
+  double d = S * exp(vDt * (2.0 * i - NUM_STEPS)) - X;
+  return (d > 0.0) ? d : 0.0;
+}
+#endif
+
+////////////////////////////////////////////////////////////////////////////////
+// GPU kernel
+////////////////////////////////////////////////////////////////////////////////
+extern "C" __global__ void binomialOptionsKernel() {
+  __shared__ real call_exchange[THREADBLOCK_SIZE + 1];
+
+  const int tid = threadIdx.x;
+  const real S = d_OptionData[blockIdx.x].S;
+  const real X = d_OptionData[blockIdx.x].X;
+  const real vDt = d_OptionData[blockIdx.x].vDt;
+  const real puByDf = d_OptionData[blockIdx.x].puByDf;
+  const real pdByDf = d_OptionData[blockIdx.x].pdByDf;
+
+  real call[ELEMS_PER_THREAD + 1];
+#pragma unroll
+  for (int i = 0; i < ELEMS_PER_THREAD; ++i)
+    call[i] = expiryCallValue(S, X, vDt, tid * ELEMS_PER_THREAD + i);
+
+  if (tid == 0)
+    call_exchange[THREADBLOCK_SIZE] = expiryCallValue(S, X, vDt, NUM_STEPS);
+
+  int final_it = max(0, tid * ELEMS_PER_THREAD - 1);
+
+#pragma unroll 16
+  for (int i = NUM_STEPS; i > 0; --i) {
+    call_exchange[tid] = call[0];
+    __syncthreads();
+    call[ELEMS_PER_THREAD] = call_exchange[tid + 1];
+    __syncthreads();
+
+    if (i > final_it) {
+#pragma unroll
+      for (int j = 0; j < ELEMS_PER_THREAD; ++j)
+        call[j] = puByDf * call[j + 1] + pdByDf * call[j];
+    }
+  }
+
+  if (tid == 0) {
+    d_CallValue[blockIdx.x] = call[0];
+  }
+}
diff --git a/src/samples/Samples/5_Domain_Specific/dxtc/dxtc.cu.hip b/src/samples/Samples/5_Domain_Specific/dxtc/dxtc.cu.hip
index 7bc32a0..04d7bcf 100755
--- a/src/samples/Samples/5_Domain_Specific/dxtc/dxtc.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/dxtc/dxtc.cu.hip
@@ -37,9 +37,9 @@ namespace cg = cooperative_groups;
 #include <helper_math.h>
 #include <float.h>  // for FLT_MAX
 
-#include "CudaMath_hipified.h"
-#include "dds_hipified.h"
-#include "permutations_hipified.h"
+#include "CudaMath.h"
+#include "dds.h"
+#include "permutations.h"
 
 // Definitions
 #define INPUT_IMAGE "teapot512_std.ppm"
@@ -611,12 +611,12 @@ int main(int argc, char **argv) {
 
   // copy into global mem
   uint *d_data = NULL;
-  checkCudaErrors(hipMalloc((void **)&d_data, memSize));
+  HIPCHECK(hipMalloc((void **)&d_data, memSize));
 
   // Result
   uint *d_result = NULL;
   const uint compressedSize = (w / 4) * (h / 4) * 8;
-  checkCudaErrors(hipMalloc((void **)&d_result, compressedSize));
+  HIPCHECK(hipMalloc((void **)&d_result, compressedSize));
   uint *h_result = (uint *)malloc(compressedSize);
 
   // Compute permutations.
@@ -625,8 +625,8 @@ int main(int argc, char **argv) {
 
   // Copy permutations host to devie.
   uint *d_permutations = NULL;
-  checkCudaErrors(hipMalloc((void **)&d_permutations, 1024 * sizeof(uint)));
-  checkCudaErrors(hipMemcpy(d_permutations, permutations, 1024 * sizeof(uint),
+  HIPCHECK(hipMalloc((void **)&d_permutations, 1024 * sizeof(uint)));
+  HIPCHECK(hipMemcpy(d_permutations, permutations, 1024 * sizeof(uint),
                              hipMemcpyHostToDevice));
 
   // create a timer
@@ -634,7 +634,7 @@ int main(int argc, char **argv) {
   sdkCreateTimer(&timer);
 
   // Copy image from host to device
-  checkCudaErrors(
+  HIPCHECK(
       hipMemcpy(d_data, block_image, memSize, hipMemcpyHostToDevice));
 
   // Determine launch configuration and run timed computation numIterations
@@ -646,8 +646,8 @@ int main(int argc, char **argv) {
   hipDeviceProp_t deviceProp;
 
   // get number of SMs on this GPU
-  checkCudaErrors(hipGetDevice(&devID));
-  checkCudaErrors(hipGetDeviceProperties(&deviceProp, devID));
+  HIPCHECK(hipGetDevice(&devID));
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, devID));
 
   // Restrict the numbers of blocks to launch on low end GPUs to avoid kernel
   // timeout
@@ -660,7 +660,7 @@ int main(int argc, char **argv) {
 
   for (int i = -1; i < numIterations; ++i) {
     if (i == 0) {
-      checkCudaErrors(hipDeviceSynchronize());
+      HIPCHECK(hipDeviceSynchronize());
       sdkStartTimer(&timer);
     }
 
@@ -673,7 +673,7 @@ int main(int argc, char **argv) {
   getLastCudaError("compress");
 
   // sync to host, stop timer, record perf
-  checkCudaErrors(hipDeviceSynchronize());
+  HIPCHECK(hipDeviceSynchronize());
   sdkStopTimer(&timer);
   double dAvgTime = 1.0e-3 * sdkGetTimerValue(&timer) / (double)numIterations;
   printf(
@@ -682,7 +682,7 @@ int main(int argc, char **argv) {
       (1.0e-6 * (double)(W * H) / dAvgTime), dAvgTime, (W * H), 1, NUM_THREADS);
 
   // copy result data from device to host
-  checkCudaErrors(
+  HIPCHECK(
       hipMemcpy(h_result, d_result, compressedSize, hipMemcpyDeviceToHost));
 
   // Write out result data to DDS file
@@ -770,9 +770,9 @@ int main(int argc, char **argv) {
   rms /= w * h * 3;
 
   // Free allocated resources and exit
-  checkCudaErrors(hipFree(d_permutations));
-  checkCudaErrors(hipFree(d_data));
-  checkCudaErrors(hipFree(d_result));
+  HIPCHECK(hipFree(d_permutations));
+  HIPCHECK(hipFree(d_data));
+  HIPCHECK(hipFree(d_result));
   free(image_path);
   free(data);
   free(block_image);
@@ -785,3 +785,7 @@ int main(int argc, char **argv) {
   /* Return zero if test passed, one otherwise */
   return rms > ERROR_THRESHOLD;
 }
+!\n");
+  /* Return zero if test passed, one otherwise */
+  return rms > ERROR_THRESHOLD;
+}
diff --git a/src/samples/Samples/5_Domain_Specific/fluidsD3D9/fluidsD3D9_kernels.cu.hip b/src/samples/Samples/5_Domain_Specific/fluidsD3D9/fluidsD3D9_kernels.cu.hip
index e69de29..f8b6a8d 100755
--- a/src/samples/Samples/5_Domain_Specific/fluidsD3D9/fluidsD3D9_kernels.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/fluidsD3D9/fluidsD3D9_kernels.cu.hip
@@ -0,0 +1,336 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <hip/hip_runtime.h>
+#include <builtin_types.h>
+#include <hipfft.h>
+#include <hip/hip_runtime.h>
+#include "helper_cuda_hipified.h"
+#include "fluidsD3D9_kernels.h"
+
+// Texture object for reading velocity field
+hipTextureObject_t texObj;
+static hipArray *array = NULL;
+
+void setupTexture(int x, int y) {
+  hipChannelFormatDesc desc = hipCreateChannelDesc<float2>();
+
+  hipMallocArray(&array, &desc, y, x);
+  getLastCudaError("hipMalloc failed");
+
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = array;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(hipCreateTextureObject(&texObj, &texRes, &texDescr, NULL));
+}
+
+void updateTexture(cData *data, size_t wib, size_t h, size_t pitch) {
+  HIPCHECK(hipMemcpy2DToArray(array, 0, 0, data, pitch, wib, h,
+                                      hipMemcpyDeviceToDevice));
+}
+
+void deleteTexture(void) {
+  HIPCHECK(hipDestroyTextureObject(texObj));
+  HIPCHECK(hipFreeArray(array));
+}
+
+// Note that these kernels are designed to work with arbitrary
+// domain sizes, not just domains that are multiples of the tile
+// size. Therefore, we have extra code that checks to make sure
+// a given thread location falls within the domain boundaries in
+// both X and Y. Also, the domain is covered by looping over
+// multiple elements in the Y direction, while there is a one-to-one
+// mapping between threads in X and the tile size in X.
+// Nolan Goodnight 9/22/06
+
+// This method adds constant force vectors to the velocity field
+// stored in 'v' according to v(x,t+1) = v(x,t) + dt * f.
+__global__ void addForces_k(cData *v, int dx, int dy, int spx, int spy,
+                            float fx, float fy, int r, size_t pitch) {
+  int tx = threadIdx.x;
+  int ty = threadIdx.y;
+  cData *fj = (cData *)((char *)v + (ty + spy) * pitch) + tx + spx;
+
+  cData vterm = *fj;
+  tx -= r;
+  ty -= r;
+  float s = 1.f / (1.f + tx * tx * tx * tx + ty * ty * ty * ty);
+  vterm.x += s * fx;
+  vterm.y += s * fy;
+  *fj = vterm;
+}
+
+// This method performs the velocity advection step, where we
+// trace velocity vectors back in time to update each grid cell.
+// That is, v(x,t+1) = v(p(x,-dt),t). Here we perform bilinear
+// interpolation in the velocity space.
+__global__ void advectVelocity_k(cData *v, float *vx, float *vy, int dx,
+                                 int pdx, int dy, float dt, int lb,
+                                 hipTextureObject_t texObject) {
+  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
+  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
+  int p;
+
+  cData vterm, ploc;
+  float vxterm, vyterm;
+
+  // gtidx is the domain location in x for this thread
+  if (gtidx < dx) {
+    for (p = 0; p < lb; p++) {
+      // fi is the domain location in y for this thread
+      int fi = gtidy + p;
+
+      if (fi < dy) {
+        int fj = fi * pdx + gtidx;
+        vterm = tex2D<cData>(texObject, (float)gtidx, (float)fi);
+        ploc.x = (gtidx + 0.5f) - (dt * vterm.x * dx);
+        ploc.y = (fi + 0.5f) - (dt * vterm.y * dy);
+        vterm = tex2D<cData>(texObject, ploc.x, ploc.y);
+        vxterm = vterm.x;
+        vyterm = vterm.y;
+        vx[fj] = vxterm;
+        vy[fj] = vyterm;
+      }
+    }
+  }
+}
+
+// This method performs velocity diffusion and forces mass conservation
+// in the frequency domain. The inputs 'vx' and 'vy' are complex-valued
+// arrays holding the Fourier coefficients of the velocity field in
+// X and Y. Diffusion in this space takes a simple form described as:
+// v(k,t) = v(k,t) / (1 + visc * dt * k^2), where visc is the viscosity,
+// and k is the wavenumber. The projection step forces the Fourier
+// velocity vectors to be orthogonal to the vectors for each
+// wavenumber: v(k,t) = v(k,t) - ((k dot v(k,t) * k) / k^2.
+__global__ void diffuseProject_k(cData *vx, cData *vy, int dx, int dy, float dt,
+                                 float visc, int lb) {
+  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
+  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
+  int p;
+
+  cData xterm, yterm;
+
+  // gtidx is the domain location in x for this thread
+  if (gtidx < dx) {
+    for (p = 0; p < lb; p++) {
+      // fi is the domain location in y for this thread
+      int fi = gtidy + p;
+
+      if (fi < dy) {
+        int fj = fi * dx + gtidx;
+        xterm = vx[fj];
+        yterm = vy[fj];
+
+        // Compute the index of the wavenumber based on the
+        // data order produced by a standard NN FFT.
+        int iix = gtidx;
+        int iiy = (fi > dy / 2) ? (fi - (dy)) : fi;
+
+        // Velocity diffusion
+        float kk = (float)(iix * iix + iiy * iiy);  // k^2
+        float diff = 1.f / (1.f + visc * dt * kk);
+        xterm.x *= diff;
+        xterm.y *= diff;
+        yterm.x *= diff;
+        yterm.y *= diff;
+
+        // Velocity projection
+        if (kk > 0.f) {
+          float rkk = 1.f / kk;
+          // Real portion of velocity projection
+          float rkp = (iix * xterm.x + iiy * yterm.x);
+          // Imaginary portion of velocity projection
+          float ikp = (iix * xterm.y + iiy * yterm.y);
+          xterm.x -= rkk * rkp * iix;
+          xterm.y -= rkk * ikp * iix;
+          yterm.x -= rkk * rkp * iiy;
+          yterm.y -= rkk * ikp * iiy;
+        }
+
+        vx[fj] = xterm;
+        vy[fj] = yterm;
+      }
+    }
+  }
+}
+
+// This method updates the velocity field 'v' using the two complex
+// arrays from the previous step: 'vx' and 'vy'. Here we scale the
+// real components by 1/(dx*dy) to account for an unnormalized FFT.
+__global__ void updateVelocity_k(cData *v, float *vx, float *vy, int dx,
+                                 int pdx, int dy, int lb, size_t pitch) {
+  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
+  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
+  int p;
+
+  float vxterm, vyterm;
+  cData nvterm;
+
+  // gtidx is the domain location in x for this thread
+  if (gtidx < dx) {
+    for (p = 0; p < lb; p++) {
+      // fi is the domain location in y for this thread
+      int fi = gtidy + p;
+
+      if (fi < dy) {
+        int fjr = fi * pdx + gtidx;
+        vxterm = vx[fjr];
+        vyterm = vy[fjr];
+
+        // Normalize the result of the inverse FFT
+        float scale = 1.f / (dx * dy);
+        nvterm.x = vxterm * scale;
+        nvterm.y = vyterm * scale;
+
+        cData *fj = (cData *)((char *)v + fi * pitch) + gtidx;
+        *fj = nvterm;
+      }
+    }  // If this thread is inside the domain in Y
+  }    // If this thread is inside the domain in X
+}
+
+// This method updates the particles by moving particle positions
+// according to the velocity field and time step. That is, for each
+// particle: p(t+1) = p(t) + dt * v(p(t)).
+__global__ void advectParticles_k(Vertex *part, cData *v, int dx, int dy,
+                                  float dt, int lb, size_t pitch) {
+  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
+  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
+  int p;
+
+  // gtidx is the domain location in x for this thread
+  cData vterm;
+  Vertex pterm;
+
+  if (gtidx < dx) {
+    for (p = 0; p < lb; p++) {
+      // fi is the domain location in y for this thread
+      int fi = gtidy + p;
+
+      if (fi < dy) {
+        int fj = fi * dx + gtidx;
+        pterm = part[fj];
+
+        int xvi = ((int)(pterm.x * dx));
+        int yvi = ((int)(pterm.y * dy));
+        vterm = *((cData *)((char *)v + yvi * pitch) + xvi);
+
+        pterm.x += dt * vterm.x;
+        pterm.x = pterm.x - (int)pterm.x;
+        pterm.x += 1.f;
+        pterm.x = pterm.x - (int)pterm.x;
+        pterm.y += dt * vterm.y;
+        pterm.y = pterm.y - (int)pterm.y;
+        pterm.y += 1.f;
+        pterm.y = pterm.y - (int)pterm.y;
+
+        part[fj] = pterm;
+      }
+    }  // If this thread is inside the domain in Y
+  }    // If this thread is inside the domain in X
+}
+
+extern "C" void addForces(cData *v, int dx, int dy, int spx, int spy, float fx,
+                          float fy, int r, size_t tPitch) {
+  dim3 tids(2 * r + 1, 2 * r + 1);
+
+  addForces_k<<<1, tids>>>(v, dx, dy, spx, spy, fx, fy, r, tPitch);
+  getLastCudaError("addForces_k failed.");
+}
+
+extern "C" void advectVelocity(cData *v, float *vx, float *vy, int dx, int pdx,
+                               int dy, float dt, size_t tPitch) {
+  dim3 grid((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
+            (dy / TILEY) + (!(dy % TILEY) ? 0 : 1));
+
+  dim3 tids(TIDSX, TIDSY);
+
+  updateTexture(v, DIM * sizeof(cData), DIM, tPitch);
+  advectVelocity_k<<<grid, tids>>>(v, vx, vy, dx, pdx, dy, dt, TILEY / TIDSY,
+                                   texObj);
+
+  getLastCudaError("advectVelocity_k failed.");
+}
+
+extern "C" void diffuseProject(cData *vx, cData *vy, int dx, int dy, float dt,
+                               float visc, size_t tPitch) {
+  // Forward FFT
+  //    hipfftExecR2C(planr2c, (hipfftReal*)vx, (hipfftComplex*)vx);
+  //    hipfftExecR2C(planr2c, (hipfftReal*)vy, (hipfftComplex*)vy);
+
+  uint3 grid = make_uint3((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
+                          (dy / TILEY) + (!(dy % TILEY) ? 0 : 1), 1);
+
+  uint3 tids = make_uint3(TIDSX, TIDSY, 1);
+
+  diffuseProject_k<<<grid, tids>>>(vx, vy, dx, dy, dt, visc, TILEY / TIDSY);
+  getLastCudaError("diffuseProject_k failed.");
+
+  // Inverse FFT
+  //    hipfftExecC2R(planc2r, (hipfftComplex*)vx, (hipfftReal*)vx);
+  //    hipfftExecC2R(planc2r, (hipfftComplex*)vy, (hipfftReal*)vy);
+}
+
+extern "C" void updateVelocity(cData *v, float *vx, float *vy, int dx, int pdx,
+                               int dy, size_t tPitch) {
+  dim3 grid((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
+            (dy / TILEY) + (!(dy % TILEY) ? 0 : 1));
+
+  dim3 tids(TIDSX, TIDSY);
+
+  updateVelocity_k<<<grid, tids>>>(v, vx, vy, dx, pdx, dy, TILEY / TIDSY,
+                                   tPitch);
+  getLastCudaError("updateVelocity_k failed.");
+}
+
+extern "C" void advectParticles(Vertex *p, cData *v, int dx, int dy, float dt,
+                                size_t tPitch) {
+  dim3 grid((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
+            (dy / TILEY) + (!(dy % TILEY) ? 0 : 1));
+
+  dim3 tids(TIDSX, TIDSY);
+
+  advectParticles_k<<<grid, tids>>>(p, v, dx, dy, dt, TILEY / TIDSY, tPitch);
+  getLastCudaError("advectParticles_k failed.");
+}
+ectParticles_k failed.");
+}
diff --git a/src/samples/Samples/5_Domain_Specific/fluidsGL/fluidsGL_kernels.cu.hip b/src/samples/Samples/5_Domain_Specific/fluidsGL/fluidsGL_kernels.cu.hip
index e69de29..d1f4bd8 100755
--- a/src/samples/Samples/5_Domain_Specific/fluidsGL/fluidsGL_kernels.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/fluidsGL/fluidsGL_kernels.cu.hip
@@ -0,0 +1,363 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+
+#include <hip/hip_runtime.h>
+#include <hipfft.h>        // CUDA FFT Libraries
+#include <helper_cuda.h>  // Helper functions for CUDA Error handling
+
+// OpenGL Graphics includes
+#define HELPERGL_EXTERN_GL_FUNC_IMPLEMENTATION
+#include <helper_gl.h>
+
+// FluidsGL CUDA kernel definitions
+#include "fluidsGL_kernels.cuh"
+
+// Texture object for reading velocity field
+hipTextureObject_t texObj;
+static hipArray *array = NULL;
+
+// Particle data
+extern GLuint vbo;  // OpenGL vertex buffer object
+extern struct hipGraphicsResource
+    *cuda_vbo_resource;  // handles OpenGL-CUDA exchange
+
+// Texture pitch
+extern size_t tPitch;
+extern hipfftHandle planr2c;
+extern hipfftHandle planc2r;
+cData *vxfield = NULL;
+cData *vyfield = NULL;
+
+void setupTexture(int x, int y) {
+  hipChannelFormatDesc desc = hipCreateChannelDesc<float2>();
+
+  hipMallocArray(&array, &desc, y, x);
+  getLastCudaError("hipMalloc failed");
+
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = array;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(hipCreateTextureObject(&texObj, &texRes, &texDescr, NULL));
+}
+
+void updateTexture(cData *data, size_t wib, size_t h, size_t pitch) {
+  HIPCHECK(hipMemcpy2DToArray(array, 0, 0, data, pitch, wib, h,
+                                      hipMemcpyDeviceToDevice));
+}
+
+void deleteTexture(void) {
+  HIPCHECK(hipDestroyTextureObject(texObj));
+  HIPCHECK(hipFreeArray(array));
+}
+
+// Note that these kernels are designed to work with arbitrary
+// domain sizes, not just domains that are multiples of the tile
+// size. Therefore, we have extra code that checks to make sure
+// a given thread location falls within the domain boundaries in
+// both X and Y. Also, the domain is covered by looping over
+// multiple elements in the Y direction, while there is a one-to-one
+// mapping between threads in X and the tile size in X.
+// Nolan Goodnight 9/22/06
+
+// This method adds constant force vectors to the velocity field
+// stored in 'v' according to v(x,t+1) = v(x,t) + dt * f.
+__global__ void addForces_k(cData *v, int dx, int dy, int spx, int spy,
+                            float fx, float fy, int r, size_t pitch) {
+  int tx = threadIdx.x;
+  int ty = threadIdx.y;
+  cData *fj = (cData *)((char *)v + (ty + spy) * pitch) + tx + spx;
+
+  cData vterm = *fj;
+  tx -= r;
+  ty -= r;
+  float s = 1.f / (1.f + tx * tx * tx * tx + ty * ty * ty * ty);
+  vterm.x += s * fx;
+  vterm.y += s * fy;
+  *fj = vterm;
+}
+
+// This method performs the velocity advection step, where we
+// trace velocity vectors back in time to update each grid cell.
+// That is, v(x,t+1) = v(p(x,-dt),t). Here we perform bilinear
+// interpolation in the velocity space.
+__global__ void advectVelocity_k(cData *v, float *vx, float *vy, int dx,
+                                 int pdx, int dy, float dt, int lb,
+                                 hipTextureObject_t texObject) {
+  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
+  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
+  int p;
+
+  cData vterm, ploc;
+  float vxterm, vyterm;
+
+  // gtidx is the domain location in x for this thread
+  if (gtidx < dx) {
+    for (p = 0; p < lb; p++) {
+      // fi is the domain location in y for this thread
+      int fi = gtidy + p;
+
+      if (fi < dy) {
+        int fj = fi * pdx + gtidx;
+        vterm = tex2D<cData>(texObject, (float)gtidx, (float)fi);
+        ploc.x = (gtidx + 0.5f) - (dt * vterm.x * dx);
+        ploc.y = (fi + 0.5f) - (dt * vterm.y * dy);
+        vterm = tex2D<cData>(texObject, ploc.x, ploc.y);
+        vxterm = vterm.x;
+        vyterm = vterm.y;
+        vx[fj] = vxterm;
+        vy[fj] = vyterm;
+      }
+    }
+  }
+}
+
+// This method performs velocity diffusion and forces mass conservation
+// in the frequency domain. The inputs 'vx' and 'vy' are complex-valued
+// arrays holding the Fourier coefficients of the velocity field in
+// X and Y. Diffusion in this space takes a simple form described as:
+// v(k,t) = v(k,t) / (1 + visc * dt * k^2), where visc is the viscosity,
+// and k is the wavenumber. The projection step forces the Fourier
+// velocity vectors to be orthogonal to the vectors for each
+// wavenumber: v(k,t) = v(k,t) - ((k dot v(k,t) * k) / k^2.
+__global__ void diffuseProject_k(cData *vx, cData *vy, int dx, int dy, float dt,
+                                 float visc, int lb) {
+  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
+  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
+  int p;
+
+  cData xterm, yterm;
+
+  // gtidx is the domain location in x for this thread
+  if (gtidx < dx) {
+    for (p = 0; p < lb; p++) {
+      // fi is the domain location in y for this thread
+      int fi = gtidy + p;
+
+      if (fi < dy) {
+        int fj = fi * dx + gtidx;
+        xterm = vx[fj];
+        yterm = vy[fj];
+
+        // Compute the index of the wavenumber based on the
+        // data order produced by a standard NN FFT.
+        int iix = gtidx;
+        int iiy = (fi > dy / 2) ? (fi - (dy)) : fi;
+
+        // Velocity diffusion
+        float kk = (float)(iix * iix + iiy * iiy);  // k^2
+        float diff = 1.f / (1.f + visc * dt * kk);
+        xterm.x *= diff;
+        xterm.y *= diff;
+        yterm.x *= diff;
+        yterm.y *= diff;
+
+        // Velocity projection
+        if (kk > 0.f) {
+          float rkk = 1.f / kk;
+          // Real portion of velocity projection
+          float rkp = (iix * xterm.x + iiy * yterm.x);
+          // Imaginary portion of velocity projection
+          float ikp = (iix * xterm.y + iiy * yterm.y);
+          xterm.x -= rkk * rkp * iix;
+          xterm.y -= rkk * ikp * iix;
+          yterm.x -= rkk * rkp * iiy;
+          yterm.y -= rkk * ikp * iiy;
+        }
+
+        vx[fj] = xterm;
+        vy[fj] = yterm;
+      }
+    }
+  }
+}
+
+// This method updates the velocity field 'v' using the two complex
+// arrays from the previous step: 'vx' and 'vy'. Here we scale the
+// real components by 1/(dx*dy) to account for an unnormalized FFT.
+__global__ void updateVelocity_k(cData *v, float *vx, float *vy, int dx,
+                                 int pdx, int dy, int lb, size_t pitch) {
+  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
+  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
+  int p;
+
+  float vxterm, vyterm;
+  cData nvterm;
+
+  // gtidx is the domain location in x for this thread
+  if (gtidx < dx) {
+    for (p = 0; p < lb; p++) {
+      // fi is the domain location in y for this thread
+      int fi = gtidy + p;
+
+      if (fi < dy) {
+        int fjr = fi * pdx + gtidx;
+        vxterm = vx[fjr];
+        vyterm = vy[fjr];
+
+        // Normalize the result of the inverse FFT
+        float scale = 1.f / (dx * dy);
+        nvterm.x = vxterm * scale;
+        nvterm.y = vyterm * scale;
+
+        cData *fj = (cData *)((char *)v + fi * pitch) + gtidx;
+        *fj = nvterm;
+      }
+    }  // If this thread is inside the domain in Y
+  }    // If this thread is inside the domain in X
+}
+
+// This method updates the particles by moving particle positions
+// according to the velocity field and time step. That is, for each
+// particle: p(t+1) = p(t) + dt * v(p(t)).
+__global__ void advectParticles_k(cData *part, cData *v, int dx, int dy,
+                                  float dt, int lb, size_t pitch) {
+  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
+  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
+  int p;
+
+  // gtidx is the domain location in x for this thread
+  cData pterm, vterm;
+
+  if (gtidx < dx) {
+    for (p = 0; p < lb; p++) {
+      // fi is the domain location in y for this thread
+      int fi = gtidy + p;
+
+      if (fi < dy) {
+        int fj = fi * dx + gtidx;
+        pterm = part[fj];
+
+        int xvi = ((int)(pterm.x * dx));
+        int yvi = ((int)(pterm.y * dy));
+        vterm = *((cData *)((char *)v + yvi * pitch) + xvi);
+
+        pterm.x += dt * vterm.x;
+        pterm.x = pterm.x - (int)pterm.x;
+        pterm.x += 1.f;
+        pterm.x = pterm.x - (int)pterm.x;
+        pterm.y += dt * vterm.y;
+        pterm.y = pterm.y - (int)pterm.y;
+        pterm.y += 1.f;
+        pterm.y = pterm.y - (int)pterm.y;
+
+        part[fj] = pterm;
+      }
+    }  // If this thread is inside the domain in Y
+  }    // If this thread is inside the domain in X
+}
+
+// These are the external function calls necessary for launching fluid
+// simulation
+extern "C" void addForces(cData *v, int dx, int dy, int spx, int spy, float fx,
+                          float fy, int r) {
+  dim3 tids(2 * r + 1, 2 * r + 1);
+
+  addForces_k<<<1, tids>>>(v, dx, dy, spx, spy, fx, fy, r, tPitch);
+  getLastCudaError("addForces_k failed.");
+}
+
+extern "C" void advectVelocity(cData *v, float *vx, float *vy, int dx, int pdx,
+                               int dy, float dt) {
+  dim3 grid((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
+            (dy / TILEY) + (!(dy % TILEY) ? 0 : 1));
+
+  dim3 tids(TIDSX, TIDSY);
+
+  updateTexture(v, DIM * sizeof(cData), DIM, tPitch);
+  advectVelocity_k<<<grid, tids>>>(v, vx, vy, dx, pdx, dy, dt, TILEY / TIDSY,
+                                   texObj);
+
+  getLastCudaError("advectVelocity_k failed.");
+}
+
+extern "C" void diffuseProject(cData *vx, cData *vy, int dx, int dy, float dt,
+                               float visc) {
+  // Forward FFT
+  HIPCHECK(hipfftExecR2C(planr2c, (hipfftReal *)vx, (hipfftComplex *)vx));
+  HIPCHECK(hipfftExecR2C(planr2c, (hipfftReal *)vy, (hipfftComplex *)vy));
+
+  uint3 grid = make_uint3((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
+                          (dy / TILEY) + (!(dy % TILEY) ? 0 : 1), 1);
+  uint3 tids = make_uint3(TIDSX, TIDSY, 1);
+
+  diffuseProject_k<<<grid, tids>>>(vx, vy, dx, dy, dt, visc, TILEY / TIDSY);
+  getLastCudaError("diffuseProject_k failed.");
+
+  // Inverse FFT
+  HIPCHECK(hipfftExecC2R(planc2r, (hipfftComplex *)vx, (hipfftReal *)vx));
+  HIPCHECK(hipfftExecC2R(planc2r, (hipfftComplex *)vy, (hipfftReal *)vy));
+}
+
+extern "C" void updateVelocity(cData *v, float *vx, float *vy, int dx, int pdx,
+                               int dy) {
+  dim3 grid((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
+            (dy / TILEY) + (!(dy % TILEY) ? 0 : 1));
+  dim3 tids(TIDSX, TIDSY);
+
+  updateVelocity_k<<<grid, tids>>>(v, vx, vy, dx, pdx, dy, TILEY / TIDSY,
+                                   tPitch);
+  getLastCudaError("updateVelocity_k failed.");
+}
+
+extern "C" void advectParticles(GLuint vbo, cData *v, int dx, int dy,
+                                float dt) {
+  dim3 grid((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
+            (dy / TILEY) + (!(dy % TILEY) ? 0 : 1));
+  dim3 tids(TIDSX, TIDSY);
+
+  cData *p;
+  hipGraphicsMapResources(1, &cuda_vbo_resource, 0);
+  getLastCudaError("hipGraphicsMapResources failed");
+
+  size_t num_bytes;
+  hipGraphicsResourceGetMappedPointer((void **)&p, &num_bytes,
+                                       cuda_vbo_resource);
+  getLastCudaError("hipGraphicsResourceGetMappedPointer failed");
+
+  advectParticles_k<<<grid, tids>>>(p, v, dx, dy, dt, TILEY / TIDSY, tPitch);
+  getLastCudaError("advectParticles_k failed.");
+
+  hipGraphicsUnmapResources(1, &cuda_vbo_resource, 0);
+  getLastCudaError("hipGraphicsUnmapResources failed");
+}
+getLastCudaError("hipGraphicsUnmapResources failed");
+}
diff --git a/src/samples/Samples/5_Domain_Specific/fluidsGLES/fluidsGLES_kernels.cu.hip b/src/samples/Samples/5_Domain_Specific/fluidsGLES/fluidsGLES_kernels.cu.hip
index e69de29..91835e8 100755
--- a/src/samples/Samples/5_Domain_Specific/fluidsGLES/fluidsGLES_kernels.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/fluidsGLES/fluidsGLES_kernels.cu.hip
@@ -0,0 +1,361 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+
+#include <hip/hip_runtime.h>
+#include <hipfft.h>        // CUDA FFT Libraries
+#include <helper_cuda.h>  // Helper functions for CUDA Error handling
+
+// OpenGL Graphics includes
+#include <GLES3/gl31.h>
+
+// FluidsGLES CUDA kernel definitions
+#include "fluidsGLES_kernels.cuh"
+
+// Texture object for reading velocity field
+hipTextureObject_t texObj;
+static hipArray *array = NULL;
+
+// Particle data
+extern GLuint vbo;  // OpenGL vertex buffer object
+extern struct hipGraphicsResource
+    *cuda_vbo_resource;  // handles OpenGL-CUDA exchange
+
+// Texture pitch
+extern size_t tPitch;
+extern hipfftHandle planr2c;
+extern hipfftHandle planc2r;
+cData *vxfield = NULL;
+cData *vyfield = NULL;
+
+void setupTexture(int x, int y) {
+  hipChannelFormatDesc desc = hipCreateChannelDesc<float2>();
+
+  hipMallocArray(&array, &desc, y, x);
+  getLastCudaError("hipMalloc failed");
+
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = array;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(hipCreateTextureObject(&texObj, &texRes, &texDescr, NULL));
+}
+
+void updateTexture(cData *data, size_t wib, size_t h, size_t pitch) {
+  HIPCHECK(hipMemcpy2DToArray(array, 0, 0, data, pitch, wib, h,
+                                      hipMemcpyDeviceToDevice));
+}
+
+void deleteTexture(void) {
+  HIPCHECK(hipDestroyTextureObject(texObj));
+  HIPCHECK(hipFreeArray(array));
+}
+
+// Note that these kernels are designed to work with arbitrary
+// domain sizes, not just domains that are multiples of the tile
+// size. Therefore, we have extra code that checks to make sure
+// a given thread location falls within the domain boundaries in
+// both X and Y. Also, the domain is covered by looping over
+// multiple elements in the Y direction, while there is a one-to-one
+// mapping between threads in X and the tile size in X.
+// Nolan Goodnight 9/22/06
+
+// This method adds constant force vectors to the velocity field
+// stored in 'v' according to v(x,t+1) = v(x,t) + dt * f.
+__global__ void addForces_k(cData *v, int dx, int dy, int spx, int spy,
+                            float fx, float fy, int r, size_t pitch) {
+  int tx = threadIdx.x;
+  int ty = threadIdx.y;
+  cData *fj = (cData *)((char *)v + (ty + spy) * pitch) + tx + spx;
+
+  cData vterm = *fj;
+  tx -= r;
+  ty -= r;
+  float s = 1.f / (1.f + tx * tx * tx * tx + ty * ty * ty * ty);
+  vterm.x += s * fx;
+  vterm.y += s * fy;
+  *fj = vterm;
+}
+
+// This method performs the velocity advection step, where we
+// trace velocity vectors back in time to update each grid cell.
+// That is, v(x,t+1) = v(p(x,-dt),t). Here we perform bilinear
+// interpolation in the velocity space.
+__global__ void advectVelocity_k(cData *v, float *vx, float *vy, int dx,
+                                 int pdx, int dy, float dt, int lb,
+                                 hipTextureObject_t texObject) {
+  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
+  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
+  int p;
+
+  cData vterm, ploc;
+  float vxterm, vyterm;
+
+  // gtidx is the domain location in x for this thread
+  if (gtidx < dx) {
+    for (p = 0; p < lb; p++) {
+      // fi is the domain location in y for this thread
+      int fi = gtidy + p;
+
+      if (fi < dy) {
+        int fj = fi * pdx + gtidx;
+        vterm = tex2D<cData>(texObject, (float)gtidx, (float)fi);
+        ploc.x = (gtidx + 0.5f) - (dt * vterm.x * dx);
+        ploc.y = (fi + 0.5f) - (dt * vterm.y * dy);
+        vterm = tex2D<cData>(texObject, ploc.x, ploc.y);
+        vxterm = vterm.x;
+        vyterm = vterm.y;
+        vx[fj] = vxterm;
+        vy[fj] = vyterm;
+      }
+    }
+  }
+}
+
+// This method performs velocity diffusion and forces mass conservation
+// in the frequency domain. The inputs 'vx' and 'vy' are complex-valued
+// arrays holding the Fourier coefficients of the velocity field in
+// X and Y. Diffusion in this space takes a simple form described as:
+// v(k,t) = v(k,t) / (1 + visc * dt * k^2), where visc is the viscosity,
+// and k is the wavenumber. The projection step forces the Fourier
+// velocity vectors to be orthogonal to the vectors for each
+// wavenumber: v(k,t) = v(k,t) - ((k dot v(k,t) * k) / k^2.
+__global__ void diffuseProject_k(cData *vx, cData *vy, int dx, int dy, float dt,
+                                 float visc, int lb) {
+  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
+  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
+  int p;
+
+  cData xterm, yterm;
+
+  // gtidx is the domain location in x for this thread
+  if (gtidx < dx) {
+    for (p = 0; p < lb; p++) {
+      // fi is the domain location in y for this thread
+      int fi = gtidy + p;
+
+      if (fi < dy) {
+        int fj = fi * dx + gtidx;
+        xterm = vx[fj];
+        yterm = vy[fj];
+
+        // Compute the index of the wavenumber based on the
+        // data order produced by a standard NN FFT.
+        int iix = gtidx;
+        int iiy = (fi > dy / 2) ? (fi - (dy)) : fi;
+
+        // Velocity diffusion
+        float kk = (float)(iix * iix + iiy * iiy);  // k^2
+        float diff = 1.f / (1.f + visc * dt * kk);
+        xterm.x *= diff;
+        xterm.y *= diff;
+        yterm.x *= diff;
+        yterm.y *= diff;
+
+        // Velocity projection
+        if (kk > 0.f) {
+          float rkk = 1.f / kk;
+          // Real portion of velocity projection
+          float rkp = (iix * xterm.x + iiy * yterm.x);
+          // Imaginary portion of velocity projection
+          float ikp = (iix * xterm.y + iiy * yterm.y);
+          xterm.x -= rkk * rkp * iix;
+          xterm.y -= rkk * ikp * iix;
+          yterm.x -= rkk * rkp * iiy;
+          yterm.y -= rkk * ikp * iiy;
+        }
+
+        vx[fj] = xterm;
+        vy[fj] = yterm;
+      }
+    }
+  }
+}
+
+// This method updates the velocity field 'v' using the two complex
+// arrays from the previous step: 'vx' and 'vy'. Here we scale the
+// real components by 1/(dx*dy) to account for an unnormalized FFT.
+__global__ void updateVelocity_k(cData *v, float *vx, float *vy, int dx,
+                                 int pdx, int dy, int lb, size_t pitch) {
+  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
+  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
+  int p;
+
+  float vxterm, vyterm;
+  cData nvterm;
+
+  // gtidx is the domain location in x for this thread
+  if (gtidx < dx) {
+    for (p = 0; p < lb; p++) {
+      // fi is the domain location in y for this thread
+      int fi = gtidy + p;
+
+      if (fi < dy) {
+        int fjr = fi * pdx + gtidx;
+        vxterm = vx[fjr];
+        vyterm = vy[fjr];
+
+        // Normalize the result of the inverse FFT
+        float scale = 1.f / (dx * dy);
+        nvterm.x = vxterm * scale;
+        nvterm.y = vyterm * scale;
+
+        cData *fj = (cData *)((char *)v + fi * pitch) + gtidx;
+        *fj = nvterm;
+      }
+    }  // If this thread is inside the domain in Y
+  }    // If this thread is inside the domain in X
+}
+
+// This method updates the particles by moving particle positions
+// according to the velocity field and time step. That is, for each
+// particle: p(t+1) = p(t) + dt * v(p(t)).
+__global__ void advectParticles_k(cData *part, cData *v, int dx, int dy,
+                                  float dt, int lb, size_t pitch) {
+  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
+  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
+  int p;
+
+  // gtidx is the domain location in x for this thread
+  cData pterm, vterm;
+
+  if (gtidx < dx) {
+    for (p = 0; p < lb; p++) {
+      // fi is the domain location in y for this thread
+      int fi = gtidy + p;
+
+      if (fi < dy) {
+        int fj = fi * dx + gtidx;
+        pterm = part[fj];
+
+        int xvi = ((int)(pterm.x * dx));
+        int yvi = ((int)(pterm.y * dy));
+        vterm = *((cData *)((char *)v + yvi * pitch) + xvi);
+
+        pterm.x += dt * vterm.x;
+        pterm.x = pterm.x - (int)pterm.x;
+        pterm.x += 1.f;
+        pterm.x = pterm.x - (int)pterm.x;
+        pterm.y += dt * vterm.y;
+        pterm.y = pterm.y - (int)pterm.y;
+        pterm.y += 1.f;
+        pterm.y = pterm.y - (int)pterm.y;
+
+        part[fj] = pterm;
+      }
+    }  // If this thread is inside the domain in Y
+  }    // If this thread is inside the domain in X
+}
+
+// These are the external function calls necessary for launching fluid simuation
+extern "C" void addForces(cData *v, int dx, int dy, int spx, int spy, float fx,
+                          float fy, int r) {
+  dim3 tids(2 * r + 1, 2 * r + 1);
+
+  addForces_k<<<1, tids>>>(v, dx, dy, spx, spy, fx, fy, r, tPitch);
+  getLastCudaError("addForces_k failed.");
+}
+
+extern "C" void advectVelocity(cData *v, float *vx, float *vy, int dx, int pdx,
+                               int dy, float dt) {
+  dim3 grid((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
+            (dy / TILEY) + (!(dy % TILEY) ? 0 : 1));
+
+  dim3 tids(TIDSX, TIDSY);
+
+  updateTexture(v, DIM * sizeof(cData), DIM, tPitch);
+  advectVelocity_k<<<grid, tids>>>(v, vx, vy, dx, pdx, dy, dt, TILEY / TIDSY,
+                                   texObj);
+  getLastCudaError("advectVelocity_k failed.");
+}
+
+extern "C" void diffuseProject(cData *vx, cData *vy, int dx, int dy, float dt,
+                               float visc) {
+  // Forward FFT
+  HIPCHECK(hipfftExecR2C(planr2c, (hipfftReal *)vx, (hipfftComplex *)vx));
+  HIPCHECK(hipfftExecR2C(planr2c, (hipfftReal *)vy, (hipfftComplex *)vy));
+
+  uint3 grid = make_uint3((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
+                          (dy / TILEY) + (!(dy % TILEY) ? 0 : 1), 1);
+  uint3 tids = make_uint3(TIDSX, TIDSY, 1);
+
+  diffuseProject_k<<<grid, tids>>>(vx, vy, dx, dy, dt, visc, TILEY / TIDSY);
+  getLastCudaError("diffuseProject_k failed.");
+
+  // Inverse FFT
+  HIPCHECK(hipfftExecC2R(planc2r, (hipfftComplex *)vx, (hipfftReal *)vx));
+  HIPCHECK(hipfftExecC2R(planc2r, (hipfftComplex *)vy, (hipfftReal *)vy));
+}
+
+extern "C" void updateVelocity(cData *v, float *vx, float *vy, int dx, int pdx,
+                               int dy) {
+  dim3 grid((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
+            (dy / TILEY) + (!(dy % TILEY) ? 0 : 1));
+  dim3 tids(TIDSX, TIDSY);
+
+  updateVelocity_k<<<grid, tids>>>(v, vx, vy, dx, pdx, dy, TILEY / TIDSY,
+                                   tPitch);
+  getLastCudaError("updateVelocity_k failed.");
+}
+
+extern "C" void advectParticles(GLuint vbo, cData *v, int dx, int dy,
+                                float dt) {
+  dim3 grid((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
+            (dy / TILEY) + (!(dy % TILEY) ? 0 : 1));
+  dim3 tids(TIDSX, TIDSY);
+
+  cData *p;
+  HIPCHECK(hipGraphicsMapResources(1, &cuda_vbo_resource, 0));
+  getLastCudaError("hipGraphicsMapResources failed");
+
+  size_t num_bytes;
+  HIPCHECK(hipGraphicsResourceGetMappedPointer((void **)&p, &num_bytes,
+                                                       cuda_vbo_resource));
+  getLastCudaError("hipGraphicsResourceGetMappedPointer failed");
+
+  advectParticles_k<<<grid, tids>>>(p, v, dx, dy, dt, TILEY / TIDSY, tPitch);
+  getLastCudaError("advectParticles_k failed.");
+
+  HIPCHECK(hipGraphicsUnmapResources(1, &cuda_vbo_resource, 0));
+}
+;
+
+  checkCudaErrors(hipGraphicsUnmapResources(1, &cuda_vbo_resource, 0));
+}
diff --git a/src/samples/Samples/5_Domain_Specific/marchingCubes/marchingCubes_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/marchingCubes/marchingCubes_kernel.cu.hip
index e69de29..20cd5e0 100755
--- a/src/samples/Samples/5_Domain_Specific/marchingCubes/marchingCubes_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/marchingCubes/marchingCubes_kernel.cu.hip
@@ -0,0 +1,673 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _MARCHING_CUBES_KERNEL_CU_
+#define _MARCHING_CUBES_KERNEL_CU_
+
+#include <stdio.h>
+#include <string.h>
+#include <helper_cuda.h>  // includes for helper CUDA functions
+#include <helper_math.h>
+#include <hip/hip_runtime_api.h>
+#include <thrust/device_vector.h>
+#include <thrust/scan.h>
+
+#include "defines.h"
+#include "tables.h"
+
+// textures containing look-up tables
+hipTextureObject_t triTex;
+hipTextureObject_t numVertsTex;
+
+// volume data
+hipTextureObject_t volumeTex;
+
+extern "C" void allocateTextures(uint **d_edgeTable, uint **d_triTable,
+                                 uint **d_numVertsTable) {
+  HIPCHECK(hipMalloc((void **)d_edgeTable, 256 * sizeof(uint)));
+  HIPCHECK(hipMemcpy((void *)*d_edgeTable, (void *)edgeTable,
+                             256 * sizeof(uint), hipMemcpyHostToDevice));
+  hipChannelFormatDesc channelDesc =
+      hipCreateChannelDesc(32, 0, 0, 0, hipChannelFormatKindUnsigned);
+
+  HIPCHECK(hipMalloc((void **)d_triTable, 256 * 16 * sizeof(uint)));
+  HIPCHECK(hipMemcpy((void *)*d_triTable, (void *)triTable,
+                             256 * 16 * sizeof(uint), hipMemcpyHostToDevice));
+
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeLinear;
+  texRes.res.linear.devPtr = *d_triTable;
+  texRes.res.linear.sizeInBytes = 256 * 16 * sizeof(uint);
+  texRes.res.linear.desc = channelDesc;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModePoint;
+  texDescr.addressMode[0] = hipAddressModeClamp;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(hipCreateTextureObject(&triTex, &texRes, &texDescr, NULL));
+
+  HIPCHECK(hipMalloc((void **)d_numVertsTable, 256 * sizeof(uint)));
+  HIPCHECK(hipMemcpy((void *)*d_numVertsTable, (void *)numVertsTable,
+                             256 * sizeof(uint), hipMemcpyHostToDevice));
+
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeLinear;
+  texRes.res.linear.devPtr = *d_numVertsTable;
+  texRes.res.linear.sizeInBytes = 256 * sizeof(uint);
+  texRes.res.linear.desc = channelDesc;
+
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModePoint;
+  texDescr.addressMode[0] = hipAddressModeClamp;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(
+      hipCreateTextureObject(&numVertsTex, &texRes, &texDescr, NULL));
+}
+
+extern "C" void createVolumeTexture(uchar *d_volume, size_t buffSize) {
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeLinear;
+  texRes.res.linear.devPtr = d_volume;
+  texRes.res.linear.sizeInBytes = buffSize;
+  texRes.res.linear.desc =
+      hipCreateChannelDesc(8, 0, 0, 0, hipChannelFormatKindUnsigned);
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModePoint;
+  texDescr.addressMode[0] = hipAddressModeClamp;
+  texDescr.readMode = hipReadModeNormalizedFloat;
+
+  HIPCHECK(
+      hipCreateTextureObject(&volumeTex, &texRes, &texDescr, NULL));
+}
+
+extern "C" void destroyAllTextureObjects() {
+  HIPCHECK(hipDestroyTextureObject(triTex));
+  HIPCHECK(hipDestroyTextureObject(numVertsTex));
+  HIPCHECK(hipDestroyTextureObject(volumeTex));
+}
+
+// an interesting field function
+__device__ float tangle(float x, float y, float z) {
+  x *= 3.0f;
+  y *= 3.0f;
+  z *= 3.0f;
+  return (x * x * x * x - 5.0f * x * x + y * y * y * y - 5.0f * y * y +
+          z * z * z * z - 5.0f * z * z + 11.8f) * 0.2f + 0.5f;
+}
+
+// evaluate field function at point
+__device__ float fieldFunc(float3 p) { return tangle(p.x, p.y, p.z); }
+
+// evaluate field function at a point
+// returns value and gradient in float4
+__device__ float4 fieldFunc4(float3 p) {
+  float v = tangle(p.x, p.y, p.z);
+  const float d = 0.001f;
+  float dx = tangle(p.x + d, p.y, p.z) - v;
+  float dy = tangle(p.x, p.y + d, p.z) - v;
+  float dz = tangle(p.x, p.y, p.z + d) - v;
+  return make_float4(dx, dy, dz, v);
+}
+
+// sample volume data set at a point
+__device__ float sampleVolume(hipTextureObject_t volumeTex, uchar *data,
+                              uint3 p, uint3 gridSize) {
+  p.x = min(p.x, gridSize.x - 1);
+  p.y = min(p.y, gridSize.y - 1);
+  p.z = min(p.z, gridSize.z - 1);
+  uint i = (p.z * gridSize.x * gridSize.y) + (p.y * gridSize.x) + p.x;
+  //    return (float) data[i] / 255.0f;
+  return tex1Dfetch<float>(volumeTex, i);
+}
+
+// compute position in 3d grid from 1d index
+// only works for power of 2 sizes
+__device__ uint3 calcGridPos(uint i, uint3 gridSizeShift, uint3 gridSizeMask) {
+  uint3 gridPos;
+  gridPos.x = i & gridSizeMask.x;
+  gridPos.y = (i >> gridSizeShift.y) & gridSizeMask.y;
+  gridPos.z = (i >> gridSizeShift.z) & gridSizeMask.z;
+  return gridPos;
+}
+
+// classify voxel based on number of vertices it will generate
+// one thread per voxel
+__global__ void classifyVoxel(uint *voxelVerts, uint *voxelOccupied,
+                              uchar *volume, uint3 gridSize,
+                              uint3 gridSizeShift, uint3 gridSizeMask,
+                              uint numVoxels, float3 voxelSize, float isoValue,
+                              hipTextureObject_t numVertsTex,
+                              hipTextureObject_t volumeTex) {
+  uint blockId = __mul24(blockIdx.y, gridDim.x) + blockIdx.x;
+  uint i = __mul24(blockId, blockDim.x) + threadIdx.x;
+
+  uint3 gridPos = calcGridPos(i, gridSizeShift, gridSizeMask);
+
+// read field values at neighbouring grid vertices
+#if SAMPLE_VOLUME
+  float field[8];
+  field[0] = sampleVolume(volumeTex, volume, gridPos, gridSize);
+  field[1] =
+      sampleVolume(volumeTex, volume, gridPos + make_uint3(1, 0, 0), gridSize);
+  field[2] =
+      sampleVolume(volumeTex, volume, gridPos + make_uint3(1, 1, 0), gridSize);
+  field[3] =
+      sampleVolume(volumeTex, volume, gridPos + make_uint3(0, 1, 0), gridSize);
+  field[4] =
+      sampleVolume(volumeTex, volume, gridPos + make_uint3(0, 0, 1), gridSize);
+  field[5] =
+      sampleVolume(volumeTex, volume, gridPos + make_uint3(1, 0, 1), gridSize);
+  field[6] =
+      sampleVolume(volumeTex, volume, gridPos + make_uint3(1, 1, 1), gridSize);
+  field[7] =
+      sampleVolume(volumeTex, volume, gridPos + make_uint3(0, 1, 1), gridSize);
+#else
+  float3 p;
+  p.x = -1.0f + (gridPos.x * voxelSize.x);
+  p.y = -1.0f + (gridPos.y * voxelSize.y);
+  p.z = -1.0f + (gridPos.z * voxelSize.z);
+
+  float field[8];
+  field[0] = fieldFunc(p);
+  field[1] = fieldFunc(p + make_float3(voxelSize.x, 0, 0));
+  field[2] = fieldFunc(p + make_float3(voxelSize.x, voxelSize.y, 0));
+  field[3] = fieldFunc(p + make_float3(0, voxelSize.y, 0));
+  field[4] = fieldFunc(p + make_float3(0, 0, voxelSize.z));
+  field[5] = fieldFunc(p + make_float3(voxelSize.x, 0, voxelSize.z));
+  field[6] = fieldFunc(p + make_float3(voxelSize.x, voxelSize.y, voxelSize.z));
+  field[7] = fieldFunc(p + make_float3(0, voxelSize.y, voxelSize.z));
+#endif
+
+  // calculate flag indicating if each vertex is inside or outside isosurface
+  uint cubeindex;
+  cubeindex = uint(field[0] < isoValue);
+  cubeindex += uint(field[1] < isoValue) * 2;
+  cubeindex += uint(field[2] < isoValue) * 4;
+  cubeindex += uint(field[3] < isoValue) * 8;
+  cubeindex += uint(field[4] < isoValue) * 16;
+  cubeindex += uint(field[5] < isoValue) * 32;
+  cubeindex += uint(field[6] < isoValue) * 64;
+  cubeindex += uint(field[7] < isoValue) * 128;
+
+  // read number of vertices from texture
+  uint numVerts = tex1Dfetch<uint>(numVertsTex, cubeindex);
+
+  if (i < numVoxels) {
+    voxelVerts[i] = numVerts;
+    voxelOccupied[i] = (numVerts > 0);
+  }
+}
+
+extern "C" void launch_classifyVoxel(dim3 grid, dim3 threads, uint *voxelVerts,
+                                     uint *voxelOccupied, uchar *volume,
+                                     uint3 gridSize, uint3 gridSizeShift,
+                                     uint3 gridSizeMask, uint numVoxels,
+                                     float3 voxelSize, float isoValue) {
+  // calculate number of vertices need per voxel
+  classifyVoxel<<<grid, threads>>>(voxelVerts, voxelOccupied, volume, gridSize,
+                                   gridSizeShift, gridSizeMask, numVoxels,
+                                   voxelSize, isoValue, numVertsTex, volumeTex);
+  getLastCudaError("classifyVoxel failed");
+}
+
+// compact voxel array
+__global__ void compactVoxels(uint *compactedVoxelArray, uint *voxelOccupied,
+                              uint *voxelOccupiedScan, uint numVoxels) {
+  uint blockId = __mul24(blockIdx.y, gridDim.x) + blockIdx.x;
+  uint i = __mul24(blockId, blockDim.x) + threadIdx.x;
+
+  if (voxelOccupied[i] && (i < numVoxels)) {
+    compactedVoxelArray[voxelOccupiedScan[i]] = i;
+  }
+}
+
+extern "C" void launch_compactVoxels(dim3 grid, dim3 threads,
+                                     uint *compactedVoxelArray,
+                                     uint *voxelOccupied,
+                                     uint *voxelOccupiedScan, uint numVoxels) {
+  compactVoxels<<<grid, threads>>>(compactedVoxelArray, voxelOccupied,
+                                   voxelOccupiedScan, numVoxels);
+  getLastCudaError("compactVoxels failed");
+}
+
+// compute interpolated vertex along an edge
+__device__ float3 vertexInterp(float isolevel, float3 p0, float3 p1, float f0,
+                               float f1) {
+  float t = (isolevel - f0) / (f1 - f0);
+  return lerp(p0, p1, t);
+}
+
+// compute interpolated vertex position and normal along an edge
+__device__ void vertexInterp2(float isolevel, float3 p0, float3 p1, float4 f0,
+                              float4 f1, float3 &p, float3 &n) {
+  float t = (isolevel - f0.w) / (f1.w - f0.w);
+  p = lerp(p0, p1, t);
+  n.x = lerp(f0.x, f1.x, t);
+  n.y = lerp(f0.y, f1.y, t);
+  n.z = lerp(f0.z, f1.z, t);
+  //    n = normalize(n);
+}
+
+// generate triangles for each voxel using marching cubes
+// interpolates normals from field function
+__global__ void generateTriangles(
+    float4 *pos, float4 *norm, uint *compactedVoxelArray, uint *numVertsScanned,
+    uint3 gridSize, uint3 gridSizeShift, uint3 gridSizeMask, float3 voxelSize,
+    float isoValue, uint activeVoxels, uint maxVerts,
+    hipTextureObject_t triTex, hipTextureObject_t numVertsTex) {
+  uint blockId = __mul24(blockIdx.y, gridDim.x) + blockIdx.x;
+  uint i = __mul24(blockId, blockDim.x) + threadIdx.x;
+
+  if (i > activeVoxels - 1) {
+    // can't return here because of syncthreads()
+    i = activeVoxels - 1;
+  }
+
+#if SKIP_EMPTY_VOXELS
+  uint voxel = compactedVoxelArray[i];
+#else
+  uint voxel = i;
+#endif
+
+  // compute position in 3d grid
+  uint3 gridPos = calcGridPos(voxel, gridSizeShift, gridSizeMask);
+
+  float3 p;
+  p.x = -1.0f + (gridPos.x * voxelSize.x);
+  p.y = -1.0f + (gridPos.y * voxelSize.y);
+  p.z = -1.0f + (gridPos.z * voxelSize.z);
+
+  // calculate cell vertex positions
+  float3 v[8];
+  v[0] = p;
+  v[1] = p + make_float3(voxelSize.x, 0, 0);
+  v[2] = p + make_float3(voxelSize.x, voxelSize.y, 0);
+  v[3] = p + make_float3(0, voxelSize.y, 0);
+  v[4] = p + make_float3(0, 0, voxelSize.z);
+  v[5] = p + make_float3(voxelSize.x, 0, voxelSize.z);
+  v[6] = p + make_float3(voxelSize.x, voxelSize.y, voxelSize.z);
+  v[7] = p + make_float3(0, voxelSize.y, voxelSize.z);
+
+  // evaluate field values
+  float4 field[8];
+  field[0] = fieldFunc4(v[0]);
+  field[1] = fieldFunc4(v[1]);
+  field[2] = fieldFunc4(v[2]);
+  field[3] = fieldFunc4(v[3]);
+  field[4] = fieldFunc4(v[4]);
+  field[5] = fieldFunc4(v[5]);
+  field[6] = fieldFunc4(v[6]);
+  field[7] = fieldFunc4(v[7]);
+
+  // recalculate flag
+  // (this is faster than storing it in global memory)
+  uint cubeindex;
+  cubeindex = uint(field[0].w < isoValue);
+  cubeindex += uint(field[1].w < isoValue) * 2;
+  cubeindex += uint(field[2].w < isoValue) * 4;
+  cubeindex += uint(field[3].w < isoValue) * 8;
+  cubeindex += uint(field[4].w < isoValue) * 16;
+  cubeindex += uint(field[5].w < isoValue) * 32;
+  cubeindex += uint(field[6].w < isoValue) * 64;
+  cubeindex += uint(field[7].w < isoValue) * 128;
+
+// find the vertices where the surface intersects the cube
+
+#if USE_SHARED
+  // use partioned shared memory to avoid using local memory
+  __shared__ float3 vertlist[12 * NTHREADS];
+  __shared__ float3 normlist[12 * NTHREADS];
+
+  vertexInterp2(isoValue, v[0], v[1], field[0], field[1], vertlist[threadIdx.x],
+                normlist[threadIdx.x]);
+  vertexInterp2(isoValue, v[1], v[2], field[1], field[2],
+                vertlist[threadIdx.x + NTHREADS],
+                normlist[threadIdx.x + NTHREADS]);
+  vertexInterp2(isoValue, v[2], v[3], field[2], field[3],
+                vertlist[threadIdx.x + (NTHREADS * 2)],
+                normlist[threadIdx.x + (NTHREADS * 2)]);
+  vertexInterp2(isoValue, v[3], v[0], field[3], field[0],
+                vertlist[threadIdx.x + (NTHREADS * 3)],
+                normlist[threadIdx.x + (NTHREADS * 3)]);
+  vertexInterp2(isoValue, v[4], v[5], field[4], field[5],
+                vertlist[threadIdx.x + (NTHREADS * 4)],
+                normlist[threadIdx.x + (NTHREADS * 4)]);
+  vertexInterp2(isoValue, v[5], v[6], field[5], field[6],
+                vertlist[threadIdx.x + (NTHREADS * 5)],
+                normlist[threadIdx.x + (NTHREADS * 5)]);
+  vertexInterp2(isoValue, v[6], v[7], field[6], field[7],
+                vertlist[threadIdx.x + (NTHREADS * 6)],
+                normlist[threadIdx.x + (NTHREADS * 6)]);
+  vertexInterp2(isoValue, v[7], v[4], field[7], field[4],
+                vertlist[threadIdx.x + (NTHREADS * 7)],
+                normlist[threadIdx.x + (NTHREADS * 7)]);
+  vertexInterp2(isoValue, v[0], v[4], field[0], field[4],
+                vertlist[threadIdx.x + (NTHREADS * 8)],
+                normlist[threadIdx.x + (NTHREADS * 8)]);
+  vertexInterp2(isoValue, v[1], v[5], field[1], field[5],
+                vertlist[threadIdx.x + (NTHREADS * 9)],
+                normlist[threadIdx.x + (NTHREADS * 9)]);
+  vertexInterp2(isoValue, v[2], v[6], field[2], field[6],
+                vertlist[threadIdx.x + (NTHREADS * 10)],
+                normlist[threadIdx.x + (NTHREADS * 10)]);
+  vertexInterp2(isoValue, v[3], v[7], field[3], field[7],
+                vertlist[threadIdx.x + (NTHREADS * 11)],
+                normlist[threadIdx.x + (NTHREADS * 11)]);
+  __syncthreads();
+
+#else
+  float3 vertlist[12];
+  float3 normlist[12];
+
+  vertexInterp2(isoValue, v[0], v[1], field[0], field[1], vertlist[0],
+                normlist[0]);
+  vertexInterp2(isoValue, v[1], v[2], field[1], field[2], vertlist[1],
+                normlist[1]);
+  vertexInterp2(isoValue, v[2], v[3], field[2], field[3], vertlist[2],
+                normlist[2]);
+  vertexInterp2(isoValue, v[3], v[0], field[3], field[0], vertlist[3],
+                normlist[3]);
+
+  vertexInterp2(isoValue, v[4], v[5], field[4], field[5], vertlist[4],
+                normlist[4]);
+  vertexInterp2(isoValue, v[5], v[6], field[5], field[6], vertlist[5],
+                normlist[5]);
+  vertexInterp2(isoValue, v[6], v[7], field[6], field[7], vertlist[6],
+                normlist[6]);
+  vertexInterp2(isoValue, v[7], v[4], field[7], field[4], vertlist[7],
+                normlist[7]);
+
+  vertexInterp2(isoValue, v[0], v[4], field[0], field[4], vertlist[8],
+                normlist[8]);
+  vertexInterp2(isoValue, v[1], v[5], field[1], field[5], vertlist[9],
+                normlist[9]);
+  vertexInterp2(isoValue, v[2], v[6], field[2], field[6], vertlist[10],
+                normlist[10]);
+  vertexInterp2(isoValue, v[3], v[7], field[3], field[7], vertlist[11],
+                normlist[11]);
+#endif
+
+  // output triangle vertices
+  uint numVerts = tex1Dfetch<uint>(numVertsTex, cubeindex);
+
+  for (int i = 0; i < numVerts; i++) {
+    uint edge = tex1Dfetch<uint>(triTex, cubeindex * 16 + i);
+
+    uint index = numVertsScanned[voxel] + i;
+
+    if (index < maxVerts) {
+#if USE_SHARED
+      pos[index] = make_float4(vertlist[(edge * NTHREADS) + threadIdx.x], 1.0f);
+      norm[index] =
+          make_float4(normlist[(edge * NTHREADS) + threadIdx.x], 0.0f);
+#else
+      pos[index] = make_float4(vertlist[edge], 1.0f);
+      norm[index] = make_float4(normlist[edge], 0.0f);
+#endif
+    }
+  }
+}
+
+extern "C" void launch_generateTriangles(
+    dim3 grid, dim3 threads, float4 *pos, float4 *norm,
+    uint *compactedVoxelArray, uint *numVertsScanned, uint3 gridSize,
+    uint3 gridSizeShift, uint3 gridSizeMask, float3 voxelSize, float isoValue,
+    uint activeVoxels, uint maxVerts) {
+  generateTriangles<<<grid, NTHREADS>>>(
+      pos, norm, compactedVoxelArray, numVertsScanned, gridSize, gridSizeShift,
+      gridSizeMask, voxelSize, isoValue, activeVoxels, maxVerts, triTex,
+      numVertsTex);
+  getLastCudaError("generateTriangles failed");
+}
+
+// calculate triangle normal
+__device__ float3 calcNormal(float3 *v0, float3 *v1, float3 *v2) {
+  float3 edge0 = *v1 - *v0;
+  float3 edge1 = *v2 - *v0;
+  // note - it's faster to perform normalization in vertex shader rather than
+  // here
+  return cross(edge0, edge1);
+}
+
+// version that calculates flat surface normal for each triangle
+__global__ void generateTriangles2(
+    float4 *pos, float4 *norm, uint *compactedVoxelArray, uint *numVertsScanned,
+    uchar *volume, uint3 gridSize, uint3 gridSizeShift, uint3 gridSizeMask,
+    float3 voxelSize, float isoValue, uint activeVoxels, uint maxVerts,
+    hipTextureObject_t triTex, hipTextureObject_t numVertsTex,
+    hipTextureObject_t volumeTex) {
+  uint blockId = __mul24(blockIdx.y, gridDim.x) + blockIdx.x;
+  uint i = __mul24(blockId, blockDim.x) + threadIdx.x;
+
+  if (i > activeVoxels - 1) {
+    i = activeVoxels - 1;
+  }
+
+#if SKIP_EMPTY_VOXELS
+  uint voxel = compactedVoxelArray[i];
+#else
+  uint voxel = i;
+#endif
+
+  // compute position in 3d grid
+  uint3 gridPos = calcGridPos(voxel, gridSizeShift, gridSizeMask);
+
+  float3 p;
+  p.x = -1.0f + (gridPos.x * voxelSize.x);
+  p.y = -1.0f + (gridPos.y * voxelSize.y);
+  p.z = -1.0f + (gridPos.z * voxelSize.z);
+
+  // calculate cell vertex positions
+  float3 v[8];
+  v[0] = p;
+  v[1] = p + make_float3(voxelSize.x, 0, 0);
+  v[2] = p + make_float3(voxelSize.x, voxelSize.y, 0);
+  v[3] = p + make_float3(0, voxelSize.y, 0);
+  v[4] = p + make_float3(0, 0, voxelSize.z);
+  v[5] = p + make_float3(voxelSize.x, 0, voxelSize.z);
+  v[6] = p + make_float3(voxelSize.x, voxelSize.y, voxelSize.z);
+  v[7] = p + make_float3(0, voxelSize.y, voxelSize.z);
+
+#if SAMPLE_VOLUME
+  float field[8];
+  field[0] = sampleVolume(volumeTex, volume, gridPos, gridSize);
+  field[1] =
+      sampleVolume(volumeTex, volume, gridPos + make_uint3(1, 0, 0), gridSize);
+  field[2] =
+      sampleVolume(volumeTex, volume, gridPos + make_uint3(1, 1, 0), gridSize);
+  field[3] =
+      sampleVolume(volumeTex, volume, gridPos + make_uint3(0, 1, 0), gridSize);
+  field[4] =
+      sampleVolume(volumeTex, volume, gridPos + make_uint3(0, 0, 1), gridSize);
+  field[5] =
+      sampleVolume(volumeTex, volume, gridPos + make_uint3(1, 0, 1), gridSize);
+  field[6] =
+      sampleVolume(volumeTex, volume, gridPos + make_uint3(1, 1, 1), gridSize);
+  field[7] =
+      sampleVolume(volumeTex, volume, gridPos + make_uint3(0, 1, 1), gridSize);
+#else
+  // evaluate field values
+  float field[8];
+  field[0] = fieldFunc(v[0]);
+  field[1] = fieldFunc(v[1]);
+  field[2] = fieldFunc(v[2]);
+  field[3] = fieldFunc(v[3]);
+  field[4] = fieldFunc(v[4]);
+  field[5] = fieldFunc(v[5]);
+  field[6] = fieldFunc(v[6]);
+  field[7] = fieldFunc(v[7]);
+#endif
+
+  // recalculate flag
+  uint cubeindex;
+  cubeindex = uint(field[0] < isoValue);
+  cubeindex += uint(field[1] < isoValue) * 2;
+  cubeindex += uint(field[2] < isoValue) * 4;
+  cubeindex += uint(field[3] < isoValue) * 8;
+  cubeindex += uint(field[4] < isoValue) * 16;
+  cubeindex += uint(field[5] < isoValue) * 32;
+  cubeindex += uint(field[6] < isoValue) * 64;
+  cubeindex += uint(field[7] < isoValue) * 128;
+
+// find the vertices where the surface intersects the cube
+
+#if USE_SHARED
+  // use shared memory to avoid using local
+  __shared__ float3 vertlist[12 * NTHREADS];
+
+  vertlist[threadIdx.x] =
+      vertexInterp(isoValue, v[0], v[1], field[0], field[1]);
+  vertlist[NTHREADS + threadIdx.x] =
+      vertexInterp(isoValue, v[1], v[2], field[1], field[2]);
+  vertlist[(NTHREADS * 2) + threadIdx.x] =
+      vertexInterp(isoValue, v[2], v[3], field[2], field[3]);
+  vertlist[(NTHREADS * 3) + threadIdx.x] =
+      vertexInterp(isoValue, v[3], v[0], field[3], field[0]);
+  vertlist[(NTHREADS * 4) + threadIdx.x] =
+      vertexInterp(isoValue, v[4], v[5], field[4], field[5]);
+  vertlist[(NTHREADS * 5) + threadIdx.x] =
+      vertexInterp(isoValue, v[5], v[6], field[5], field[6]);
+  vertlist[(NTHREADS * 6) + threadIdx.x] =
+      vertexInterp(isoValue, v[6], v[7], field[6], field[7]);
+  vertlist[(NTHREADS * 7) + threadIdx.x] =
+      vertexInterp(isoValue, v[7], v[4], field[7], field[4]);
+  vertlist[(NTHREADS * 8) + threadIdx.x] =
+      vertexInterp(isoValue, v[0], v[4], field[0], field[4]);
+  vertlist[(NTHREADS * 9) + threadIdx.x] =
+      vertexInterp(isoValue, v[1], v[5], field[1], field[5]);
+  vertlist[(NTHREADS * 10) + threadIdx.x] =
+      vertexInterp(isoValue, v[2], v[6], field[2], field[6]);
+  vertlist[(NTHREADS * 11) + threadIdx.x] =
+      vertexInterp(isoValue, v[3], v[7], field[3], field[7]);
+  __syncthreads();
+#else
+
+  float3 vertlist[12];
+
+  vertlist[0] = vertexInterp(isoValue, v[0], v[1], field[0], field[1]);
+  vertlist[1] = vertexInterp(isoValue, v[1], v[2], field[1], field[2]);
+  vertlist[2] = vertexInterp(isoValue, v[2], v[3], field[2], field[3]);
+  vertlist[3] = vertexInterp(isoValue, v[3], v[0], field[3], field[0]);
+
+  vertlist[4] = vertexInterp(isoValue, v[4], v[5], field[4], field[5]);
+  vertlist[5] = vertexInterp(isoValue, v[5], v[6], field[5], field[6]);
+  vertlist[6] = vertexInterp(isoValue, v[6], v[7], field[6], field[7]);
+  vertlist[7] = vertexInterp(isoValue, v[7], v[4], field[7], field[4]);
+
+  vertlist[8] = vertexInterp(isoValue, v[0], v[4], field[0], field[4]);
+  vertlist[9] = vertexInterp(isoValue, v[1], v[5], field[1], field[5]);
+  vertlist[10] = vertexInterp(isoValue, v[2], v[6], field[2], field[6]);
+  vertlist[11] = vertexInterp(isoValue, v[3], v[7], field[3], field[7]);
+#endif
+
+  // output triangle vertices
+  uint numVerts = tex1Dfetch<uint>(numVertsTex, cubeindex);
+
+  for (int i = 0; i < numVerts; i += 3) {
+    uint index = numVertsScanned[voxel] + i;
+
+    float3 *v[3];
+    uint edge;
+    edge = tex1Dfetch<uint>(triTex, (cubeindex * 16) + i);
+#if USE_SHARED
+    v[0] = &vertlist[(edge * NTHREADS) + threadIdx.x];
+#else
+    v[0] = &vertlist[edge];
+#endif
+
+    edge = tex1Dfetch<uint>(triTex, (cubeindex * 16) + i + 1);
+#if USE_SHARED
+    v[1] = &vertlist[(edge * NTHREADS) + threadIdx.x];
+#else
+    v[1] = &vertlist[edge];
+#endif
+
+    edge = tex1Dfetch<uint>(triTex, (cubeindex * 16) + i + 2);
+#if USE_SHARED
+    v[2] = &vertlist[(edge * NTHREADS) + threadIdx.x];
+#else
+    v[2] = &vertlist[edge];
+#endif
+
+    // calculate triangle surface normal
+    float3 n = calcNormal(v[0], v[1], v[2]);
+
+    if (index < (maxVerts - 3)) {
+      pos[index] = make_float4(*v[0], 1.0f);
+      norm[index] = make_float4(n, 0.0f);
+
+      pos[index + 1] = make_float4(*v[1], 1.0f);
+      norm[index + 1] = make_float4(n, 0.0f);
+
+      pos[index + 2] = make_float4(*v[2], 1.0f);
+      norm[index + 2] = make_float4(n, 0.0f);
+    }
+  }
+}
+
+extern "C" void launch_generateTriangles2(
+    dim3 grid, dim3 threads, float4 *pos, float4 *norm,
+    uint *compactedVoxelArray, uint *numVertsScanned, uchar *volume,
+    uint3 gridSize, uint3 gridSizeShift, uint3 gridSizeMask, float3 voxelSize,
+    float isoValue, uint activeVoxels, uint maxVerts) {
+  generateTriangles2<<<grid, NTHREADS>>>(
+      pos, norm, compactedVoxelArray, numVertsScanned, volume, gridSize,
+      gridSizeShift, gridSizeMask, voxelSize, isoValue, activeVoxels, maxVerts,
+      triTex, numVertsTex, volumeTex);
+  getLastCudaError("generateTriangles2 failed");
+}
+
+extern "C" void ThrustScanWrapper(unsigned int *output, unsigned int *input,
+                                  unsigned int numElements) {
+  thrust::exclusive_scan(thrust::device_ptr<unsigned int>(input),
+                         thrust::device_ptr<unsigned int>(input + numElements),
+                         thrust::device_ptr<unsigned int>(output));
+}
+
+#endif
+nts),
+                         thrust::device_ptr<unsigned int>(output));
+}
+
+#endif
diff --git a/src/samples/Samples/5_Domain_Specific/nbody/bodysystemcuda.cu.hip b/src/samples/Samples/5_Domain_Specific/nbody/bodysystemcuda.cu.hip
index e69de29..8823fdd 100755
--- a/src/samples/Samples/5_Domain_Specific/nbody/bodysystemcuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/nbody/bodysystemcuda.cu.hip
@@ -0,0 +1,288 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "helper_cuda_hipified.h"
+#include <math.h>
+
+#if defined(__APPLE__) || defined(MACOSX)
+#pragma clang diagnostic ignored "-Wdeprecated-declarations"
+#include <GLUT/glut.h>
+#else
+#include <GL/freeglut.h>
+#endif
+
+// CUDA standard includes
+#include <hip/hip_runtime.h>
+#include <cuda_gl_interop.h>
+
+#include <hip/hip_cooperative_groups.h>
+
+namespace cg = cooperative_groups;
+
+#include "bodysystem.h"
+
+__constant__ float softeningSquared;
+__constant__ double softeningSquared_fp64;
+
+hipError_t setSofteningSquared(float softeningSq) {
+  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared), &softeningSq, sizeof(float), 0,
+                            hipMemcpyHostToDevice);
+}
+
+hipError_t setSofteningSquared(double softeningSq) {
+  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared_fp64), &softeningSq, sizeof(double),
+                            0, hipMemcpyHostToDevice);
+}
+
+template <class T>
+struct SharedMemory {
+  __device__ inline operator T *() {
+    extern __shared__ int __smem[];
+    return (T *)__smem;
+  }
+
+  __device__ inline operator const T *() const {
+    extern __shared__ int __smem[];
+    return (T *)__smem;
+  }
+};
+
+template <typename T>
+__device__ T rsqrt_T(T x) {
+  return rsqrt(x);
+}
+
+template <>
+__device__ float rsqrt_T<float>(float x) {
+  return rsqrtf(x);
+}
+
+template <>
+__device__ double rsqrt_T<double>(double x) {
+  return rsqrt(x);
+}
+
+// Macros to simplify shared memory addressing
+#define SX(i) sharedPos[i + blockDim.x * threadIdx.y]
+// This macro is only used when multithreadBodies is true (below)
+#define SX_SUM(i, j) sharedPos[i + blockDim.x * j]
+
+template <typename T>
+__device__ T getSofteningSquared() {
+  return softeningSquared;
+}
+template <>
+__device__ double getSofteningSquared<double>() {
+  return softeningSquared_fp64;
+}
+
+template <typename T>
+struct DeviceData {
+  T *dPos[2];  // mapped host pointers
+  T *dVel;
+  hipEvent_t event;
+  unsigned int offset;
+  unsigned int numBodies;
+};
+
+template <typename T>
+__device__ typename vec3<T>::Type bodyBodyInteraction(
+    typename vec3<T>::Type ai, typename vec4<T>::Type bi,
+    typename vec4<T>::Type bj) {
+  typename vec3<T>::Type r;
+
+  // r_ij  [3 FLOPS]
+  r.x = bj.x - bi.x;
+  r.y = bj.y - bi.y;
+  r.z = bj.z - bi.z;
+
+  // distSqr = dot(r_ij, r_ij) + EPS^2  [6 FLOPS]
+  T distSqr = r.x * r.x + r.y * r.y + r.z * r.z;
+  distSqr += getSofteningSquared<T>();
+
+  // invDistCube =1/distSqr^(3/2)  [4 FLOPS (2 mul, 1 sqrt, 1 inv)]
+  T invDist = rsqrt_T(distSqr);
+  T invDistCube = invDist * invDist * invDist;
+
+  // s = m_j * invDistCube [1 FLOP]
+  T s = bj.w * invDistCube;
+
+  // a_i =  a_i + s * r_ij [6 FLOPS]
+  ai.x += r.x * s;
+  ai.y += r.y * s;
+  ai.z += r.z * s;
+
+  return ai;
+}
+
+template <typename T>
+__device__ typename vec3<T>::Type computeBodyAccel(
+    typename vec4<T>::Type bodyPos, typename vec4<T>::Type *positions,
+    int numTiles, cg::thread_block cta) {
+  typename vec4<T>::Type *sharedPos = SharedMemory<typename vec4<T>::Type>();
+
+  typename vec3<T>::Type acc = {0.0f, 0.0f, 0.0f};
+
+  for (int tile = 0; tile < numTiles; tile++) {
+    sharedPos[threadIdx.x] = positions[tile * blockDim.x + threadIdx.x];
+
+    cg::sync(cta);
+
+// This is the "tile_calculation" from the GPUG3 article.
+#pragma unroll 128
+
+    for (unsigned int counter = 0; counter < blockDim.x; counter++) {
+      acc = bodyBodyInteraction<T>(acc, bodyPos, sharedPos[counter]);
+    }
+
+    cg::sync(cta);
+  }
+
+  return acc;
+}
+
+template <typename T>
+__global__ void integrateBodies(typename vec4<T>::Type *__restrict__ newPos,
+                                typename vec4<T>::Type *__restrict__ oldPos,
+                                typename vec4<T>::Type *vel,
+                                unsigned int deviceOffset,
+                                unsigned int deviceNumBodies, float deltaTime,
+                                float damping, int numTiles) {
+  // Handle to thread block group
+  cg::thread_block cta = cg::this_thread_block();
+  int index = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (index >= deviceNumBodies) {
+    return;
+  }
+
+  typename vec4<T>::Type position = oldPos[deviceOffset + index];
+
+  typename vec3<T>::Type accel =
+      computeBodyAccel<T>(position, oldPos, numTiles, cta);
+
+  // acceleration = force / mass;
+  // new velocity = old velocity + acceleration * deltaTime
+  // note we factor out the body's mass from the equation, here and in
+  // bodyBodyInteraction
+  // (because they cancel out).  Thus here force == acceleration
+  typename vec4<T>::Type velocity = vel[deviceOffset + index];
+
+  velocity.x += accel.x * deltaTime;
+  velocity.y += accel.y * deltaTime;
+  velocity.z += accel.z * deltaTime;
+
+  velocity.x *= damping;
+  velocity.y *= damping;
+  velocity.z *= damping;
+
+  // new position = old position + velocity * deltaTime
+  position.x += velocity.x * deltaTime;
+  position.y += velocity.y * deltaTime;
+  position.z += velocity.z * deltaTime;
+
+  // store new position and velocity
+  newPos[deviceOffset + index] = position;
+  vel[deviceOffset + index] = velocity;
+}
+
+template <typename T>
+void integrateNbodySystem(DeviceData<T> *deviceData,
+                          hipGraphicsResource **pgres,
+                          unsigned int currentRead, float deltaTime,
+                          float damping, unsigned int numBodies,
+                          unsigned int numDevices, int blockSize,
+                          bool bUsePBO) {
+  if (bUsePBO) {
+    HIPCHECK(cudaGraphicsResourceSetMapFlags(
+        pgres[currentRead], cudaGraphicsMapFlagsReadOnly));
+    HIPCHECK(cudaGraphicsResourceSetMapFlags(
+        pgres[1 - currentRead], cudaGraphicsMapFlagsWriteDiscard));
+    HIPCHECK(hipGraphicsMapResources(2, pgres, 0));
+    size_t bytes;
+    HIPCHECK(hipGraphicsResourceGetMappedPointer(
+        (void **)&(deviceData[0].dPos[currentRead]), &bytes,
+        pgres[currentRead]));
+    HIPCHECK(hipGraphicsResourceGetMappedPointer(
+        (void **)&(deviceData[0].dPos[1 - currentRead]), &bytes,
+        pgres[1 - currentRead]));
+  }
+
+  for (unsigned int dev = 0; dev != numDevices; dev++) {
+    if (numDevices > 1) {
+      hipSetDevice(dev);
+    }
+
+    int numBlocks = (deviceData[dev].numBodies + blockSize - 1) / blockSize;
+    int numTiles = (numBodies + blockSize - 1) / blockSize;
+    int sharedMemSize = blockSize * 4 * sizeof(T);  // 4 floats for pos
+
+    integrateBodies<T><<<numBlocks, blockSize, sharedMemSize>>>(
+        (typename vec4<T>::Type *)deviceData[dev].dPos[1 - currentRead],
+        (typename vec4<T>::Type *)deviceData[dev].dPos[currentRead],
+        (typename vec4<T>::Type *)deviceData[dev].dVel, deviceData[dev].offset,
+        deviceData[dev].numBodies, deltaTime, damping, numTiles);
+
+    if (numDevices > 1) {
+      HIPCHECK(hipEventRecord(deviceData[dev].event));
+      // MJH: Hack on older driver versions to force kernel launches to flush!
+      hipStreamQuery(0);
+    }
+
+    // check if kernel invocation generated an error
+    getLastCudaError("Kernel execution failed");
+  }
+
+  if (numDevices > 1) {
+    for (unsigned int dev = 0; dev < numDevices; dev++) {
+      HIPCHECK(hipEventSynchronize(deviceData[dev].event));
+    }
+  }
+
+  if (bUsePBO) {
+    HIPCHECK(hipGraphicsUnmapResources(2, pgres, 0));
+  }
+}
+
+// Explicit specializations needed to generate code
+template void integrateNbodySystem<float>(DeviceData<float> *deviceData,
+                                          hipGraphicsResource **pgres,
+                                          unsigned int currentRead,
+                                          float deltaTime, float damping,
+                                          unsigned int numBodies,
+                                          unsigned int numDevices,
+                                          int blockSize, bool bUsePBO);
+
+template void integrateNbodySystem<double>(DeviceData<double> *deviceData,
+                                           hipGraphicsResource **pgres,
+                                           unsigned int currentRead,
+                                           float deltaTime, float damping,
+                                           unsigned int numBodies,
+                                           unsigned int numDevices,
+                                           int blockSize, bool bUsePBO);
+                          int blockSize, bool bUsePBO);
diff --git a/src/samples/Samples/5_Domain_Specific/nbody_opengles/bodysystemcuda.cu.hip b/src/samples/Samples/5_Domain_Specific/nbody_opengles/bodysystemcuda.cu.hip
index e69de29..986ac9a 100755
--- a/src/samples/Samples/5_Domain_Specific/nbody_opengles/bodysystemcuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/nbody_opengles/bodysystemcuda.cu.hip
@@ -0,0 +1,278 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "helper_cuda_hipified.h"
+#include <math.h>
+
+//#include <GL/glew.h>
+//#include <GL/freeglut.h>
+
+// CUDA standard includes
+#include <hip/hip_runtime.h>
+//#include <cuda_gl_interop.h>
+
+#include "bodysystem.h"
+
+__constant__ float softeningSquared;
+__constant__ double softeningSquared_fp64;
+
+hipError_t setSofteningSquared(float softeningSq) {
+  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared), &softeningSq, sizeof(float), 0,
+                            hipMemcpyHostToDevice);
+}
+
+hipError_t setSofteningSquared(double softeningSq) {
+  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared_fp64), &softeningSq, sizeof(double),
+                            0, hipMemcpyHostToDevice);
+}
+
+template <class T>
+struct SharedMemory {
+  __device__ inline operator T *() {
+    extern __shared__ int __smem[];
+    return (T *)__smem;
+  }
+
+  __device__ inline operator const T *() const {
+    extern __shared__ int __smem[];
+    return (T *)__smem;
+  }
+};
+
+template <typename T>
+__device__ T rsqrt_T(T x) {
+  return rsqrt(x);
+}
+
+template <>
+__device__ float rsqrt_T<float>(float x) {
+  return rsqrtf(x);
+}
+
+template <>
+__device__ double rsqrt_T<double>(double x) {
+  return rsqrt(x);
+}
+
+// Macros to simplify shared memory addressing
+#define SX(i) sharedPos[i + blockDim.x * threadIdx.y]
+// This macro is only used when multithreadBodies is true (below)
+#define SX_SUM(i, j) sharedPos[i + blockDim.x * j]
+
+template <typename T>
+__device__ T getSofteningSquared() {
+  return softeningSquared;
+}
+template <>
+__device__ double getSofteningSquared<double>() {
+  return softeningSquared_fp64;
+}
+
+template <typename T>
+struct DeviceData {
+  T *dPos[2];  // mapped host pointers
+  T *dVel;
+  hipEvent_t event;
+  unsigned int offset;
+  unsigned int numBodies;
+};
+
+template <typename T>
+__device__ typename vec3<T>::Type bodyBodyInteraction(
+    typename vec3<T>::Type ai, typename vec4<T>::Type bi,
+    typename vec4<T>::Type bj) {
+  typename vec3<T>::Type r;
+
+  // r_ij  [3 FLOPS]
+  r.x = bj.x - bi.x;
+  r.y = bj.y - bi.y;
+  r.z = bj.z - bi.z;
+
+  // distSqr = dot(r_ij, r_ij) + EPS^2  [6 FLOPS]
+  T distSqr = r.x * r.x + r.y * r.y + r.z * r.z;
+  distSqr += getSofteningSquared<T>();
+
+  // invDistCube =1/distSqr^(3/2)  [4 FLOPS (2 mul, 1 sqrt, 1 inv)]
+  T invDist = rsqrt_T(distSqr);
+  T invDistCube = invDist * invDist * invDist;
+
+  // s = m_j * invDistCube [1 FLOP]
+  T s = bj.w * invDistCube;
+
+  // a_i =  a_i + s * r_ij [6 FLOPS]
+  ai.x += r.x * s;
+  ai.y += r.y * s;
+  ai.z += r.z * s;
+
+  return ai;
+}
+
+template <typename T>
+__device__ typename vec3<T>::Type computeBodyAccel(
+    typename vec4<T>::Type bodyPos, typename vec4<T>::Type *positions,
+    int numTiles) {
+  typename vec4<T>::Type *sharedPos = SharedMemory<typename vec4<T>::Type>();
+
+  typename vec3<T>::Type acc = {0.0f, 0.0f, 0.0f};
+
+  for (int tile = 0; tile < numTiles; tile++) {
+    sharedPos[threadIdx.x] = positions[tile * blockDim.x + threadIdx.x];
+
+    __syncthreads();
+
+    // This is the "tile_calculation" from the GPUG3 article.
+#pragma unroll 128
+
+    for (unsigned int counter = 0; counter < blockDim.x; counter++) {
+      acc = bodyBodyInteraction<T>(acc, bodyPos, sharedPos[counter]);
+    }
+
+    __syncthreads();
+  }
+
+  return acc;
+}
+
+template <typename T>
+__global__ void integrateBodies(typename vec4<T>::Type *__restrict__ newPos,
+                                typename vec4<T>::Type *__restrict__ oldPos,
+                                typename vec4<T>::Type *vel,
+                                unsigned int deviceOffset,
+                                unsigned int deviceNumBodies, float deltaTime,
+                                float damping, int numTiles) {
+  int index = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (index >= deviceNumBodies) {
+    return;
+  }
+
+  typename vec4<T>::Type position = oldPos[deviceOffset + index];
+
+  typename vec3<T>::Type accel =
+      computeBodyAccel<T>(position, oldPos, numTiles);
+
+  // acceleration = force / mass;
+  // new velocity = old velocity + acceleration * deltaTime
+  // note we factor out the body's mass from the equation, here and in
+  // bodyBodyInteraction (because they cancel out).  Thus here force ==
+  // acceleration
+  typename vec4<T>::Type velocity = vel[deviceOffset + index];
+
+  velocity.x += accel.x * deltaTime;
+  velocity.y += accel.y * deltaTime;
+  velocity.z += accel.z * deltaTime;
+
+  velocity.x *= damping;
+  velocity.y *= damping;
+  velocity.z *= damping;
+
+  // new position = old position + velocity * deltaTime
+  position.x += velocity.x * deltaTime;
+  position.y += velocity.y * deltaTime;
+  position.z += velocity.z * deltaTime;
+
+  // store new position and velocity
+  newPos[deviceOffset + index] = position;
+  vel[deviceOffset + index] = velocity;
+}
+
+template <typename T>
+void integrateNbodySystem(DeviceData<T> *deviceData,
+                          hipGraphicsResource **pgres,
+                          unsigned int currentRead, float deltaTime,
+                          float damping, unsigned int numBodies,
+                          unsigned int numDevices, int blockSize,
+                          bool bUsePBO) {
+  if (bUsePBO) {
+    HIPCHECK(cudaGraphicsResourceSetMapFlags(
+        pgres[currentRead], cudaGraphicsMapFlagsReadOnly));
+    HIPCHECK(cudaGraphicsResourceSetMapFlags(
+        pgres[1 - currentRead], cudaGraphicsMapFlagsWriteDiscard));
+    HIPCHECK(hipGraphicsMapResources(2, pgres, 0));
+    size_t bytes;
+    HIPCHECK(hipGraphicsResourceGetMappedPointer(
+        (void **)&(deviceData[0].dPos[currentRead]), &bytes,
+        pgres[currentRead]));
+    HIPCHECK(hipGraphicsResourceGetMappedPointer(
+        (void **)&(deviceData[0].dPos[1 - currentRead]), &bytes,
+        pgres[1 - currentRead]));
+  }
+
+  for (unsigned int dev = 0; dev != numDevices; dev++) {
+    if (numDevices > 1) {
+      hipSetDevice(dev);
+    }
+
+    int numBlocks = (deviceData[dev].numBodies + blockSize - 1) / blockSize;
+    int numTiles = (numBodies + blockSize - 1) / blockSize;
+    int sharedMemSize = blockSize * 4 * sizeof(T);  // 4 floats for pos
+
+    integrateBodies<T><<<numBlocks, blockSize, sharedMemSize>>>(
+        (typename vec4<T>::Type *)deviceData[dev].dPos[1 - currentRead],
+        (typename vec4<T>::Type *)deviceData[dev].dPos[currentRead],
+        (typename vec4<T>::Type *)deviceData[dev].dVel, deviceData[dev].offset,
+        deviceData[dev].numBodies, deltaTime, damping, numTiles);
+
+    if (numDevices > 1) {
+      HIPCHECK(hipEventRecord(deviceData[dev].event));
+      // MJH: Hack on older driver versions to force kernel launches to flush!
+      hipStreamQuery(0);
+    }
+
+    // check if kernel invocation generated an error
+    getLastCudaError("Kernel execution failed");
+  }
+
+  if (numDevices > 1) {
+    for (unsigned int dev = 0; dev < numDevices; dev++) {
+      HIPCHECK(hipEventSynchronize(deviceData[dev].event));
+    }
+  }
+
+  if (bUsePBO) {
+    HIPCHECK(hipGraphicsUnmapResources(2, pgres, 0));
+  }
+}
+
+// Explicit specializations needed to generate code
+template void integrateNbodySystem<float>(DeviceData<float> *deviceData,
+                                          hipGraphicsResource **pgres,
+                                          unsigned int currentRead,
+                                          float deltaTime, float damping,
+                                          unsigned int numBodies,
+                                          unsigned int numDevices,
+                                          int blockSize, bool bUsePBO);
+
+template void integrateNbodySystem<double>(DeviceData<double> *deviceData,
+                                           hipGraphicsResource **pgres,
+                                           unsigned int currentRead,
+                                           float deltaTime, float damping,
+                                           unsigned int numBodies,
+                                           unsigned int numDevices,
+                                           int blockSize, bool bUsePBO);
+                          int blockSize, bool bUsePBO);
diff --git a/src/samples/Samples/5_Domain_Specific/nbody_screen/bodysystemcuda.cu.hip b/src/samples/Samples/5_Domain_Specific/nbody_screen/bodysystemcuda.cu.hip
index e69de29..986ac9a 100755
--- a/src/samples/Samples/5_Domain_Specific/nbody_screen/bodysystemcuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/nbody_screen/bodysystemcuda.cu.hip
@@ -0,0 +1,278 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "helper_cuda_hipified.h"
+#include <math.h>
+
+//#include <GL/glew.h>
+//#include <GL/freeglut.h>
+
+// CUDA standard includes
+#include <hip/hip_runtime.h>
+//#include <cuda_gl_interop.h>
+
+#include "bodysystem.h"
+
+__constant__ float softeningSquared;
+__constant__ double softeningSquared_fp64;
+
+hipError_t setSofteningSquared(float softeningSq) {
+  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared), &softeningSq, sizeof(float), 0,
+                            hipMemcpyHostToDevice);
+}
+
+hipError_t setSofteningSquared(double softeningSq) {
+  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared_fp64), &softeningSq, sizeof(double),
+                            0, hipMemcpyHostToDevice);
+}
+
+template <class T>
+struct SharedMemory {
+  __device__ inline operator T *() {
+    extern __shared__ int __smem[];
+    return (T *)__smem;
+  }
+
+  __device__ inline operator const T *() const {
+    extern __shared__ int __smem[];
+    return (T *)__smem;
+  }
+};
+
+template <typename T>
+__device__ T rsqrt_T(T x) {
+  return rsqrt(x);
+}
+
+template <>
+__device__ float rsqrt_T<float>(float x) {
+  return rsqrtf(x);
+}
+
+template <>
+__device__ double rsqrt_T<double>(double x) {
+  return rsqrt(x);
+}
+
+// Macros to simplify shared memory addressing
+#define SX(i) sharedPos[i + blockDim.x * threadIdx.y]
+// This macro is only used when multithreadBodies is true (below)
+#define SX_SUM(i, j) sharedPos[i + blockDim.x * j]
+
+template <typename T>
+__device__ T getSofteningSquared() {
+  return softeningSquared;
+}
+template <>
+__device__ double getSofteningSquared<double>() {
+  return softeningSquared_fp64;
+}
+
+template <typename T>
+struct DeviceData {
+  T *dPos[2];  // mapped host pointers
+  T *dVel;
+  hipEvent_t event;
+  unsigned int offset;
+  unsigned int numBodies;
+};
+
+template <typename T>
+__device__ typename vec3<T>::Type bodyBodyInteraction(
+    typename vec3<T>::Type ai, typename vec4<T>::Type bi,
+    typename vec4<T>::Type bj) {
+  typename vec3<T>::Type r;
+
+  // r_ij  [3 FLOPS]
+  r.x = bj.x - bi.x;
+  r.y = bj.y - bi.y;
+  r.z = bj.z - bi.z;
+
+  // distSqr = dot(r_ij, r_ij) + EPS^2  [6 FLOPS]
+  T distSqr = r.x * r.x + r.y * r.y + r.z * r.z;
+  distSqr += getSofteningSquared<T>();
+
+  // invDistCube =1/distSqr^(3/2)  [4 FLOPS (2 mul, 1 sqrt, 1 inv)]
+  T invDist = rsqrt_T(distSqr);
+  T invDistCube = invDist * invDist * invDist;
+
+  // s = m_j * invDistCube [1 FLOP]
+  T s = bj.w * invDistCube;
+
+  // a_i =  a_i + s * r_ij [6 FLOPS]
+  ai.x += r.x * s;
+  ai.y += r.y * s;
+  ai.z += r.z * s;
+
+  return ai;
+}
+
+template <typename T>
+__device__ typename vec3<T>::Type computeBodyAccel(
+    typename vec4<T>::Type bodyPos, typename vec4<T>::Type *positions,
+    int numTiles) {
+  typename vec4<T>::Type *sharedPos = SharedMemory<typename vec4<T>::Type>();
+
+  typename vec3<T>::Type acc = {0.0f, 0.0f, 0.0f};
+
+  for (int tile = 0; tile < numTiles; tile++) {
+    sharedPos[threadIdx.x] = positions[tile * blockDim.x + threadIdx.x];
+
+    __syncthreads();
+
+    // This is the "tile_calculation" from the GPUG3 article.
+#pragma unroll 128
+
+    for (unsigned int counter = 0; counter < blockDim.x; counter++) {
+      acc = bodyBodyInteraction<T>(acc, bodyPos, sharedPos[counter]);
+    }
+
+    __syncthreads();
+  }
+
+  return acc;
+}
+
+template <typename T>
+__global__ void integrateBodies(typename vec4<T>::Type *__restrict__ newPos,
+                                typename vec4<T>::Type *__restrict__ oldPos,
+                                typename vec4<T>::Type *vel,
+                                unsigned int deviceOffset,
+                                unsigned int deviceNumBodies, float deltaTime,
+                                float damping, int numTiles) {
+  int index = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (index >= deviceNumBodies) {
+    return;
+  }
+
+  typename vec4<T>::Type position = oldPos[deviceOffset + index];
+
+  typename vec3<T>::Type accel =
+      computeBodyAccel<T>(position, oldPos, numTiles);
+
+  // acceleration = force / mass;
+  // new velocity = old velocity + acceleration * deltaTime
+  // note we factor out the body's mass from the equation, here and in
+  // bodyBodyInteraction (because they cancel out).  Thus here force ==
+  // acceleration
+  typename vec4<T>::Type velocity = vel[deviceOffset + index];
+
+  velocity.x += accel.x * deltaTime;
+  velocity.y += accel.y * deltaTime;
+  velocity.z += accel.z * deltaTime;
+
+  velocity.x *= damping;
+  velocity.y *= damping;
+  velocity.z *= damping;
+
+  // new position = old position + velocity * deltaTime
+  position.x += velocity.x * deltaTime;
+  position.y += velocity.y * deltaTime;
+  position.z += velocity.z * deltaTime;
+
+  // store new position and velocity
+  newPos[deviceOffset + index] = position;
+  vel[deviceOffset + index] = velocity;
+}
+
+template <typename T>
+void integrateNbodySystem(DeviceData<T> *deviceData,
+                          hipGraphicsResource **pgres,
+                          unsigned int currentRead, float deltaTime,
+                          float damping, unsigned int numBodies,
+                          unsigned int numDevices, int blockSize,
+                          bool bUsePBO) {
+  if (bUsePBO) {
+    HIPCHECK(cudaGraphicsResourceSetMapFlags(
+        pgres[currentRead], cudaGraphicsMapFlagsReadOnly));
+    HIPCHECK(cudaGraphicsResourceSetMapFlags(
+        pgres[1 - currentRead], cudaGraphicsMapFlagsWriteDiscard));
+    HIPCHECK(hipGraphicsMapResources(2, pgres, 0));
+    size_t bytes;
+    HIPCHECK(hipGraphicsResourceGetMappedPointer(
+        (void **)&(deviceData[0].dPos[currentRead]), &bytes,
+        pgres[currentRead]));
+    HIPCHECK(hipGraphicsResourceGetMappedPointer(
+        (void **)&(deviceData[0].dPos[1 - currentRead]), &bytes,
+        pgres[1 - currentRead]));
+  }
+
+  for (unsigned int dev = 0; dev != numDevices; dev++) {
+    if (numDevices > 1) {
+      hipSetDevice(dev);
+    }
+
+    int numBlocks = (deviceData[dev].numBodies + blockSize - 1) / blockSize;
+    int numTiles = (numBodies + blockSize - 1) / blockSize;
+    int sharedMemSize = blockSize * 4 * sizeof(T);  // 4 floats for pos
+
+    integrateBodies<T><<<numBlocks, blockSize, sharedMemSize>>>(
+        (typename vec4<T>::Type *)deviceData[dev].dPos[1 - currentRead],
+        (typename vec4<T>::Type *)deviceData[dev].dPos[currentRead],
+        (typename vec4<T>::Type *)deviceData[dev].dVel, deviceData[dev].offset,
+        deviceData[dev].numBodies, deltaTime, damping, numTiles);
+
+    if (numDevices > 1) {
+      HIPCHECK(hipEventRecord(deviceData[dev].event));
+      // MJH: Hack on older driver versions to force kernel launches to flush!
+      hipStreamQuery(0);
+    }
+
+    // check if kernel invocation generated an error
+    getLastCudaError("Kernel execution failed");
+  }
+
+  if (numDevices > 1) {
+    for (unsigned int dev = 0; dev < numDevices; dev++) {
+      HIPCHECK(hipEventSynchronize(deviceData[dev].event));
+    }
+  }
+
+  if (bUsePBO) {
+    HIPCHECK(hipGraphicsUnmapResources(2, pgres, 0));
+  }
+}
+
+// Explicit specializations needed to generate code
+template void integrateNbodySystem<float>(DeviceData<float> *deviceData,
+                                          hipGraphicsResource **pgres,
+                                          unsigned int currentRead,
+                                          float deltaTime, float damping,
+                                          unsigned int numBodies,
+                                          unsigned int numDevices,
+                                          int blockSize, bool bUsePBO);
+
+template void integrateNbodySystem<double>(DeviceData<double> *deviceData,
+                                           hipGraphicsResource **pgres,
+                                           unsigned int currentRead,
+                                           float deltaTime, float damping,
+                                           unsigned int numBodies,
+                                           unsigned int numDevices,
+                                           int blockSize, bool bUsePBO);
+                          int blockSize, bool bUsePBO);
diff --git a/src/samples/Samples/5_Domain_Specific/postProcessGL/postProcessGL.cu.hip b/src/samples/Samples/5_Domain_Specific/postProcessGL/postProcessGL.cu.hip
index e69de29..d62ec63 100755
--- a/src/samples/Samples/5_Domain_Specific/postProcessGL/postProcessGL.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/postProcessGL/postProcessGL.cu.hip
@@ -0,0 +1,261 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// Utilities and system includes
+
+#include <hip/hip_cooperative_groups.h>
+
+namespace cg = cooperative_groups;
+
+#include "helper_cuda_hipified.h"
+
+hipTextureObject_t inTexObject;
+
+// clamp x to range [a, b]
+__device__ float clamp(float x, float a, float b) { return max(a, min(b, x)); }
+
+__device__ int clamp(int x, int a, int b) { return max(a, min(b, x)); }
+
+// convert floating point rgb color to 8-bit integer
+__device__ int rgbToInt(float r, float g, float b) {
+  r = clamp(r, 0.0f, 255.0f);
+  g = clamp(g, 0.0f, 255.0f);
+  b = clamp(b, 0.0f, 255.0f);
+  return (int(b) << 16) | (int(g) << 8) | int(r);
+}
+
+// get pixel from 2D image, with clamping to border
+__device__ uchar4 getPixel(int x, int y, hipTextureObject_t inTex) {
+#ifndef USE_TEXTURE_RGBA8UI
+  float4 res = tex2D<float4>(inTex, x, y);
+  uchar4 ucres = make_uchar4(res.x * 255.0f, res.y * 255.0f, res.z * 255.0f,
+                             res.w * 255.0f);
+#else
+  uchar4 ucres = tex2D<uchar4>(inTex, x, y);
+#endif
+  return ucres;
+}
+
+// macros to make indexing shared memory easier
+#define SMEM(X, Y) sdata[(Y)*tilew + (X)]
+
+/*
+    2D convolution using shared memory
+    - operates on 8-bit RGB data stored in 32-bit int
+    - assumes kernel radius is less than or equal to block size
+    - not optimized for performance
+     _____________
+    |   :     :   |
+    |_ _:_____:_ _|
+    |   |     |   |
+    |   |     |   |
+    |_ _|_____|_ _|
+  r |   :     :   |
+    |___:_____:___|
+      r    bw   r
+    <----tilew---->
+*/
+
+__global__ void cudaProcess(unsigned int *g_odata, int imgw, int imgh,
+                            int tilew, int r, float threshold, float highlight,
+                            hipTextureObject_t inTex) {
+  // Handle to thread block group
+  cg::thread_block cta = cg::this_thread_block();
+  extern __shared__ uchar4 sdata[];
+
+  int tx = threadIdx.x;
+  int ty = threadIdx.y;
+  int bw = blockDim.x;
+  int bh = blockDim.y;
+  int x = blockIdx.x * bw + tx;
+  int y = blockIdx.y * bh + ty;
+
+#if 0
+    uchar4 c4 = getPixel(x, y);
+    g_odata[y*imgw+x] = rgbToInt(c4.z, c4.y, c4.x);
+#else
+  // copy tile to shared memory
+  // center region
+  SMEM(r + tx, r + ty) = getPixel(x, y, inTex);
+
+  // borders
+  if (threadIdx.x < r) {
+    // left
+    SMEM(tx, r + ty) = getPixel(x - r, y, inTex);
+    // right
+    SMEM(r + bw + tx, r + ty) = getPixel(x + bw, y, inTex);
+  }
+
+  if (threadIdx.y < r) {
+    // top
+    SMEM(r + tx, ty) = getPixel(x, y - r, inTex);
+    // bottom
+    SMEM(r + tx, r + bh + ty) = getPixel(x, y + bh, inTex);
+  }
+
+  // load corners
+  if ((threadIdx.x < r) && (threadIdx.y < r)) {
+    // tl
+    SMEM(tx, ty) = getPixel(x - r, y - r, inTex);
+    // bl
+    SMEM(tx, r + bh + ty) = getPixel(x - r, y + bh, inTex);
+    // tr
+    SMEM(r + bw + tx, ty) = getPixel(x + bh, y - r, inTex);
+    // br
+    SMEM(r + bw + tx, r + bh + ty) = getPixel(x + bw, y + bh, inTex);
+  }
+
+  // wait for loads to complete
+  cg::sync(cta);
+
+  // perform convolution
+  float rsum = 0.0f;
+  float gsum = 0.0f;
+  float bsum = 0.0f;
+  float samples = 0.0f;
+
+  for (int dy = -r; dy <= r; dy++) {
+    for (int dx = -r; dx <= r; dx++) {
+#if 0
+            // try this to see the benefit of using shared memory
+            uchar4 pixel = getPixel(x+dx, y+dy);
+#else
+      uchar4 pixel = SMEM(r + tx + dx, r + ty + dy);
+#endif
+
+      // only sum pixels within disc-shaped kernel
+      float l = dx * dx + dy * dy;
+
+      if (l <= r * r) {
+        float r = float(pixel.x);
+        float g = float(pixel.y);
+        float b = float(pixel.z);
+#if 1
+        // brighten highlights
+        float lum = (r + g + b) / (255 * 3);
+
+        if (lum > threshold) {
+          r *= highlight;
+          g *= highlight;
+          b *= highlight;
+        }
+
+#endif
+        rsum += r;
+        gsum += g;
+        bsum += b;
+        samples += 1.0f;
+      }
+    }
+  }
+
+  rsum /= samples;
+  gsum /= samples;
+  bsum /= samples;
+  // ABGR
+  g_odata[y * imgw + x] = rgbToInt(rsum, gsum, bsum);
+// g_odata[y*imgw+x] = rgbToInt(x,y,0);
+#endif
+}
+
+extern "C" void launch_cudaProcess(dim3 grid, dim3 block, int sbytes,
+                                   hipArray *g_data_array,
+                                   unsigned int *g_odata, int imgw, int imgh,
+                                   int tilew, int radius, float threshold,
+                                   float highlight) {
+  struct hipChannelFormatDesc desc;
+  HIPCHECK(hipGetChannelDesc(&desc, g_data_array));
+
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = g_data_array;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModePoint;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(
+      hipCreateTextureObject(&inTexObject, &texRes, &texDescr, NULL));
+
+#if 0
+    printf("CUDA Array channel descriptor, bits per component:\n");
+    printf("X %d Y %d Z %d W %d, kind %d\n",
+           desc.x,desc.y,desc.z,desc.w,desc.f);
+
+    printf("Possible values for channel format kind: i %d, u%d, f%d:\n",
+           hipChannelFormatKindSigned, hipChannelFormatKindUnsigned,
+           hipChannelFormatKindFloat);
+#endif
+
+// printf("\n");
+#ifdef GPU_PROFILING
+  StopWatchInterface *timer = 0;
+  sdkCreateTimer(&timer);
+
+  int nIter = 30;
+
+  for (int i = -1; i < nIter; ++i) {
+    if (i == 0) {
+      sdkStartTimer(&timer);
+    }
+
+#endif
+
+    cudaProcess<<<grid, block, sbytes>>>(g_odata, imgw, imgh,
+                                         block.x + (2 * radius), radius, 0.8f,
+                                         4.0f, inTexObject);
+
+#ifdef GPU_PROFILING
+  }
+
+  hipDeviceSynchronize();
+  sdkStopTimer(&timer);
+  double dSeconds = sdkGetTimerValue(&timer) / ((double)nIter * 1000.0);
+  double dNumTexels = (double)imgw * (double)imgh;
+  double mtexps = 1.0e-6 * dNumTexels / dSeconds;
+
+  if (radius == 4) {
+    printf("\n");
+    printf(
+        "postprocessGL, Throughput = %.4f MTexels/s, Time = %.5f s, Size = "
+        "%.0f Texels, NumDevsUsed = %d, Workgroup = %u\n",
+        mtexps, dSeconds, dNumTexels, 1, block.x * block.y);
+  }
+
+#endif
+}
+  }
+
+#endif
+}
diff --git a/src/samples/Samples/5_Domain_Specific/quasirandomGenerator_nvrtc/quasirandomGenerator_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/quasirandomGenerator_nvrtc/quasirandomGenerator_kernel.cu.hip
index e69de29..1e51076 100755
--- a/src/samples/Samples/5_Domain_Specific/quasirandomGenerator_nvrtc/quasirandomGenerator_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/quasirandomGenerator_nvrtc/quasirandomGenerator_kernel.cu.hip
@@ -0,0 +1,160 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef QUASIRANDOMGENERATOR_KERNEL_CUH
+#define QUASIRANDOMGENERATOR_KERNEL_CUH
+
+#include "quasirandomGenerator_common.h"
+
+// Fast integer multiplication
+#define MUL(a, b) __umul24(a, b)
+
+////////////////////////////////////////////////////////////////////////////////
+// Niederreiter quasirandom number generation kernel
+////////////////////////////////////////////////////////////////////////////////
+__constant__ unsigned int c_Table[QRNG_DIMENSIONS][QRNG_RESOLUTION];
+
+extern "C" __global__ void quasirandomGeneratorKernel(float *d_Output,
+                                                      unsigned int seed,
+                                                      unsigned int N) {
+  unsigned int *dimBase = &c_Table[threadIdx.y][0];
+  unsigned int tid = MUL(blockDim.x, blockIdx.x) + threadIdx.x;
+  unsigned int threadN = MUL(blockDim.x, gridDim.x);
+
+  for (unsigned int pos = tid; pos < N; pos += threadN) {
+    unsigned int result = 0;
+    unsigned int data = seed + pos;
+
+    for (int bit = 0; bit < QRNG_RESOLUTION; bit++, data >>= 1)
+      if (data & 1) {
+        result ^= dimBase[bit];
+      }
+
+    d_Output[MUL(threadIdx.y, N) + pos] = (float)(result + 1) * INT_SCALE;
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Moro's Inverse Cumulative Normal Distribution function approximation
+////////////////////////////////////////////////////////////////////////////////
+__device__ inline float MoroInvCNDgpu(unsigned int x) {
+  const float a1 = 2.50662823884f;
+  const float a2 = -18.61500062529f;
+  const float a3 = 41.39119773534f;
+  const float a4 = -25.44106049637f;
+  const float b1 = -8.4735109309f;
+  const float b2 = 23.08336743743f;
+  const float b3 = -21.06224101826f;
+  const float b4 = 3.13082909833f;
+  const float c1 = 0.337475482272615f;
+  const float c2 = 0.976169019091719f;
+  const float c3 = 0.160797971491821f;
+  const float c4 = 2.76438810333863E-02f;
+  const float c5 = 3.8405729373609E-03f;
+  const float c6 = 3.951896511919E-04f;
+  const float c7 = 3.21767881768E-05f;
+  const float c8 = 2.888167364E-07f;
+  const float c9 = 3.960315187E-07f;
+
+  float z;
+
+  bool negate = false;
+
+  // Ensure the conversion to floating point will give a value in the
+  // range (0,0.5] by restricting the input to the bottom half of the
+  // input domain. We will later reflect the result if the input was
+  // originally in the top half of the input domain
+  if (x >= 0x80000000UL) {
+    x = 0xffffffffUL - x;
+    negate = true;
+  }
+
+  // x is now in the range [0,0x80000000) (i.e. [0,0x7fffffff])
+  // Convert to floating point in (0,0.5]
+  const float x1 = 1.0f / static_cast<float>(0xffffffffUL);
+  const float x2 = x1 / 2.0f;
+  float p1 = x * x1 + x2;
+  // Convert to floating point in (-0.5,0]
+  float p2 = p1 - 0.5f;
+
+  // The input to the Moro inversion is p2 which is in the range
+  // (-0.5,0]. This means that our output will be the negative side
+  // of the bell curve (which we will reflect if "negate" is true).
+
+  // Main body of the bell curve for |p| < 0.42
+  if (p2 > -0.42f) {
+    z = p2 * p2;
+    z = p2 * (((a4 * z + a3) * z + a2) * z + a1) /
+        ((((b4 * z + b3) * z + b2) * z + b1) * z + 1.0f);
+  }
+  // Special case (Chebychev) for tail
+  else {
+    z = __logf(-__logf(p1));
+    z = -(c1 + z * (c2 + z * (c3 + z * (c4 + z * (c5 + z * (c6 + z * 
+        (c7 + z * (c8 + z * c9))))))));
+  }
+
+  // If the original input (x) was in the top half of the range, reflect
+  // to get the positive side of the bell curve
+
+  return negate ? -z : z;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Main kernel. Choose between transforming
+// input sequence and uniform ascending (0, 1) sequence
+////////////////////////////////////////////////////////////////////////////////
+
+extern "C" __global__ void inverseCNDKernel(float *d_Output,
+                                            unsigned int pathN) {
+  unsigned int distance = ((unsigned int)-1) / (pathN + 1);
+  unsigned int tid = MUL(blockDim.x, blockIdx.x) + threadIdx.x;
+  unsigned int threadN = MUL(blockDim.x, gridDim.x);
+
+  // Transform input number sequence if it's supplied
+  if (0)  // d_Input)
+  {
+    /*
+      for (unsigned int pos = tid; pos < pathN; pos += threadN)
+      {
+          unsigned int d = d_Input[pos];
+          d_Output[pos] = (float)MoroInvCNDgpu(d);
+      }
+      */
+  }
+  // Else generate input uniformly placed samples on the fly
+  // and write to destination
+  else {
+    for (unsigned int pos = tid; pos < pathN; pos += threadN) {
+      unsigned int d = (pos + 1) * distance;
+      d_Output[pos] = (float)MoroInvCNDgpu(d);
+    }
+  }
+}
+
+#endif
diff --git a/src/samples/Samples/5_Domain_Specific/recursiveGaussian/recursiveGaussian_cuda.cu.hip b/src/samples/Samples/5_Domain_Specific/recursiveGaussian/recursiveGaussian_cuda.cu.hip
index e69de29..44ae3c1 100755
--- a/src/samples/Samples/5_Domain_Specific/recursiveGaussian/recursiveGaussian_cuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/recursiveGaussian/recursiveGaussian_cuda.cu.hip
@@ -0,0 +1,160 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+  Recursive Gaussian filter
+  sgreen 8/1/08
+
+  This code sample implements a Gaussian blur using Deriche's recursive method:
+  http://citeseer.ist.psu.edu/deriche93recursively.html
+
+  This is similar to the box filter sample in the SDK, but it uses the previous
+  outputs of the filter as well as the previous inputs. This is also known as an
+  IIR (infinite impulse response) filter, since its response to an input impulse
+  can last forever.
+
+  The main advantage of this method is that the execution time is independent of
+  the filter width.
+
+  The GPU processes columns of the image in parallel. To avoid uncoalesced reads
+  for the row pass we transpose the image and then transpose it back again
+  afterwards.
+
+  The implementation is based on code from the CImg library:
+  http://cimg.sourceforge.net/
+  Thanks to David Tschumperl and all the CImg contributors!
+*/
+
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+
+#include <hip/hip_runtime.h>
+#include "helper_cuda_hipified.h"
+#include <helper_math.h>
+
+#include "recursiveGaussian_kernel.cuh"
+
+#define USE_SIMPLE_FILTER 0
+
+// Round a / b to nearest higher integer value
+int iDivUp(int a, int b) { return (a % b != 0) ? (a / b + 1) : (a / b); }
+
+/*
+  Transpose a 2D array (see SDK transpose example)
+*/
+extern "C" void transpose(uint *d_src, uint *d_dest, uint width, int height) {
+  dim3 grid(iDivUp(width, BLOCK_DIM), iDivUp(height, BLOCK_DIM), 1);
+  dim3 threads(BLOCK_DIM, BLOCK_DIM, 1);
+  d_transpose<<<grid, threads>>>(d_dest, d_src, width, height);
+  getLastCudaError("Kernel execution failed");
+}
+
+/*
+  Perform Gaussian filter on a 2D image using CUDA
+
+  Parameters:
+  d_src  - pointer to input image in device memory
+  d_dest - pointer to destination image in device memory
+  d_temp - pointer to temporary storage in device memory
+  width  - image width
+  height - image height
+  sigma  - sigma of Gaussian
+  order  - filter order (0, 1 or 2)
+*/
+
+// 8-bit RGBA version
+extern "C" void gaussianFilterRGBA(uint *d_src, uint *d_dest, uint *d_temp,
+                                   int width, int height, float sigma,
+                                   int order, int nthreads) {
+  // compute filter coefficients
+  const float nsigma = sigma < 0.1f ? 0.1f : sigma, alpha = 1.695f / nsigma,
+              ema = (float)std::exp(-alpha), ema2 = (float)std::exp(-2 * alpha),
+              b1 = -2 * ema, b2 = ema2;
+
+  float a0 = 0, a1 = 0, a2 = 0, a3 = 0, coefp = 0, coefn = 0;
+
+  switch (order) {
+    case 0: {
+      const float k = (1 - ema) * (1 - ema) / (1 + 2 * alpha * ema - ema2);
+      a0 = k;
+      a1 = k * (alpha - 1) * ema;
+      a2 = k * (alpha + 1) * ema;
+      a3 = -k * ema2;
+    } break;
+
+    case 1: {
+      const float k = (1 - ema) * (1 - ema) / ema;
+      a0 = k * ema;
+      a1 = a3 = 0;
+      a2 = -a0;
+    } break;
+
+    case 2: {
+      const float ea = (float)std::exp(-alpha),
+                  k = -(ema2 - 1) / (2 * alpha * ema),
+                  kn = (-2 * (-1 + 3 * ea - 3 * ea * ea + ea * ea * ea) /
+                        (3 * ea + 1 + 3 * ea * ea + ea * ea * ea));
+      a0 = kn;
+      a1 = -kn * (1 + k * alpha) * ema;
+      a2 = kn * (1 - k * alpha) * ema;
+      a3 = -kn * ema2;
+    } break;
+
+    default:
+      fprintf(stderr, "gaussianFilter: invalid order parameter!\n");
+      return;
+  }
+
+  coefp = (a0 + a1) / (1 + b1 + b2);
+  coefn = (a2 + a3) / (1 + b1 + b2);
+
+// process columns
+#if USE_SIMPLE_FILTER
+  d_simpleRecursive_rgba<<<iDivUp(width, nthreads), nthreads>>>(
+      d_src, d_temp, width, height, ema);
+#else
+  d_recursiveGaussian_rgba<<<iDivUp(width, nthreads), nthreads>>>(
+      d_src, d_temp, width, height, a0, a1, a2, a3, b1, b2, coefp, coefn);
+#endif
+  getLastCudaError("Kernel execution failed");
+
+  transpose(d_temp, d_dest, width, height);
+  getLastCudaError("transpose: Kernel execution failed");
+
+// process rows
+#if USE_SIMPLE_FILTER
+  d_simpleRecursive_rgba<<<iDivUp(height, nthreads), nthreads>>>(
+      d_dest, d_temp, height, width, ema);
+#else
+  d_recursiveGaussian_rgba<<<iDivUp(height, nthreads), nthreads>>>(
+      d_dest, d_temp, height, width, a0, a1, a2, a3, b1, b2, coefp, coefn);
+#endif
+  getLastCudaError("Kernel execution failed");
+
+  transpose(d_temp, d_dest, height, width);
+}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleD3D10/simpleD3D10_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleD3D10/simpleD3D10_kernel.cu.hip
index e69de29..50d3b9b 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleD3D10/simpleD3D10_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleD3D10/simpleD3D10_kernel.cu.hip
@@ -0,0 +1,86 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* This example demonstrates how to use the CUDA Direct3D bindings with the
+ * runtime API.
+ * Device code.
+ */
+
+#ifndef _SIMPLED3D_KERNEL_CU_
+#define _SIMPLED3D_KERNEL_CU_
+
+// includes, C string library
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+// includes, cuda
+#include <hip/hip_runtime.h>
+#include <builtin_types.h>
+#include <hip/hip_runtime_api.h>
+
+///////////////////////////////////////////////////////////////////////////////
+//! Simple kernel to modify vertex positions in sine wave pattern
+//! @param pos  pos in global memory
+///////////////////////////////////////////////////////////////////////////////
+__global__ void kernel(float4 *pos, unsigned int width, unsigned int height,
+                       float time) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  // calculate uv coordinates
+  float u = x / (float)width;
+  float v = y / (float)height;
+  u = u * 2.0f - 1.0f;
+  v = v * 2.0f - 1.0f;
+
+  // calculate simple sine wave pattern
+  float freq = 4.0f;
+  float w = sinf(u * freq + time) * cosf(v * freq + time) * 0.5f;
+
+  // write output vertex
+  pos[y * width + x] = make_float4(u, w, v, __int_as_float(0xff00ff00));
+}
+
+extern "C" void simpleD3DKernel(float4 *pos, unsigned int width,
+                                unsigned int height, float time) {
+  hipError_t error = hipSuccess;
+
+  dim3 block(8, 8, 1);
+  dim3 grid(width / block.x, height / block.y, 1);
+
+  kernel<<<grid, block>>>(pos, width, height, time);
+
+  error = hipGetLastError();
+
+  if (error != hipSuccess) {
+    printf("kernel() failed to launch error = %d\n", error);
+  }
+}
+
+#endif  // #ifndef _SIMPLED3D_KERNEL_CU_
diff --git a/src/samples/Samples/5_Domain_Specific/simpleD3D10RenderTarget/simpleD3D10RenderTarget_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleD3D10RenderTarget/simpleD3D10RenderTarget_kernel.cu.hip
index e69de29..f6c6156 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleD3D10RenderTarget/simpleD3D10RenderTarget_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleD3D10RenderTarget/simpleD3D10RenderTarget_kernel.cu.hip
@@ -0,0 +1,225 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* This example demonstrates how to use the CUDA Direct3D bindings with the
+ * runtime API.
+ * Device code.
+ */
+
+#ifndef SIMPLED3D10RENDERTARGET_KERNEL_CU
+#define SIMPLED3D10RENDERTARGET_KERNEL_CU
+
+// includes, C string library
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+// includes, cuda
+#include <hip/hip_runtime.h>
+#include <builtin_types.h>
+#include <hip/hip_runtime_api.h>
+
+// includes, project
+#include <helper_cuda.h>  // includes hip/hip_runtime.h and hip/hip_runtime_api.h
+//#include "HIPCHECK"
+
+#define BIN_COUNT 256
+#define HISTOGRAM_SIZE (BIN_COUNT * sizeof(unsigned int))
+
+texture<uchar4, 2, hipReadModeElementType> colorTex;
+
+////////////////////////////////////////////////////////////////////////////////
+// GPU-specific definitions
+////////////////////////////////////////////////////////////////////////////////
+// Fast mul on G8x / G9x / G100
+#define IMUL(a, b) __mul24(a, b)
+
+// Machine warp size
+// G80's warp size is 32 threads
+#define WARP_LOG2SIZE 5
+
+// Warps in thread block for histogram256Kernel()
+#define WARP_N 6
+
+// Corresponding thread block size in threads for histogram256Kernel()
+#define THREAD_N (WARP_N << WARP_LOG2SIZE)
+
+// Total histogram size (in counters) per thread block for histogram256Kernel()
+#define BLOCK_MEMORY (WARP_N * BIN_COUNT)
+
+// Thread block count for histogram256Kernel()
+#define BLOCK_N 64
+
+////////////////////////////////////////////////////////////////////////////////
+// If threadPos == threadIdx.x, there are always  4-way bank conflicts,
+// since each group of 16 threads (half-warp) accesses different bytes,
+// but only within 4 shared memory banks. Having shuffled bits of threadIdx.x
+// as in histogram64GPU(), each half-warp accesses different shared memory banks
+// avoiding any bank conflicts at all.
+// Refer to the supplied whitepaper for detailed explanations.
+////////////////////////////////////////////////////////////////////////////////
+__device__ inline void addData256(volatile unsigned int *s_WarpHist,
+                                  unsigned int data, unsigned int threadTag) {
+  unsigned int count;
+
+  do {
+    count = s_WarpHist[data] & 0x07FFFFFFU;
+    count = threadTag | (count + 1);
+    s_WarpHist[data] = count;
+  } while (s_WarpHist[data] != count);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Main histogram calculation kernel
+////////////////////////////////////////////////////////////////////////////////
+static __global__ void histogramTex256Kernel(unsigned int *d_Result,
+                                             unsigned int width,
+                                             unsigned int height, int dataN) {
+  // Current global thread index
+  const int globalTid = IMUL(blockIdx.x, blockDim.x) + threadIdx.x;
+  // Total number of threads in the compute grid
+  const int numThreads = IMUL(blockDim.x, gridDim.x);
+
+  // Thread tag for addData256()
+  // WARP_LOG2SIZE higher bits of counter values are tagged
+  // by lower WARP_LOG2SIZE threadID bits
+  const unsigned int threadTag = threadIdx.x << (32 - WARP_LOG2SIZE);
+
+  // Shared memory storage for each warp
+  volatile __shared__ unsigned int s_Hist[BLOCK_MEMORY];
+
+  // Current warp shared memory base
+  const int warpBase = (threadIdx.x >> WARP_LOG2SIZE) * BIN_COUNT;
+
+  // Clear shared memory buffer for current thread block before processing
+  for (int pos = threadIdx.x; pos < BLOCK_MEMORY; pos += blockDim.x)
+    s_Hist[pos] = 0;
+
+  // Cycle through the entire data set, update subhistograms for each warp
+  __syncthreads();
+
+  for (int pos = globalTid; pos < dataN; pos += numThreads) {
+    // NOTE: check this... Not sure this is what needs to be done
+    int py = pos / width;
+    int px = pos - (py * width);
+    uchar4 data4 = tex2D(colorTex, px, py);
+
+    addData256(s_Hist + warpBase, (data4.x), threadTag);
+    addData256(s_Hist + warpBase, (data4.y), threadTag);
+    addData256(s_Hist + warpBase, (data4.z), threadTag);
+    addData256(s_Hist + warpBase, (data4.w), threadTag);
+  }
+
+  __syncthreads();
+
+  // Merge per-warp histograms into per-block and write to global memory
+  for (int pos = threadIdx.x; pos < BIN_COUNT; pos += blockDim.x) {
+    unsigned int sum = 0;
+
+    for (int base = 0; base < BLOCK_MEMORY; base += BIN_COUNT)
+      sum += s_Hist[base + pos] & 0x07FFFFFFU;
+
+    d_Result[blockIdx.x * BIN_COUNT + pos] = sum;
+  }
+}
+
+///////////////////////////////////////////////////////////////////////////////
+// Merge BLOCK_N subhistograms of BIN_COUNT bins into final histogram
+///////////////////////////////////////////////////////////////////////////////
+// gridDim.x   == BIN_COUNT
+// blockDim.x  == BLOCK_N
+// blockIdx.x  == bin counter processed by current block
+// threadIdx.x == subhistogram index
+static __global__ void mergeHistogramTex256Kernel(unsigned int *d_Result) {
+  __shared__ unsigned int data[BLOCK_N];
+
+  // Reads are uncoalesced, but this final stage takes
+  // only a fraction of total processing time
+  data[threadIdx.x] = d_Result[threadIdx.x * BIN_COUNT + blockIdx.x];
+
+  for (int stride = BLOCK_N / 2; stride > 0; stride >>= 1) {
+    __syncthreads();
+
+    if (threadIdx.x < stride) data[threadIdx.x] += data[threadIdx.x + stride];
+  }
+
+  if (threadIdx.x == 0) d_Result[blockIdx.x] = data[0];
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Host interface to GPU histogram
+////////////////////////////////////////////////////////////////////////////////
+
+extern "C" void checkCudaError() {
+  hipError_t err = hipGetLastError();
+
+  if (hipSuccess != err) {
+    fprintf(stderr, "Cuda error: %s.\n", hipGetErrorString(err));
+    exit(2);
+  }
+}
+
+// Maximum block count for histogram64kernel()
+// Limits input data size to 756MB
+// const int MAX_BLOCK_N = 16384;
+
+// Internal memory allocation
+// const int BLOCK_N2 = 32;
+
+extern "C" void createHistogramTex(unsigned int *h_Result, unsigned int width,
+                                   unsigned int height, hipArray *colorArray) {
+  hipBindTextureToArray(colorTex, colorArray);
+  checkCudaError();
+
+  histogramTex256Kernel<<<BLOCK_N, THREAD_N>>>(h_Result, width, height,
+                                               width * height / 4);
+  checkCudaError();
+
+  mergeHistogramTex256Kernel<<<BIN_COUNT, BLOCK_N>>>(h_Result);
+  checkCudaError();
+
+  hipUnbindTexture(colorTex);
+  checkCudaError();
+
+#if 0
+    // Dummy fill test
+    unsigned int toto[256];
+
+    for (int i=0; i<256; i++)
+    {
+        toto[i] = i * 100;
+    }
+    hipMemcpy(h_Result, toto, HISTOGRAM_SIZE, hipMemcpyHostToDevice);
+#endif
+  checkCudaError();
+}
+
+extern "C" void bindArrayToTexture(hipArray *pArray) {}
+
+#endif  // #ifndef SIMPLED3D10RENDERTARGET_KERNEL_CU
+NEL_CU
diff --git a/src/samples/Samples/5_Domain_Specific/simpleD3D10Texture/texture_2d.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleD3D10Texture/texture_2d.cu.hip
index e69de29..7d933ab 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleD3D10Texture/texture_2d.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleD3D10Texture/texture_2d.cu.hip
@@ -0,0 +1,79 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+#define PI 3.1415926536f
+
+/*
+ * Paint a 2D texture with a moving red/green hatch pattern on a
+ * strobing blue background.  Note that this kernel reads to and
+ * writes from the texture, hence why this texture was not mapped
+ * as WriteDiscard.
+ */
+__global__ void cuda_kernel_texture_2d(unsigned char *surface, int width,
+                                       int height, size_t pitch, float t) {
+  int x = blockIdx.x * blockDim.x + threadIdx.x;
+  int y = blockIdx.y * blockDim.y + threadIdx.y;
+  float *pixel;
+
+  // in the case where, due to quantization into grids, we have
+  // more threads than pixels, skip the threads which don't
+  // correspond to valid pixels
+  if (x >= width || y >= height) return;
+
+  // get a pointer to the pixel at (x,y)
+  pixel = (float *)(surface + y * pitch) + 4 * x;
+
+  // populate it
+  float value_x = 0.5f + 0.5f * cos(t + 10.0f * ((2.0f * x) / width - 1.0f));
+  float value_y = 0.5f + 0.5f * cos(t + 10.0f * ((2.0f * y) / height - 1.0f));
+  pixel[0] = 0.5 * pixel[0] + 0.5 * pow(value_x, 3.0f);  // red
+  pixel[1] = 0.5 * pixel[1] + 0.5 * pow(value_y, 3.0f);  // green
+  pixel[2] = 0.5f + 0.5f * cos(t);                       // blue
+  pixel[3] = 1;                                          // alpha
+}
+
+extern "C" void cuda_texture_2d(void *surface, int width, int height,
+                                size_t pitch, float t) {
+  hipError_t error = hipSuccess;
+
+  dim3 Db = dim3(16, 16);  // block dimensions are fixed to be 256 threads
+  dim3 Dg = dim3((width + Db.x - 1) / Db.x, (height + Db.y - 1) / Db.y);
+
+  cuda_kernel_texture_2d<<<Dg, Db>>>((unsigned char *)surface, width, height,
+                                     pitch, t);
+
+  error = hipGetLastError();
+
+  if (error != hipSuccess) {
+    printf("cuda_kernel_texture_2d() failed to launch error = %d\n", error);
+  }
+}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleD3D10Texture/texture_3d.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleD3D10Texture/texture_3d.cu.hip
index e69de29..059c524 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleD3D10Texture/texture_3d.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleD3D10Texture/texture_3d.cu.hip
@@ -0,0 +1,77 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+/*
+ * Paint a 3D texture with a gradient in X (blue) and Z (green), and have every
+ * other Z slice have full red.
+ */
+__global__ void cuda_kernel_texture_3d(unsigned char *surface, int width,
+                                       int height, int depth, size_t pitch,
+                                       size_t pitchSlice, float t) {
+  int x = blockIdx.x * blockDim.x + threadIdx.x;
+  int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  // in the case where, due to quantization into grids, we have
+  // more threads than pixels, skip the threads which don't
+  // correspond to valid pixels
+  if (x >= width || y >= height) return;
+
+  // walk across the Z slices of this texture.  it should be noted that
+  // this is far from optimal data access.
+  for (int z = 0; z < depth; ++z) {
+    // get a pointer to this pixel
+    unsigned char *pixel = surface + z * pitchSlice + y * pitch + 4 * x;
+    pixel[0] = (unsigned char)(255.f * (0.5f + 0.5f * 
+        cos(t + (x * x + y * y + z * z) * 0.0001f * 3.14f)));  // red
+    pixel[1] = (unsigned char)(255.f * (0.5f + 0.5f * 
+        sin(t + (x * x + y * y + z * z) * 0.0001f * 3.14f)));  // green
+    pixel[2] = (unsigned char)0;                               // blue
+    pixel[3] = 255;                                            // alpha
+  }
+}
+
+extern "C" void cuda_texture_3d(void *surface, int width, int height, int depth,
+                                size_t pitch, size_t pitchSlice, float t) {
+  hipError_t error = hipSuccess;
+
+  dim3 Db = dim3(16, 16);  // block dimensions are fixed to be 256 threads
+  dim3 Dg = dim3((width + Db.x - 1) / Db.x, (height + Db.y - 1) / Db.y);
+
+  cuda_kernel_texture_3d<<<Dg, Db>>>((unsigned char *)surface, width, height,
+                                     depth, pitch, pitchSlice, t);
+
+  error = hipGetLastError();
+
+  if (error != hipSuccess) {
+    printf("cuda_kernel_texture_3d() failed to launch error = %d\n", error);
+  }
+}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleD3D10Texture/texture_cube.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleD3D10Texture/texture_cube.cu.hip
index 39ef766..e23b2a0 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleD3D10Texture/texture_cube.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleD3D10Texture/texture_cube.cu.hip
@@ -27,8 +27,6 @@
  */
 
 #include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
 #include <stdlib.h>
 #include <string.h>
 
diff --git a/src/samples/Samples/5_Domain_Specific/simpleD3D11/sinewave_cuda.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleD3D11/sinewave_cuda.cu.hip
index e69de29..f6b7480 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleD3D11/sinewave_cuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleD3D11/sinewave_cuda.cu.hip
@@ -0,0 +1,137 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <stdio.h>
+#include "ShaderStructs.h"
+#include "helper_cuda.h"
+#include "sinewave_cuda.h"
+
+__global__ void sinewave_gen_kernel(Vertex *vertices, unsigned int width, unsigned int height, float time)
+{
+    unsigned int x = blockIdx.x*blockDim.x + threadIdx.x;
+    unsigned int y = blockIdx.y*blockDim.y + threadIdx.y;
+
+    // calculate uv coordinates
+    float u = x / (float) width;
+    float v = y / (float) height;
+    u = u*2.0f - 1.0f;
+    v = v*2.0f - 1.0f;
+
+    // calculate simple sine wave pattern
+    float freq = 4.0f;
+    float w = sinf(u*freq + time) * cosf(v*freq + time) * 0.5f;
+
+    if (y < height && x < width)
+    {
+        // write output vertex
+        vertices[y*width+x].position.x = u;
+        vertices[y*width+x].position.y = w;
+        vertices[y*width+x].position.z = v;
+        vertices[y*width+x].color.x = 1.0f;
+        vertices[y*width+x].color.y = 0.0f;
+        vertices[y*width+x].color.z = 0.0f;
+        vertices[y*width + x].color.w = 0.0f;
+    }
+}
+
+Vertex* cudaImportVertexBuffer(void*sharedHandle, hipExternalMemory_t &externalMemory, int meshWidth, int meshHeight)
+{
+    hipExternalMemoryHandleDesc externalMemoryHandleDesc;
+    memset(&externalMemoryHandleDesc, 0, sizeof(externalMemoryHandleDesc));
+
+    externalMemoryHandleDesc.type = hipExternalMemoryHandleTypeD3D11ResourceKmt;
+    externalMemoryHandleDesc.size = sizeof(Vertex) * meshHeight * meshWidth;
+    externalMemoryHandleDesc.flags = cudaExternalMemoryDedicated;
+    externalMemoryHandleDesc.handle.win32.handle = sharedHandle;
+
+    HIPCHECK(hipImportExternalMemory(&externalMemory, &externalMemoryHandleDesc));
+
+    hipExternalMemoryBufferDesc externalMemoryBufferDesc;
+    memset(&externalMemoryBufferDesc, 0, sizeof(externalMemoryBufferDesc));
+    externalMemoryBufferDesc.offset = 0;
+    externalMemoryBufferDesc.size = sizeof(Vertex) * meshHeight * meshWidth;
+    externalMemoryBufferDesc.flags = 0;
+
+    Vertex* cudaDevVertptr = NULL;
+    HIPCHECK(hipExternalMemoryGetMappedBuffer((void**)&cudaDevVertptr, externalMemory, &externalMemoryBufferDesc));
+
+    return cudaDevVertptr;
+}
+
+void cudaImportKeyedMutex(void*sharedHandle, hipExternalSemaphore_t &extSemaphore)
+{
+    hipExternalSemaphoreHandleDesc extSemaDesc;
+    memset(&extSemaDesc, 0, sizeof(extSemaDesc));
+    extSemaDesc.type = cudaExternalSemaphoreHandleTypeKeyedMutexKmt;
+    extSemaDesc.handle.win32.handle = sharedHandle;
+    extSemaDesc.flags = 0;
+
+    HIPCHECK(hipImportExternalSemaphore(&extSemaphore, &extSemaDesc));
+}
+
+void cudaAcquireSync(hipExternalSemaphore_t &extSemaphore, uint64_t key, unsigned int timeoutMs, hipStream_t streamToRun)
+{
+    hipExternalSemaphoreWaitParams extSemWaitParams;
+    memset(&extSemWaitParams, 0, sizeof(extSemWaitParams));
+    extSemWaitParams.params.keyedMutex.key = key;
+    extSemWaitParams.params.keyedMutex.timeoutMs = timeoutMs;
+
+    HIPCHECK(hipWaitExternalSemaphoresAsync(&extSemaphore, &extSemWaitParams, 1, streamToRun));
+}
+
+void cudaReleaseSync(hipExternalSemaphore_t &extSemaphore, uint64_t key, hipStream_t streamToRun)
+{
+    hipExternalSemaphoreSignalParams extSemSigParams;
+    memset(&extSemSigParams, 0, sizeof(extSemSigParams));
+    extSemSigParams.params.keyedMutex.key = key;
+
+    HIPCHECK(hipSignalExternalSemaphoresAsync(&extSemaphore, &extSemSigParams, 1, streamToRun));
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run the Cuda part of the computation
+////////////////////////////////////////////////////////////////////////////////
+void RunSineWaveKernel(hipExternalSemaphore_t &extSemaphore, uint64_t &key, unsigned int timeoutMs, 
+                        size_t mesh_width, size_t mesh_height, Vertex *cudaDevVertptr, hipStream_t streamToRun)
+{
+    static float t = 0.0f;
+    cudaAcquireSync(extSemaphore, key++, timeoutMs, streamToRun);
+
+    dim3 block(16, 16, 1);
+    dim3 grid(mesh_width / 16, mesh_height / 16, 1);
+    sinewave_gen_kernel<<< grid, block, 0, streamToRun >>>(cudaDevVertptr, mesh_width, mesh_height, t);
+    getLastCudaError("sinewave_gen_kernel execution failed.\n");
+
+    cudaReleaseSync(extSemaphore, key, streamToRun);
+    t += 0.01f;
+}
+
+, streamToRun);
+    t += 0.01f;
+}
+
diff --git a/src/samples/Samples/5_Domain_Specific/simpleD3D11Texture/texture_2d.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleD3D11Texture/texture_2d.cu.hip
index e69de29..7d933ab 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleD3D11Texture/texture_2d.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleD3D11Texture/texture_2d.cu.hip
@@ -0,0 +1,79 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+#define PI 3.1415926536f
+
+/*
+ * Paint a 2D texture with a moving red/green hatch pattern on a
+ * strobing blue background.  Note that this kernel reads to and
+ * writes from the texture, hence why this texture was not mapped
+ * as WriteDiscard.
+ */
+__global__ void cuda_kernel_texture_2d(unsigned char *surface, int width,
+                                       int height, size_t pitch, float t) {
+  int x = blockIdx.x * blockDim.x + threadIdx.x;
+  int y = blockIdx.y * blockDim.y + threadIdx.y;
+  float *pixel;
+
+  // in the case where, due to quantization into grids, we have
+  // more threads than pixels, skip the threads which don't
+  // correspond to valid pixels
+  if (x >= width || y >= height) return;
+
+  // get a pointer to the pixel at (x,y)
+  pixel = (float *)(surface + y * pitch) + 4 * x;
+
+  // populate it
+  float value_x = 0.5f + 0.5f * cos(t + 10.0f * ((2.0f * x) / width - 1.0f));
+  float value_y = 0.5f + 0.5f * cos(t + 10.0f * ((2.0f * y) / height - 1.0f));
+  pixel[0] = 0.5 * pixel[0] + 0.5 * pow(value_x, 3.0f);  // red
+  pixel[1] = 0.5 * pixel[1] + 0.5 * pow(value_y, 3.0f);  // green
+  pixel[2] = 0.5f + 0.5f * cos(t);                       // blue
+  pixel[3] = 1;                                          // alpha
+}
+
+extern "C" void cuda_texture_2d(void *surface, int width, int height,
+                                size_t pitch, float t) {
+  hipError_t error = hipSuccess;
+
+  dim3 Db = dim3(16, 16);  // block dimensions are fixed to be 256 threads
+  dim3 Dg = dim3((width + Db.x - 1) / Db.x, (height + Db.y - 1) / Db.y);
+
+  cuda_kernel_texture_2d<<<Dg, Db>>>((unsigned char *)surface, width, height,
+                                     pitch, t);
+
+  error = hipGetLastError();
+
+  if (error != hipSuccess) {
+    printf("cuda_kernel_texture_2d() failed to launch error = %d\n", error);
+  }
+}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleD3D11Texture/texture_3d.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleD3D11Texture/texture_3d.cu.hip
index e69de29..321aa66 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleD3D11Texture/texture_3d.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleD3D11Texture/texture_3d.cu.hip
@@ -0,0 +1,79 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+/*
+ * Paint a 3D texture with a gradient in X (blue) and Z (green), and have every
+ * other Z slice have full red.
+ */
+__global__ void cuda_kernel_texture_3d(unsigned char *surface, int width,
+                                       int height, int depth, size_t pitch,
+                                       size_t pitchSlice, float t) {
+  int x = blockIdx.x * blockDim.x + threadIdx.x;
+  int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  // in the case where, due to quantization into grids, we have
+  // more threads than pixels, skip the threads which don't
+  // correspond to valid pixels
+  if (x >= width || y >= height) return;
+
+  // walk across the Z slices of this texture.  it should be noted that
+  // this is far from optimal data access.
+  for (int z = 0; z < depth; ++z) {
+    // get a pointer to this pixel
+    unsigned char *pixel = surface + z * pitchSlice + y * pitch + 4 * x;
+    pixel[0] =
+        (unsigned char)(255.f * (0.5f + 0.5f * 
+        cos(t + (x * x + y * y + z * z) * 0.0001f * 3.14f)));  // red
+    pixel[1] =
+        (unsigned char)(255.f * (0.5f + 0.5f * 
+        sin(t + (x * x + y * y + z * z) * 0.0001f * 3.14f)));  // green
+    pixel[2] = (unsigned char)0;                               // blue
+    pixel[3] = 255;                                            // alpha
+  }
+}
+
+extern "C" void cuda_texture_3d(void *surface, int width, int height, int depth,
+                                size_t pitch, size_t pitchSlice, float t) {
+  hipError_t error = hipSuccess;
+
+  dim3 Db = dim3(16, 16);  // block dimensions are fixed to be 256 threads
+  dim3 Dg = dim3((width + Db.x - 1) / Db.x, (height + Db.y - 1) / Db.y);
+
+  cuda_kernel_texture_3d<<<Dg, Db>>>((unsigned char *)surface, width, height,
+                                     depth, pitch, pitchSlice, t);
+
+  error = hipGetLastError();
+
+  if (error != hipSuccess) {
+    printf("cuda_kernel_texture_3d() failed to launch error = %d\n", error);
+  }
+}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleD3D11Texture/texture_cube.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleD3D11Texture/texture_cube.cu.hip
index e69de29..e23b2a0 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleD3D11Texture/texture_cube.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleD3D11Texture/texture_cube.cu.hip
@@ -0,0 +1,92 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+#define PI 3.1415926536f
+
+/*
+ * Paint a 2D surface with a moving bulls-eye pattern.  The "face" parameter
+ * selects
+ * between 6 different colors to use.  We will use a different color on each
+ * face of a
+ * cube map.
+ */
+__global__ void cuda_kernel_texture_cube(char *surface, int width, int height,
+                                         size_t pitch, int face, float t) {
+  int x = blockIdx.x * blockDim.x + threadIdx.x;
+  int y = blockIdx.y * blockDim.y + threadIdx.y;
+  unsigned char *pixel;
+
+  // in the case where, due to quantization into grids, we have
+  // more threads than pixels, skip the threads which don't
+  // correspond to valid pixels
+  if (x >= width || y >= height) return;
+
+  // get a pointer to this pixel
+  pixel = (unsigned char *)(surface + y * pitch) + 4 * x;
+
+  // populate it
+  float theta_x = (2.0f * x) / width - 1.0f;
+  float theta_y = (2.0f * y) / height - 1.0f;
+  float theta = 2.0f * PI * sqrt(theta_x * theta_x + theta_y * theta_y);
+  unsigned char value = 255 * (0.6f + 0.4f * cos(theta + t));
+
+  pixel[3] = 255;  // alpha
+
+  if (face % 2) {
+    pixel[0] =           // blue
+        pixel[1] =       // green
+        pixel[2] = 0.5;  // red
+    pixel[face / 2] = value;
+  } else {
+    pixel[0] =             // blue
+        pixel[1] =         // green
+        pixel[2] = value;  // red
+    pixel[face / 2] = 0.5;
+  }
+}
+
+extern "C" void cuda_texture_cube(void *surface, int width, int height,
+                                  size_t pitch, int face, float t) {
+  hipError_t error = hipSuccess;
+
+  dim3 Db = dim3(16, 16);  // block dimensions are fixed to be 256 threads
+  dim3 Dg = dim3((width + Db.x - 1) / Db.x, (height + Db.y - 1) / Db.y);
+
+  cuda_kernel_texture_cube<<<Dg, Db>>>((char *)surface, width, height, pitch,
+                                       face, t);
+
+  error = hipGetLastError();
+
+  if (error != hipSuccess) {
+    printf("cuda_kernel_texture_cube() failed to launch error = %d\n", error);
+  }
+}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleD3D12/sinewave_cuda.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleD3D12/sinewave_cuda.cu.hip
index e69de29..7220a2e 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleD3D12/sinewave_cuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleD3D12/sinewave_cuda.cu.hip
@@ -0,0 +1,70 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "ShaderStructs.h"
+
+__global__ void sinewave_gen_kernel(Vertex *vertices, unsigned int width,
+                                    unsigned int height, float time) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  // calculate uv coordinates
+  float u = x / (float)width;
+  float v = y / (float)height;
+  u = u * 2.0f - 1.0f;
+  v = v * 2.0f - 1.0f;
+
+  // calculate simple sine wave pattern
+  float freq = 4.0f;
+  float w = sinf(u * freq + time) * cosf(v * freq + time) * 0.5f;
+
+  if (y < height && x < width) {
+    // write output vertex
+    vertices[y * width + x].position.x = u;
+    vertices[y * width + x].position.y = w;
+    vertices[y * width + x].position.z = v;
+    // vertices[y*width+x].position[3] = 1.0f;
+    vertices[y * width + x].color.x = 1.0f;
+    vertices[y * width + x].color.y = 0.0f;
+    vertices[y * width + x].color.z = 0.0f;
+    vertices[y * width + x].color.w = 0.0f;
+  }
+}
+
+// The host CPU Sinewave thread spawner
+void RunSineWaveKernel(size_t mesh_width, size_t mesh_height,
+                       Vertex *cudaDevVertptr, hipStream_t streamToRun,
+                       float AnimTime) {
+  dim3 block(16, 16, 1);
+  dim3 grid(mesh_width / 16, mesh_height / 16, 1);
+  Vertex *vertices = (Vertex *)cudaDevVertptr;
+  sinewave_gen_kernel<<<grid, block, 0, streamToRun>>>(vertices, mesh_width,
+                                                       mesh_height, AnimTime);
+
+  getLastCudaError("sinewave_gen_kernel execution failed.\n");
+}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleD3D9/simpleD3D9_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleD3D9/simpleD3D9_kernel.cu.hip
index e69de29..88c4a97 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleD3D9/simpleD3D9_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleD3D9/simpleD3D9_kernel.cu.hip
@@ -0,0 +1,80 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// This example demonstrates how to use the CUDA Direct3D bindings with the
+// runtime API.
+// Device code.
+
+#ifndef _SIMPLED3D_KERNEL_CU_
+#define _SIMPLED3D_KERNEL_CU_
+
+// includes, C string library
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+///////////////////////////////////////////////////////////////////////////////
+//! Simple kernel to modify vertex positions in sine wave pattern
+//! @param pos  pos in global memory
+///////////////////////////////////////////////////////////////////////////////
+__global__ void kernel(float4 *pos, unsigned int width, unsigned int height,
+                       float time) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  // calculate uv coordinates
+  float u = x / (float)width;
+  float v = y / (float)height;
+  u = u * 2.0f - 1.0f;
+  v = v * 2.0f - 1.0f;
+
+  // calculate simple sine wave pattern
+  float freq = 4.0f;
+  float w = sinf(u * freq + time) * cosf(v * freq + time) * 0.5f;
+
+  // write output vertex
+  pos[y * width + x] = make_float4(u, w, v, __int_as_float(0xff00ff00));
+}
+
+extern "C" void simpleD3DKernel(float4 *pos, unsigned int width,
+                                unsigned int height, float time) {
+  hipError_t error = hipSuccess;
+
+  dim3 block(8, 8, 1);
+  dim3 grid(width / block.x, height / block.y, 1);
+
+  kernel<<<grid, block>>>(pos, width, height, time);
+
+  error = hipGetLastError();
+
+  if (error != hipSuccess) {
+    printf("kernel() failed to launch error = %d\n", error);
+  }
+}
+
+#endif  // #ifndef _SIMPLED3D_KERNEL_CU_
diff --git a/src/samples/Samples/5_Domain_Specific/simpleD3D9Texture/texture_2d.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleD3D9Texture/texture_2d.cu.hip
index e69de29..7d933ab 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleD3D9Texture/texture_2d.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleD3D9Texture/texture_2d.cu.hip
@@ -0,0 +1,79 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+#define PI 3.1415926536f
+
+/*
+ * Paint a 2D texture with a moving red/green hatch pattern on a
+ * strobing blue background.  Note that this kernel reads to and
+ * writes from the texture, hence why this texture was not mapped
+ * as WriteDiscard.
+ */
+__global__ void cuda_kernel_texture_2d(unsigned char *surface, int width,
+                                       int height, size_t pitch, float t) {
+  int x = blockIdx.x * blockDim.x + threadIdx.x;
+  int y = blockIdx.y * blockDim.y + threadIdx.y;
+  float *pixel;
+
+  // in the case where, due to quantization into grids, we have
+  // more threads than pixels, skip the threads which don't
+  // correspond to valid pixels
+  if (x >= width || y >= height) return;
+
+  // get a pointer to the pixel at (x,y)
+  pixel = (float *)(surface + y * pitch) + 4 * x;
+
+  // populate it
+  float value_x = 0.5f + 0.5f * cos(t + 10.0f * ((2.0f * x) / width - 1.0f));
+  float value_y = 0.5f + 0.5f * cos(t + 10.0f * ((2.0f * y) / height - 1.0f));
+  pixel[0] = 0.5 * pixel[0] + 0.5 * pow(value_x, 3.0f);  // red
+  pixel[1] = 0.5 * pixel[1] + 0.5 * pow(value_y, 3.0f);  // green
+  pixel[2] = 0.5f + 0.5f * cos(t);                       // blue
+  pixel[3] = 1;                                          // alpha
+}
+
+extern "C" void cuda_texture_2d(void *surface, int width, int height,
+                                size_t pitch, float t) {
+  hipError_t error = hipSuccess;
+
+  dim3 Db = dim3(16, 16);  // block dimensions are fixed to be 256 threads
+  dim3 Dg = dim3((width + Db.x - 1) / Db.x, (height + Db.y - 1) / Db.y);
+
+  cuda_kernel_texture_2d<<<Dg, Db>>>((unsigned char *)surface, width, height,
+                                     pitch, t);
+
+  error = hipGetLastError();
+
+  if (error != hipSuccess) {
+    printf("cuda_kernel_texture_2d() failed to launch error = %d\n", error);
+  }
+}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleD3D9Texture/texture_cube.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleD3D9Texture/texture_cube.cu.hip
index e69de29..b74301a 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleD3D9Texture/texture_cube.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleD3D9Texture/texture_cube.cu.hip
@@ -0,0 +1,90 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+#define PI 3.1415926536f
+
+/*
+ * Paint a 2D surface with a moving bulls-eye pattern.  The "face" parameter
+ * selects  between 6 different colors to use.  We will use a different color on
+ * each face of a  cube map.
+ */
+__global__ void cuda_kernel_texture_cube(char *surface, int width, int height,
+                                         size_t pitch, int face, float t) {
+  int x = blockIdx.x * blockDim.x + threadIdx.x;
+  int y = blockIdx.y * blockDim.y + threadIdx.y;
+  unsigned char *pixel;
+
+  // in the case where, due to quantization into grids, we have
+  // more threads than pixels, skip the threads which don't
+  // correspond to valid pixels
+  if (x >= width || y >= height) return;
+
+  // get a pointer to this pixel
+  pixel = (unsigned char *)(surface + y * pitch) + 4 * x;
+
+  // populate it
+  float theta_x = (2.0f * x) / width - 1.0f;
+  float theta_y = (2.0f * y) / height - 1.0f;
+  float theta = 2.0f * PI * sqrt(theta_x * theta_x + theta_y * theta_y);
+  unsigned char value = 255 * (0.6f + 0.4f * cos(theta + t));
+
+  pixel[3] = 255;  // alpha
+
+  if (face % 2) {
+    pixel[0] =         // blue
+        pixel[1] =     // green
+        pixel[2] = 0;  // red
+    pixel[face / 2] = value;
+  } else {
+    pixel[0] =             // blue
+        pixel[1] =         // green
+        pixel[2] = value;  // red
+    pixel[face / 2] = 0;
+  }
+}
+
+extern "C" void cuda_texture_cube(void *surface, int width, int height,
+                                  size_t pitch, int face, float t) {
+  hipError_t error = hipSuccess;
+
+  dim3 Db = dim3(16, 16);  // block dimensions are fixed to be 256 threads
+  dim3 Dg = dim3((width + Db.x - 1) / Db.x, (height + Db.y - 1) / Db.y);
+
+  cuda_kernel_texture_cube<<<Dg, Db>>>((char *)surface, width, height, pitch,
+                                       face, t);
+
+  error = hipGetLastError();
+
+  if (error != hipSuccess) {
+    printf("cuda_kernel_texture_cube() failed to launch error = %d\n", error);
+  }
+}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleD3D9Texture/texture_volume.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleD3D9Texture/texture_volume.cu.hip
index e69de29..edfadad 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleD3D9Texture/texture_volume.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleD3D9Texture/texture_volume.cu.hip
@@ -0,0 +1,76 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+/*
+ * Paint a 3D texture with a gradient in X (blue) and Z (green), and have every
+ * other Z slice have full red.
+ */
+__global__ void cuda_kernel_texture_volume(unsigned char *surface, int width,
+                                           int height, int depth, size_t pitch,
+                                           size_t pitchSlice) {
+  int x = blockIdx.x * blockDim.x + threadIdx.x;
+  int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  // in the case where, due to quantization into grids, we have
+  // more threads than pixels, skip the threads which don't
+  // correspond to valid pixels
+  if (x >= width || y >= height) return;
+
+  // walk across the Z slices of this texture.  it should be noted that
+  // this is far from optimal data access.
+  for (int z = 0; z < depth; ++z) {
+    // get a pointer to this pixel
+    unsigned char *pixel = surface + z * pitchSlice + y * pitch + 4 * x;
+    pixel[0] = 255 * x / (width - 1);  // blue
+    pixel[1] = 255 * z / (depth - 1);  // green
+    pixel[2] = 255 * (z % 2);          // red
+    pixel[3] = 255;                    // alpha
+  }
+}
+
+extern "C" void cuda_texture_volume(void *surface, int width, int height,
+                                    int depth, size_t pitch, size_t pitchSlice,
+                                    float t) {
+  hipError_t error = hipSuccess;
+
+  dim3 Db = dim3(16, 16);  // block dimensions are fixed to be 256 threads
+  dim3 Dg = dim3((width + Db.x - 1) / Db.x, (height + Db.y - 1) / Db.y);
+
+  cuda_kernel_texture_volume<<<Dg, Db>>>((unsigned char *)surface, width,
+                                         height, depth, pitch, pitchSlice);
+
+  error = hipGetLastError();
+
+  if (error != hipSuccess) {
+    printf("cuda_kernel_texture_volume() failed to launch error = %d\n", error);
+  }
+}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleGL/simpleGL.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleGL/simpleGL.cu.hip
index e69de29..7773caf 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleGL/simpleGL.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleGL/simpleGL.cu.hip
@@ -0,0 +1,592 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+    This example demonstrates how to use the Cuda OpenGL bindings to
+    dynamically modify a vertex buffer using a Cuda kernel.
+
+    The steps are:
+    1. Create an empty vertex buffer object (VBO)
+    2. Register the VBO with Cuda
+    3. Map the VBO for writing from Cuda
+    4. Run Cuda kernel to modify the vertex positions
+    5. Unmap the VBO
+    6. Render the results using OpenGL
+
+    Host code
+*/
+
+// includes, system
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <math.h>
+
+#ifdef _WIN32
+#  define WINDOWS_LEAN_AND_MEAN
+#  define NOMINMAX
+#  include <windows.h>
+#endif
+
+// OpenGL Graphics includes
+#include <helper_gl.h>
+#if defined (__APPLE__) || defined(MACOSX)
+  #pragma clang diagnostic ignored "-Wdeprecated-declarations"
+  #include <GLUT/glut.h>
+  #ifndef glutCloseFunc
+  #define glutCloseFunc glutWMCloseFunc
+  #endif
+#else
+#include <GL/freeglut.h>
+#endif
+
+// includes, cuda
+#include <hip/hip_runtime.h>
+#include <cuda_gl_interop.h>
+
+// Utilities and timing functions
+#include <helper_functions.h>    // includes hip/hip_runtime.h and hip/hip_runtime_api.h
+
+// CUDA helper functions
+#include <helper_cuda.h>         // helper functions for CUDA error check
+
+#include <hip/hip_vector_types.h>
+
+#define MAX_EPSILON_ERROR 10.0f
+#define THRESHOLD          0.30f
+#define REFRESH_DELAY     10 //ms
+
+////////////////////////////////////////////////////////////////////////////////
+// constants
+const unsigned int window_width  = 512;
+const unsigned int window_height = 512;
+
+const unsigned int mesh_width    = 256;
+const unsigned int mesh_height   = 256;
+
+// vbo variables
+GLuint vbo;
+struct hipGraphicsResource *cuda_vbo_resource;
+void *d_vbo_buffer = NULL;
+
+float g_fAnim = 0.0;
+
+// mouse controls
+int mouse_old_x, mouse_old_y;
+int mouse_buttons = 0;
+float rotate_x = 0.0, rotate_y = 0.0;
+float translate_z = -3.0;
+
+StopWatchInterface *timer = NULL;
+
+// Auto-Verification Code
+int fpsCount = 0;        // FPS count for averaging
+int fpsLimit = 1;        // FPS limit for sampling
+int g_Index = 0;
+float avgFPS = 0.0f;
+unsigned int frameCount = 0;
+unsigned int g_TotalErrors = 0;
+bool g_bQAReadback = false;
+
+int *pArgc = NULL;
+char **pArgv = NULL;
+
+#define MAX(a,b) ((a > b) ? a : b)
+
+////////////////////////////////////////////////////////////////////////////////
+// declaration, forward
+bool runTest(int argc, char **argv, char *ref_file);
+void cleanup();
+
+// GL functionality
+bool initGL(int *argc, char **argv);
+void createVBO(GLuint *vbo, struct hipGraphicsResource **vbo_res,
+               unsigned int vbo_res_flags);
+void deleteVBO(GLuint *vbo, struct hipGraphicsResource *vbo_res);
+
+// rendering callbacks
+void display();
+void keyboard(unsigned char key, int x, int y);
+void mouse(int button, int state, int x, int y);
+void motion(int x, int y);
+void timerEvent(int value);
+
+// Cuda functionality
+void runCuda(struct hipGraphicsResource **vbo_resource);
+void runAutoTest(int devID, char **argv, char *ref_file);
+void checkResultCuda(int argc, char **argv, const GLuint &vbo);
+
+const char *sSDKsample = "simpleGL (VBO)";
+
+///////////////////////////////////////////////////////////////////////////////
+//! Simple kernel to modify vertex positions in sine wave pattern
+//! @param data  data in global memory
+///////////////////////////////////////////////////////////////////////////////
+__global__ void simple_vbo_kernel(float4 *pos, unsigned int width, unsigned int height, float time)
+{
+    unsigned int x = blockIdx.x*blockDim.x + threadIdx.x;
+    unsigned int y = blockIdx.y*blockDim.y + threadIdx.y;
+
+    // calculate uv coordinates
+    float u = x / (float) width;
+    float v = y / (float) height;
+    u = u*2.0f - 1.0f;
+    v = v*2.0f - 1.0f;
+
+    // calculate simple sine wave pattern
+    float freq = 4.0f;
+    float w = sinf(u*freq + time) * cosf(v*freq + time) * 0.5f;
+
+    // write output vertex
+    pos[y*width+x] = make_float4(u, w, v, 1.0f);
+}
+
+
+void launch_kernel(float4 *pos, unsigned int mesh_width,
+                   unsigned int mesh_height, float time)
+{
+    // execute the kernel
+    dim3 block(8, 8, 1);
+    dim3 grid(mesh_width / block.x, mesh_height / block.y, 1);
+    simple_vbo_kernel<<< grid, block>>>(pos, mesh_width, mesh_height, time);
+}
+
+
+////////////////////////////////////////////////////////////////////////////////
+// Program main
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv)
+{
+    char *ref_file = NULL;
+
+    pArgc = &argc;
+    pArgv = argv;
+
+#if defined(__linux__)
+    setenv ("DISPLAY", ":0", 0);
+#endif
+
+    printf("%s starting...\n", sSDKsample);
+
+    if (argc > 1)
+    {
+        if (checkCmdLineFlag(argc, (const char **)argv, "file"))
+        {
+            // In this mode, we are running non-OpenGL and doing a compare of the VBO was generated correctly
+            getCmdLineArgumentString(argc, (const char **)argv, "file", (char **)&ref_file);
+        }
+    }
+
+    printf("\n");
+
+    runTest(argc, argv, ref_file);
+
+    printf("%s completed, returned %s\n", sSDKsample, (g_TotalErrors == 0) ? "OK" : "ERROR!");
+    exit(g_TotalErrors == 0 ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+
+void computeFPS()
+{
+    frameCount++;
+    fpsCount++;
+
+    if (fpsCount == fpsLimit)
+    {
+        avgFPS = 1.f / (sdkGetAverageTimerValue(&timer) / 1000.f);
+        fpsCount = 0;
+        fpsLimit = (int)MAX(avgFPS, 1.f);
+
+        sdkResetTimer(&timer);
+    }
+
+    char fps[256];
+    sprintf(fps, "Cuda GL Interop (VBO): %3.1f fps (Max 100Hz)", avgFPS);
+    glutSetWindowTitle(fps);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Initialize GL
+////////////////////////////////////////////////////////////////////////////////
+bool initGL(int *argc, char **argv)
+{
+    glutInit(argc, argv);
+    glutInitDisplayMode(GLUT_RGBA | GLUT_DOUBLE);
+    glutInitWindowSize(window_width, window_height);
+    glutCreateWindow("Cuda GL Interop (VBO)");
+    glutDisplayFunc(display);
+    glutKeyboardFunc(keyboard);
+    glutMotionFunc(motion);
+    glutTimerFunc(REFRESH_DELAY, timerEvent,0);
+
+    // initialize necessary OpenGL extensions
+    if (! isGLVersionSupported(2,0))
+    {
+        fprintf(stderr, "ERROR: Support for necessary OpenGL extensions missing.");
+        fflush(stderr);
+        return false;
+    }
+
+    // default initialization
+    glClearColor(0.0, 0.0, 0.0, 1.0);
+    glDisable(GL_DEPTH_TEST);
+
+    // viewport
+    glViewport(0, 0, window_width, window_height);
+
+    // projection
+    glMatrixMode(GL_PROJECTION);
+    glLoadIdentity();
+    gluPerspective(60.0, (GLfloat)window_width / (GLfloat) window_height, 0.1, 10.0);
+
+    SDK_CHECK_ERROR_GL();
+
+    return true;
+}
+
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run a simple test for CUDA
+////////////////////////////////////////////////////////////////////////////////
+bool runTest(int argc, char **argv, char *ref_file)
+{
+    // Create the CUTIL timer
+    sdkCreateTimer(&timer);
+
+    // use command-line specified CUDA device, otherwise use device with highest Gflops/s
+    int devID = findCudaDevice(argc, (const char **)argv);
+
+    // command line mode only
+    if (ref_file != NULL)
+    {
+        // create VBO
+        HIPCHECK(hipMalloc((void **)&d_vbo_buffer, mesh_width*mesh_height*4*sizeof(float)));
+
+        // run the cuda part
+        runAutoTest(devID, argv, ref_file);
+
+        // check result of Cuda step
+        checkResultCuda(argc, argv, vbo);
+
+        hipFree(d_vbo_buffer);
+        d_vbo_buffer = NULL;
+    }
+    else
+    {
+        // First initialize OpenGL context, so we can properly set the GL for CUDA.
+        // This is necessary in order to achieve optimal performance with OpenGL/CUDA interop.
+        if (false == initGL(&argc, argv))
+        {
+            return false;
+        }
+
+        // register callbacks
+        glutDisplayFunc(display);
+        glutKeyboardFunc(keyboard);
+        glutMouseFunc(mouse);
+        glutMotionFunc(motion);
+#if defined (__APPLE__) || defined(MACOSX)
+        atexit(cleanup);
+#else
+        glutCloseFunc(cleanup);
+#endif
+
+        // create VBO
+        createVBO(&vbo, &cuda_vbo_resource, cudaGraphicsMapFlagsWriteDiscard);
+
+        // run the cuda part
+        runCuda(&cuda_vbo_resource);
+
+        // start rendering mainloop
+        glutMainLoop();
+    }
+
+    return true;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run the Cuda part of the computation
+////////////////////////////////////////////////////////////////////////////////
+void runCuda(struct hipGraphicsResource **vbo_resource)
+{
+    // map OpenGL buffer object for writing from CUDA
+    float4 *dptr;
+    HIPCHECK(hipGraphicsMapResources(1, vbo_resource, 0));
+    size_t num_bytes;
+    HIPCHECK(hipGraphicsResourceGetMappedPointer((void **)&dptr, &num_bytes,
+                                                         *vbo_resource));
+    //printf("CUDA mapped VBO: May access %ld bytes\n", num_bytes);
+
+    // execute the kernel
+    //    dim3 block(8, 8, 1);
+    //    dim3 grid(mesh_width / block.x, mesh_height / block.y, 1);
+    //    kernel<<< grid, block>>>(dptr, mesh_width, mesh_height, g_fAnim);
+
+    launch_kernel(dptr, mesh_width, mesh_height, g_fAnim);
+
+    // unmap buffer object
+    HIPCHECK(hipGraphicsUnmapResources(1, vbo_resource, 0));
+}
+
+#ifdef _WIN32
+#ifndef FOPEN
+#define FOPEN(fHandle,filename,mode) fopen_s(&fHandle, filename, mode)
+#endif
+#else
+#ifndef FOPEN
+#define FOPEN(fHandle,filename,mode) (fHandle = fopen(filename, mode))
+#endif
+#endif
+
+void sdkDumpBin2(void *data, unsigned int bytes, const char *filename)
+{
+    printf("sdkDumpBin: <%s>\n", filename);
+    FILE *fp;
+    FOPEN(fp, filename, "wb");
+    fwrite(data, bytes, 1, fp);
+    fflush(fp);
+    fclose(fp);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run the Cuda part of the computation
+////////////////////////////////////////////////////////////////////////////////
+void runAutoTest(int devID, char **argv, char *ref_file)
+{
+    char *reference_file = NULL;
+    void *imageData = malloc(mesh_width*mesh_height*sizeof(float));
+
+    // execute the kernel
+    launch_kernel((float4 *)d_vbo_buffer, mesh_width, mesh_height, g_fAnim);
+
+    hipDeviceSynchronize();
+    getLastCudaError("launch_kernel failed");
+
+    HIPCHECK(hipMemcpy(imageData, d_vbo_buffer, mesh_width*mesh_height*sizeof(float), hipMemcpyDeviceToHost));
+
+    sdkDumpBin2(imageData, mesh_width*mesh_height*sizeof(float), "simpleGL.bin");
+    reference_file = sdkFindFilePath(ref_file, argv[0]);
+
+    if (reference_file &&
+        !sdkCompareBin2BinFloat("simpleGL.bin", reference_file,
+                                mesh_width*mesh_height*sizeof(float),
+                                MAX_EPSILON_ERROR, THRESHOLD, pArgv[0]))
+    {
+        g_TotalErrors++;
+    }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Create VBO
+////////////////////////////////////////////////////////////////////////////////
+void createVBO(GLuint *vbo, struct hipGraphicsResource **vbo_res,
+               unsigned int vbo_res_flags)
+{
+    assert(vbo);
+
+    // create buffer object
+    glGenBuffers(1, vbo);
+    glBindBuffer(GL_ARRAY_BUFFER, *vbo);
+
+    // initialize buffer object
+    unsigned int size = mesh_width * mesh_height * 4 * sizeof(float);
+    glBufferData(GL_ARRAY_BUFFER, size, 0, GL_DYNAMIC_DRAW);
+
+    glBindBuffer(GL_ARRAY_BUFFER, 0);
+
+    // register this buffer object with CUDA
+    HIPCHECK(hipGraphicsGLRegisterBuffer(vbo_res, *vbo, vbo_res_flags));
+
+    SDK_CHECK_ERROR_GL();
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Delete VBO
+////////////////////////////////////////////////////////////////////////////////
+void deleteVBO(GLuint *vbo, struct hipGraphicsResource *vbo_res)
+{
+
+    // unregister this buffer object with CUDA
+    HIPCHECK(hipGraphicsUnregisterResource(vbo_res));
+
+    glBindBuffer(1, *vbo);
+    glDeleteBuffers(1, vbo);
+
+    *vbo = 0;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Display callback
+////////////////////////////////////////////////////////////////////////////////
+void display()
+{
+    sdkStartTimer(&timer);
+
+    // run CUDA kernel to generate vertex positions
+    runCuda(&cuda_vbo_resource);
+
+    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
+
+    // set view matrix
+    glMatrixMode(GL_MODELVIEW);
+    glLoadIdentity();
+    glTranslatef(0.0, 0.0, translate_z);
+    glRotatef(rotate_x, 1.0, 0.0, 0.0);
+    glRotatef(rotate_y, 0.0, 1.0, 0.0);
+
+    // render from the vbo
+    glBindBuffer(GL_ARRAY_BUFFER, vbo);
+    glVertexPointer(4, GL_FLOAT, 0, 0);
+
+    glEnableClientState(GL_VERTEX_ARRAY);
+    glColor3f(1.0, 0.0, 0.0);
+    glDrawArrays(GL_POINTS, 0, mesh_width * mesh_height);
+    glDisableClientState(GL_VERTEX_ARRAY);
+
+    glutSwapBuffers();
+
+    g_fAnim += 0.01f;
+
+    sdkStopTimer(&timer);
+    computeFPS();
+}
+
+void timerEvent(int value)
+{
+    if (glutGetWindow())
+    {
+        glutPostRedisplay();
+        glutTimerFunc(REFRESH_DELAY, timerEvent,0);
+    }
+}
+
+void cleanup()
+{
+    sdkDeleteTimer(&timer);
+
+    if (vbo)
+    {
+        deleteVBO(&vbo, cuda_vbo_resource);
+    }
+}
+
+
+////////////////////////////////////////////////////////////////////////////////
+//! Keyboard events handler
+////////////////////////////////////////////////////////////////////////////////
+void keyboard(unsigned char key, int /*x*/, int /*y*/)
+{
+    switch (key)
+    {
+        case (27) :
+            #if defined(__APPLE__) || defined(MACOSX)
+                exit(EXIT_SUCCESS);
+            #else
+                glutDestroyWindow(glutGetWindow());
+                return;
+            #endif
+    }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Mouse event handlers
+////////////////////////////////////////////////////////////////////////////////
+void mouse(int button, int state, int x, int y)
+{
+    if (state == GLUT_DOWN)
+    {
+        mouse_buttons |= 1<<button;
+    }
+    else if (state == GLUT_UP)
+    {
+        mouse_buttons = 0;
+    }
+
+    mouse_old_x = x;
+    mouse_old_y = y;
+}
+
+void motion(int x, int y)
+{
+    float dx, dy;
+    dx = (float)(x - mouse_old_x);
+    dy = (float)(y - mouse_old_y);
+
+    if (mouse_buttons & 1)
+    {
+        rotate_x += dy * 0.2f;
+        rotate_y += dx * 0.2f;
+    }
+    else if (mouse_buttons & 4)
+    {
+        translate_z += dy * 0.01f;
+    }
+
+    mouse_old_x = x;
+    mouse_old_y = y;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Check if the result is correct or write data to file for external
+//! regression testing
+////////////////////////////////////////////////////////////////////////////////
+void checkResultCuda(int argc, char **argv, const GLuint &vbo)
+{
+    if (!d_vbo_buffer)
+    {
+        HIPCHECK(hipGraphicsUnregisterResource(cuda_vbo_resource));
+
+        // map buffer object
+        glBindBuffer(GL_ARRAY_BUFFER, vbo);
+        float *data = (float *) glMapBuffer(GL_ARRAY_BUFFER, GL_READ_ONLY);
+
+        // check result
+        if (checkCmdLineFlag(argc, (const char **) argv, "regression"))
+        {
+            // write file for regression test
+            sdkWriteFile<float>("./data/regression.dat",
+                                data, mesh_width * mesh_height * 3, 0.0, false);
+        }
+
+        // unmap GL buffer object
+        if (!glUnmapBuffer(GL_ARRAY_BUFFER))
+        {
+            fprintf(stderr, "Unmap buffer failed.\n");
+            fflush(stderr);
+        }
+
+        HIPCHECK(hipGraphicsGLRegisterBuffer(&cuda_vbo_resource, vbo,
+                                                     cudaGraphicsMapFlagsWriteDiscard));
+
+        SDK_CHECK_ERROR_GL();
+    }
+}
+MapFlagsWriteDiscard));
+
+        SDK_CHECK_ERROR_GL();
+    }
+}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleGLES/simpleGLES.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleGLES/simpleGLES.cu.hip
index e69de29..9b3210f 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleGLES/simpleGLES.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleGLES/simpleGLES.cu.hip
@@ -0,0 +1,630 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+    This example demonstrates how to use the CUDA C bindings to OpenGL ES to
+    dynamically modify a vertex buffer using a CUDA C kernel.
+
+    The steps are:
+    1. Create an empty vertex buffer object (VBO)
+    2. Register the VBO with CUDA C
+    3. Map the VBO for writing from CUDA C
+    4. Run CUDA C kernel to modify the vertex positions
+    5. Unmap the VBO
+    6. Render the results using OpenGL ES
+
+    Host code
+*/
+
+// includes, system
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <math.h>
+
+#include <stdarg.h>
+#include <unistd.h>
+#include <X11/Xlib.h>
+#include <X11/Xutil.h>
+
+void error_exit(const char *format, ...) {
+  va_list args;
+  va_start(args, format);
+  vfprintf(stderr, format, args);
+  va_end(args);
+  exit(1);
+}
+
+#include "graphics_interface.c"
+
+#ifdef _WIN32
+#define WINDOWS_LEAN_AND_MEAN
+#define NOMINMAX
+#include <windows.h>
+#endif
+
+// includes, cuda
+#include <hip/hip_runtime.h>
+#include <cuda_gl_interop.h>
+
+// Utilities and timing functions
+#include <helper_functions.h>  // includes hip/hip_runtime.h and hip/hip_runtime_api.h
+
+// CUDA helper functions
+#include <helper_cuda.h>  // helper functions for CUDA error check
+//#include <helper_cuda_gl.h>      // helper functions for CUDA/GL interop
+
+#include <hip/hip_vector_types.h>
+
+#define MAX_EPSILON_ERROR 0.0f
+#define THRESHOLD 0.0f
+#define REFRESH_DELAY 1  // ms
+
+#define GUI_IDLE 0x100
+#define GUI_ROTATE 0x101
+#define GUI_TRANSLATE 0x102
+
+int gui_mode;
+
+////////////////////////////////////////////////////////////////////////////////
+// constants
+const unsigned int window_width = 512;
+const unsigned int window_height = 512;
+
+const unsigned int mesh_width = 256;
+const unsigned int mesh_height = 256;
+
+// OpenGL ES variables and interop with CUDA C
+GLuint mesh_vao, mesh_vbo;
+struct hipGraphicsResource *cuda_vbo_resource;
+void *d_vbo_buffer = NULL;
+
+float g_fAnim = 0.0;
+
+// UI / mouse controls
+int mouse_old_x, mouse_old_y;
+int mouse_buttons = 0;
+float rotate_x = 0.0, rotate_y = 0.0;
+float translate_z = -3.0;
+
+StopWatchInterface *timer = NULL;
+
+// Frame statistics
+int frame;
+int fpsCount = 0;  // FPS count for averaging
+int fpsLimit = 1;  // FPS limit for sampling
+int g_Index = 0;
+float avgFPS = 0.0f;
+unsigned int frameCount = 0;
+unsigned int g_TotalErrors = 0;
+
+// Auto-Verification Code
+bool g_bQAReadback = false;
+
+int *pArgc = NULL;
+char **pArgv = NULL;
+
+#define MAX(a, b) ((a > b) ? a : b)
+
+////////////////////////////////////////////////////////////////////////////////
+// declaration, forward
+
+// CUDA functionality
+void runCuda(struct hipGraphicsResource **vbo_resource);
+void runAutoTest(int devID, char **argv, char *ref_file);
+void checkResultCuda(int argc, char **argv, const GLuint &vbo);
+
+const char *sSDKsample = "simpleGLES (VBO)";
+
+void computeFPS() {
+  frameCount++;
+  fpsCount++;
+
+  if (fpsCount == fpsLimit) {
+    avgFPS = 1.f / (sdkGetAverageTimerValue(&timer) / 1000.f);
+    fpsCount = 0;
+    fpsLimit = (int)MAX(avgFPS, 1.f);
+
+    sdkResetTimer(&timer);
+  }
+
+  char fps[256];
+  sprintf(fps, "Cuda/OpenGL ES Interop (VBO): %3.1f fps (Max 1000 fps)",
+          avgFPS);
+  graphics_set_windowtitle(fps);
+}
+
+///////////////////////////////////////////////////////////////////////////////
+//! Simple kernel to modify vertex positions in sine wave pattern
+//! @param data  data in global memory
+///////////////////////////////////////////////////////////////////////////////
+__global__ void simple_vbo_kernel(float4 *pos, unsigned int width,
+                                  unsigned int height, float time) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  // calculate uv coordinates
+  float u = x / (float)width;
+  float v = y / (float)height;
+  u = u * 2.0f - 1.0f;
+  v = v * 2.0f - 1.0f;
+
+  // calculate simple sine wave pattern
+  float freq = 4.0f;
+  float w = sinf(u * freq + time) * cosf(v * freq + time) * 0.5f;
+
+  // write output vertex
+  pos[y * width + x] = make_float4(u, w, v, 1.0f);
+}
+
+void launch_kernel(float4 *pos, unsigned int mesh_width,
+                   unsigned int mesh_height, float time) {
+  // execute the kernel
+  dim3 block(8, 8, 1);
+  dim3 grid(mesh_width / block.x, mesh_height / block.y, 1);
+  simple_vbo_kernel<<<grid, block>>>(pos, mesh_width, mesh_height, time);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run the Cuda part of the computation
+////////////////////////////////////////////////////////////////////////////////
+void runCuda(struct hipGraphicsResource **vbo_resource) {
+  // map OpenGL buffer object for writing from CUDA
+  float4 *dptr;
+  hipGraphicsMapResources(1, vbo_resource, 0);
+  size_t num_bytes;
+  hipGraphicsResourceGetMappedPointer((void **)&dptr, &num_bytes,
+                                       *vbo_resource);
+  // printf("Sample CUDA mapped VBO: May access %ld bytes\n", num_bytes);
+
+  // execute the kernel
+  //    dim3 block(8, 8, 1);
+  //    dim3 grid(mesh_width / block.x, mesh_height / block.y, 1);
+  //    kernel<<< grid, block>>>(dptr, mesh_width, mesh_height, g_fAnim);
+
+  launch_kernel(dptr, mesh_width, mesh_height, g_fAnim);
+
+  // unmap buffer object
+  hipGraphicsUnmapResources(1, vbo_resource, 0);
+}
+
+#ifdef _WIN32
+#ifndef FOPEN
+#define FOPEN(fHandle, filename, mode) fopen_s(&fHandle, filename, mode)
+#endif
+#else
+#ifndef FOPEN
+#define FOPEN(fHandle, filename, mode) (fHandle = fopen(filename, mode))
+#endif
+#endif
+
+void sdkDumpBin2(void *data, unsigned int bytes, const char *filename) {
+  printf("sdkDumpBin: <%s>\n", filename);
+  FILE *fp;
+  FOPEN(fp, filename, "wb");
+  fwrite(data, bytes, 1, fp);
+  fflush(fp);
+  fclose(fp);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run the Cuda part of the computation
+////////////////////////////////////////////////////////////////////////////////
+void runAutoTest(int devID, char **argv, char *ref_file) {
+  char *reference_file = NULL;
+  void *imageData = malloc(mesh_width * mesh_height * sizeof(float));
+
+  // execute the kernel
+  launch_kernel((float4 *)d_vbo_buffer, mesh_width, mesh_height, g_fAnim);
+
+  hipDeviceSynchronize();
+  getLastCudaError("launch_kernel failed");
+
+  hipMemcpy(imageData, d_vbo_buffer, mesh_width * mesh_height * sizeof(float),
+             hipMemcpyDeviceToHost);
+
+  sdkDumpBin2(imageData, mesh_width * mesh_height * sizeof(float),
+              "simpleGL.bin");
+  reference_file = sdkFindFilePath(ref_file, argv[0]);
+
+  if (reference_file &&
+      !sdkCompareBin2BinFloat("simpleGL.bin", reference_file,
+                              mesh_width * mesh_height * sizeof(float),
+                              MAX_EPSILON_ERROR, THRESHOLD, pArgv[0])) {
+    g_TotalErrors++;
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Display callback
+////////////////////////////////////////////////////////////////////////////////
+void display_thisframe(float time_delta) {
+  sdkStartTimer(&timer);
+
+  // run CUDA kernel to generate vertex positions
+  runCuda(&cuda_vbo_resource);
+
+  glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
+  // GET_GLERROR(0);
+
+  // set view matrix: broken, it doesn't work in OpenGL ES! Must put into shader
+  // glMatrixMode(GL_MODELVIEW);
+  // glLoadIdentity();
+  // glTranslatef(0.0, 0.0, translate_z);
+  // glRotatef(rotate_x, 1.0, 0.0, 0.0);
+  // glRotatef(rotate_y, 0.0, 1.0, 0.0);
+
+  glDrawArrays(GL_POINTS, 0, mesh_width * mesh_height);
+
+  // GET_GLERROR(0);
+  glFinish();
+  // GET_GLERROR(0);
+
+  g_fAnim += time_delta;
+
+  sdkStopTimer(&timer);
+  computeFPS();
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Check if the result is correct or write data to file for external
+//! regression testing
+////////////////////////////////////////////////////////////////////////////////
+void checkResultCuda(int argc, char **argv, const GLuint &vbo) {
+  if (!d_vbo_buffer) {
+    printf("%s: Mapping result buffer from OpenGL ES\n", __FUNCTION__);
+
+    hipGraphicsUnregisterResource(cuda_vbo_resource);
+
+    // map buffer object
+    glBindBuffer(GL_ARRAY_BUFFER, vbo);
+    float *data = (float *)glMapBufferRange(
+        GL_ARRAY_BUFFER, 0, mesh_width * mesh_height * 4 * sizeof(float),
+        GL_READ_ONLY);
+
+    // check result
+    if (checkCmdLineFlag(argc, (const char **)argv, "regression")) {
+      // write file for regression test
+      sdkWriteFile<float>("./data/regression.dat", data,
+                          mesh_width * mesh_height * 3, 0.0, false);
+    }
+
+    // unmap GL buffer object
+    if (!glUnmapBuffer(GL_ARRAY_BUFFER)) {
+      fprintf(stderr, "Unmap buffer failed.\n");
+      fflush(stderr);
+    }
+
+    HIPCHECK(hipGraphicsGLRegisterBuffer(
+        &cuda_vbo_resource, vbo, cudaGraphicsMapFlagsWriteDiscard));
+
+    GET_GLERROR(0);
+  }
+}
+
+GLuint mesh_shader = 0;
+
+void readAndCompileShaderFromGLSLFile(GLuint new_shaderprogram,
+                                      const char *filename, GLenum shaderType) {
+  FILE *file = fopen(filename, "rb");  // open shader text file
+  if (!file) error_exit("Filename %s does not exist\n", filename);
+
+  /* get the size of the file and read it */
+  fseek(file, 0, SEEK_END);
+  GLint size = ftell(file);
+  char *data = (char *)malloc(sizeof(char) * (size + 1));
+  memset(data, 0, sizeof(char) * (size + 1));
+  fseek(file, 0, SEEK_SET);
+  size_t res = fread(data, 1, size, file);
+  fclose(file);
+
+  GLuint shader = glCreateShader(shaderType);
+  glShaderSource(shader, 1, (const GLchar **)&data, &size);
+  glCompileShader(shader);
+
+  GET_GLERROR(0);
+  GLint compile_success = 0;
+  glGetShaderiv(shader, GL_COMPILE_STATUS, &compile_success);
+  GET_GLERROR(0);
+
+  if (compile_success == GL_FALSE) {
+    printf("Compilation of %s failed!\n Reason:\n", filename);
+
+    GLint maxLength = 0;
+    glGetShaderiv(shader, GL_INFO_LOG_LENGTH, &maxLength);
+
+    char errorLog[maxLength];
+    glGetShaderInfoLog(shader, maxLength, &maxLength, &errorLog[0]);
+
+    printf("%s", errorLog);
+
+    glDeleteShader(shader);
+    exit(1);
+  }
+
+  glAttachShader(new_shaderprogram, shader);
+  glDeleteShader(shader);  // good to do?
+
+  free(data);
+}
+
+GLuint ShaderCreate(const char *vshader_filename,
+                    const char *fshader_filename) {
+  printf("Loading GLSL shaders %s %s\n", vshader_filename, fshader_filename);
+
+  GLuint new_shaderprogram = glCreateProgram();
+
+  GET_GLERROR(0);
+  if (vshader_filename)
+    readAndCompileShaderFromGLSLFile(new_shaderprogram, vshader_filename,
+                                     GL_VERTEX_SHADER);
+
+  GET_GLERROR(0);
+  if (fshader_filename)
+    readAndCompileShaderFromGLSLFile(new_shaderprogram, fshader_filename,
+                                     GL_FRAGMENT_SHADER);
+
+  GET_GLERROR(0);
+
+  glLinkProgram(new_shaderprogram);
+
+  GET_GLERROR(0);
+  GLint link_success;
+  glGetProgramiv(new_shaderprogram, GL_LINK_STATUS, &link_success);
+
+  if (link_success == GL_FALSE) {
+    printf("Linking of %s with %s failed!\n Reason:\n", vshader_filename,
+           fshader_filename);
+
+    GLint maxLength = 0;
+    glGetShaderiv(new_shaderprogram, GL_INFO_LOG_LENGTH, &maxLength);
+
+    char errorLog[maxLength];
+    glGetShaderInfoLog(new_shaderprogram, maxLength, &maxLength, &errorLog[0]);
+
+    printf("%s", errorLog);
+
+    exit(EXIT_FAILURE);
+  }
+
+  return new_shaderprogram;
+}
+
+//===========================================================================
+// InitGraphicsState() - initialize OpenGL
+//===========================================================================
+static void InitGraphicsState(void) {
+  char *GL_version = (char *)glGetString(GL_VERSION);
+  char *GL_vendor = (char *)glGetString(GL_VENDOR);
+  char *GL_renderer = (char *)glGetString(GL_RENDERER);
+
+  printf("Version: %s\n", GL_version);
+  printf("Vendor: %s\n", GL_vendor);
+  printf("Renderer: %s\n", GL_renderer);
+
+  // RENDERING SETUP (OpenGL ES or OpenGL Core Profile!)
+  glGenVertexArrays(1, &mesh_vao);  // Features' Vertex Array Object allocation
+  glBindVertexArray(mesh_vao);      // bind VAO
+
+  // initialize buffer object
+  glGenBuffers(1, &mesh_vbo);
+  glBindBuffer(GL_ARRAY_BUFFER, mesh_vbo);
+
+  unsigned int size = mesh_width * mesh_height * 4 * sizeof(float);
+  glBufferData(GL_ARRAY_BUFFER, size, NULL, GL_DYNAMIC_DRAW);
+  glVertexAttribPointer((GLuint)0, 4, GL_FLOAT, GL_FALSE, 0, 0);
+  glEnableVertexAttribArray(0);
+
+  HIPCHECK(hipGraphicsGLRegisterBuffer(&cuda_vbo_resource, mesh_vbo,
+                                               cudaGraphicsMapFlagsNone));
+  // glBindVertexArray(0); // keep above Vertex Array Object bound (it's the
+  // only one throughout)
+
+  // GLSL stuff
+  char *vertex_shader_path = sdkFindFilePath("mesh.vert.glsl", pArgv[0]);
+  char *fragment_shader_path = sdkFindFilePath("mesh.frag.glsl", pArgv[0]);
+
+  if (vertex_shader_path == NULL || fragment_shader_path == NULL) {
+    printf("Error finding shader file\n");
+    exit(EXIT_FAILURE);
+  }
+
+  mesh_shader = ShaderCreate(vertex_shader_path, fragment_shader_path);
+  GET_GLERROR(0);
+
+  free(vertex_shader_path);
+  free(fragment_shader_path);
+
+  glUseProgram(mesh_shader);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run a simple test for CUDA
+////////////////////////////////////////////////////////////////////////////////
+bool runTest(int argc, char **argv, char *ref_file) {
+  // Create the CUTIL timer
+  sdkCreateTimer(&timer);
+
+  int devID = 0;
+#if defined(__aarch64__) || defined(__arm__)
+  // find iGPU on the system which is compute capable which will perform
+  // GLES-CUDA interop
+  devID = findIntegratedGPU();
+#else
+  // use command-line specified CUDA device, otherwise use device with highest
+  // Gflops/s
+  devID = findCudaDevice(argc, (const char **)argv);
+#endif
+
+  // command line mode only
+  if (ref_file != NULL) {
+    // create VBO
+    HIPCHECK(hipMalloc((void **)&d_vbo_buffer,
+                               mesh_width * mesh_height * 4 * sizeof(float)));
+
+    // run the cuda part
+    runAutoTest(devID, argv, ref_file);
+
+    // check result of Cuda step
+    checkResultCuda(argc, argv, mesh_vbo);
+
+    hipFree(d_vbo_buffer);
+    d_vbo_buffer = NULL;
+  } else {
+    // this would use command-line specified CUDA device, note that CUDA
+    // defaults to highest Gflops/s device
+    if (checkCmdLineFlag(argc, (const char **)argv, "device"))
+      error_exit("Device setting not yet implemented!\n");
+
+    // create X11 window and set up associated OpenGL ES context
+    graphics_setup_window(0, 0, window_width, window_height, sSDKsample);
+
+    InitGraphicsState();  // set up GLES stuff
+
+    glClearColor(0, 0.5, 1, 1);  // blue-ish background
+    glClear(GL_COLOR_BUFFER_BIT);
+
+    // printf("WP%d\n", __LINE__);
+    graphics_swap_buffers();
+
+    XEvent event;
+    KeySym key;
+    char text[255];
+
+    int frame = 0;
+
+    while (frame < 100000) {
+      if (XPending(display)) {
+        XNextEvent(display, &event);
+
+        if (event.type == Expose && event.xexpose.count == 0) {
+          printf("Redraw requested!\n");
+        }
+        if (event.type == KeyPress &&
+            XLookupString(&event.xkey, text, 255, &key, 0) == 1) {
+          if (text[0] == 27) goto label_stop_x;
+
+          printf("You pressed the %c key!\n", text[0]);
+        }
+
+        if (event.type == ButtonPress) {
+          printf("Mouse button %d press at (%d,%d)\n", event.xbutton.button,
+                 event.xbutton.x, event.xbutton.y);
+
+          if (event.xbutton.button == Button1) gui_mode = GUI_TRANSLATE;
+          if (event.xbutton.button == Button3) gui_mode = GUI_ROTATE;
+          mouse_old_x = event.xbutton.x;
+          mouse_old_y = event.xbutton.y;
+        }
+
+        if (event.type == ButtonRelease) {
+          printf("Mouse button %d released at (%d,%d)\n", event.xbutton.button,
+                 event.xbutton.x, event.xbutton.y);
+
+          gui_mode = GUI_IDLE;
+          mouse_old_x = event.xbutton.x;
+          mouse_old_y = event.xbutton.y;
+        }
+
+        if (event.type == MotionNotify) {
+          // printf("Mouse motion towards %d %d, GUI mode is 0x%x\n",
+          //	   event.xmotion.x, event.xmotion.y, gui_mode);
+          float dx, dy;
+          dx = (float)(event.xmotion.x - mouse_old_x);
+          dy = (float)(event.xmotion.y - mouse_old_y);
+
+          if (gui_mode == GUI_ROTATE) {
+            rotate_x += dy * 0.2f;
+            rotate_y += dx * 0.2f;
+            printf("rot x %f y %f\n", rotate_x, rotate_y);
+          }
+          if (gui_mode == GUI_TRANSLATE) {
+            translate_z += dy * 0.01f;
+            printf("translate z %f\n", translate_z);
+          }
+
+          mouse_old_x = event.xmotion.x;
+          mouse_old_y = event.xmotion.y;
+        }
+      }
+
+      display_thisframe(0.010);
+      usleep(1000);  // need not take full CPU and GPU
+
+      graphics_swap_buffers();
+      // printf("frame %d\n",frame++);
+    }
+
+  label_stop_x:
+    // NOTE: Before destroying OpenGL ES context, must unregister all shared
+    // resources from CUDA !
+    hipGraphicsUnregisterResource(cuda_vbo_resource);
+
+    graphics_close_window();  // close window and destroy OpenGL ES context
+
+    sdkDeleteTimer(&timer);
+  }
+
+  return true;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Program main
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv) {
+  char *ref_file = NULL;
+
+  pArgc = &argc;
+  pArgv = argv;
+
+#if defined(__linux__)
+  setenv("DISPLAY", ":0", 0);
+#endif
+
+  printf("%s starting...\n", sSDKsample);
+
+  if (argc > 1) {
+    if (checkCmdLineFlag(argc, (const char **)argv, "file")) {
+      // In this mode, we run without OpenGL and see if VBO is generated
+      // correctly
+      getCmdLineArgumentString(argc, (const char **)argv, "file",
+                               (char **)&ref_file);
+    }
+  }
+
+  printf("\n");
+
+  runTest(argc, argv, ref_file);
+
+  printf("%s completed, returned %s\n", sSDKsample,
+         (g_TotalErrors == 0) ? "OK" : "ERROR!");
+
+  exit(g_TotalErrors == 0 ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+S : EXIT_FAILURE);
+}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleGLES_EGLOutput/simpleGLES_EGLOutput.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleGLES_EGLOutput/simpleGLES_EGLOutput.cu.hip
index e69de29..c595d42 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleGLES_EGLOutput/simpleGLES_EGLOutput.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleGLES_EGLOutput/simpleGLES_EGLOutput.cu.hip
@@ -0,0 +1,574 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+    This example demonstrates how to use the CUDA C bindings to OpenGL ES to
+    dynamically modify a vertex buffer using a CUDA C kernel.
+
+    The steps are:
+    1. Create an empty vertex buffer object (VBO)
+    2. Register the VBO with CUDA C
+    3. Map the VBO for writing from CUDA C
+    4. Run CUDA C kernel to modify the vertex positions
+    5. Unmap the VBO
+    6. Render the results using OpenGL ES
+
+    Host code
+*/
+
+// includes, system
+#include <math.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+
+#include <stdarg.h>
+#include <unistd.h>
+
+void error_exit(const char *format, ...) {
+  va_list args;
+  va_start(args, format);
+  vfprintf(stderr, format, args);
+  va_end(args);
+  exit(1);
+}
+
+#if 0
+#include "graphics_interface.c"
+#else
+#include "graphics_interface_egloutput_via_egl.c"
+#endif
+
+#ifdef _WIN32
+#define WINDOWS_LEAN_AND_MEAN
+#define NOMINMAX
+#include <windows.h>
+#endif
+
+// includes, cuda
+#include <cuda_gl_interop.h>
+#include <hip/hip_runtime.h>
+
+// Utilities and timing functions
+#include <helper_functions.h>  // includes hip/hip_runtime.h and hip/hip_runtime_api.h
+
+// CUDA helper functions
+#include <helper_cuda.h>  // helper functions for CUDA error check
+  //#include <helper_cuda_gl.h>      // helper functions for CUDA/GL interop
+
+#include <hip/hip_vector_types.h>
+
+#define MAX_EPSILON_ERROR 0.0f
+#define THRESHOLD 0.0f
+#define REFRESH_DELAY 1  // ms
+
+#define GUI_IDLE 0x100
+#define GUI_ROTATE 0x101
+#define GUI_TRANSLATE 0x102
+
+int gui_mode;
+
+////////////////////////////////////////////////////////////////////////////////
+// constants
+const unsigned int window_width = 512;
+const unsigned int window_height = 512;
+
+const unsigned int mesh_width = 256;
+const unsigned int mesh_height = 256;
+
+// OpenGL ES variables and interop with CUDA C
+GLuint mesh_vao, mesh_vbo;
+struct hipGraphicsResource *cuda_vbo_resource;
+void *d_vbo_buffer = NULL;
+
+float g_fAnim = 0.0;
+
+// UI / mouse controls
+int mouse_old_x, mouse_old_y;
+int mouse_buttons = 0;
+float rotate_x = 0.0, rotate_y = 0.0;
+float translate_z = -3.0;
+
+StopWatchInterface *timer = NULL;
+
+// Frame statistics
+int frame;
+int fpsCount = 0;  // FPS count for averaging
+int fpsLimit = 1;  // FPS limit for sampling
+int g_Index = 0;
+float avgFPS = 0.0f;
+unsigned int frameCount = 0;
+unsigned int g_TotalErrors = 0;
+
+// Auto-Verification Code
+bool g_bQAReadback = false;
+
+int *pArgc = NULL;
+char **pArgv = NULL;
+
+#define MAX(a, b) ((a > b) ? a : b)
+
+////////////////////////////////////////////////////////////////////////////////
+// declaration, forward
+
+// CUDA functionality
+void runCuda(struct hipGraphicsResource **vbo_resource);
+void runAutoTest(int devID, char **argv, char *ref_file);
+void checkResultCuda(int argc, char **argv, const GLuint &vbo);
+
+const char *sSDKsample = "simpleGLES (VBO)";
+
+void computeFPS() {
+  frameCount++;
+  fpsCount++;
+
+  if (fpsCount == fpsLimit) {
+    avgFPS = 1.f / (sdkGetAverageTimerValue(&timer) / 1000.f);
+    fpsCount = 0;
+    fpsLimit = (int)MAX(avgFPS, 1.f);
+
+    sdkResetTimer(&timer);
+  }
+
+  char fps[256];
+  sprintf(fps, "Cuda/OpenGL ES Interop (VBO): %3.1f fps (Max 1000 fps)",
+          avgFPS);
+  graphics_set_windowtitle(fps);
+}
+
+///////////////////////////////////////////////////////////////////////////////
+//! Simple kernel to modify vertex positions in sine wave pattern
+//! @param data  data in global memory
+///////////////////////////////////////////////////////////////////////////////
+__global__ void simple_vbo_kernel(float4 *pos, unsigned int width,
+                                  unsigned int height, float time) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  // calculate uv coordinates
+  float u = x / (float)width;
+  float v = y / (float)height;
+  u = u * 2.0f - 1.0f;
+  v = v * 2.0f - 1.0f;
+
+  // calculate simple sine wave pattern
+  float freq = 4.0f;
+  float w = sinf(u * freq + time) * cosf(v * freq + time) * 0.5f;
+
+  // write output vertex
+  pos[y * width + x] = make_float4(u, w, v, 1.0f);
+}
+
+void launch_kernel(float4 *pos, unsigned int mesh_width,
+                   unsigned int mesh_height, float time) {
+  // execute the kernel
+  dim3 block(8, 8, 1);
+  dim3 grid(mesh_width / block.x, mesh_height / block.y, 1);
+  simple_vbo_kernel<<<grid, block>>>(pos, mesh_width, mesh_height, time);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run the Cuda part of the computation
+////////////////////////////////////////////////////////////////////////////////
+void runCuda(struct hipGraphicsResource **vbo_resource) {
+  // map OpenGL buffer object for writing from CUDA
+  float4 *dptr;
+  hipGraphicsMapResources(1, vbo_resource, 0);
+  size_t num_bytes;
+  hipGraphicsResourceGetMappedPointer((void **)&dptr, &num_bytes,
+                                       *vbo_resource);
+  // printf("Sample CUDA mapped VBO: May access %ld bytes\n", num_bytes);
+
+  // execute the kernel
+  //    dim3 block(8, 8, 1);
+  //    dim3 grid(mesh_width / block.x, mesh_height / block.y, 1);
+  //    kernel<<< grid, block>>>(dptr, mesh_width, mesh_height, g_fAnim);
+
+  launch_kernel(dptr, mesh_width, mesh_height, g_fAnim);
+
+  // unmap buffer object
+  hipGraphicsUnmapResources(1, vbo_resource, 0);
+}
+
+#ifdef _WIN32
+#ifndef FOPEN
+#define FOPEN(fHandle, filename, mode) fopen_s(&fHandle, filename, mode)
+#endif
+#else
+#ifndef FOPEN
+#define FOPEN(fHandle, filename, mode) (fHandle = fopen(filename, mode))
+#endif
+#endif
+
+void sdkDumpBin2(void *data, unsigned int bytes, const char *filename) {
+  printf("sdkDumpBin: <%s>\n", filename);
+  FILE *fp;
+  FOPEN(fp, filename, "wb");
+  fwrite(data, bytes, 1, fp);
+  fflush(fp);
+  fclose(fp);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run the Cuda part of the computation
+////////////////////////////////////////////////////////////////////////////////
+void runAutoTest(int devID, char **argv, char *ref_file) {
+  char *reference_file = NULL;
+  void *imageData = malloc(mesh_width * mesh_height * sizeof(float));
+
+  // execute the kernel
+  launch_kernel((float4 *)d_vbo_buffer, mesh_width, mesh_height, g_fAnim);
+
+  hipDeviceSynchronize();
+  getLastCudaError("launch_kernel failed");
+
+  hipMemcpy(imageData, d_vbo_buffer, mesh_width * mesh_height * sizeof(float),
+             hipMemcpyDeviceToHost);
+
+  sdkDumpBin2(imageData, mesh_width * mesh_height * sizeof(float),
+              "simpleGL.bin");
+  reference_file = sdkFindFilePath(ref_file, argv[0]);
+
+  if (reference_file &&
+      !sdkCompareBin2BinFloat("simpleGL.bin", reference_file,
+                              mesh_width * mesh_height * sizeof(float),
+                              MAX_EPSILON_ERROR, THRESHOLD, pArgv[0])) {
+    g_TotalErrors++;
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Display callback
+////////////////////////////////////////////////////////////////////////////////
+void display_thisframe(float time_delta) {
+  sdkStartTimer(&timer);
+
+  // run CUDA kernel to generate vertex positions
+  runCuda(&cuda_vbo_resource);
+
+  glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
+  // GET_GLERROR(0);
+
+  // set view matrix: broken, it doesn't work in OpenGL ES! Must put into shader
+  // glMatrixMode(GL_MODELVIEW);
+  // glLoadIdentity();
+  // glTranslatef(0.0, 0.0, translate_z);
+  // glRotatef(rotate_x, 1.0, 0.0, 0.0);
+  // glRotatef(rotate_y, 0.0, 1.0, 0.0);
+
+  glDrawArrays(GL_POINTS, 0, mesh_width * mesh_height);
+
+  // GET_GLERROR(0);
+  glFinish();
+  // GET_GLERROR(0);
+
+  g_fAnim += time_delta;
+
+  sdkStopTimer(&timer);
+  computeFPS();
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Check if the result is correct or write data to file for external
+//! regression testing
+////////////////////////////////////////////////////////////////////////////////
+void checkResultCuda(int argc, char **argv, const GLuint &vbo) {
+  if (!d_vbo_buffer) {
+    printf("%s: Mapping result buffer from OpenGL ES\n", __FUNCTION__);
+
+    hipGraphicsUnregisterResource(cuda_vbo_resource);
+
+    // map buffer object
+    glBindBuffer(GL_ARRAY_BUFFER, vbo);
+    float *data = (float *)glMapBufferRange(
+        GL_ARRAY_BUFFER, 0, mesh_width * mesh_height * 4 * sizeof(float),
+        GL_READ_ONLY);
+
+    // check result
+    if (checkCmdLineFlag(argc, (const char **)argv, "regression")) {
+      // write file for regression test
+      sdkWriteFile<float>("./data/regression.dat", data,
+                          mesh_width * mesh_height * 3, 0.0, false);
+    }
+
+    // unmap GL buffer object
+    if (!glUnmapBuffer(GL_ARRAY_BUFFER)) {
+      fprintf(stderr, "Unmap buffer failed.\n");
+      fflush(stderr);
+    }
+
+    HIPCHECK(hipGraphicsGLRegisterBuffer(
+        &cuda_vbo_resource, vbo, cudaGraphicsMapFlagsWriteDiscard));
+
+    GET_GLERROR(0);
+  }
+}
+
+GLuint mesh_shader = 0;
+
+void readAndCompileShaderFromGLSLFile(GLuint new_shaderprogram,
+                                      const char *filename, GLenum shaderType) {
+  FILE *file = fopen(filename, "rb");  // open shader text file
+  if (!file) error_exit("Filename %s does not exist\n", filename);
+
+  /* get the size of the file and read it */
+  fseek(file, 0, SEEK_END);
+  GLint size = ftell(file);
+  char *data = (char *)malloc(sizeof(char) * (size + 1));
+  memset(data, 0, sizeof(char) * (size + 1));
+  fseek(file, 0, SEEK_SET);
+  size_t res = fread(data, 1, size, file);
+  fclose(file);
+
+  GLuint shader = glCreateShader(shaderType);
+  glShaderSource(shader, 1, (const GLchar **)&data, &size);
+  glCompileShader(shader);
+
+  GET_GLERROR(0);
+  GLint compile_success = 0;
+  glGetShaderiv(shader, GL_COMPILE_STATUS, &compile_success);
+  GET_GLERROR(0);
+
+  if (compile_success == GL_FALSE) {
+    printf("Compilation of %s failed!\n Reason:\n", filename);
+
+    GLint maxLength = 0;
+    glGetShaderiv(shader, GL_INFO_LOG_LENGTH, &maxLength);
+
+    char errorLog[maxLength];
+    glGetShaderInfoLog(shader, maxLength, &maxLength, &errorLog[0]);
+
+    printf("%s", errorLog);
+
+    glDeleteShader(shader);
+    exit(1);
+  }
+
+  glAttachShader(new_shaderprogram, shader);
+  glDeleteShader(shader);  // good to do?
+
+  free(data);
+}
+
+GLuint ShaderCreate(const char *vshader_filename,
+                    const char *fshader_filename) {
+  printf("Loading GLSL shaders %s %s\n", vshader_filename, fshader_filename);
+
+  GLuint new_shaderprogram = glCreateProgram();
+
+  GET_GLERROR(0);
+  if (vshader_filename)
+    readAndCompileShaderFromGLSLFile(new_shaderprogram, vshader_filename,
+                                     GL_VERTEX_SHADER);
+
+  GET_GLERROR(0);
+  if (fshader_filename)
+    readAndCompileShaderFromGLSLFile(new_shaderprogram, fshader_filename,
+                                     GL_FRAGMENT_SHADER);
+
+  GET_GLERROR(0);
+
+  glLinkProgram(new_shaderprogram);
+
+  GET_GLERROR(0);
+  GLint link_success;
+  glGetProgramiv(new_shaderprogram, GL_LINK_STATUS, &link_success);
+
+  if (link_success == GL_FALSE) {
+    printf("Linking of %s with %s failed!\n Reason:\n", vshader_filename,
+           fshader_filename);
+
+    GLint maxLength = 0;
+    glGetShaderiv(new_shaderprogram, GL_INFO_LOG_LENGTH, &maxLength);
+
+    char errorLog[maxLength];
+    glGetShaderInfoLog(new_shaderprogram, maxLength, &maxLength, &errorLog[0]);
+
+    printf("%s", errorLog);
+
+    exit(EXIT_FAILURE);
+  }
+
+  return new_shaderprogram;
+}
+
+//===========================================================================
+// InitGraphicsState() - initialize OpenGL
+//===========================================================================
+static void InitGraphicsState(char **argv) {
+  char *GL_version = (char *)glGetString(GL_VERSION);
+  char *GL_vendor = (char *)glGetString(GL_VENDOR);
+  char *GL_renderer = (char *)glGetString(GL_RENDERER);
+
+  printf("Version: %s\n", GL_version);
+  printf("Vendor: %s\n", GL_vendor);
+  printf("Renderer: %s\n", GL_renderer);
+
+  // RENDERING SETUP (OpenGL ES or OpenGL Core Profile!)
+  glGenVertexArrays(1, &mesh_vao);  // Features' Vertex Array Object allocation
+  glBindVertexArray(mesh_vao);      // bind VAO
+
+  // initialize buffer object
+  glGenBuffers(1, &mesh_vbo);
+  glBindBuffer(GL_ARRAY_BUFFER, mesh_vbo);
+
+  unsigned int size = mesh_width * mesh_height * 4 * sizeof(float);
+  glBufferData(GL_ARRAY_BUFFER, size, NULL, GL_DYNAMIC_DRAW);
+  glVertexAttribPointer((GLuint)0, 4, GL_FLOAT, GL_FALSE, 0, 0);
+  glEnableVertexAttribArray(0);
+
+  HIPCHECK(hipGraphicsGLRegisterBuffer(&cuda_vbo_resource, mesh_vbo,
+                                               cudaGraphicsMapFlagsNone));
+
+  // glBindVertexArray(0); // keep above Vertex Array Object bound (it's the
+  // only one throughout)
+
+  // GLSL stuff
+  char *vertex_shader_path = sdkFindFilePath("mesh.vert.glsl", argv[0]);
+  char *fragment_shader_path = sdkFindFilePath("mesh.frag.glsl", argv[0]);
+
+  if (vertex_shader_path == NULL || fragment_shader_path == NULL) {
+    printf("Error finding shader file\n");
+    exit(EXIT_FAILURE);
+  }
+
+  mesh_shader = ShaderCreate(vertex_shader_path, fragment_shader_path);
+  GET_GLERROR(0);
+
+  free(vertex_shader_path);
+  free(fragment_shader_path);
+
+  glUseProgram(mesh_shader);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run a simple test for CUDA
+////////////////////////////////////////////////////////////////////////////////
+bool runTest(int argc, char **argv, char *ref_file) {
+  // Create the CUTIL timer
+  sdkCreateTimer(&timer);
+
+  int devID = 0;
+#if defined(__aarch64__) || defined(__arm__)
+  // find iGPU on the system which is compute capable which will perform
+  // GLES-CUDA interop
+  devID = findIntegratedGPU();
+#else
+  // use command-line specified CUDA device, otherwise use device with highest
+  // Gflops/s
+  devID = findCudaDevice(argc, (const char **)argv);
+#endif
+
+  // command line mode only
+  if (ref_file != NULL) {
+    // create VBO
+    HIPCHECK(hipMalloc((void **)&d_vbo_buffer,
+                               mesh_width * mesh_height * 4 * sizeof(float)));
+
+    // run the cuda part
+    runAutoTest(devID, argv, ref_file);
+
+    // check result of Cuda step
+    checkResultCuda(argc, argv, mesh_vbo);
+
+    hipFree(d_vbo_buffer);
+    d_vbo_buffer = NULL;
+  } else {
+    // this would use command-line specified CUDA device, note that CUDA
+    // defaults to highest Gflops/s device
+    if (checkCmdLineFlag(argc, (const char **)argv, "device"))
+      error_exit("Device setting not yet implemented!\n");
+
+    // create X11 window and set up associated OpenGL ES context
+    graphics_setup_window(0, 0, window_width, window_height, sSDKsample);
+
+    InitGraphicsState(argv);  // set up GLES stuff
+
+    glClearColor(0, 0.5, 1, 1);  // blue-ish background
+    glClear(GL_COLOR_BUFFER_BIT);
+
+    // printf("WP%d\n", __LINE__);
+    graphics_swap_buffers();
+
+    int frame = 0;
+
+    while (frame < 1000) {
+      display_thisframe(0.010);
+      usleep(1000);  // need not take full CPU and GPU
+
+      graphics_swap_buffers();
+      // printf("frame %d\n",frame++);
+    }
+
+    // NOTE: Before destroying OpenGL ES context, must unregister all shared
+    // resources from CUDA !
+    hipGraphicsUnregisterResource(cuda_vbo_resource);
+
+    graphics_close_window();  // close window and destroy OpenGL ES context
+
+    sdkDeleteTimer(&timer);
+  }
+
+  return true;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Program main
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv) {
+  char *ref_file = NULL;
+
+  pArgc = &argc;
+  pArgv = argv;
+
+#if defined(__linux__)
+  setenv("DISPLAY", ":0", 0);
+#endif
+
+  printf("%s starting...\n", sSDKsample);
+
+  if (argc > 1) {
+    if (checkCmdLineFlag(argc, (const char **)argv, "file")) {
+      // In this mode, we run without OpenGL and see if VBO is generated
+      // correctly
+      getCmdLineArgumentString(argc, (const char **)argv, "file",
+                               (char **)&ref_file);
+    }
+  }
+
+  printf("\n");
+
+  runTest(argc, argv, ref_file);
+
+  printf("%s completed, returned %s\n", sSDKsample,
+         (g_TotalErrors == 0) ? "OK" : "ERROR!");
+
+  exit(g_TotalErrors == 0 ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+S : EXIT_FAILURE);
+}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleGLES_screen/simpleGLES_screen.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleGLES_screen/simpleGLES_screen.cu.hip
index e69de29..8fd4bf7 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleGLES_screen/simpleGLES_screen.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleGLES_screen/simpleGLES_screen.cu.hip
@@ -0,0 +1,604 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+   This example demonstrates how to use the CUDA C bindings to OpenGL ES to
+   dynamically modify a vertex buffer using a CUDA C kernel.
+
+   The steps are:
+   1. Create an empty vertex buffer object (VBO)
+   2. Register the VBO with CUDA C
+   3. Map the VBO for writing from CUDA C
+   4. Run CUDA C kernel to modify the vertex positions
+   5. Unmap the VBO
+   6. Render the results using OpenGL ES
+
+   Host code
+ */
+
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <math.h>
+
+#include <stdarg.h>
+#include <unistd.h>
+#include <screen/screen.h>
+
+#include "graphics_interface.c"
+
+// includes, cuda
+#include <hip/hip_runtime.h>
+#include <cuda_gl_interop.h>
+
+// Utilities and timing functions
+#include <helper_functions.h>    // includes hip/hip_runtime.h and hip/hip_runtime_api.h
+
+// CUDA helper functions
+#include <helper_cuda.h>         // helper functions for CUDA error check
+
+#include <hip/hip_vector_types.h>
+
+#define MAX_EPSILON_ERROR 0.0f
+#define THRESHOLD 0.0f
+#define REFRESH_DELAY 1  // ms
+
+#define GUI_IDLE 0x100
+#define GUI_ROTATE 0x101
+#define GUI_TRANSLATE 0x102
+
+int gui_mode;
+
+////////////////////////////////////////////////////////////////////////////////
+// Default configuration
+unsigned int window_width = 512;
+unsigned int window_height = 512;
+unsigned int dispno = 0;
+
+// constants
+const unsigned int mesh_width = 256;
+const unsigned int mesh_height = 256;
+
+// OpenGL ES variables and interop with CUDA C
+GLuint mesh_vao, mesh_vbo;
+struct hipGraphicsResource *cuda_vbo_resource;
+void *d_vbo_buffer = NULL;
+
+float g_fAnim = 0.0;
+
+// UI / mouse controls
+int mouse_old_x, mouse_old_y;
+int mouse_buttons = 0;
+float rotate_x = 0.0, rotate_y = 0.0;
+float translate_z = -3.0;
+
+StopWatchInterface *timer = NULL;
+
+// Frame statistics
+int frame;
+int fpsCount = 0;  // FPS count for averaging
+int fpsLimit = 1;  // FPS limit for sampling
+int g_Index = 0;
+float avgFPS = 0.0f;
+unsigned int frameCount = 0;
+unsigned int g_TotalErrors = 0;
+
+// The default number of seconds after which the test will end.
+#define TIME_LIMIT 10.0  // 10 secs
+
+// Flag indicating it is time to shut down
+static GLboolean shutdown = GL_FALSE;
+
+// Callback to close window
+static void closeCB_app(void) { shutdown = GL_TRUE; }
+
+// Callback to handle key presses
+static void keyCB_app(char key, int state) {
+  // Ignoring releases
+  if (!state) return;
+
+  if ((key == 'q') || (key == 'Q') || (key == NvGlDemoKeyCode_Escape))
+    shutdown = GL_TRUE;
+}
+
+// Auto-Verification Code
+bool g_bQAReadback = false;
+
+int *pArgc = NULL;
+char **pArgv = NULL;
+
+#define MAX(a, b) ((a > b) ? a : b)
+
+////////////////////////////////////////////////////////////////////////////////
+// declaration, forward
+
+// CUDA functionality
+void runCuda(struct hipGraphicsResource **vbo_resource);
+void runAutoTest(int devID, char **argv, char *ref_file);
+void checkResultCuda(int argc, char **argv, const GLuint &vbo);
+
+const char *sSDKsample = "simpleGLES on Screen (VBO)";
+
+void computeFPS() {
+  frameCount++;
+  fpsCount++;
+
+  if (fpsCount == fpsLimit) {
+    avgFPS = 1.f / (sdkGetAverageTimerValue(&timer) / 1000.f);
+    fpsCount = 0;
+    fpsLimit = (int)MAX(avgFPS, 1.f);
+
+    sdkResetTimer(&timer);
+  }
+
+  char fps[256];
+  sprintf(fps, "Cuda/OpenGL ES Interop (VBO): %3.1f fps (Max 1000 fps)",
+          avgFPS);
+  graphics_set_windowtitle(fps);
+}
+
+///////////////////////////////////////////////////////////////////////////////
+//! Simple kernel to modify vertex positions in sine wave pattern
+//! @param data  data in global memory
+///////////////////////////////////////////////////////////////////////////////
+__global__ void simple_vbo_kernel(float4 *pos, unsigned int width,
+                                  unsigned int height, float time) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  // calculate uv coordinates
+  float u = x / (float)width;
+  float v = y / (float)height;
+  u = u * 2.0f - 1.0f;
+  v = v * 2.0f - 1.0f;
+
+  // calculate simple sine wave pattern
+  float freq = 4.0f;
+  float w = sinf(u * freq + time) * cosf(v * freq + time) * 0.5f;
+
+  // write output vertex
+  pos[y * width + x] = make_float4(u, w, v, 1.0f);
+}
+
+void launch_kernel(float4 *pos, unsigned int mesh_width,
+                   unsigned int mesh_height, float time) {
+  // execute the kernel
+  dim3 block(8, 8, 1);
+  dim3 grid(mesh_width / block.x, mesh_height / block.y, 1);
+  simple_vbo_kernel<<<grid, block>>>(pos, mesh_width, mesh_height, time);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run the Cuda part of the computation
+////////////////////////////////////////////////////////////////////////////////
+void runCuda(struct hipGraphicsResource **vbo_resource) {
+  // map OpenGL buffer object for writing from CUDA
+  float4 *dptr;
+  hipGraphicsMapResources(1, vbo_resource, 0);
+  size_t num_bytes;
+  hipGraphicsResourceGetMappedPointer((void **)&dptr, &num_bytes,
+                                       *vbo_resource);
+
+  launch_kernel(dptr, mesh_width, mesh_height, g_fAnim);
+
+  // unmap buffer object
+  hipGraphicsUnmapResources(1, vbo_resource, 0);
+}
+
+#ifndef FOPEN
+#define FOPEN(fHandle, filename, mode) (fHandle = fopen(filename, mode))
+#endif
+
+void sdkDumpBin2(void *data, unsigned int bytes, const char *filename) {
+  printf("sdkDumpBin: <%s>\n", filename);
+  FILE *fp;
+  FOPEN(fp, filename, "wb");
+  fwrite(data, bytes, 1, fp);
+  fflush(fp);
+  fclose(fp);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run the Cuda part of the computation
+////////////////////////////////////////////////////////////////////////////////
+void runAutoTest(int devID, char **argv, char *ref_file) {
+  char *reference_file = NULL;
+  void *imageData = malloc(mesh_width * mesh_height * sizeof(float));
+
+  // execute the kernel
+  launch_kernel((float4 *)d_vbo_buffer, mesh_width, mesh_height, g_fAnim);
+
+  hipDeviceSynchronize();
+  getLastCudaError("launch_kernel failed");
+
+  hipMemcpy(imageData, d_vbo_buffer, mesh_width * mesh_height * sizeof(float),
+             hipMemcpyDeviceToHost);
+
+  sdkDumpBin2(imageData, mesh_width * mesh_height * sizeof(float),
+              "simpleGLES_screen.bin");
+  reference_file = sdkFindFilePath(ref_file, argv[0]);
+
+  if (reference_file &&
+      !sdkCompareBin2BinFloat("simpleGLES_screen.bin", reference_file,
+                              mesh_width * mesh_height * sizeof(float),
+                              MAX_EPSILON_ERROR, THRESHOLD, pArgv[0])) {
+    g_TotalErrors++;
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Display callback
+////////////////////////////////////////////////////////////////////////////////
+void display_thisframe(float time_delta) {
+  sdkStartTimer(&timer);
+
+  // run CUDA kernel to generate vertex positions
+  runCuda(&cuda_vbo_resource);
+
+  glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
+
+  glDrawArrays(GL_POINTS, 0, mesh_width * mesh_height);
+
+  glFinish();
+
+  g_fAnim += time_delta;
+
+  sdkStopTimer(&timer);
+  computeFPS();
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Check if the result is correct or write data to file for external
+//! regression testing
+////////////////////////////////////////////////////////////////////////////////
+void checkResultCuda(int argc, char **argv, const GLuint &vbo) {
+  if (!d_vbo_buffer) {
+    printf("%s: Mapping result buffer from OpenGL ES\n", __FUNCTION__);
+
+    hipGraphicsUnregisterResource(cuda_vbo_resource);
+
+    // map buffer object
+    glBindBuffer(GL_ARRAY_BUFFER, vbo);
+    float *data = (float *)glMapBufferRange(
+        GL_ARRAY_BUFFER, 0, mesh_width * mesh_height * 4 * sizeof(float),
+        GL_READ_ONLY);
+
+    // check result
+    if (checkCmdLineFlag(argc, (const char **)argv, "regression")) {
+      // write file for regression test
+      sdkWriteFile<float>("./data/regression.dat", data,
+                          mesh_width * mesh_height * 3, 0.0, false);
+    }
+
+    // unmap GL buffer object
+    if (!glUnmapBuffer(GL_ARRAY_BUFFER)) {
+      fprintf(stderr, "Unmap buffer failed.\n");
+      fflush(stderr);
+    }
+
+    HIPCHECK(hipGraphicsGLRegisterBuffer(
+        &cuda_vbo_resource, vbo, cudaGraphicsMapFlagsWriteDiscard));
+
+    CHECK_GLERROR();
+  }
+}
+
+GLuint mesh_shader = 0;
+
+void readAndCompileShaderFromGLSLFile(GLuint new_shaderprogram,
+                                      const char *filename, GLenum shaderType) {
+  FILE *file = fopen(filename, "rb");  // open shader text file
+  if (!file) {
+    error_exit("Filename %s does not exist\n", filename);
+  }
+
+  // get the size of the file and read it
+  fseek(file, 0, SEEK_END);
+  GLint size = ftell(file);
+  char *data = (char *)malloc(sizeof(char) * (size + 1));
+  memset(data, 0, sizeof(char) * (size + 1));
+  fseek(file, 0, SEEK_SET);
+  size_t res = fread(data, 1, size, file);
+  fclose(file);
+
+  GLuint shader = glCreateShader(shaderType);
+  glShaderSource(shader, 1, (const GLchar **)&data, &size);
+  glCompileShader(shader);
+
+  CHECK_GLERROR();
+  GLint compile_success = 0;
+  glGetShaderiv(shader, GL_COMPILE_STATUS, &compile_success);
+  CHECK_GLERROR();
+
+  if (compile_success == GL_FALSE) {
+    printf("Compilation of %s failed!\n Reason:\n", filename);
+
+    GLint maxLength = 0;
+    glGetShaderiv(shader, GL_INFO_LOG_LENGTH, &maxLength);
+
+    char errorLog[maxLength];
+    glGetShaderInfoLog(shader, maxLength, &maxLength, &errorLog[0]);
+
+    printf("%s", errorLog);
+
+    glDeleteShader(shader);
+    exit(1);
+  }
+
+  glAttachShader(new_shaderprogram, shader);
+  glDeleteShader(shader);
+
+  free(data);
+}
+
+GLuint ShaderCreate(const char *vshader_filename,
+                    const char *fshader_filename) {
+  printf("Loading GLSL shaders %s %s\n", vshader_filename, fshader_filename);
+
+  GLuint new_shaderprogram = glCreateProgram();
+
+  CHECK_GLERROR();
+  if (vshader_filename) {
+    readAndCompileShaderFromGLSLFile(new_shaderprogram, vshader_filename,
+                                     GL_VERTEX_SHADER);
+  }
+
+  CHECK_GLERROR();
+  if (fshader_filename) {
+    readAndCompileShaderFromGLSLFile(new_shaderprogram, fshader_filename,
+                                     GL_FRAGMENT_SHADER);
+  }
+
+  CHECK_GLERROR();
+
+  glLinkProgram(new_shaderprogram);
+
+  CHECK_GLERROR();
+  GLint link_success;
+  glGetProgramiv(new_shaderprogram, GL_LINK_STATUS, &link_success);
+
+  if (link_success == GL_FALSE) {
+    printf("Linking of %s with %s failed!\n Reason:\n", vshader_filename,
+           fshader_filename);
+
+    GLint maxLength = 0;
+    glGetShaderiv(new_shaderprogram, GL_INFO_LOG_LENGTH, &maxLength);
+
+    char errorLog[maxLength];
+    glGetShaderInfoLog(new_shaderprogram, maxLength, &maxLength, &errorLog[0]);
+
+    printf("%s", errorLog);
+
+    exit(EXIT_FAILURE);
+  }
+
+  return new_shaderprogram;
+}
+
+//===========================================================================
+// InitGraphicsState() - initialize OpenGL
+//===========================================================================
+static void InitGraphicsState(void) {
+  char *GL_version = (char *)glGetString(GL_VERSION);
+  char *GL_vendor = (char *)glGetString(GL_VENDOR);
+  char *GL_renderer = (char *)glGetString(GL_RENDERER);
+
+  printf("Version: %s\n", GL_version);
+  printf("Vendor: %s\n", GL_vendor);
+  printf("Renderer: %s\n", GL_renderer);
+
+  // RENDERING SETUP (OpenGL ES or OpenGL Core Profile!)
+  glGenVertexArrays(1, &mesh_vao);  // Features' Vertex Array Object allocation
+  glBindVertexArray(mesh_vao);      // bind VAO
+
+  // initialize buffer object
+  glGenBuffers(1, &mesh_vbo);
+  glBindBuffer(GL_ARRAY_BUFFER, mesh_vbo);
+
+  unsigned int size = mesh_width * mesh_height * 4 * sizeof(float);
+  glBufferData(GL_ARRAY_BUFFER, size, NULL, GL_DYNAMIC_DRAW);
+  glVertexAttribPointer((GLuint)0, 4, GL_FLOAT, GL_FALSE, 0, 0);
+  glEnableVertexAttribArray(0);
+
+  HIPCHECK(hipGraphicsGLRegisterBuffer(&cuda_vbo_resource, mesh_vbo,
+                                               cudaGraphicsMapFlagsNone));
+
+  // GLSL stuff
+  char *vertex_shader_path = sdkFindFilePath("mesh.vert.glsl", pArgv[0]);
+  char *fragment_shader_path = sdkFindFilePath("mesh.frag.glsl", pArgv[0]);
+
+  if (vertex_shader_path == NULL || fragment_shader_path == NULL) {
+    printf("Error finding shader file\n");
+    exit(EXIT_FAILURE);
+  }
+
+  mesh_shader = ShaderCreate(vertex_shader_path, fragment_shader_path);
+  CHECK_GLERROR();
+
+  free(vertex_shader_path);
+  free(fragment_shader_path);
+
+  glUseProgram(mesh_shader);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run a simple test for CUDA
+////////////////////////////////////////////////////////////////////////////////
+bool runTest(int argc, char **argv, char *ref_file) {
+  // command line mode only
+  if (ref_file != NULL) {
+    // This will pick the best possible CUDA capable device
+    // int devID = findCudaDevice(argc, (const char **)argv);
+#if defined(__aarch64__) || defined(__arm__)
+    // find iGPU on the system which is compute capable which will perform
+    // GLES-CUDA interop
+    int devID = findIntegratedGPU();
+#else
+    // use command-line specified CUDA device, otherwise use device with highest
+    // Gflops/s
+    int devID = findCudaDevice(argc, (const char **)argv);
+#endif
+
+    // create VBO
+    HIPCHECK(hipMalloc((void **)&d_vbo_buffer,
+                               mesh_width * mesh_height * 4 * sizeof(float)));
+
+    // run the cuda part
+    runAutoTest(devID, argv, ref_file);
+
+    // check result of Cuda step
+    checkResultCuda(argc, argv, mesh_vbo);
+
+    hipFree(d_vbo_buffer);
+    d_vbo_buffer = NULL;
+  } else {
+    double endTime = TIME_LIMIT;
+
+    // this would use command-line specified CUDA device, note that CUDA
+    // defaults to highest Gflops/s device
+    if (checkCmdLineFlag(argc, (const char **)argv, "device")) {
+      error_exit("Device setting not yet implemented!\n");
+    }
+
+    // display selection
+    if (checkCmdLineFlag(argc, (const char **)argv, "dispno")) {
+      dispno = getCmdLineArgumentInt(argc, (const char **)argv, "dispno");
+    }
+
+    // Window width
+    if (checkCmdLineFlag(argc, (const char **)argv, "width")) {
+      window_width = getCmdLineArgumentInt(argc, (const char **)argv, "width");
+    }
+
+    // Window Height
+    if (checkCmdLineFlag(argc, (const char **)argv, "height")) {
+      window_height =
+          getCmdLineArgumentInt(argc, (const char **)argv, "height");
+    }
+
+    // Determine how long to run for in secs: default is 10s
+    if (checkCmdLineFlag(argc, (const char **)argv, "runtime")) {
+      endTime = getCmdLineArgumentInt(argc, (const char **)argv, "runtime");
+    }
+
+    SetCloseCB(closeCB_app);
+    SetKeyCB(keyCB_app);
+
+    // create QNX screen window and set up associated OpenGL ES context
+    graphics_setup_window(0, 0, window_width, window_height, sSDKsample,
+                          dispno);
+
+#if defined(__aarch64__) || defined(__arm__)
+    // find iGPU on the system which is compute capable which will perform
+    // GLES-CUDA interop
+    int devID = findIntegratedGPU();
+#else
+    // use command-line specified CUDA device, otherwise use device with highest
+    // Gflops/s
+    int devID = findCudaDevice(argc, (const char **)argv);
+#endif
+    InitGraphicsState();  // set up GLES stuff
+
+    glClearColor(0, 0.5, 1, 1);  // blue-ish background
+    glClear(GL_COLOR_BUFFER_BIT);
+
+    graphics_swap_buffers();
+
+    int frame = 0;
+
+    struct timeval begin, end;
+    gettimeofday(&begin, NULL);
+
+    // Print runtime
+    if (endTime < 0.0) {
+      endTime = TIME_LIMIT;
+      printf(" running forever...\n");
+    } else {
+      printf(" running for %f seconds...\n", endTime);
+    }
+
+    while (!shutdown) {
+      frame++;
+      display_thisframe(0.010);
+      usleep(1000);
+      graphics_swap_buffers();
+      CheckEvents();
+
+      gettimeofday(&end, 0);
+      double elapsed = (end.tv_sec - begin.tv_sec) +
+                       ((end.tv_usec - begin.tv_usec) / 1000000.0);
+
+      // Check whether time limit has been exceeded
+      if (!shutdown) shutdown = (endTime <= elapsed);
+    }
+
+    // NOTE: Before destroying OpenGL ES context, must unregister all shared
+    // resources from CUDA !
+    HIPCHECK(hipGraphicsUnregisterResource(cuda_vbo_resource));
+
+    graphics_close_window();  // close window and destroy OpenGL ES context
+  }
+
+  return true;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Program main
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv) {
+  char *ref_file = NULL;
+
+  pArgc = &argc;
+  pArgv = argv;
+
+#if defined(__linux__)
+  setenv("DISPLAY", ":0", 0);
+#endif
+
+  printf("%s starting...\n", sSDKsample);
+
+  if (argc > 1) {
+    if (checkCmdLineFlag(argc, (const char **)argv, "file")) {
+      // In this mode, we run without OpenGL and see if VBO is generated
+      // correctly
+      getCmdLineArgumentString(argc, (const char **)argv, "file",
+                               (char **)&ref_file);
+    }
+  }
+
+  printf("\n");
+
+  runTest(argc, argv, ref_file);
+
+  printf("%s completed, returned %s\n", sSDKsample,
+         (g_TotalErrors == 0) ? "OK" : "ERROR!");
+
+  exit(g_TotalErrors == 0 ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+_SUCCESS : EXIT_FAILURE);
+}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleVulkan/SineWaveSimulation.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleVulkan/SineWaveSimulation.cu.hip
index e69de29..e0408d3 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleVulkan/SineWaveSimulation.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleVulkan/SineWaveSimulation.cu.hip
@@ -0,0 +1,137 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "SineWaveSimulation.h"
+#include <algorithm>
+#include "helper_cuda_hipified.h"
+
+__global__ void sinewave(float *heightMap, unsigned int width,
+                         unsigned int height, float time) {
+  const float freq = 4.0f;
+  const size_t stride = gridDim.x * blockDim.x;
+
+  // Iterate through the entire array in a way that is
+  // independent of the grid configuration
+  for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < width * height;
+       tid += stride) {
+    // Calculate the x, y coordinates
+    const size_t y = tid / width;
+    const size_t x = tid - y * width;
+    // Normalize x, y to [0,1]
+    const float u = ((2.0f * x) / width) - 1.0f;
+    const float v = ((2.0f * y) / height) - 1.0f;
+    // Calculate the new height value
+    const float w = 0.5f * sinf(u * freq + time) * cosf(v * freq + time);
+    // Store this new height value
+    heightMap[tid] = w;
+  }
+}
+
+SineWaveSimulation::SineWaveSimulation(size_t width, size_t height)
+    : m_heightMap(nullptr), m_width(width), m_height(height) {}
+
+void SineWaveSimulation::initCudaLaunchConfig(int device) {
+  hipDeviceProp_t prop = {};
+  HIPCHECK(hipSetDevice(device));
+  HIPCHECK(hipGetDeviceProperties(&prop, device));
+
+  // We don't need large block sizes, since there's not much inter-thread
+  // communication
+  m_threads = prop.warpSize;
+
+  // Use the occupancy calculator and fill the gpu as best as we can
+  HIPCHECK(hipOccupancyMaxActiveBlocksPerMultiprocessor(
+      &m_blocks, sinewave, prop.warpSize, 0));
+  m_blocks *= prop.multiProcessorCount;
+
+  // Go ahead and the clamp the blocks to the minimum needed for this
+  // height/width
+  m_blocks = std::min(m_blocks,
+                      (int)((m_width * m_height + m_threads - 1) / m_threads));
+}
+
+int SineWaveSimulation::initCuda(uint8_t *vkDeviceUUID, size_t UUID_SIZE) {
+  int current_device = 0;
+  int device_count = 0;
+  int devices_prohibited = 0;
+
+  hipDeviceProp_t deviceProp;
+  HIPCHECK(hipGetDeviceCount(&device_count));
+
+  if (device_count == 0) {
+    fprintf(stderr, "CUDA error: no devices supporting CUDA.\n");
+    exit(EXIT_FAILURE);
+  }
+
+  // Find the GPU which is selected by Vulkan
+  while (current_device < device_count) {
+    hipGetDeviceProperties(&deviceProp, current_device);
+
+    if ((deviceProp.computeMode != hipComputeModeProhibited)) {
+      // Compare the cuda device UUID with vulkan UUID
+      int ret = memcmp((void *)&deviceProp.uuid, vkDeviceUUID, UUID_SIZE);
+      if (ret == 0) {
+        HIPCHECK(hipSetDevice(current_device));
+        HIPCHECK(hipGetDeviceProperties(&deviceProp, current_device));
+        printf("GPU Device %d: \"%s\" with compute capability %d.%d\n\n",
+               current_device, deviceProp.name, deviceProp.major,
+               deviceProp.minor);
+
+        return current_device;
+      }
+
+    } else {
+      devices_prohibited++;
+    }
+
+    current_device++;
+  }
+
+  if (devices_prohibited == device_count) {
+    fprintf(stderr,
+            "CUDA error:"
+            " No Vulkan-CUDA Interop capable GPU found.\n");
+    exit(EXIT_FAILURE);
+  }
+
+  return -1;
+}
+
+SineWaveSimulation::~SineWaveSimulation() { m_heightMap = NULL; }
+
+void SineWaveSimulation::initSimulation(float *heights) {
+  m_heightMap = heights;
+}
+
+void SineWaveSimulation::stepSimulation(float time, hipStream_t stream) {
+  sinewave<<<m_blocks, m_threads, 0, stream>>>(m_heightMap, m_width, m_height,
+                                               time);
+  getLastCudaError("Failed to launch CUDA simulation");
+}
+or("Failed to launch CUDA simulation");
+}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleVulkanMMAP/MonteCarloPi.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleVulkanMMAP/MonteCarloPi.cu.hip
index 0e359ed..97c1700 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleVulkanMMAP/MonteCarloPi.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleVulkanMMAP/MonteCarloPi.cu.hip
@@ -306,3 +306,4 @@ void MonteCarloPiSimulation::cleanupSimulationAllocations() {
     m_xyVector = nullptr;
     m_pointsInsideCircle = nullptr;
   }
+}
diff --git a/src/samples/Samples/5_Domain_Specific/smokeParticles/ParticleSystem_cuda.cu.hip b/src/samples/Samples/5_Domain_Specific/smokeParticles/ParticleSystem_cuda.cu.hip
index e69de29..a0b5f06 100755
--- a/src/samples/Samples/5_Domain_Specific/smokeParticles/ParticleSystem_cuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/smokeParticles/ParticleSystem_cuda.cu.hip
@@ -0,0 +1,155 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+This file contains simple wrapper functions that call the CUDA kernels
+*/
+#define HELPERGL_EXTERN_GL_FUNC_IMPLEMENTATION
+#include <helper_gl.h>
+#include "helper_cuda_hipified.h"
+#include <cstdlib>
+#include <cstdio>
+#include <string.h>
+#include <cuda_gl_interop.h>
+
+#include "thrust/device_ptr.h"
+#include "thrust/for_each.h"
+#include "thrust/iterator/zip_iterator.h"
+#include "thrust/sort.h"
+
+#include "particles_kernel_device.cuh"
+#include "ParticleSystem.cuh"
+
+extern "C" {
+
+hipArray *noiseArray;
+
+void setParameters(SimParams *hostParams) {
+  // copy parameters to constant memory
+  HIPCHECK(hipMemcpyToSymbol(HIP_SYMBOL(params), hostParams, sizeof(SimParams)));
+}
+
+// Round a / b to nearest higher integer value
+int iDivUp(int a, int b) { return (a % b != 0) ? (a / b + 1) : (a / b); }
+
+// compute grid and thread block size for a given number of elements
+void computeGridSize(int n, int blockSize, int &numBlocks, int &numThreads) {
+  numThreads = min(blockSize, n);
+  numBlocks = iDivUp(n, numThreads);
+}
+
+inline float frand() { return rand() / (float)RAND_MAX; }
+
+// create 3D texture containing random values
+void createNoiseTexture(int w, int h, int d) {
+  hipExtent size = make_hipExtent(w, h, d);
+  size_t elements = size.width * size.height * size.depth;
+
+  float *volumeData = (float *)malloc(elements * 4 * sizeof(float));
+  float *ptr = volumeData;
+
+  for (size_t i = 0; i < elements; i++) {
+    *ptr++ = frand() * 2.0f - 1.0f;
+    *ptr++ = frand() * 2.0f - 1.0f;
+    *ptr++ = frand() * 2.0f - 1.0f;
+    *ptr++ = frand() * 2.0f - 1.0f;
+  }
+
+  hipChannelFormatDesc channelDesc = hipCreateChannelDesc<float4>();
+  HIPCHECK(hipMalloc3DArray(&noiseArray, &channelDesc, size));
+
+  hipMemcpy3DParms copyParams = {0};
+  copyParams.srcPtr = make_hipPitchedPtr(
+      (void *)volumeData, size.width * sizeof(float4), size.width, size.height);
+  copyParams.dstArray = noiseArray;
+  copyParams.extent = size;
+  copyParams.kind = hipMemcpyHostToDevice;
+  HIPCHECK(hipMemcpy3D(&copyParams));
+
+  free(volumeData);
+
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = noiseArray;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = true;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.addressMode[1] = hipAddressModeWrap;
+  texDescr.addressMode[2] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(hipCreateTextureObject(&noiseTex, &texRes, &texDescr, NULL));
+}
+
+void integrateSystem(float4 *oldPos, float4 *newPos, float4 *oldVel,
+                     float4 *newVel, float deltaTime, int numParticles) {
+  thrust::device_ptr<float4> d_newPos(newPos);
+  thrust::device_ptr<float4> d_newVel(newVel);
+  thrust::device_ptr<float4> d_oldPos(oldPos);
+  thrust::device_ptr<float4> d_oldVel(oldVel);
+
+  thrust::for_each(thrust::make_zip_iterator(thrust::make_tuple(
+                       d_newPos, d_newVel, d_oldPos, d_oldVel)),
+                   thrust::make_zip_iterator(thrust::make_tuple(
+                       d_newPos + numParticles, d_newVel + numParticles,
+                       d_oldPos + numParticles, d_oldVel + numParticles)),
+                   integrate_functor(deltaTime, noiseTex));
+}
+
+void calcDepth(float4 *pos,
+               float *keys,    // output
+               uint *indices,  // output
+               float3 sortVector, int numParticles) {
+  thrust::device_ptr<float4> d_pos(pos);
+  thrust::device_ptr<float> d_keys(keys);
+  thrust::device_ptr<uint> d_indices(indices);
+
+  thrust::for_each(thrust::make_zip_iterator(thrust::make_tuple(d_pos, d_keys)),
+                   thrust::make_zip_iterator(thrust::make_tuple(
+                       d_pos + numParticles, d_keys + numParticles)),
+                   calcDepth_functor(sortVector));
+
+  thrust::sequence(d_indices, d_indices + numParticles);
+}
+
+void sortParticles(float *sortKeys, uint *indices, uint numParticles) {
+  thrust::sort_by_key(thrust::device_ptr<float>(sortKeys),
+                      thrust::device_ptr<float>(sortKeys + numParticles),
+                      thrust::device_ptr<uint>(indices));
+}
+
+}  // extern "C"
+ices));
+}
+
+}  // extern "C"
diff --git a/src/samples/Samples/5_Domain_Specific/stereoDisparity/stereoDisparity.cu.hip b/src/samples/Samples/5_Domain_Specific/stereoDisparity/stereoDisparity.cu.hip
index e69de29..9177090 100755
--- a/src/samples/Samples/5_Domain_Specific/stereoDisparity/stereoDisparity.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/stereoDisparity/stereoDisparity.cu.hip
@@ -0,0 +1,283 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* A CUDA program that demonstrates how to compute a stereo disparity map using
+ * SIMD SAD (Sum of Absolute Difference) intrinsics
+ */
+
+// includes, system
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <math.h>
+
+// includes, kernels
+#include <hip/hip_runtime.h>
+#include "stereoDisparity_kernel.cuh"
+
+// includes, project
+#include <helper_functions.h>  // helper for shared that are common to CUDA Samples
+#include "helper_cuda_hipified.h"  // helper for checking cuda initialization and error checking
+#include "helper_string.h"  // helper functions for string parsing
+#include "HIPCHECK.h"
+static const char *sSDKsample = "[stereoDisparity]\0";
+
+int iDivUp(int a, int b) { return ((a % b) != 0) ? (a / b + 1) : (a / b); }
+
+////////////////////////////////////////////////////////////////////////////////
+// declaration, forward
+void runTest(int argc, char **argv);
+
+////////////////////////////////////////////////////////////////////////////////
+// Program main
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv) {
+  printf("%s Starting...\n\n", sSDKsample);
+  runTest(argc, argv);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! CUDA Sample for calculating depth maps
+////////////////////////////////////////////////////////////////////////////////
+void runTest(int argc, char **argv) {
+  hipDeviceProp_t deviceProp;
+  deviceProp.major = 0;
+  deviceProp.minor = 0;
+  int dev = 0;
+
+  // This will pick the best possible CUDA capable device
+  dev = findCudaDevice(argc, (const char **)argv);
+
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, dev));
+
+  // Statistics about the GPU device
+  printf(
+      "> GPU device has %d Multi-Processors, SM %d.%d compute capabilities\n\n",
+      deviceProp.multiProcessorCount, deviceProp.major, deviceProp.minor);
+
+  StopWatchInterface *timer;
+  sdkCreateTimer(&timer);
+
+  // Search parameters
+  int minDisp = -16;
+  int maxDisp = 0;
+
+  // Load image data
+  // allocate mem for the images on host side
+  // initialize pointers to NULL to request lib call to allocate as needed
+  // PPM images are loaded into 4 byte/pixel memory (RGBX)
+  unsigned char *h_img0 = NULL;
+  unsigned char *h_img1 = NULL;
+  unsigned int w, h;
+  char *fname0 = sdkFindFilePath("stereo.im0.640x533.ppm", argv[0]);
+  char *fname1 = sdkFindFilePath("stereo.im1.640x533.ppm", argv[0]);
+
+  printf("Loaded <%s> as image 0\n", fname0);
+
+  if (!sdkLoadPPM4ub(fname0, &h_img0, &w, &h)) {
+    fprintf(stderr, "Failed to load <%s>\n", fname0);
+  }
+
+  printf("Loaded <%s> as image 1\n", fname1);
+
+  if (!sdkLoadPPM4ub(fname1, &h_img1, &w, &h)) {
+    fprintf(stderr, "Failed to load <%s>\n", fname1);
+  }
+
+  dim3 numThreads = dim3(blockSize_x, blockSize_y, 1);
+  dim3 numBlocks = dim3(iDivUp(w, numThreads.x), iDivUp(h, numThreads.y));
+  unsigned int numData = w * h;
+  unsigned int memSize = sizeof(int) * numData;
+
+  // allocate mem for the result on host side
+  unsigned int *h_odata = (unsigned int *)malloc(memSize);
+
+  // initialize the memory
+  for (unsigned int i = 0; i < numData; i++) h_odata[i] = 0;
+
+  // allocate device memory for result
+  unsigned int *d_odata, *d_img0, *d_img1;
+
+  HIPCHECK(hipMalloc((void **)&d_odata, memSize));
+  HIPCHECK(hipMalloc((void **)&d_img0, memSize));
+  HIPCHECK(hipMalloc((void **)&d_img1, memSize));
+
+  // copy host memory to device to initialize to zeros
+  HIPCHECK(hipMemcpy(d_img0, h_img0, memSize, hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(d_img1, h_img1, memSize, hipMemcpyHostToDevice));
+  HIPCHECK(
+      hipMemcpy(d_odata, h_odata, memSize, hipMemcpyHostToDevice));
+
+  hipChannelFormatDesc ca_desc0 = hipCreateChannelDesc<unsigned int>();
+  hipChannelFormatDesc ca_desc1 = hipCreateChannelDesc<unsigned int>();
+
+  hipTextureObject_t tex2Dleft, tex2Dright;
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypePitch2D;
+  texRes.res.pitch2D.devPtr = d_img0;
+  texRes.res.pitch2D.desc = ca_desc0;
+  texRes.res.pitch2D.width = w;
+  texRes.res.pitch2D.height = h;
+  texRes.res.pitch2D.pitchInBytes = w * 4;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModePoint;
+  texDescr.addressMode[0] = hipAddressModeClamp;
+  texDescr.addressMode[1] = hipAddressModeClamp;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(
+      hipCreateTextureObject(&tex2Dleft, &texRes, &texDescr, NULL));
+
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypePitch2D;
+  texRes.res.pitch2D.devPtr = d_img1;
+  texRes.res.pitch2D.desc = ca_desc1;
+  texRes.res.pitch2D.width = w;
+  texRes.res.pitch2D.height = h;
+  texRes.res.pitch2D.pitchInBytes = w * 4;
+
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModePoint;
+  texDescr.addressMode[0] = hipAddressModeClamp;
+  texDescr.addressMode[1] = hipAddressModeClamp;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(
+      hipCreateTextureObject(&tex2Dright, &texRes, &texDescr, NULL));
+
+  // First run the warmup kernel (which we'll use to get the GPU in the correct
+  // max power state
+  stereoDisparityKernel<<<numBlocks, numThreads>>>(
+      d_img0, d_img1, d_odata, w, h, minDisp, maxDisp, tex2Dleft, tex2Dright);
+  hipDeviceSynchronize();
+
+  // Allocate CUDA events that we'll use for timing
+  hipEvent_t start, stop;
+  HIPCHECK(hipEventCreate(&start));
+  HIPCHECK(hipEventCreate(&stop));
+
+  printf("Launching CUDA stereoDisparityKernel()\n");
+
+  // Record the start event
+  HIPCHECK(hipEventRecord(start, NULL));
+
+  // launch the stereoDisparity kernel
+  stereoDisparityKernel<<<numBlocks, numThreads>>>(
+      d_img0, d_img1, d_odata, w, h, minDisp, maxDisp, tex2Dleft, tex2Dright);
+
+  // Record the stop event
+  HIPCHECK(hipEventRecord(stop, NULL));
+
+  // Wait for the stop event to complete
+  HIPCHECK(hipEventSynchronize(stop));
+
+  // Check to make sure the kernel didn't fail
+  getLastCudaError("Kernel execution failed");
+
+  float msecTotal = 0.0f;
+  HIPCHECK(hipEventElapsedTime(&msecTotal, start, stop));
+
+  // Copy result from device to host for verification
+  HIPCHECK(
+      hipMemcpy(h_odata, d_odata, memSize, hipMemcpyDeviceToHost));
+
+  printf("Input Size  [%dx%d], ", w, h);
+  printf("Kernel size [%dx%d], ", (2 * RAD + 1), (2 * RAD + 1));
+  printf("Disparities [%d:%d]\n", minDisp, maxDisp);
+
+  printf("GPU processing time : %.4f (ms)\n", msecTotal);
+  printf("Pixel throughput    : %.3f Mpixels/sec\n",
+         ((float)(w * h * 1000.f) / msecTotal) / 1000000);
+
+  // calculate sum of resultant GPU image
+  unsigned int checkSum = 0;
+
+  for (unsigned int i = 0; i < w * h; i++) {
+    checkSum += h_odata[i];
+  }
+
+  printf("GPU Checksum = %u, ", checkSum);
+
+  // write out the resulting disparity image.
+  unsigned char *dispOut = (unsigned char *)malloc(numData);
+  int mult = 20;
+  const char *fnameOut = "output_GPU.pgm";
+
+  for (unsigned int i = 0; i < numData; i++) {
+    dispOut[i] = (int)h_odata[i] * mult;
+  }
+
+  printf("GPU image: <%s>\n", fnameOut);
+  sdkSavePGM(fnameOut, dispOut, w, h);
+
+  // compute reference solution
+  printf("Computing CPU reference...\n");
+  cpu_gold_stereo((unsigned int *)h_img0, (unsigned int *)h_img1,
+                  (unsigned int *)h_odata, w, h, minDisp, maxDisp);
+  unsigned int cpuCheckSum = 0;
+
+  for (unsigned int i = 0; i < w * h; i++) {
+    cpuCheckSum += h_odata[i];
+  }
+
+  printf("CPU Checksum = %u, ", cpuCheckSum);
+  const char *cpuFnameOut = "output_CPU.pgm";
+
+  for (unsigned int i = 0; i < numData; i++) {
+    dispOut[i] = (int)h_odata[i] * mult;
+  }
+
+  printf("CPU image: <%s>\n", cpuFnameOut);
+  sdkSavePGM(cpuFnameOut, dispOut, w, h);
+
+  // cleanup memory
+  HIPCHECK(hipFree(d_odata));
+  HIPCHECK(hipFree(d_img0));
+  HIPCHECK(hipFree(d_img1));
+
+  if (h_odata != NULL) free(h_odata);
+
+  if (h_img0 != NULL) free(h_img0);
+
+  if (h_img1 != NULL) free(h_img1);
+
+  if (dispOut != NULL) free(dispOut);
+
+  sdkDeleteTimer(&timer);
+
+  exit((checkSum == cpuCheckSum) ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+
diff --git a/src/samples/Samples/5_Domain_Specific/volumeFiltering/volumeFilter_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/volumeFiltering/volumeFilter_kernel.cu.hip
index e69de29..c397a84 100755
--- a/src/samples/Samples/5_Domain_Specific/volumeFiltering/volumeFilter_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/volumeFiltering/volumeFilter_kernel.cu.hip
@@ -0,0 +1,117 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _VOLUMEFILTER_KERNEL_CU_
+#define _VOLUMEFILTER_KERNEL_CU_
+
+#include "helper_cuda_hipified.h"
+#include <helper_math.h>
+#include "volumeFilter.h"
+
+typedef unsigned int uint;
+typedef unsigned char uchar;
+typedef unsigned short ushort;
+
+__constant__ float4 c_filterData[VOLUMEFILTER_MAXWEIGHTS];
+
+__global__ void d_filter_surface3d(int filterSize, float filter_offset,
+                                   hipExtent volumeSize,
+                                   hipTextureObject_t volumeTexIn,
+                                   hipSurfaceObject_t volumeTexOut) {
+  int x = blockIdx.x * blockDim.x + threadIdx.x;
+  int y = blockIdx.y * blockDim.y + threadIdx.y;
+  int z = blockIdx.z * blockDim.z + threadIdx.z;
+
+  if (x >= volumeSize.width || y >= volumeSize.height ||
+      z >= volumeSize.depth) {
+    return;
+  }
+
+  float filtered = 0;
+  float4 basecoord = make_float4(x, y, z, 0);
+
+  for (int i = 0; i < filterSize; i++) {
+    float4 coord = basecoord + c_filterData[i];
+    filtered += tex3D<float>(volumeTexIn, coord.x, coord.y, coord.z) *
+                c_filterData[i].w;
+  }
+
+  filtered += filter_offset;
+
+  VolumeType output = VolumeTypeInfo<VolumeType>::convert(filtered);
+
+  // surface writes need byte offsets for x!
+  surf3Dwrite(output, volumeTexOut, x * sizeof(VolumeType), y, z);
+}
+
+static unsigned int iDivUp(size_t a, size_t b) {
+  size_t val = (a % b != 0) ? (a / b + 1) : (a / b);
+  if (val > UINT_MAX) {
+    fprintf(stderr, "\nUINT_MAX limit exceeded in iDivUp() exiting.....\n");
+    exit(EXIT_FAILURE);  // val exceeds limit
+  }
+
+  return static_cast<unsigned int>(val);
+}
+
+extern "C" Volume *VolumeFilter_runFilter(Volume *input, Volume *output0,
+                                          Volume *output1, int iterations,
+                                          int numWeights, float4 *weights,
+                                          float postWeightOffset) {
+  Volume *swap = 0;
+  hipExtent size = input->size;
+  unsigned int dim = 32 / sizeof(VolumeType);
+  dim3 blockSize(dim, dim, 1);
+  dim3 gridSize(iDivUp(size.width, blockSize.x),
+                iDivUp(size.height, blockSize.y),
+                iDivUp(size.depth, blockSize.z));
+
+  // set weights
+  HIPCHECK(
+      hipMemcpyToSymbol(HIP_SYMBOL(c_filterData), weights, sizeof(float4) * numWeights));
+
+  for (int i = 0; i < iterations; i++) {
+    d_filter_surface3d<<<gridSize, blockSize>>>(numWeights, postWeightOffset,
+                                                size, input->volumeTex,
+                                                output0->volumeSurf);
+
+    getLastCudaError("filter kernel failed");
+
+    swap = input;
+    input = output0;
+    output0 = swap;
+
+    if (i == 0) {
+      output0 = output1;
+    }
+  }
+
+  return input;
+}
+#endif
+#endif
diff --git a/src/samples/Samples/5_Domain_Specific/volumeFiltering/volumeRender_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/volumeFiltering/volumeRender_kernel.cu.hip
index e69de29..1a111e0 100755
--- a/src/samples/Samples/5_Domain_Specific/volumeFiltering/volumeRender_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/volumeFiltering/volumeRender_kernel.cu.hip
@@ -0,0 +1,575 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// Simple 3D volume renderer
+
+#ifndef _VOLUMERENDER_KERNEL_CU_
+#define _VOLUMERENDER_KERNEL_CU_
+
+#include "helper_cuda_hipified.h"
+#include <helper_math.h>
+#include "volumeRender.h"
+
+#define VOLUMERENDER_TFS 2
+#define VOLUMERENDER_TF_PREINTSIZE 1024
+#define VOLUMERENDER_TF_PREINTSTEPS 1024
+#define VOLUMERENDER_TF_PREINTRAY 4
+
+enum TFMode {
+  TF_SINGLE_1D = 0,          // single 1D TF for everything
+  TF_LAYERED_2D_PREINT = 1,  // layered 2D TF uses pre-integration
+  TF_LAYERED_2D = 2,         // layered 2D TF without pre-integration behavior
+};
+
+typedef unsigned int uint;
+typedef unsigned char uchar;
+
+static bool usePreInt = true;
+static hipArray *d_transferIntegrate = 0;
+static hipArray *d_transferFunc = 0;
+static hipArray *d_transferArray = 0;
+
+// 1D transfer function texture
+hipTextureObject_t transferTex;
+// 1D transfer integration texture
+hipTextureObject_t transferIntegrateTex;
+hipSurfaceObject_t transferIntegrateSurf;
+// 2D layered preintegrated transfer function texture
+hipTextureObject_t transferLayerPreintTex;
+hipSurfaceObject_t transferLayerPreintSurf;
+
+typedef struct { float4 m[3]; } float3x4;
+
+__constant__ float3x4 c_invViewMatrix;  // inverse view matrix
+
+struct Ray {
+  float3 o;  // origin
+  float3 d;  // direction
+};
+
+// intersect ray with a box
+// http://www.siggraph.org/education/materials/HyperGraph/raytrace/rtinter3.htm
+
+__device__ int intersectBox(Ray r, float3 boxmin, float3 boxmax, float *tnear,
+                            float *tfar) {
+  // compute intersection of ray with all six bbox planes
+  float3 invR = make_float3(1.0f) / r.d;
+  float3 tbot = invR * (boxmin - r.o);
+  float3 ttop = invR * (boxmax - r.o);
+
+  // re-order intersections to find smallest and largest on each axis
+  float3 tmin = fminf(ttop, tbot);
+  float3 tmax = fmaxf(ttop, tbot);
+
+  // find the largest tmin and the smallest tmax
+  float largest_tmin = fmaxf(fmaxf(tmin.x, tmin.y), fmaxf(tmin.x, tmin.z));
+  float smallest_tmax = fminf(fminf(tmax.x, tmax.y), fminf(tmax.x, tmax.z));
+
+  *tnear = largest_tmin;
+  *tfar = smallest_tmax;
+
+  return smallest_tmax > largest_tmin;
+}
+
+// transform vector by matrix (no translation)
+__device__ float3 mul(const float3x4 &M, const float3 &v) {
+  float3 r;
+  r.x = dot(v, make_float3(M.m[0]));
+  r.y = dot(v, make_float3(M.m[1]));
+  r.z = dot(v, make_float3(M.m[2]));
+  return r;
+}
+
+// transform vector by matrix with translation
+__device__ float4 mul(const float3x4 &M, const float4 &v) {
+  float4 r;
+  r.x = dot(v, M.m[0]);
+  r.y = dot(v, M.m[1]);
+  r.z = dot(v, M.m[2]);
+  r.w = 1.0f;
+  return r;
+}
+
+__device__ uint rgbaFloatToInt(float4 rgba) {
+  rgba.x = __saturatef(rgba.x);  // clamp to [0.0, 1.0]
+  rgba.y = __saturatef(rgba.y);
+  rgba.z = __saturatef(rgba.z);
+  rgba.w = __saturatef(rgba.w);
+  return (uint(rgba.w * 255) << 24) | (uint(rgba.z * 255) << 16) |
+         (uint(rgba.y * 255) << 8) | uint(rgba.x * 255);
+}
+
+template <int TFMODE>
+__device__ void d_render(uint *d_output, uint imageW, uint imageH,
+                         float density, float brightness, float transferOffset,
+                         float transferScale, hipTextureObject_t volumeTex,
+                         hipTextureObject_t transferTex,
+                         hipTextureObject_t transferLayerPreintTex,
+                         float transferWeight = 0.0f) {
+  const float rayscale =
+      float(TFMODE != TF_SINGLE_1D ? VOLUMERENDER_TF_PREINTRAY : 1);
+  const int maxSteps = 512;
+  const float tstep = 0.01f * rayscale;
+  const float opacityThreshold = 0.95f;
+  const float3 boxMin = make_float3(-1.0f, -1.0f, -1.0f);
+  const float3 boxMax = make_float3(1.0f, 1.0f, 1.0f);
+
+  density *= rayscale;
+
+  uint x = blockIdx.x * blockDim.x + threadIdx.x;
+  uint y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  if ((x >= imageW) || (y >= imageH)) return;
+
+  float u = (x / (float)imageW) * 2.0f - 1.0f;
+  float v = (y / (float)imageH) * 2.0f - 1.0f;
+
+  // calculate eye ray in world space
+  Ray eyeRay;
+  eyeRay.o =
+      make_float3(mul(c_invViewMatrix, make_float4(0.0f, 0.0f, 0.0f, 1.0f)));
+  eyeRay.d = normalize(make_float3(u, v, -2.0f));
+  eyeRay.d = mul(c_invViewMatrix, eyeRay.d);
+
+  // find intersection with box
+  float tnear, tfar;
+  int hit = intersectBox(eyeRay, boxMin, boxMax, &tnear, &tfar);
+
+  if (!hit) return;
+
+  if (tnear < 0.0f) tnear = 0.0f;  // clamp to near plane
+
+  // march along ray from front to back, accumulating color
+  float4 sum = make_float4(0.0f);
+  float t = tnear;
+  float3 pos = eyeRay.o + eyeRay.d * tnear;
+  float3 step = eyeRay.d * tstep;
+
+  float lastsample = 0;
+
+  // lastsample = (lastsample-transferOffset)*transferScale;
+  for (int i = 0; i < maxSteps; i++) {
+    // read from 3D texture
+    // remap position to [0, 1] coordinates
+    float3 coord = make_float3(pos.x * 0.5f + 0.5f, pos.y * 0.5f + 0.5f,
+                               pos.z * 0.5f + 0.5f);
+    float sample = tex3D<float>(volumeTex, coord.x, coord.y, coord.z);
+    // sample = (sample-transferOffset)*transferScale;
+    // sample *= 64.0f;    // scale for 10-bit data
+
+    // lookup in transfer function texture
+    float4 col;
+    int tfid = (pos.x < 0);
+
+    if (TFMODE != TF_SINGLE_1D) {
+      col = tex2DLayered<float4>(transferLayerPreintTex, sample,
+                                 TFMODE == TF_LAYERED_2D ? sample : lastsample,
+                                 tfid);
+      col.w *= density;
+      lastsample = sample;
+    } else {
+      col = tex1D<float4>(transferTex, sample);
+      col.w *= 0;
+    }
+
+    // "under" operator for back-to-front blending
+    // sum = lerp(sum, col, col.w);
+
+    // pre-multiply alpha
+    col.x *= col.w;
+    col.y *= col.w;
+    col.z *= col.w;
+    // "over" operator for front-to-back blending
+    sum = sum + col * (1.0f - sum.w);
+
+    // exit early if opaque
+    if (sum.w > opacityThreshold) break;
+
+    t += tstep;
+
+    if (t > tfar) break;
+
+    pos += step;
+  }
+
+  sum *= brightness;
+
+  // write output color
+  d_output[y * imageW + x] = rgbaFloatToInt(sum);
+}
+
+__global__ void d_render_regular(uint *d_output, uint imageW, uint imageH,
+                                 float density, float brightness,
+                                 float transferOffset, float transferScale,
+                                 hipTextureObject_t volumeTex,
+                                 hipTextureObject_t transferTex,
+                                 hipTextureObject_t transferLayerPreintTex,
+                                 float transferWeight = 0.0f) {
+  d_render<TF_SINGLE_1D>(d_output, imageW, imageH, density, brightness,
+                         transferOffset, transferScale, volumeTex, transferTex,
+                         transferLayerPreintTex, transferWeight);
+}
+
+__global__ void d_render_preint(uint *d_output, uint imageW, uint imageH,
+                                float density, float brightness,
+                                float transferOffset, float transferScale,
+                                hipTextureObject_t volumeTex,
+                                hipTextureObject_t transferTex,
+                                hipTextureObject_t transferLayerPreintTex,
+                                float transferWeight = 0.0f) {
+  d_render<TF_LAYERED_2D_PREINT>(d_output, imageW, imageH, density, brightness,
+                                 transferOffset, transferScale, volumeTex,
+                                 transferTex, transferLayerPreintTex,
+                                 transferWeight);
+}
+
+__global__ void d_render_preint_off(uint *d_output, uint imageW, uint imageH,
+                                    float density, float brightness,
+                                    float transferOffset, float transferScale,
+                                    hipTextureObject_t volumeTex,
+                                    hipTextureObject_t transferTex,
+                                    hipTextureObject_t transferLayerPreintTex,
+                                    float transferWeight = 0.0f) {
+  d_render<TF_LAYERED_2D>(d_output, imageW, imageH, density, brightness,
+                          transferOffset, transferScale, volumeTex, transferTex,
+                          transferLayerPreintTex, transferWeight);
+}
+
+//////////////////////////////////////////////////////////////////////////
+
+__global__ void d_integrate_trapezoidal(
+    hipExtent extent, hipTextureObject_t transferTex,
+    hipSurfaceObject_t transferIntegrateSurf) {
+  uint x = blockIdx.x * blockDim.x + threadIdx.x;
+
+  // for higher speed could use hierarchical approach for sum
+  if (x >= extent.width) {
+    return;
+  }
+
+  float stepsize = 1.0 / float(extent.width - 1);
+  float to = float(x) * stepsize;
+
+  float4 outclr = make_float4(0, 0, 0, 0);
+  float incr = stepsize;
+
+  float4 lastval = tex1D<float4>(transferTex, 0);
+
+  float cur = incr;
+
+  while (cur < to + incr * 0.5) {
+    float4 val = tex1D<float4>(transferTex, cur);
+    float4 trapezoid = (lastval + val) / 2.0f;
+    lastval = val;
+
+    outclr += trapezoid;
+    cur += incr;
+  }
+
+  // surface writes need byte offsets for x!
+  surf1Dwrite(outclr, transferIntegrateSurf, x * sizeof(float4));
+}
+
+__global__ void d_preintegrate(int layer, float steps, hipExtent extent,
+                               hipTextureObject_t transferTex,
+                               hipTextureObject_t transferIntegrateTex,
+                               hipSurfaceObject_t transferLayerPreintSurf) {
+  uint x = blockIdx.x * blockDim.x + threadIdx.x;
+  uint y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  if (x >= extent.width || y >= extent.height) {
+    return;
+  }
+
+  float sx = float(x) / float(extent.width);
+  float sy = float(y) / float(extent.height);
+
+  float smax = max(sx, sy);
+  float smin = min(sx, sy);
+
+  float4 iv;
+
+  if (x != y) {
+    // assumes square textures!
+    float fracc = smax - smin;
+    fracc = 1.0 / (fracc * steps);
+
+    float4 intmax = tex1D<float4>(transferIntegrateTex, smax);
+    float4 intmin = tex1D<float4>(transferIntegrateTex, smin);
+    iv.x = (intmax.x - intmin.x) * fracc;
+    iv.y = (intmax.y - intmin.y) * fracc;
+    iv.z = (intmax.z - intmin.z) * fracc;
+    // iv.w = (intmax.w - intmin.w)*fracc;
+    iv.w = (1.0 - exp(-(intmax.w - intmin.w) * fracc));
+  } else {
+    float4 sample = tex1D<float4>(transferTex, smin);
+    iv.x = sample.x;
+    iv.y = sample.y;
+    iv.z = sample.z;
+    // iv.w = sample.w;
+    iv.w = (1.0 - exp(-sample.w));
+  }
+
+  iv.x = __saturatef(iv.x);
+  iv.y = __saturatef(iv.y);
+  iv.z = __saturatef(iv.z);
+  iv.w = __saturatef(iv.w);
+
+  // surface writes need byte offsets for x!
+  surf2DLayeredwrite(iv, transferLayerPreintSurf, x * sizeof(float4), y, layer);
+}
+
+//////////////////////////////////////////////////////////////////////////
+
+void VolumeRender_setTextureFilterMode(bool bLinearFilter, Volume *vol) {
+  if (vol->volumeTex) {
+    HIPCHECK(hipDestroyTextureObject(vol->volumeTex));
+  }
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = vol->content;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = true;
+  texDescr.filterMode =
+      bLinearFilter ? hipFilterModeLinear : hipFilterModePoint;
+
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.addressMode[1] = hipAddressModeWrap;
+  texDescr.addressMode[2] = hipAddressModeWrap;
+
+  texDescr.readMode = VolumeTypeInfo<VolumeType>::readMode;
+
+  HIPCHECK(
+      hipCreateTextureObject(&vol->volumeTex, &texRes, &texDescr, NULL));
+}
+
+static unsigned int iDivUp(size_t a, size_t b) {
+  size_t val = (a % b != 0) ? (a / b + 1) : (a / b);
+  if (val > UINT_MAX) {
+    fprintf(stderr, "\nUINT_MAX limit exceeded in iDivUp() exiting.....\n");
+    exit(EXIT_FAILURE);  // val exceeds limit
+  }
+
+  return static_cast<unsigned int>(val);
+}
+
+void VolumeRender_updateTF(int tfIdx, int numColors, float4 *colors) {
+  if (d_transferFunc) {
+    HIPCHECK(hipFreeArray(d_transferFunc));
+    d_transferFunc = 0;
+  }
+
+  hipChannelFormatDesc channelFloat4 = hipCreateChannelDesc<float4>();
+  HIPCHECK(
+      hipMallocArray(&d_transferFunc, &channelFloat4, numColors, 1));
+  HIPCHECK(hipMemcpy2DToArray(d_transferFunc, 0, 0, colors, 0,
+                                      sizeof(float4) * numColors, 1,
+                                      hipMemcpyHostToDevice));
+
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = d_transferFunc;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = true;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeClamp;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(
+      hipCreateTextureObject(&transferTex, &texRes, &texDescr, NULL));
+
+  if (tfIdx < 0 || tfIdx >= VOLUMERENDER_TFS) {
+    return;
+  }
+
+  {
+    hipExtent extent = {VOLUMERENDER_TF_PREINTSTEPS, 0, 0};
+    dim3 blockSize(32, 1, 1);
+    dim3 gridSize(iDivUp(extent.width, blockSize.x), 1, 1);
+    d_integrate_trapezoidal<<<gridSize, blockSize>>>(extent, transferTex,
+                                                     transferIntegrateSurf);
+  }
+
+  {
+    hipExtent extent = {VOLUMERENDER_TF_PREINTSIZE, VOLUMERENDER_TF_PREINTSIZE,
+                         VOLUMERENDER_TFS};
+    dim3 blockSize(16, 16, 1);
+    dim3 gridSize(iDivUp(extent.width, blockSize.x),
+                  iDivUp(extent.height, blockSize.y), 1);
+    d_preintegrate<<<gridSize, blockSize>>>(
+        tfIdx, float(VOLUMERENDER_TF_PREINTSTEPS), extent, transferTex,
+        transferIntegrateTex, transferLayerPreintSurf);
+  }
+}
+
+void VolumeRender_init() {
+  hipResourceDesc texRes;
+  hipTextureDesc texDescr;
+
+  hipChannelFormatDesc channelFloat4 = hipCreateChannelDesc<float4>();
+  hipExtent extent = {VOLUMERENDER_TF_PREINTSIZE, VOLUMERENDER_TF_PREINTSIZE,
+                       VOLUMERENDER_TFS};
+  HIPCHECK(
+      hipMalloc3DArray(&d_transferArray, &channelFloat4, extent,
+                        hipArrayLayered | hipArraySurfaceLoadStore));
+
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = d_transferArray;
+
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = true;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeClamp;
+  texDescr.addressMode[1] = hipAddressModeClamp;
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(hipCreateTextureObject(&transferLayerPreintTex, &texRes,
+                                          &texDescr, NULL));
+
+  hipResourceDesc surfRes;
+  memset(&surfRes, 0, sizeof(hipResourceDesc));
+  surfRes.resType = hipResourceTypeArray;
+  surfRes.res.array.array = d_transferArray;
+
+  HIPCHECK(hipCreateSurfaceObject(&transferLayerPreintSurf, &surfRes));
+
+  HIPCHECK(hipMallocArray(&d_transferIntegrate, &channelFloat4,
+                                  VOLUMERENDER_TF_PREINTSTEPS, 0,
+                                  hipArraySurfaceLoadStore));
+
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = d_transferIntegrate;
+
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = true;
+  texDescr.filterMode = hipFilterModeLinear;
+
+  texDescr.addressMode[0] = hipAddressModeClamp;
+  texDescr.addressMode[1] = hipAddressModeClamp;
+  texDescr.addressMode[2] = hipAddressModeClamp;
+
+  texDescr.readMode = hipReadModeElementType;
+
+  HIPCHECK(
+      hipCreateTextureObject(&transferIntegrateTex, &texRes, &texDescr, NULL));
+
+  memset(&surfRes, 0, sizeof(hipResourceDesc));
+  surfRes.resType = hipResourceTypeArray;
+  surfRes.res.array.array = d_transferIntegrate;
+
+  HIPCHECK(hipCreateSurfaceObject(&transferIntegrateSurf, &surfRes));
+
+  // create transfer function texture
+  float4 transferFunc0[] = {
+    {  0.0, 0.0, 0.0, 0.0, },
+    {  1.0, 0.0, 0.0, 1.0, },
+    {  1.0, 0.5, 0.0, 1.0, },
+    {  1.0, 1.0, 0.0, 1.0, },
+    {  0.0, 1.0, 0.0, 1.0, },
+    {  0.0, 1.0, 1.0, 1.0, },
+    {  0.0, 0.0, 1.0, 1.0, },
+    {  1.0, 0.0, 1.0, 1.0, },
+    {  0.0, 0.0, 0.0, 0.0, },
+  };
+
+  float4 transferFunc1[] = {
+    {  0.0, 0.0, 0.0, 0.0, },
+    {  0.0, 1.0, 0.0, 0.125, },
+    {  0.0, 0.5, 1.0, 0.125, },
+    {  0.0, 1.0, 1.0, 0.125, },
+    {  0.0, 1.0, 0.0, 0.125, },
+    {  0.25, 0.75, 0.0, 1.0, },
+    {  0.75, 0.25, 0.0, 0.125, },
+    {  1.0, 0.75, 0.0, 0.125, },
+    {  0.0, 0.0, 0.0, 0.0, },
+  };
+
+  VolumeRender_updateTF(1, sizeof(transferFunc1) / sizeof(float4),
+                        transferFunc1);
+  VolumeRender_updateTF(0, sizeof(transferFunc0) / sizeof(float4),
+                        transferFunc0);
+}
+
+void VolumeRender_deinit() {
+  HIPCHECK(hipDestroyTextureObject(transferTex));
+  HIPCHECK(hipDestroyTextureObject(transferIntegrateTex));
+  HIPCHECK(hipDestroySurfaceObject(transferIntegrateSurf));
+  HIPCHECK(hipDestroyTextureObject(transferLayerPreintTex));
+  HIPCHECK(hipDestroySurfaceObject(transferLayerPreintSurf));
+  HIPCHECK(hipFreeArray(d_transferFunc));
+  HIPCHECK(hipFreeArray(d_transferArray));
+  HIPCHECK(hipFreeArray(d_transferIntegrate));
+  d_transferArray = 0;
+  d_transferFunc = 0;
+  d_transferIntegrate = 0;
+}
+
+void VolumeRender_setPreIntegrated(int state) { usePreInt = !!state; }
+
+void VolumeRender_render(dim3 gridSize, dim3 blockSize, uint *d_output,
+                         uint imageW, uint imageH, float density,
+                         float brightness, float transferOffset,
+                         float transferScale, hipTextureObject_t volumeTex) {
+  if (usePreInt) {
+    d_render_preint<<<gridSize, blockSize>>>(
+        d_output, imageW, imageH, density, brightness, transferOffset,
+        transferScale, volumeTex, transferTex, transferLayerPreintTex);
+  } else {
+    d_render_preint_off<<<gridSize, blockSize>>>(
+        d_output, imageW, imageH, density, brightness, transferOffset,
+        transferScale, volumeTex, transferTex, transferLayerPreintTex);
+  }
+}
+
+void VolumeRender_copyInvViewMatrix(float *invViewMatrix, size_t sizeofMatrix) {
+  HIPCHECK(
+      hipMemcpyToSymbol(HIP_SYMBOL(c_invViewMatrix), invViewMatrix, sizeofMatrix));
+}
+
+#endif  // #ifndef _VOLUMERENDER_KERNEL_CU_
+heckCudaErrors(
+      hipMemcpyToSymbol(HIP_SYMBOL(c_invViewMatrix), invViewMatrix, sizeofMatrix));
+}
+
+#endif  // #ifndef _VOLUMERENDER_KERNEL_CU_
diff --git a/src/samples/Samples/5_Domain_Specific/volumeRender/volumeRender_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/volumeRender/volumeRender_kernel.cu.hip
index 9df9611..5a56fe1 100755
--- a/src/samples/Samples/5_Domain_Specific/volumeRender/volumeRender_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/volumeRender/volumeRender_kernel.cu.hip
@@ -312,3 +312,5 @@ extern "C" void copyInvViewMatrix(float *invViewMatrix, size_t sizeofMatrix) {
 #endif  // #ifndef _VOLUMERENDER_KERNEL_CU_
 ViewMatrix), invViewMatrix, sizeofMatrix));
 }
+
+#endif  // #ifndef _VOLUMERENDER_KERNEL_CU_
diff --git a/src/samples/Samples/5_Domain_Specific/vulkanImageCUDA/vulkanImageCUDA.cu.hip b/src/samples/Samples/5_Domain_Specific/vulkanImageCUDA/vulkanImageCUDA.cu.hip
index e69de29..5d94434 100755
--- a/src/samples/Samples/5_Domain_Specific/vulkanImageCUDA/vulkanImageCUDA.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/vulkanImageCUDA/vulkanImageCUDA.cu.hip
@@ -0,0 +1,2650 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#define GLFW_INCLUDE_VULKAN
+#ifdef _WIN64
+#include <aclapi.h>
+#include <dxgi1_2.h>
+#include <windows.h>
+#include <VersionHelpers.h>
+#define _USE_MATH_DEFINES
+#endif
+
+#include <GLFW/glfw3.h>
+#include <vulkan/vulkan.h>
+#ifdef _WIN64
+#include <vulkan/vulkan_win32.h>
+#endif
+
+#include <algorithm>
+#include <array>
+#include <chrono>
+#include <cstdlib>
+#include <cstring>
+#include <fstream>
+#include <iostream>
+#include <set>
+#include <stdexcept>
+#include <thread>
+#include <vector>
+
+#include <hip/hip_runtime.h>
+#include <hip/hip_runtime.h>
+#include "helper_cuda_hipified.h"
+#include <helper_image.h>
+#include <helper_math.h>
+
+#include "linmath.h"
+
+#define WIDTH 800
+#define HEIGHT 600
+
+const int MAX_FRAMES = 4;
+
+const std::vector<const char*> validationLayers = {
+    "VK_LAYER_KHRONOS_validation"};
+
+#ifdef NDEBUG
+const bool enableValidationLayers = true;
+#else
+const bool enableValidationLayers = false;
+#endif
+
+std::string execution_path;
+
+VkResult CreateDebugUtilsMessengerEXT(
+    VkInstance instance, const VkDebugUtilsMessengerCreateInfoEXT* pCreateInfo,
+    const VkAllocationCallbacks* pAllocator,
+    VkDebugUtilsMessengerEXT* pDebugMessenger) {
+  auto func = (PFN_vkCreateDebugUtilsMessengerEXT)vkGetInstanceProcAddr(
+      instance, "vkCreateDebugUtilsMessengerEXT");
+  if (func != nullptr) {
+    return func(instance, pCreateInfo, pAllocator, pDebugMessenger);
+  } else {
+    return VK_ERROR_EXTENSION_NOT_PRESENT;
+  }
+};
+
+const std::vector<const char*> deviceExtensions = {
+    VK_KHR_SWAPCHAIN_EXTENSION_NAME,
+    VK_KHR_EXTERNAL_MEMORY_EXTENSION_NAME,
+    VK_KHR_EXTERNAL_SEMAPHORE_EXTENSION_NAME,
+#ifdef _WIN64
+    VK_KHR_EXTERNAL_MEMORY_WIN32_EXTENSION_NAME,
+    VK_KHR_EXTERNAL_SEMAPHORE_WIN32_EXTENSION_NAME,
+#else
+    VK_KHR_EXTERNAL_MEMORY_FD_EXTENSION_NAME,
+    VK_KHR_EXTERNAL_SEMAPHORE_FD_EXTENSION_NAME,
+#endif
+};
+
+#ifdef _WIN64
+class WindowsSecurityAttributes {
+ protected:
+  SECURITY_ATTRIBUTES m_winSecurityAttributes;
+  PSECURITY_DESCRIPTOR m_winPSecurityDescriptor;
+
+ public:
+  WindowsSecurityAttributes();
+  SECURITY_ATTRIBUTES* operator&();
+  ~WindowsSecurityAttributes();
+};
+
+WindowsSecurityAttributes::WindowsSecurityAttributes() {
+  m_winPSecurityDescriptor = (PSECURITY_DESCRIPTOR)calloc(
+      1, SECURITY_DESCRIPTOR_MIN_LENGTH + 2 * sizeof(void**));
+
+  PSID* ppSID =
+      (PSID*)((PBYTE)m_winPSecurityDescriptor + SECURITY_DESCRIPTOR_MIN_LENGTH);
+  PACL* ppACL = (PACL*)((PBYTE)ppSID + sizeof(PSID*));
+
+  InitializeSecurityDescriptor(m_winPSecurityDescriptor,
+                               SECURITY_DESCRIPTOR_REVISION);
+
+  SID_IDENTIFIER_AUTHORITY sidIdentifierAuthority =
+      SECURITY_WORLD_SID_AUTHORITY;
+  AllocateAndInitializeSid(&sidIdentifierAuthority, 1, SECURITY_WORLD_RID, 0, 0,
+                           0, 0, 0, 0, 0, ppSID);
+
+  EXPLICIT_ACCESS explicitAccess;
+  ZeroMemory(&explicitAccess, sizeof(EXPLICIT_ACCESS));
+  explicitAccess.grfAccessPermissions =
+      STANDARD_RIGHTS_ALL | SPECIFIC_RIGHTS_ALL;
+  explicitAccess.grfAccessMode = SET_ACCESS;
+  explicitAccess.grfInheritance = INHERIT_ONLY;
+  explicitAccess.Trustee.TrusteeForm = TRUSTEE_IS_SID;
+  explicitAccess.Trustee.TrusteeType = TRUSTEE_IS_WELL_KNOWN_GROUP;
+  explicitAccess.Trustee.ptstrName = (LPTSTR)*ppSID;
+
+  SetEntriesInAcl(1, &explicitAccess, NULL, ppACL);
+
+  SetSecurityDescriptorDacl(m_winPSecurityDescriptor, TRUE, *ppACL, FALSE);
+
+  m_winSecurityAttributes.nLength = sizeof(m_winSecurityAttributes);
+  m_winSecurityAttributes.lpSecurityDescriptor = m_winPSecurityDescriptor;
+  m_winSecurityAttributes.bInheritHandle = TRUE;
+}
+
+SECURITY_ATTRIBUTES* WindowsSecurityAttributes::operator&() {
+  return &m_winSecurityAttributes;
+}
+
+WindowsSecurityAttributes::~WindowsSecurityAttributes() {
+  PSID* ppSID =
+      (PSID*)((PBYTE)m_winPSecurityDescriptor + SECURITY_DESCRIPTOR_MIN_LENGTH);
+  PACL* ppACL = (PACL*)((PBYTE)ppSID + sizeof(PSID*));
+
+  if (*ppSID) {
+    FreeSid(*ppSID);
+  }
+  if (*ppACL) {
+    LocalFree(*ppACL);
+  }
+  free(m_winPSecurityDescriptor);
+}
+#endif
+
+void DestroyDebugUtilsMessengerEXT(VkInstance instance,
+                                   VkDebugUtilsMessengerEXT debugMessenger,
+                                   const VkAllocationCallbacks* pAllocator) {
+  auto func = (PFN_vkDestroyDebugUtilsMessengerEXT)vkGetInstanceProcAddr(
+      instance, "vkDestroyDebugUtilsMessengerEXT");
+  if (func != nullptr) {
+    func(instance, debugMessenger, pAllocator);
+  }
+}
+
+struct QueueFamilyIndices {
+  int graphicsFamily = -1;
+  int presentFamily = -1;
+
+  bool isComplete() { return graphicsFamily >= 0 && presentFamily >= 0; }
+};
+
+struct SwapChainSupportDetails {
+  VkSurfaceCapabilitiesKHR capabilities;
+  std::vector<VkSurfaceFormatKHR> formats;
+  std::vector<VkPresentModeKHR> presentModes;
+};
+
+typedef float vec2[2];
+
+struct Vertex {
+  vec4 pos;
+  vec3 color;
+  vec2 texCoord;
+
+  static VkVertexInputBindingDescription getBindingDescription() {
+    VkVertexInputBindingDescription bindingDescription = {};
+    bindingDescription.binding = 0;
+    bindingDescription.stride = sizeof(Vertex);
+    bindingDescription.inputRate = VK_VERTEX_INPUT_RATE_VERTEX;
+
+    return bindingDescription;
+  }
+
+  static std::array<VkVertexInputAttributeDescription, 3>
+  getAttributeDescriptions() {
+    std::array<VkVertexInputAttributeDescription, 3> attributeDescriptions = {};
+
+    attributeDescriptions[0].binding = 0;
+    attributeDescriptions[0].location = 0;
+    attributeDescriptions[0].format = VK_FORMAT_R32G32B32A32_SFLOAT;
+    attributeDescriptions[0].offset = offsetof(Vertex, pos);
+
+    attributeDescriptions[1].binding = 0;
+    attributeDescriptions[1].location = 1;
+    attributeDescriptions[1].format = VK_FORMAT_R32G32B32_SFLOAT;
+    attributeDescriptions[1].offset = offsetof(Vertex, color);
+
+    attributeDescriptions[2].binding = 0;
+    attributeDescriptions[2].location = 2;
+    attributeDescriptions[2].format = VK_FORMAT_R32G32_SFLOAT;
+    attributeDescriptions[2].offset = offsetof(Vertex, texCoord);
+
+    return attributeDescriptions;
+  }
+};
+
+struct UniformBufferObject {
+  alignas(16) mat4x4 model;
+  alignas(16) mat4x4 view;
+  alignas(16) mat4x4 proj;
+};
+
+const std::vector<Vertex> vertices = {
+    {{-1.0f, -1.0f, 0.0f, 1.0f}, {1.0f, 0.0f, 0.0f}, {0.0f, 0.0f}},
+    {{1.0f, -1.0f, 0.0f, 1.0f}, {0.0f, 1.0f, 0.0f}, {1.0f, 0.0f}},
+    {{1.0f, 1.0f, 0.0f, 1.0f}, {0.0f, 0.0f, 1.0f}, {1.0f, 1.0f}},
+    {{-1.0f, 1.0f, 0.0f, 1.0f}, {1.0f, 1.0f, 1.0f}, {0.0f, 1.0f}}};
+
+const std::vector<uint16_t> indices = {0, 1, 2, 2, 3, 0};
+
+// convert floating point rgba color to 32-bit integer
+__device__ unsigned int rgbaFloatToInt(float4 rgba) {
+  rgba.x = __saturatef(rgba.x);  // clamp to [0.0, 1.0]
+  rgba.y = __saturatef(rgba.y);
+  rgba.z = __saturatef(rgba.z);
+  rgba.w = __saturatef(rgba.w);
+  return ((unsigned int)(rgba.w * 255.0f) << 24) |
+         ((unsigned int)(rgba.z * 255.0f) << 16) |
+         ((unsigned int)(rgba.y * 255.0f) << 8) |
+         ((unsigned int)(rgba.x * 255.0f));
+}
+
+__device__ float4 rgbaIntToFloat(unsigned int c) {
+  float4 rgba;
+  rgba.x = (c & 0xff) * 0.003921568627f;          //  /255.0f;
+  rgba.y = ((c >> 8) & 0xff) * 0.003921568627f;   //  /255.0f;
+  rgba.z = ((c >> 16) & 0xff) * 0.003921568627f;  //  /255.0f;
+  rgba.w = ((c >> 24) & 0xff) * 0.003921568627f;  //  /255.0f;
+  return rgba;
+}
+
+int filter_radius = 14;
+int g_nFilterSign = 1;
+
+// This varies the filter radius, so we can see automatic animation
+void varySigma() {
+  filter_radius += g_nFilterSign;
+
+  if (filter_radius > 64) {
+    filter_radius = 64;  // clamp to 64 and then negate sign
+    g_nFilterSign = -1;
+  } else if (filter_radius < 0) {
+    filter_radius = 0;
+    g_nFilterSign = 1;
+  }
+}
+
+// row pass using texture lookups
+__global__ void d_boxfilter_rgba_x(hipSurfaceObject_t* dstSurfMipMapArray,
+                                   hipTextureObject_t textureMipMapInput,
+                                   size_t baseWidth, size_t baseHeight,
+                                   size_t mipLevels, int filter_radius) {
+  float scale = 1.0f / (float)((filter_radius << 1) + 1);
+  unsigned int y = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (y < baseHeight) {
+    for (uint32_t mipLevelIdx = 0; mipLevelIdx < mipLevels; mipLevelIdx++) {
+      uint32_t width =
+          (baseWidth >> mipLevelIdx) ? (baseWidth >> mipLevelIdx) : 1;
+      uint32_t height =
+          (baseHeight >> mipLevelIdx) ? (baseHeight >> mipLevelIdx) : 1;
+      if (y < height && filter_radius < width) {
+        float px = 1.0 / width;
+        float py = 1.0 / height;
+        float4 t = make_float4(0.0f);
+        for (int x = -filter_radius; x <= filter_radius; x++) {
+          t += tex2DLod<float4>(textureMipMapInput, x * px, y * py,
+                                (float)mipLevelIdx);
+        }
+
+        unsigned int dataB = rgbaFloatToInt(t * scale);
+        surf2Dwrite(dataB, dstSurfMipMapArray[mipLevelIdx], 0, y);
+
+        for (int x = 1; x < width; x++) {
+          t += tex2DLod<float4>(textureMipMapInput, (x + filter_radius) * px,
+                                y * py, (float)mipLevelIdx);
+          t -=
+              tex2DLod<float4>(textureMipMapInput, (x - filter_radius - 1) * px,
+                               y * py, (float)mipLevelIdx);
+          unsigned int dataB = rgbaFloatToInt(t * scale);
+          surf2Dwrite(dataB, dstSurfMipMapArray[mipLevelIdx],
+                      x * sizeof(uchar4), y);
+        }
+      }
+    }
+  }
+}
+
+// column pass using coalesced global memory reads
+__global__ void d_boxfilter_rgba_y(hipSurfaceObject_t* dstSurfMipMapArray,
+                                   hipSurfaceObject_t* srcSurfMipMapArray,
+                                   size_t baseWidth, size_t baseHeight,
+                                   size_t mipLevels, int filter_radius) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  float scale = 1.0f / (float)((filter_radius << 1) + 1);
+
+  for (uint32_t mipLevelIdx = 0; mipLevelIdx < mipLevels; mipLevelIdx++) {
+    uint32_t width =
+        (baseWidth >> mipLevelIdx) ? (baseWidth >> mipLevelIdx) : 1;
+    uint32_t height =
+        (baseHeight >> mipLevelIdx) ? (baseHeight >> mipLevelIdx) : 1;
+
+    if (x < width && height > filter_radius) {
+      float4 t;
+      // do left edge
+      int colInBytes = x * sizeof(uchar4);
+      unsigned int pixFirst = surf2Dread<unsigned int>(
+          srcSurfMipMapArray[mipLevelIdx], colInBytes, 0);
+      t = rgbaIntToFloat(pixFirst) * filter_radius;
+
+      for (int y = 0; (y < (filter_radius + 1)) && (y < height); y++) {
+        unsigned int pix = surf2Dread<unsigned int>(
+            srcSurfMipMapArray[mipLevelIdx], colInBytes, y);
+        t += rgbaIntToFloat(pix);
+      }
+
+      unsigned int dataB = rgbaFloatToInt(t * scale);
+      surf2Dwrite(dataB, dstSurfMipMapArray[mipLevelIdx], colInBytes, 0);
+
+      for (int y = 1; (y < filter_radius + 1) && ((y + filter_radius) < height);
+           y++) {
+        unsigned int pix = surf2Dread<unsigned int>(
+            srcSurfMipMapArray[mipLevelIdx], colInBytes, y + filter_radius);
+        t += rgbaIntToFloat(pix);
+        t -= rgbaIntToFloat(pixFirst);
+
+        dataB = rgbaFloatToInt(t * scale);
+        surf2Dwrite(dataB, dstSurfMipMapArray[mipLevelIdx], colInBytes, y);
+      }
+
+      // main loop
+      for (int y = (filter_radius + 1); y < (height - filter_radius); y++) {
+        unsigned int pix = surf2Dread<unsigned int>(
+            srcSurfMipMapArray[mipLevelIdx], colInBytes, y + filter_radius);
+        t += rgbaIntToFloat(pix);
+
+        pix = surf2Dread<unsigned int>(srcSurfMipMapArray[mipLevelIdx],
+                                       colInBytes, y - filter_radius - 1);
+        t -= rgbaIntToFloat(pix);
+
+        dataB = rgbaFloatToInt(t * scale);
+        surf2Dwrite(dataB, dstSurfMipMapArray[mipLevelIdx], colInBytes, y);
+      }
+
+      // do right edge
+      unsigned int pixLast = surf2Dread<unsigned int>(
+          srcSurfMipMapArray[mipLevelIdx], colInBytes, height - 1);
+      for (int y = height - filter_radius;
+           (y < height) && ((y - filter_radius - 1) > 1); y++) {
+        t += rgbaIntToFloat(pixLast);
+        unsigned int pix = surf2Dread<unsigned int>(
+            srcSurfMipMapArray[mipLevelIdx], colInBytes, y - filter_radius - 1);
+        t -= rgbaIntToFloat(pix);
+        dataB = rgbaFloatToInt(t * scale);
+        surf2Dwrite(dataB, dstSurfMipMapArray[mipLevelIdx], colInBytes, y);
+      }
+    }
+  }
+}
+
+class vulkanImageCUDA {
+ public:
+  void loadImageData(const std::string& filename) {
+    // load image (needed so we can get the width and height before we create
+    // the window
+    char* image_path =
+        sdkFindFilePath(filename.c_str(), execution_path.c_str());
+
+    if (image_path == 0) {
+      printf("Error finding image file '%s'\n", filename.c_str());
+      exit(EXIT_FAILURE);
+    }
+
+    sdkLoadPPM4(image_path, (unsigned char**)&image_data, &imageWidth,
+                &imageHeight);
+
+    if (!image_data) {
+      printf("Error opening file '%s'\n", image_path);
+      exit(EXIT_FAILURE);
+    }
+
+    printf("Loaded '%s', %d x %d pixels\n", image_path, imageWidth,
+           imageHeight);
+  }
+
+  void run() {
+    initWindow();
+    initVulkan();
+    initCuda();
+    mainLoop();
+    cleanup();
+  }
+
+ private:
+  GLFWwindow* window;
+
+  VkInstance instance;
+  VkDebugUtilsMessengerEXT debugMessenger;
+  VkSurfaceKHR surface;
+
+  VkPhysicalDevice physicalDevice = VK_NULL_HANDLE;
+  VkDevice device;
+  uint8_t vkDeviceUUID[VK_UUID_SIZE];
+
+  VkQueue graphicsQueue;
+  VkQueue presentQueue;
+
+  VkSwapchainKHR swapChain;
+  std::vector<VkImage> swapChainImages;
+  VkFormat swapChainImageFormat;
+  VkExtent2D swapChainExtent;
+  std::vector<VkImageView> swapChainImageViews;
+  std::vector<VkFramebuffer> swapChainFramebuffers;
+
+  VkRenderPass renderPass;
+  VkDescriptorSetLayout descriptorSetLayout;
+  VkPipelineLayout pipelineLayout;
+  VkPipeline graphicsPipeline;
+
+  VkCommandPool commandPool;
+
+  VkImage textureImage;
+  VkDeviceMemory textureImageMemory;
+  VkImageView textureImageView;
+  VkSampler textureSampler;
+
+  VkBuffer vertexBuffer;
+  VkDeviceMemory vertexBufferMemory;
+  VkBuffer indexBuffer;
+  VkDeviceMemory indexBufferMemory;
+
+  std::vector<VkBuffer> uniformBuffers;
+  std::vector<VkDeviceMemory> uniformBuffersMemory;
+
+  VkDescriptorPool descriptorPool;
+  std::vector<VkDescriptorSet> descriptorSets;
+
+  std::vector<VkCommandBuffer> commandBuffers;
+
+  std::vector<VkSemaphore> imageAvailableSemaphores;
+  std::vector<VkSemaphore> renderFinishedSemaphores;
+  VkSemaphore cudaUpdateVkSemaphore, vkUpdateCudaSemaphore;
+  std::vector<VkFence> inFlightFences;
+
+  size_t currentFrame = 0;
+
+  bool framebufferResized = false;
+
+#ifdef _WIN64
+  PFN_vkGetMemoryWin32HandleKHR fpGetMemoryWin32HandleKHR;
+  PFN_vkGetSemaphoreWin32HandleKHR fpGetSemaphoreWin32HandleKHR;
+#else
+  PFN_vkGetMemoryFdKHR fpGetMemoryFdKHR = NULL;
+  PFN_vkGetSemaphoreFdKHR fpGetSemaphoreFdKHR = NULL;
+#endif
+
+  PFN_vkGetPhysicalDeviceProperties2 fpGetPhysicalDeviceProperties2;
+
+  unsigned int* image_data = NULL;
+  unsigned int imageWidth, imageHeight;
+  unsigned int mipLevels = 1;
+  size_t totalImageMemSize;
+
+  // CUDA objects
+  hipExternalMemory_t cudaExtMemImageBuffer;
+  hipMipmappedArray_t cudaMipmappedImageArray, cudaMipmappedImageArrayTemp,
+      cudaMipmappedImageArrayOrig;
+  std::vector<hipSurfaceObject_t> surfaceObjectList, surfaceObjectListTemp;
+  hipSurfaceObject_t *d_surfaceObjectList, *d_surfaceObjectListTemp;
+  hipTextureObject_t textureObjMipMapInput;
+
+  hipExternalSemaphore_t cudaExtCudaUpdateVkSemaphore;
+  hipExternalSemaphore_t cudaExtVkUpdateCudaSemaphore;
+  hipStream_t streamToRun;
+
+  void initWindow() {
+    glfwInit();
+
+    glfwWindowHint(GLFW_CLIENT_API, GLFW_NO_API);
+
+    window = glfwCreateWindow(WIDTH, HEIGHT, "Vulkan Image CUDA Box Filter",
+                              nullptr, nullptr);
+    glfwSetWindowUserPointer(window, this);
+    glfwSetFramebufferSizeCallback(window, framebufferResizeCallback);
+  }
+
+  static void framebufferResizeCallback(GLFWwindow* window, int width,
+                                        int height) {
+    auto app =
+        reinterpret_cast<vulkanImageCUDA*>(glfwGetWindowUserPointer(window));
+    app->framebufferResized = true;
+  }
+
+  void initVulkan() {
+    createInstance();
+    setupDebugMessenger();
+    createSurface();
+    pickPhysicalDevice();
+    createLogicalDevice();
+    getKhrExtensionsFn();
+    createSwapChain();
+    createImageViews();
+    createRenderPass();
+    createDescriptorSetLayout();
+    createGraphicsPipeline();
+    createFramebuffers();
+    createCommandPool();
+    createTextureImage();
+    createTextureImageView();
+    createTextureSampler();
+    createVertexBuffer();
+    createIndexBuffer();
+    createUniformBuffers();
+    createDescriptorPool();
+    createDescriptorSets();
+    createCommandBuffers();
+    createSyncObjects();
+    createSyncObjectsExt();
+  }
+
+  void initCuda() {
+    setCudaVkDevice();
+    HIPCHECK(hipStreamCreate(&streamToRun));
+    cudaVkImportImageMem();
+    cudaVkImportSemaphore();
+  }
+
+  void mainLoop() {
+    updateUniformBuffer();
+    while (!glfwWindowShouldClose(window)) {
+      glfwPollEvents();
+      drawFrame();
+    }
+
+    vkDeviceWaitIdle(device);
+  }
+
+  void cleanupSwapChain() {
+    for (auto framebuffer : swapChainFramebuffers) {
+      vkDestroyFramebuffer(device, framebuffer, nullptr);
+    }
+
+    vkFreeCommandBuffers(device, commandPool,
+                         static_cast<uint32_t>(commandBuffers.size()),
+                         commandBuffers.data());
+
+    vkDestroyPipeline(device, graphicsPipeline, nullptr);
+    vkDestroyPipelineLayout(device, pipelineLayout, nullptr);
+    vkDestroyRenderPass(device, renderPass, nullptr);
+
+    for (auto imageView : swapChainImageViews) {
+      vkDestroyImageView(device, imageView, nullptr);
+    }
+
+    vkDestroySwapchainKHR(device, swapChain, nullptr);
+
+    for (size_t i = 0; i < swapChainImages.size(); i++) {
+      vkDestroyBuffer(device, uniformBuffers[i], nullptr);
+      vkFreeMemory(device, uniformBuffersMemory[i], nullptr);
+    }
+
+    vkDestroyDescriptorPool(device, descriptorPool, nullptr);
+  }
+
+  void cleanup() {
+    cleanupSwapChain();
+
+    vkDestroySampler(device, textureSampler, nullptr);
+    vkDestroyImageView(device, textureImageView, nullptr);
+
+    for (int i = 0; i < mipLevels; i++) {
+      HIPCHECK(hipDestroySurfaceObject(surfaceObjectList[i]));
+      HIPCHECK(hipDestroySurfaceObject(surfaceObjectListTemp[i]));
+    }
+
+    HIPCHECK(hipFree(d_surfaceObjectList));
+    HIPCHECK(hipFree(d_surfaceObjectListTemp));
+    HIPCHECK(hipFreeMipmappedArray(cudaMipmappedImageArrayTemp));
+    HIPCHECK(hipFreeMipmappedArray(cudaMipmappedImageArrayOrig));
+    HIPCHECK(hipFreeMipmappedArray(cudaMipmappedImageArray));
+    HIPCHECK(hipDestroyTextureObject(textureObjMipMapInput));
+    HIPCHECK(hipDestroyExternalMemory(cudaExtMemImageBuffer));
+    HIPCHECK(hipDestroyExternalSemaphore(cudaExtCudaUpdateVkSemaphore));
+    HIPCHECK(hipDestroyExternalSemaphore(cudaExtVkUpdateCudaSemaphore));
+
+    vkDestroyImage(device, textureImage, nullptr);
+    vkFreeMemory(device, textureImageMemory, nullptr);
+
+    vkDestroyDescriptorSetLayout(device, descriptorSetLayout, nullptr);
+
+    vkDestroyBuffer(device, indexBuffer, nullptr);
+    vkFreeMemory(device, indexBufferMemory, nullptr);
+
+    vkDestroyBuffer(device, vertexBuffer, nullptr);
+    vkFreeMemory(device, vertexBufferMemory, nullptr);
+
+    vkDestroySemaphore(device, cudaUpdateVkSemaphore, nullptr);
+    vkDestroySemaphore(device, vkUpdateCudaSemaphore, nullptr);
+
+    for (size_t i = 0; i < MAX_FRAMES; i++) {
+      vkDestroySemaphore(device, renderFinishedSemaphores[i], nullptr);
+      vkDestroySemaphore(device, imageAvailableSemaphores[i], nullptr);
+      vkDestroyFence(device, inFlightFences[i], nullptr);
+    }
+
+    vkDestroyCommandPool(device, commandPool, nullptr);
+
+    vkDestroyDevice(device, nullptr);
+
+    if (enableValidationLayers) {
+      DestroyDebugUtilsMessengerEXT(instance, debugMessenger, nullptr);
+    }
+
+    vkDestroySurfaceKHR(instance, surface, nullptr);
+    vkDestroyInstance(instance, nullptr);
+
+    glfwDestroyWindow(window);
+
+    glfwTerminate();
+  }
+
+  void recreateSwapChain() {
+    int width = 0, height = 0;
+    while (width == 0 || height == 0) {
+      glfwGetFramebufferSize(window, &width, &height);
+      glfwWaitEvents();
+    }
+
+    vkDeviceWaitIdle(device);
+
+    cleanupSwapChain();
+
+    createSwapChain();
+    createImageViews();
+    createRenderPass();
+    createGraphicsPipeline();
+    createFramebuffers();
+    createUniformBuffers();
+    createDescriptorPool();
+    createDescriptorSets();
+    createCommandBuffers();
+  }
+
+  void createInstance() {
+    if (enableValidationLayers && !checkValidationLayerSupport()) {
+      throw std::runtime_error(
+          "validation layers requested, but not available!");
+    }
+
+    VkApplicationInfo appInfo = {};
+    appInfo.sType = VK_STRUCTURE_TYPE_APPLICATION_INFO;
+    appInfo.pApplicationName = "Vulkan Image CUDA Interop";
+    appInfo.applicationVersion = VK_MAKE_VERSION(1, 0, 0);
+    appInfo.pEngineName = "No Engine";
+    appInfo.engineVersion = VK_MAKE_VERSION(1, 0, 0);
+    appInfo.apiVersion = VK_API_VERSION_1_1;
+
+    VkInstanceCreateInfo createInfo = {};
+    createInfo.sType = VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO;
+    createInfo.pApplicationInfo = &appInfo;
+
+    auto extensions = getRequiredExtensions();
+    createInfo.enabledExtensionCount = static_cast<uint32_t>(extensions.size());
+    createInfo.ppEnabledExtensionNames = extensions.data();
+
+    VkDebugUtilsMessengerCreateInfoEXT debugCreateInfo;
+    if (enableValidationLayers) {
+      createInfo.enabledLayerCount =
+          static_cast<uint32_t>(validationLayers.size());
+      createInfo.ppEnabledLayerNames = validationLayers.data();
+
+      populateDebugMessengerCreateInfo(debugCreateInfo);
+      createInfo.pNext = (VkDebugUtilsMessengerCreateInfoEXT*)&debugCreateInfo;
+    } else {
+      createInfo.enabledLayerCount = 0;
+
+      createInfo.pNext = nullptr;
+    }
+
+    if (vkCreateInstance(&createInfo, nullptr, &instance) != VK_SUCCESS) {
+      throw std::runtime_error("failed to create instance!");
+    }
+
+    fpGetPhysicalDeviceProperties2 =
+        (PFN_vkGetPhysicalDeviceProperties2)vkGetInstanceProcAddr(
+            instance, "vkGetPhysicalDeviceProperties2");
+    if (fpGetPhysicalDeviceProperties2 == NULL) {
+      throw std::runtime_error(
+          "Vulkan: Proc address for \"vkGetPhysicalDeviceProperties2KHR\" not "
+          "found.\n");
+    }
+
+#ifdef _WIN64
+    fpGetMemoryWin32HandleKHR =
+        (PFN_vkGetMemoryWin32HandleKHR)vkGetInstanceProcAddr(
+            instance, "vkGetMemoryWin32HandleKHR");
+    if (fpGetMemoryWin32HandleKHR == NULL) {
+      throw std::runtime_error(
+          "Vulkan: Proc address for \"vkGetMemoryWin32HandleKHR\" not "
+          "found.\n");
+    }
+#else
+    fpGetMemoryFdKHR = (PFN_vkGetMemoryFdKHR)vkGetInstanceProcAddr(
+        instance, "vkGetMemoryFdKHR");
+    if (fpGetMemoryFdKHR == NULL) {
+      throw std::runtime_error(
+          "Vulkan: Proc address for \"vkGetMemoryFdKHR\" not found.\n");
+    } else {
+      std::cout << "Vulkan proc address for vkGetMemoryFdKHR - "
+                << fpGetMemoryFdKHR << std::endl;
+    }
+#endif
+  }
+
+  void populateDebugMessengerCreateInfo(
+      VkDebugUtilsMessengerCreateInfoEXT& createInfo) {
+    createInfo = {};
+    createInfo.sType = VK_STRUCTURE_TYPE_DEBUG_UTILS_MESSENGER_CREATE_INFO_EXT;
+    createInfo.messageSeverity =
+        VK_DEBUG_UTILS_MESSAGE_SEVERITY_VERBOSE_BIT_EXT |
+        VK_DEBUG_UTILS_MESSAGE_SEVERITY_WARNING_BIT_EXT |
+        VK_DEBUG_UTILS_MESSAGE_SEVERITY_ERROR_BIT_EXT;
+    createInfo.messageType = VK_DEBUG_UTILS_MESSAGE_TYPE_GENERAL_BIT_EXT |
+                             VK_DEBUG_UTILS_MESSAGE_TYPE_VALIDATION_BIT_EXT |
+                             VK_DEBUG_UTILS_MESSAGE_TYPE_PERFORMANCE_BIT_EXT;
+    createInfo.pfnUserCallback = debugCallback;
+  }
+
+  void setupDebugMessenger() {
+    if (!enableValidationLayers) return;
+
+    VkDebugUtilsMessengerCreateInfoEXT createInfo;
+    populateDebugMessengerCreateInfo(createInfo);
+
+    if (CreateDebugUtilsMessengerEXT(instance, &createInfo, nullptr,
+                                     &debugMessenger) != VK_SUCCESS) {
+      throw std::runtime_error("failed to set up debug messenger!");
+    }
+  }
+
+  void createSurface() {
+    if (glfwCreateWindowSurface(instance, window, nullptr, &surface) !=
+        VK_SUCCESS) {
+      throw std::runtime_error("failed to create window surface!");
+    }
+  }
+
+  void pickPhysicalDevice() {
+    uint32_t deviceCount = 0;
+    vkEnumeratePhysicalDevices(instance, &deviceCount, nullptr);
+
+    if (deviceCount == 0) {
+      throw std::runtime_error("failed to find GPUs with Vulkan support!");
+    }
+
+    std::vector<VkPhysicalDevice> devices(deviceCount);
+    vkEnumeratePhysicalDevices(instance, &deviceCount, devices.data());
+
+    for (const auto& device : devices) {
+      if (isDeviceSuitable(device)) {
+        physicalDevice = device;
+        break;
+      }
+    }
+
+    if (physicalDevice == VK_NULL_HANDLE) {
+      throw std::runtime_error("failed to find a suitable GPU!");
+    }
+
+    std::cout << "Selected physical device = " << physicalDevice << std::endl;
+
+    VkPhysicalDeviceIDProperties vkPhysicalDeviceIDProperties = {};
+    vkPhysicalDeviceIDProperties.sType =
+        VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_ID_PROPERTIES;
+    vkPhysicalDeviceIDProperties.pNext = NULL;
+
+    VkPhysicalDeviceProperties2 vkPhysicalDeviceProperties2 = {};
+    vkPhysicalDeviceProperties2.sType =
+        VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_PROPERTIES_2;
+    vkPhysicalDeviceProperties2.pNext = &vkPhysicalDeviceIDProperties;
+
+    fpGetPhysicalDeviceProperties2(physicalDevice,
+                                   &vkPhysicalDeviceProperties2);
+
+    memcpy(vkDeviceUUID, vkPhysicalDeviceIDProperties.deviceUUID,
+           sizeof(vkDeviceUUID));
+  }
+
+  void getKhrExtensionsFn() {
+#ifdef _WIN64
+
+    fpGetSemaphoreWin32HandleKHR =
+        (PFN_vkGetSemaphoreWin32HandleKHR)vkGetDeviceProcAddr(
+            device, "vkGetSemaphoreWin32HandleKHR");
+    if (fpGetSemaphoreWin32HandleKHR == NULL) {
+      throw std::runtime_error(
+          "Vulkan: Proc address for \"vkGetSemaphoreWin32HandleKHR\" not "
+          "found.\n");
+    }
+#else
+    fpGetSemaphoreFdKHR = (PFN_vkGetSemaphoreFdKHR)vkGetDeviceProcAddr(
+        device, "vkGetSemaphoreFdKHR");
+    if (fpGetSemaphoreFdKHR == NULL) {
+      throw std::runtime_error(
+          "Vulkan: Proc address for \"vkGetSemaphoreFdKHR\" not found.\n");
+    }
+#endif
+  }
+
+  int setCudaVkDevice() {
+    int current_device = 0;
+    int device_count = 0;
+    int devices_prohibited = 0;
+
+    hipDeviceProp_t deviceProp;
+    HIPCHECK(hipGetDeviceCount(&device_count));
+
+    if (device_count == 0) {
+      fprintf(stderr, "CUDA error: no devices supporting CUDA.\n");
+      exit(EXIT_FAILURE);
+    }
+
+    // Find the GPU which is selected by Vulkan
+    while (current_device < device_count) {
+      hipGetDeviceProperties(&deviceProp, current_device);
+
+      if ((deviceProp.computeMode != hipComputeModeProhibited)) {
+        // Compare the cuda device UUID with vulkan UUID
+        int ret = memcmp(&deviceProp.uuid, &vkDeviceUUID, VK_UUID_SIZE);
+        if (ret == 0) {
+          HIPCHECK(hipSetDevice(current_device));
+          HIPCHECK(hipGetDeviceProperties(&deviceProp, current_device));
+          printf("GPU Device %d: \"%s\" with compute capability %d.%d\n\n",
+                 current_device, deviceProp.name, deviceProp.major,
+                 deviceProp.minor);
+
+          return current_device;
+        }
+
+      } else {
+        devices_prohibited++;
+      }
+
+      current_device++;
+    }
+
+    if (devices_prohibited == device_count) {
+      fprintf(stderr,
+              "CUDA error:"
+              " No Vulkan-CUDA Interop capable GPU found.\n");
+      exit(EXIT_FAILURE);
+    }
+
+    return -1;
+  }
+
+  void createLogicalDevice() {
+    QueueFamilyIndices indices = findQueueFamilies(physicalDevice);
+
+    std::vector<VkDeviceQueueCreateInfo> queueCreateInfos;
+    std::set<int> uniqueQueueFamilies = {indices.graphicsFamily,
+                                         indices.presentFamily};
+
+    float queuePriority = 1.0f;
+    for (int queueFamily : uniqueQueueFamilies) {
+      VkDeviceQueueCreateInfo queueCreateInfo = {};
+      queueCreateInfo.sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO;
+      queueCreateInfo.queueFamilyIndex = queueFamily;
+      queueCreateInfo.queueCount = 1;
+      queueCreateInfo.pQueuePriorities = &queuePriority;
+      queueCreateInfos.push_back(queueCreateInfo);
+    }
+
+    VkPhysicalDeviceFeatures deviceFeatures = {};
+    deviceFeatures.samplerAnisotropy = VK_TRUE;
+
+    VkDeviceCreateInfo createInfo = {};
+    createInfo.sType = VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO;
+
+    createInfo.pQueueCreateInfos = queueCreateInfos.data();
+    createInfo.queueCreateInfoCount = queueCreateInfos.size();
+
+    createInfo.pEnabledFeatures = &deviceFeatures;
+    std::vector<const char*> enabledExtensionNameList;
+
+    for (int i = 0; i < deviceExtensions.size(); i++) {
+      enabledExtensionNameList.push_back(deviceExtensions[i]);
+    }
+    if (enableValidationLayers) {
+      createInfo.enabledLayerCount =
+          static_cast<uint32_t>(validationLayers.size());
+      createInfo.ppEnabledLayerNames = validationLayers.data();
+    } else {
+      createInfo.enabledLayerCount = 0;
+    }
+    createInfo.enabledExtensionCount =
+        static_cast<uint32_t>(enabledExtensionNameList.size());
+    createInfo.ppEnabledExtensionNames = enabledExtensionNameList.data();
+
+    if (vkCreateDevice(physicalDevice, &createInfo, nullptr, &device) !=
+        VK_SUCCESS) {
+      throw std::runtime_error("failed to create logical device!");
+    }
+    vkGetDeviceQueue(device, indices.graphicsFamily, 0, &graphicsQueue);
+    vkGetDeviceQueue(device, indices.presentFamily, 0, &presentQueue);
+  }
+
+  void createSwapChain() {
+    SwapChainSupportDetails swapChainSupport =
+        querySwapChainSupport(physicalDevice);
+
+    VkSurfaceFormatKHR surfaceFormat =
+        chooseSwapSurfaceFormat(swapChainSupport.formats);
+    VkPresentModeKHR presentMode =
+        chooseSwapPresentMode(swapChainSupport.presentModes);
+    VkExtent2D extent = chooseSwapExtent(swapChainSupport.capabilities);
+
+    uint32_t imageCount = swapChainSupport.capabilities.minImageCount + 1;
+    if (swapChainSupport.capabilities.maxImageCount > 0 &&
+        imageCount > swapChainSupport.capabilities.maxImageCount) {
+      imageCount = swapChainSupport.capabilities.maxImageCount;
+    }
+
+    VkSwapchainCreateInfoKHR createInfo = {};
+    createInfo.sType = VK_STRUCTURE_TYPE_SWAPCHAIN_CREATE_INFO_KHR;
+    createInfo.surface = surface;
+
+    createInfo.minImageCount = imageCount;
+    createInfo.imageFormat = surfaceFormat.format;
+    createInfo.imageColorSpace = surfaceFormat.colorSpace;
+    createInfo.imageExtent = extent;
+    createInfo.imageArrayLayers = 1;
+    createInfo.imageUsage = VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT;
+
+    QueueFamilyIndices indices = findQueueFamilies(physicalDevice);
+    uint32_t queueFamilyIndices[] = {(uint32_t)indices.graphicsFamily,
+                                     (uint32_t)indices.presentFamily};
+
+    if (indices.graphicsFamily != indices.presentFamily) {
+      createInfo.imageSharingMode = VK_SHARING_MODE_CONCURRENT;
+      createInfo.queueFamilyIndexCount = 2;
+      createInfo.pQueueFamilyIndices = queueFamilyIndices;
+    } else {
+      createInfo.imageSharingMode = VK_SHARING_MODE_EXCLUSIVE;
+    }
+
+    createInfo.preTransform = swapChainSupport.capabilities.currentTransform;
+    createInfo.compositeAlpha = VK_COMPOSITE_ALPHA_OPAQUE_BIT_KHR;
+    createInfo.presentMode = presentMode;
+    createInfo.clipped = VK_TRUE;
+
+    if (vkCreateSwapchainKHR(device, &createInfo, nullptr, &swapChain) !=
+        VK_SUCCESS) {
+      throw std::runtime_error("failed to create swap chain!");
+    }
+
+    vkGetSwapchainImagesKHR(device, swapChain, &imageCount, nullptr);
+    swapChainImages.resize(imageCount);
+    vkGetSwapchainImagesKHR(device, swapChain, &imageCount,
+                            swapChainImages.data());
+
+    swapChainImageFormat = surfaceFormat.format;
+    swapChainExtent = extent;
+  }
+
+  void createImageViews() {
+    swapChainImageViews.resize(swapChainImages.size());
+
+    for (size_t i = 0; i < swapChainImages.size(); i++) {
+      swapChainImageViews[i] =
+          createImageView(swapChainImages[i], swapChainImageFormat);
+    }
+  }
+
+  void createRenderPass() {
+    VkAttachmentDescription colorAttachment = {};
+    colorAttachment.format = swapChainImageFormat;
+    colorAttachment.samples = VK_SAMPLE_COUNT_1_BIT;
+    colorAttachment.loadOp = VK_ATTACHMENT_LOAD_OP_CLEAR;
+    colorAttachment.storeOp = VK_ATTACHMENT_STORE_OP_STORE;
+    colorAttachment.stencilLoadOp = VK_ATTACHMENT_LOAD_OP_DONT_CARE;
+    colorAttachment.stencilStoreOp = VK_ATTACHMENT_STORE_OP_DONT_CARE;
+    colorAttachment.initialLayout = VK_IMAGE_LAYOUT_UNDEFINED;
+    colorAttachment.finalLayout = VK_IMAGE_LAYOUT_PRESENT_SRC_KHR;
+
+    VkAttachmentReference colorAttachmentRef = {};
+    colorAttachmentRef.attachment = 0;
+    colorAttachmentRef.layout = VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL;
+
+    VkSubpassDescription subpass = {};
+    subpass.pipelineBindPoint = VK_PIPELINE_BIND_POINT_GRAPHICS;
+    subpass.colorAttachmentCount = 1;
+    subpass.pColorAttachments = &colorAttachmentRef;
+
+    VkSubpassDependency dependency = {};
+    dependency.srcSubpass = VK_SUBPASS_EXTERNAL;
+    dependency.dstSubpass = 0;
+    dependency.srcStageMask = VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT;
+    dependency.srcAccessMask = 0;
+    dependency.dstStageMask = VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT;
+    dependency.dstAccessMask = VK_ACCESS_COLOR_ATTACHMENT_READ_BIT |
+                               VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT;
+
+    VkRenderPassCreateInfo renderPassInfo = {};
+    renderPassInfo.sType = VK_STRUCTURE_TYPE_RENDER_PASS_CREATE_INFO;
+    renderPassInfo.attachmentCount = 1;
+    renderPassInfo.pAttachments = &colorAttachment;
+    renderPassInfo.subpassCount = 1;
+    renderPassInfo.pSubpasses = &subpass;
+    renderPassInfo.dependencyCount = 1;
+    renderPassInfo.pDependencies = &dependency;
+
+    if (vkCreateRenderPass(device, &renderPassInfo, nullptr, &renderPass) !=
+        VK_SUCCESS) {
+      throw std::runtime_error("failed to create render pass!");
+    }
+  }
+
+  void createDescriptorSetLayout() {
+    VkDescriptorSetLayoutBinding uboLayoutBinding = {};
+    uboLayoutBinding.binding = 0;
+    uboLayoutBinding.descriptorCount = 1;
+    uboLayoutBinding.descriptorType = VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER;
+    uboLayoutBinding.pImmutableSamplers = nullptr;
+    uboLayoutBinding.stageFlags = VK_SHADER_STAGE_VERTEX_BIT;
+
+    VkDescriptorSetLayoutBinding samplerLayoutBinding = {};
+    samplerLayoutBinding.binding = 1;
+    samplerLayoutBinding.descriptorCount = 1;
+    samplerLayoutBinding.descriptorType =
+        VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER;
+    samplerLayoutBinding.pImmutableSamplers = nullptr;
+    samplerLayoutBinding.stageFlags = VK_SHADER_STAGE_FRAGMENT_BIT;
+
+    std::array<VkDescriptorSetLayoutBinding, 2> bindings = {
+        uboLayoutBinding, samplerLayoutBinding};
+    VkDescriptorSetLayoutCreateInfo layoutInfo = {};
+    layoutInfo.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO;
+    layoutInfo.bindingCount = static_cast<uint32_t>(bindings.size());
+    layoutInfo.pBindings = bindings.data();
+
+    if (vkCreateDescriptorSetLayout(device, &layoutInfo, nullptr,
+                                    &descriptorSetLayout) != VK_SUCCESS) {
+      throw std::runtime_error("failed to create descriptor set layout!");
+    }
+  }
+
+  void createGraphicsPipeline() {
+    auto vertShaderCode = readFile("vert.spv");
+    auto fragShaderCode = readFile("frag.spv");
+
+    VkShaderModule vertShaderModule = createShaderModule(vertShaderCode);
+    VkShaderModule fragShaderModule = createShaderModule(fragShaderCode);
+
+    VkPipelineShaderStageCreateInfo vertShaderStageInfo = {};
+    vertShaderStageInfo.sType =
+        VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO;
+    vertShaderStageInfo.stage = VK_SHADER_STAGE_VERTEX_BIT;
+    vertShaderStageInfo.module = vertShaderModule;
+    vertShaderStageInfo.pName = "main";
+
+    VkPipelineShaderStageCreateInfo fragShaderStageInfo = {};
+    fragShaderStageInfo.sType =
+        VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO;
+    fragShaderStageInfo.stage = VK_SHADER_STAGE_FRAGMENT_BIT;
+    fragShaderStageInfo.module = fragShaderModule;
+    fragShaderStageInfo.pName = "main";
+
+    VkPipelineShaderStageCreateInfo shaderStages[] = {vertShaderStageInfo,
+                                                      fragShaderStageInfo};
+
+    VkPipelineVertexInputStateCreateInfo vertexInputInfo = {};
+    vertexInputInfo.sType =
+        VK_STRUCTURE_TYPE_PIPELINE_VERTEX_INPUT_STATE_CREATE_INFO;
+
+    auto bindingDescription = Vertex::getBindingDescription();
+    auto attributeDescriptions = Vertex::getAttributeDescriptions();
+
+    vertexInputInfo.vertexBindingDescriptionCount = 1;
+    vertexInputInfo.vertexAttributeDescriptionCount =
+        static_cast<uint32_t>(attributeDescriptions.size());
+    vertexInputInfo.pVertexBindingDescriptions = &bindingDescription;
+    vertexInputInfo.pVertexAttributeDescriptions = attributeDescriptions.data();
+
+    VkPipelineInputAssemblyStateCreateInfo inputAssembly = {};
+    inputAssembly.sType =
+        VK_STRUCTURE_TYPE_PIPELINE_INPUT_ASSEMBLY_STATE_CREATE_INFO;
+    inputAssembly.topology = VK_PRIMITIVE_TOPOLOGY_TRIANGLE_LIST;
+    inputAssembly.primitiveRestartEnable = VK_FALSE;
+
+    VkViewport viewport = {};
+    viewport.x = 0.0f;
+    viewport.y = 0.0f;
+    viewport.width = (float)swapChainExtent.width;
+    viewport.height = (float)swapChainExtent.height;
+    viewport.minDepth = 0.0f;
+    viewport.maxDepth = 1.0f;
+
+    VkRect2D scissor = {};
+    scissor.offset = {0, 0};
+    scissor.extent = swapChainExtent;
+
+    VkPipelineViewportStateCreateInfo viewportState = {};
+    viewportState.sType = VK_STRUCTURE_TYPE_PIPELINE_VIEWPORT_STATE_CREATE_INFO;
+    viewportState.viewportCount = 1;
+    viewportState.pViewports = &viewport;
+    viewportState.scissorCount = 1;
+    viewportState.pScissors = &scissor;
+
+    VkPipelineRasterizationStateCreateInfo rasterizer = {};
+    rasterizer.sType =
+        VK_STRUCTURE_TYPE_PIPELINE_RASTERIZATION_STATE_CREATE_INFO;
+    rasterizer.depthClampEnable = VK_FALSE;
+    rasterizer.rasterizerDiscardEnable = VK_FALSE;
+    rasterizer.polygonMode = VK_POLYGON_MODE_FILL;
+    rasterizer.lineWidth = 1.0f;
+    rasterizer.cullMode = VK_CULL_MODE_BACK_BIT;
+    rasterizer.frontFace = VK_FRONT_FACE_COUNTER_CLOCKWISE;
+    rasterizer.depthBiasEnable = VK_FALSE;
+
+    VkPipelineMultisampleStateCreateInfo multisampling = {};
+    multisampling.sType =
+        VK_STRUCTURE_TYPE_PIPELINE_MULTISAMPLE_STATE_CREATE_INFO;
+    multisampling.sampleShadingEnable = VK_FALSE;
+    multisampling.rasterizationSamples = VK_SAMPLE_COUNT_1_BIT;
+
+    VkPipelineColorBlendAttachmentState colorBlendAttachment = {};
+    colorBlendAttachment.colorWriteMask =
+        VK_COLOR_COMPONENT_R_BIT | VK_COLOR_COMPONENT_G_BIT |
+        VK_COLOR_COMPONENT_B_BIT | VK_COLOR_COMPONENT_A_BIT;
+    colorBlendAttachment.blendEnable = VK_FALSE;
+
+    VkPipelineColorBlendStateCreateInfo colorBlending = {};
+    colorBlending.sType =
+        VK_STRUCTURE_TYPE_PIPELINE_COLOR_BLEND_STATE_CREATE_INFO;
+    colorBlending.logicOpEnable = VK_FALSE;
+    colorBlending.logicOp = VK_LOGIC_OP_COPY;
+    colorBlending.attachmentCount = 1;
+    colorBlending.pAttachments = &colorBlendAttachment;
+    colorBlending.blendConstants[0] = 0.0f;
+    colorBlending.blendConstants[1] = 0.0f;
+    colorBlending.blendConstants[2] = 0.0f;
+    colorBlending.blendConstants[3] = 0.0f;
+
+    VkPipelineLayoutCreateInfo pipelineLayoutInfo = {};
+    pipelineLayoutInfo.sType = VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO;
+    pipelineLayoutInfo.setLayoutCount = 1;
+    pipelineLayoutInfo.pSetLayouts = &descriptorSetLayout;
+
+    if (vkCreatePipelineLayout(device, &pipelineLayoutInfo, nullptr,
+                               &pipelineLayout) != VK_SUCCESS) {
+      throw std::runtime_error("failed to create pipeline layout!");
+    }
+
+    VkGraphicsPipelineCreateInfo pipelineInfo = {};
+    pipelineInfo.sType = VK_STRUCTURE_TYPE_GRAPHICS_PIPELINE_CREATE_INFO;
+    pipelineInfo.stageCount = 2;
+    pipelineInfo.pStages = shaderStages;
+    pipelineInfo.pVertexInputState = &vertexInputInfo;
+    pipelineInfo.pInputAssemblyState = &inputAssembly;
+    pipelineInfo.pViewportState = &viewportState;
+    pipelineInfo.pRasterizationState = &rasterizer;
+    pipelineInfo.pMultisampleState = &multisampling;
+    pipelineInfo.pColorBlendState = &colorBlending;
+    pipelineInfo.layout = pipelineLayout;
+    pipelineInfo.renderPass = renderPass;
+    pipelineInfo.subpass = 0;
+    pipelineInfo.basePipelineHandle = VK_NULL_HANDLE;
+
+    if (vkCreateGraphicsPipelines(device, VK_NULL_HANDLE, 1, &pipelineInfo,
+                                  nullptr, &graphicsPipeline) != VK_SUCCESS) {
+      throw std::runtime_error("failed to create graphics pipeline!");
+    }
+
+    vkDestroyShaderModule(device, fragShaderModule, nullptr);
+    vkDestroyShaderModule(device, vertShaderModule, nullptr);
+  }
+
+  void createFramebuffers() {
+    swapChainFramebuffers.resize(swapChainImageViews.size());
+
+    for (size_t i = 0; i < swapChainImageViews.size(); i++) {
+      VkImageView attachments[] = {swapChainImageViews[i]};
+
+      VkFramebufferCreateInfo framebufferInfo = {};
+      framebufferInfo.sType = VK_STRUCTURE_TYPE_FRAMEBUFFER_CREATE_INFO;
+      framebufferInfo.renderPass = renderPass;
+      framebufferInfo.attachmentCount = 1;
+      framebufferInfo.pAttachments = attachments;
+      framebufferInfo.width = swapChainExtent.width;
+      framebufferInfo.height = swapChainExtent.height;
+      framebufferInfo.layers = 1;
+
+      if (vkCreateFramebuffer(device, &framebufferInfo, nullptr,
+                              &swapChainFramebuffers[i]) != VK_SUCCESS) {
+        throw std::runtime_error("failed to create framebuffer!");
+      }
+    }
+  }
+
+  void createCommandPool() {
+    QueueFamilyIndices queueFamilyIndices = findQueueFamilies(physicalDevice);
+
+    VkCommandPoolCreateInfo poolInfo = {};
+    poolInfo.sType = VK_STRUCTURE_TYPE_COMMAND_POOL_CREATE_INFO;
+    poolInfo.queueFamilyIndex = queueFamilyIndices.graphicsFamily;
+
+    if (vkCreateCommandPool(device, &poolInfo, nullptr, &commandPool) !=
+        VK_SUCCESS) {
+      throw std::runtime_error("failed to create graphics command pool!");
+    }
+  }
+
+  void createTextureImage() {
+    VkDeviceSize imageSize = imageWidth * imageHeight * 4;
+    mipLevels = static_cast<uint32_t>(
+                    std::floor(std::log2(std::max(imageWidth, imageHeight)))) +
+                1;
+    printf("mipLevels = %d\n", mipLevels);
+
+    if (!image_data) {
+      throw std::runtime_error("failed to load texture image!");
+    }
+
+    VkBuffer stagingBuffer;
+    VkDeviceMemory stagingBufferMemory;
+    createBuffer(imageSize, VK_BUFFER_USAGE_TRANSFER_SRC_BIT,
+                 VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT |
+                     VK_MEMORY_PROPERTY_HOST_COHERENT_BIT,
+                 stagingBuffer, stagingBufferMemory);
+
+    void* data;
+    vkMapMemory(device, stagingBufferMemory, 0, imageSize, 0, &data);
+    memcpy(data, image_data, static_cast<size_t>(imageSize));
+    vkUnmapMemory(device, stagingBufferMemory);
+
+    // VK_FORMAT_R8G8B8A8_UNORM changed to VK_FORMAT_R8G8B8A8_UINT
+    createImage(
+        imageWidth, imageHeight, VK_FORMAT_R8G8B8A8_UNORM,
+        VK_IMAGE_TILING_OPTIMAL,
+        VK_IMAGE_USAGE_STORAGE_BIT | VK_IMAGE_USAGE_TRANSFER_SRC_BIT |
+            VK_IMAGE_USAGE_TRANSFER_DST_BIT | VK_IMAGE_USAGE_SAMPLED_BIT,
+        VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT, textureImage, textureImageMemory);
+
+    transitionImageLayout(textureImage, VK_FORMAT_R8G8B8A8_UINT,
+                          VK_IMAGE_LAYOUT_UNDEFINED,
+                          VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL);
+    copyBufferToImage(stagingBuffer, textureImage,
+                      static_cast<uint32_t>(imageWidth),
+                      static_cast<uint32_t>(imageHeight));
+
+    vkDestroyBuffer(device, stagingBuffer, nullptr);
+    vkFreeMemory(device, stagingBufferMemory, nullptr);
+
+    generateMipmaps(textureImage, VK_FORMAT_R8G8B8A8_UNORM);
+  }
+
+  void generateMipmaps(VkImage image, VkFormat imageFormat) {
+    VkFormatProperties formatProperties;
+    vkGetPhysicalDeviceFormatProperties(physicalDevice, imageFormat,
+                                        &formatProperties);
+
+    if (!(formatProperties.optimalTilingFeatures &
+          VK_FORMAT_FEATURE_SAMPLED_IMAGE_FILTER_LINEAR_BIT)) {
+      throw std::runtime_error(
+          "texture image format does not support linear blitting!");
+    }
+
+    VkCommandBuffer commandBuffer = beginSingleTimeCommands();
+
+    VkImageMemoryBarrier barrier = {};
+    barrier.sType = VK_STRUCTURE_TYPE_IMAGE_MEMORY_BARRIER;
+    barrier.image = image;
+    barrier.srcQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
+    barrier.dstQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
+    barrier.subresourceRange.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
+    barrier.subresourceRange.baseArrayLayer = 0;
+    barrier.subresourceRange.layerCount = 1;
+    barrier.subresourceRange.levelCount = 1;
+
+    int32_t mipWidth = imageWidth;
+    int32_t mipHeight = imageHeight;
+
+    for (uint32_t i = 1; i < mipLevels; i++) {
+      barrier.subresourceRange.baseMipLevel = i - 1;
+      barrier.oldLayout = VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL;
+      barrier.newLayout = VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL;
+      barrier.srcAccessMask = VK_ACCESS_TRANSFER_WRITE_BIT;
+      barrier.dstAccessMask = VK_ACCESS_TRANSFER_READ_BIT;
+
+      vkCmdPipelineBarrier(commandBuffer, VK_PIPELINE_STAGE_TRANSFER_BIT,
+                           VK_PIPELINE_STAGE_TRANSFER_BIT, 0, 0, nullptr, 0,
+                           nullptr, 1, &barrier);
+
+      VkImageBlit blit = {};
+      blit.srcOffsets[0] = {0, 0, 0};
+      blit.srcOffsets[1] = {mipWidth, mipHeight, 1};
+      blit.srcSubresource.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
+      blit.srcSubresource.mipLevel = i - 1;
+      blit.srcSubresource.baseArrayLayer = 0;
+      blit.srcSubresource.layerCount = 1;
+      blit.dstOffsets[0] = {0, 0, 0};
+      blit.dstOffsets[1] = {mipWidth > 1 ? mipWidth / 2 : 1,
+                            mipHeight > 1 ? mipHeight / 2 : 1, 1};
+      blit.dstSubresource.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
+      blit.dstSubresource.mipLevel = i;
+      blit.dstSubresource.baseArrayLayer = 0;
+      blit.dstSubresource.layerCount = 1;
+
+      vkCmdBlitImage(commandBuffer, image, VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL,
+                     image, VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, 1, &blit,
+                     VK_FILTER_LINEAR);
+
+      barrier.oldLayout = VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL;
+      barrier.newLayout = VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL;
+      barrier.srcAccessMask = VK_ACCESS_TRANSFER_READ_BIT;
+      barrier.dstAccessMask = VK_ACCESS_SHADER_READ_BIT;
+
+      vkCmdPipelineBarrier(commandBuffer, VK_PIPELINE_STAGE_TRANSFER_BIT,
+                           VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT, 0, 0, nullptr,
+                           0, nullptr, 1, &barrier);
+
+      if (mipWidth > 1) mipWidth /= 2;
+      if (mipHeight > 1) mipHeight /= 2;
+    }
+
+    barrier.subresourceRange.baseMipLevel = mipLevels - 1;
+    barrier.oldLayout = VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL;
+    barrier.newLayout = VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL;
+    barrier.srcAccessMask = VK_ACCESS_TRANSFER_WRITE_BIT;
+    barrier.dstAccessMask = VK_ACCESS_SHADER_READ_BIT;
+
+    vkCmdPipelineBarrier(commandBuffer, VK_PIPELINE_STAGE_TRANSFER_BIT,
+                         VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT, 0, 0, nullptr,
+                         0, nullptr, 1, &barrier);
+
+    endSingleTimeCommands(commandBuffer);
+  }
+
+#ifdef _WIN64  // For windows
+  HANDLE getVkImageMemHandle(
+      VkExternalMemoryHandleTypeFlagsKHR externalMemoryHandleType) {
+    HANDLE handle;
+
+    VkMemoryGetWin32HandleInfoKHR vkMemoryGetWin32HandleInfoKHR = {};
+    vkMemoryGetWin32HandleInfoKHR.sType =
+        VK_STRUCTURE_TYPE_MEMORY_GET_WIN32_HANDLE_INFO_KHR;
+    vkMemoryGetWin32HandleInfoKHR.pNext = NULL;
+    vkMemoryGetWin32HandleInfoKHR.memory = textureImageMemory;
+    vkMemoryGetWin32HandleInfoKHR.handleType =
+        (VkExternalMemoryHandleTypeFlagBitsKHR)externalMemoryHandleType;
+
+    fpGetMemoryWin32HandleKHR(device, &vkMemoryGetWin32HandleInfoKHR, &handle);
+    return handle;
+  }
+  HANDLE getVkSemaphoreHandle(
+      VkExternalSemaphoreHandleTypeFlagBitsKHR externalSemaphoreHandleType,
+      VkSemaphore& semVkCuda) {
+    HANDLE handle;
+
+    VkSemaphoreGetWin32HandleInfoKHR vulkanSemaphoreGetWin32HandleInfoKHR = {};
+    vulkanSemaphoreGetWin32HandleInfoKHR.sType =
+        VK_STRUCTURE_TYPE_SEMAPHORE_GET_WIN32_HANDLE_INFO_KHR;
+    vulkanSemaphoreGetWin32HandleInfoKHR.pNext = NULL;
+    vulkanSemaphoreGetWin32HandleInfoKHR.semaphore = semVkCuda;
+    vulkanSemaphoreGetWin32HandleInfoKHR.handleType =
+        externalSemaphoreHandleType;
+
+    fpGetSemaphoreWin32HandleKHR(device, &vulkanSemaphoreGetWin32HandleInfoKHR,
+                                 &handle);
+
+    return handle;
+  }
+#else
+  int getVkImageMemHandle(
+      VkExternalMemoryHandleTypeFlagsKHR externalMemoryHandleType) {
+    if (externalMemoryHandleType ==
+        VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT_KHR) {
+      int fd;
+
+      VkMemoryGetFdInfoKHR vkMemoryGetFdInfoKHR = {};
+      vkMemoryGetFdInfoKHR.sType = VK_STRUCTURE_TYPE_MEMORY_GET_FD_INFO_KHR;
+      vkMemoryGetFdInfoKHR.pNext = NULL;
+      vkMemoryGetFdInfoKHR.memory = textureImageMemory;
+      vkMemoryGetFdInfoKHR.handleType =
+          VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT_KHR;
+
+      fpGetMemoryFdKHR(device, &vkMemoryGetFdInfoKHR, &fd);
+
+      return fd;
+    }
+    return -1;
+  }
+
+  int getVkSemaphoreHandle(
+      VkExternalSemaphoreHandleTypeFlagBitsKHR externalSemaphoreHandleType,
+      VkSemaphore& semVkCuda) {
+    if (externalSemaphoreHandleType ==
+        VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_FD_BIT) {
+      int fd;
+
+      VkSemaphoreGetFdInfoKHR vulkanSemaphoreGetFdInfoKHR = {};
+      vulkanSemaphoreGetFdInfoKHR.sType =
+          VK_STRUCTURE_TYPE_SEMAPHORE_GET_FD_INFO_KHR;
+      vulkanSemaphoreGetFdInfoKHR.pNext = NULL;
+      vulkanSemaphoreGetFdInfoKHR.semaphore = semVkCuda;
+      vulkanSemaphoreGetFdInfoKHR.handleType =
+          VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_FD_BIT_KHR;
+
+      fpGetSemaphoreFdKHR(device, &vulkanSemaphoreGetFdInfoKHR, &fd);
+
+      return fd;
+    }
+    return -1;
+  }
+#endif
+
+  void createTextureImageView() {
+    textureImageView = createImageView(textureImage, VK_FORMAT_R8G8B8A8_UNORM);
+  }
+
+  void createTextureSampler() {
+    VkSamplerCreateInfo samplerInfo = {};
+    samplerInfo.sType = VK_STRUCTURE_TYPE_SAMPLER_CREATE_INFO;
+    samplerInfo.magFilter = VK_FILTER_LINEAR;
+    samplerInfo.minFilter = VK_FILTER_LINEAR;
+    samplerInfo.addressModeU = VK_SAMPLER_ADDRESS_MODE_REPEAT;
+    samplerInfo.addressModeV = VK_SAMPLER_ADDRESS_MODE_REPEAT;
+    samplerInfo.addressModeW = VK_SAMPLER_ADDRESS_MODE_REPEAT;
+    samplerInfo.anisotropyEnable = VK_TRUE;
+    samplerInfo.maxAnisotropy = 16;
+    samplerInfo.borderColor = VK_BORDER_COLOR_INT_OPAQUE_BLACK;
+    samplerInfo.unnormalizedCoordinates = VK_FALSE;
+    samplerInfo.compareEnable = VK_FALSE;
+    samplerInfo.compareOp = VK_COMPARE_OP_ALWAYS;
+    samplerInfo.mipmapMode = VK_SAMPLER_MIPMAP_MODE_LINEAR;
+    samplerInfo.minLod = 0;  // Optional
+    samplerInfo.maxLod = static_cast<float>(mipLevels);
+    samplerInfo.mipLodBias = 0;  // Optional
+
+    if (vkCreateSampler(device, &samplerInfo, nullptr, &textureSampler) !=
+        VK_SUCCESS) {
+      throw std::runtime_error("failed to create texture sampler!");
+    }
+  }
+
+  VkImageView createImageView(VkImage image, VkFormat format) {
+    VkImageViewCreateInfo viewInfo = {};
+    viewInfo.sType = VK_STRUCTURE_TYPE_IMAGE_VIEW_CREATE_INFO;
+    viewInfo.image = image;
+    viewInfo.viewType = VK_IMAGE_VIEW_TYPE_2D;
+    viewInfo.format = format;
+    viewInfo.subresourceRange.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
+    viewInfo.subresourceRange.baseMipLevel = 0;
+    viewInfo.subresourceRange.levelCount = mipLevels;
+    viewInfo.subresourceRange.baseArrayLayer = 0;
+    viewInfo.subresourceRange.layerCount = 1;
+
+    VkImageView imageView;
+    if (vkCreateImageView(device, &viewInfo, nullptr, &imageView) !=
+        VK_SUCCESS) {
+      throw std::runtime_error("failed to create texture image view!");
+    }
+
+    return imageView;
+  }
+
+  void createImage(uint32_t width, uint32_t height, VkFormat format,
+                   VkImageTiling tiling, VkImageUsageFlags usage,
+                   VkMemoryPropertyFlags properties, VkImage& image,
+                   VkDeviceMemory& imageMemory) {
+    VkImageCreateInfo imageInfo = {};
+    imageInfo.sType = VK_STRUCTURE_TYPE_IMAGE_CREATE_INFO;
+    imageInfo.imageType = VK_IMAGE_TYPE_2D;
+    imageInfo.extent.width = width;
+    imageInfo.extent.height = height;
+    imageInfo.extent.depth = 1;
+    imageInfo.mipLevels = mipLevels;
+    imageInfo.arrayLayers = 1;
+    imageInfo.format = format;
+    imageInfo.tiling = tiling;
+    imageInfo.initialLayout = VK_IMAGE_LAYOUT_UNDEFINED;
+    imageInfo.usage = usage;
+    imageInfo.samples = VK_SAMPLE_COUNT_1_BIT;
+    imageInfo.sharingMode = VK_SHARING_MODE_EXCLUSIVE;
+
+    VkExternalMemoryImageCreateInfo vkExternalMemImageCreateInfo = {};
+    vkExternalMemImageCreateInfo.sType =
+        VK_STRUCTURE_TYPE_EXTERNAL_MEMORY_IMAGE_CREATE_INFO;
+    vkExternalMemImageCreateInfo.pNext = NULL;
+#ifdef _WIN64
+    vkExternalMemImageCreateInfo.handleTypes =
+        VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_WIN32_BIT;
+#else
+    vkExternalMemImageCreateInfo.handleTypes =
+        VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT_KHR;
+#endif
+
+    imageInfo.pNext = &vkExternalMemImageCreateInfo;
+
+    if (vkCreateImage(device, &imageInfo, nullptr, &image) != VK_SUCCESS) {
+      throw std::runtime_error("failed to create image!");
+    }
+
+    VkMemoryRequirements memRequirements;
+    vkGetImageMemoryRequirements(device, image, &memRequirements);
+
+#ifdef _WIN64
+    WindowsSecurityAttributes winSecurityAttributes;
+
+    VkExportMemoryWin32HandleInfoKHR vulkanExportMemoryWin32HandleInfoKHR = {};
+    vulkanExportMemoryWin32HandleInfoKHR.sType =
+        VK_STRUCTURE_TYPE_EXPORT_MEMORY_WIN32_HANDLE_INFO_KHR;
+    vulkanExportMemoryWin32HandleInfoKHR.pNext = NULL;
+    vulkanExportMemoryWin32HandleInfoKHR.pAttributes = &winSecurityAttributes;
+    vulkanExportMemoryWin32HandleInfoKHR.dwAccess =
+        DXGI_SHARED_RESOURCE_READ | DXGI_SHARED_RESOURCE_WRITE;
+    vulkanExportMemoryWin32HandleInfoKHR.name = (LPCWSTR)NULL;
+#endif
+    VkExportMemoryAllocateInfoKHR vulkanExportMemoryAllocateInfoKHR = {};
+    vulkanExportMemoryAllocateInfoKHR.sType =
+        VK_STRUCTURE_TYPE_EXPORT_MEMORY_ALLOCATE_INFO_KHR;
+#ifdef _WIN64
+    vulkanExportMemoryAllocateInfoKHR.pNext =
+        IsWindows8OrGreater() ? &vulkanExportMemoryWin32HandleInfoKHR : NULL;
+    vulkanExportMemoryAllocateInfoKHR.handleTypes =
+        IsWindows8OrGreater()
+            ? VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_WIN32_BIT
+            : VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_WIN32_KMT_BIT;
+#else
+    vulkanExportMemoryAllocateInfoKHR.pNext = NULL;
+    vulkanExportMemoryAllocateInfoKHR.handleTypes =
+        VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT_KHR;
+#endif
+
+    VkMemoryAllocateInfo allocInfo = {};
+    allocInfo.sType = VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_INFO;
+    allocInfo.allocationSize = memRequirements.size;
+    allocInfo.pNext = &vulkanExportMemoryAllocateInfoKHR;
+    allocInfo.memoryTypeIndex =
+        findMemoryType(memRequirements.memoryTypeBits, properties);
+
+    VkMemoryRequirements vkMemoryRequirements = {};
+    vkGetImageMemoryRequirements(device, image, &vkMemoryRequirements);
+    totalImageMemSize = vkMemoryRequirements.size;
+
+    if (vkAllocateMemory(device, &allocInfo, nullptr, &textureImageMemory) !=
+        VK_SUCCESS) {
+      throw std::runtime_error("failed to allocate image memory!");
+    }
+
+    vkBindImageMemory(device, image, textureImageMemory, 0);
+  }
+
+  void cudaVkImportSemaphore() {
+    hipExternalSemaphoreHandleDesc externalSemaphoreHandleDesc;
+    memset(&externalSemaphoreHandleDesc, 0,
+           sizeof(externalSemaphoreHandleDesc));
+#ifdef _WIN64
+    externalSemaphoreHandleDesc.type =
+        IsWindows8OrGreater() ? hipExternalSemaphoreHandleTypeOpaqueWin32
+                              : hipExternalSemaphoreHandleTypeOpaqueWin32Kmt;
+    externalSemaphoreHandleDesc.handle.win32.handle = getVkSemaphoreHandle(
+        IsWindows8OrGreater()
+            ? VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32_BIT
+            : VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32_KMT_BIT,
+        cudaUpdateVkSemaphore);
+#else
+    externalSemaphoreHandleDesc.type = hipExternalSemaphoreHandleTypeOpaqueFd;
+    externalSemaphoreHandleDesc.handle.fd = getVkSemaphoreHandle(
+        VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_FD_BIT, cudaUpdateVkSemaphore);
+#endif
+    externalSemaphoreHandleDesc.flags = 0;
+
+    HIPCHECK(hipImportExternalSemaphore(&cudaExtCudaUpdateVkSemaphore,
+                                                &externalSemaphoreHandleDesc));
+
+    memset(&externalSemaphoreHandleDesc, 0,
+           sizeof(externalSemaphoreHandleDesc));
+#ifdef _WIN64
+    externalSemaphoreHandleDesc.type =
+        IsWindows8OrGreater() ? hipExternalSemaphoreHandleTypeOpaqueWin32
+                              : hipExternalSemaphoreHandleTypeOpaqueWin32Kmt;
+    ;
+    externalSemaphoreHandleDesc.handle.win32.handle = getVkSemaphoreHandle(
+        IsWindows8OrGreater()
+            ? VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32_BIT
+            : VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32_KMT_BIT,
+        vkUpdateCudaSemaphore);
+#else
+    externalSemaphoreHandleDesc.type = hipExternalSemaphoreHandleTypeOpaqueFd;
+    externalSemaphoreHandleDesc.handle.fd = getVkSemaphoreHandle(
+        VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_FD_BIT, vkUpdateCudaSemaphore);
+#endif
+    externalSemaphoreHandleDesc.flags = 0;
+    HIPCHECK(hipImportExternalSemaphore(&cudaExtVkUpdateCudaSemaphore,
+                                                &externalSemaphoreHandleDesc));
+    printf("CUDA Imported Vulkan semaphore\n");
+  }
+
+  void cudaVkImportImageMem() {
+    hipExternalMemoryHandleDesc cudaExtMemHandleDesc;
+    memset(&cudaExtMemHandleDesc, 0, sizeof(cudaExtMemHandleDesc));
+#ifdef _WIN64
+    cudaExtMemHandleDesc.type =
+        IsWindows8OrGreater() ? hipExternalMemoryHandleTypeOpaqueWin32
+                              : hipExternalMemoryHandleTypeOpaqueWin32Kmt;
+    cudaExtMemHandleDesc.handle.win32.handle = getVkImageMemHandle(
+        IsWindows8OrGreater()
+            ? VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_WIN32_BIT
+            : VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_WIN32_KMT_BIT);
+#else
+    cudaExtMemHandleDesc.type = hipExternalMemoryHandleTypeOpaqueFd;
+
+    cudaExtMemHandleDesc.handle.fd =
+        getVkImageMemHandle(VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT_KHR);
+#endif
+    cudaExtMemHandleDesc.size = totalImageMemSize;
+
+    HIPCHECK(hipImportExternalMemory(&cudaExtMemImageBuffer,
+                                             &cudaExtMemHandleDesc));
+
+    cudaExternalMemoryMipmappedArrayDesc externalMemoryMipmappedArrayDesc;
+
+    memset(&externalMemoryMipmappedArrayDesc, 0,
+           sizeof(externalMemoryMipmappedArrayDesc));
+
+    hipExtent extent = make_hipExtent(imageWidth, imageHeight, 0);
+    hipChannelFormatDesc formatDesc;
+    formatDesc.x = 8;
+    formatDesc.y = 8;
+    formatDesc.z = 8;
+    formatDesc.w = 8;
+    formatDesc.f = hipChannelFormatKindUnsigned;
+
+    externalMemoryMipmappedArrayDesc.offset = 0;
+    externalMemoryMipmappedArrayDesc.formatDesc = formatDesc;
+    externalMemoryMipmappedArrayDesc.extent = extent;
+    externalMemoryMipmappedArrayDesc.flags = 0;
+    externalMemoryMipmappedArrayDesc.numLevels = mipLevels;
+
+    HIPCHECK(cudaExternalMemoryGetMappedMipmappedArray(
+        &cudaMipmappedImageArray, cudaExtMemImageBuffer,
+        &externalMemoryMipmappedArrayDesc));
+
+    HIPCHECK(hipMallocMipmappedArray(&cudaMipmappedImageArrayTemp,
+                                             &formatDesc, extent, mipLevels));
+    HIPCHECK(hipMallocMipmappedArray(&cudaMipmappedImageArrayOrig,
+                                             &formatDesc, extent, mipLevels));
+
+    for (int mipLevelIdx = 0; mipLevelIdx < mipLevels; mipLevelIdx++) {
+      hipArray_t cudaMipLevelArray, cudaMipLevelArrayTemp,
+          cudaMipLevelArrayOrig;
+      hipResourceDesc resourceDesc;
+
+      HIPCHECK(hipGetMipmappedArrayLevel(
+          &cudaMipLevelArray, cudaMipmappedImageArray, mipLevelIdx));
+      HIPCHECK(hipGetMipmappedArrayLevel(
+          &cudaMipLevelArrayTemp, cudaMipmappedImageArrayTemp, mipLevelIdx));
+      HIPCHECK(hipGetMipmappedArrayLevel(
+          &cudaMipLevelArrayOrig, cudaMipmappedImageArrayOrig, mipLevelIdx));
+
+      uint32_t width =
+          (imageWidth >> mipLevelIdx) ? (imageWidth >> mipLevelIdx) : 1;
+      uint32_t height =
+          (imageHeight >> mipLevelIdx) ? (imageHeight >> mipLevelIdx) : 1;
+      HIPCHECK(cudaMemcpy2DArrayToArray(
+          cudaMipLevelArrayOrig, 0, 0, cudaMipLevelArray, 0, 0,
+          width * sizeof(uchar4), height, hipMemcpyDeviceToDevice));
+
+      memset(&resourceDesc, 0, sizeof(resourceDesc));
+      resourceDesc.resType = hipResourceTypeArray;
+      resourceDesc.res.array.array = cudaMipLevelArray;
+
+      hipSurfaceObject_t surfaceObject;
+      HIPCHECK(hipCreateSurfaceObject(&surfaceObject, &resourceDesc));
+
+      surfaceObjectList.push_back(surfaceObject);
+
+      memset(&resourceDesc, 0, sizeof(resourceDesc));
+      resourceDesc.resType = hipResourceTypeArray;
+      resourceDesc.res.array.array = cudaMipLevelArrayTemp;
+
+      hipSurfaceObject_t surfaceObjectTemp;
+      HIPCHECK(
+          hipCreateSurfaceObject(&surfaceObjectTemp, &resourceDesc));
+      surfaceObjectListTemp.push_back(surfaceObjectTemp);
+    }
+
+    hipResourceDesc resDescr;
+    memset(&resDescr, 0, sizeof(hipResourceDesc));
+
+    resDescr.resType = hipResourceTypeMipmappedArray;
+    resDescr.res.mipmap.mipmap = cudaMipmappedImageArrayOrig;
+
+    hipTextureDesc texDescr;
+    memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+    texDescr.normalizedCoords = true;
+    texDescr.filterMode = hipFilterModeLinear;
+    texDescr.mipmapFilterMode = hipFilterModeLinear;
+
+    texDescr.addressMode[0] = hipAddressModeWrap;
+    texDescr.addressMode[1] = hipAddressModeWrap;
+
+    texDescr.maxMipmapLevelClamp = float(mipLevels - 1);
+
+    texDescr.readMode = hipReadModeNormalizedFloat;
+
+    HIPCHECK(hipCreateTextureObject(&textureObjMipMapInput, &resDescr,
+                                            &texDescr, NULL));
+
+    HIPCHECK(hipMalloc((void**)&d_surfaceObjectList,
+                               sizeof(hipSurfaceObject_t) * mipLevels));
+    HIPCHECK(hipMalloc((void**)&d_surfaceObjectListTemp,
+                               sizeof(hipSurfaceObject_t) * mipLevels));
+
+    HIPCHECK(hipMemcpy(d_surfaceObjectList, surfaceObjectList.data(),
+                               sizeof(hipSurfaceObject_t) * mipLevels,
+                               hipMemcpyHostToDevice));
+    HIPCHECK(hipMemcpy(
+        d_surfaceObjectListTemp, surfaceObjectListTemp.data(),
+        sizeof(hipSurfaceObject_t) * mipLevels, hipMemcpyHostToDevice));
+
+    printf("CUDA Kernel Vulkan image buffer\n");
+  }
+
+  void cudaUpdateVkImage() {
+    cudaVkSemaphoreWait(cudaExtVkUpdateCudaSemaphore);
+
+    int nthreads = 128;
+
+    /*Perform 2D box filter on image using CUDA */
+    d_boxfilter_rgba_x<<<imageHeight / nthreads, nthreads, 0, streamToRun>>>(
+        d_surfaceObjectListTemp, textureObjMipMapInput, imageWidth, imageHeight,
+        mipLevels, filter_radius);
+
+    d_boxfilter_rgba_y<<<imageWidth / nthreads, nthreads, 0, streamToRun>>>(
+        d_surfaceObjectList, d_surfaceObjectListTemp, imageWidth, imageHeight,
+        mipLevels, filter_radius);
+
+    varySigma();
+
+    cudaVkSemaphoreSignal(cudaExtCudaUpdateVkSemaphore);
+  }
+
+  void transitionImageLayout(VkImage image, VkFormat format,
+                             VkImageLayout oldLayout, VkImageLayout newLayout) {
+    VkCommandBuffer commandBuffer = beginSingleTimeCommands();
+
+    VkImageMemoryBarrier barrier = {};
+    barrier.sType = VK_STRUCTURE_TYPE_IMAGE_MEMORY_BARRIER;
+    barrier.oldLayout = oldLayout;
+    barrier.newLayout = newLayout;
+    barrier.srcQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
+    barrier.dstQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
+    barrier.image = image;
+    barrier.subresourceRange.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
+    barrier.subresourceRange.baseMipLevel = 0;
+    barrier.subresourceRange.levelCount = mipLevels;
+    barrier.subresourceRange.baseArrayLayer = 0;
+    barrier.subresourceRange.layerCount = 1;
+
+    VkPipelineStageFlags sourceStage;
+    VkPipelineStageFlags destinationStage;
+
+    if (oldLayout == VK_IMAGE_LAYOUT_UNDEFINED &&
+        newLayout == VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL) {
+      barrier.srcAccessMask = 0;
+      barrier.dstAccessMask = VK_ACCESS_TRANSFER_WRITE_BIT;
+
+      sourceStage = VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT;
+      destinationStage = VK_PIPELINE_STAGE_TRANSFER_BIT;
+    } else if (oldLayout == VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL &&
+               newLayout == VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL) {
+      barrier.srcAccessMask = VK_ACCESS_TRANSFER_WRITE_BIT;
+      barrier.dstAccessMask = VK_ACCESS_SHADER_READ_BIT;
+
+      sourceStage = VK_PIPELINE_STAGE_TRANSFER_BIT;
+      destinationStage = VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT;
+    } else {
+      throw std::invalid_argument("unsupported layout transition!");
+    }
+
+    vkCmdPipelineBarrier(commandBuffer, sourceStage, destinationStage, 0, 0,
+                         nullptr, 0, nullptr, 1, &barrier);
+
+    endSingleTimeCommands(commandBuffer);
+  }
+
+  void copyBufferToImage(VkBuffer buffer, VkImage image, uint32_t width,
+                         uint32_t height) {
+    VkCommandBuffer commandBuffer = beginSingleTimeCommands();
+
+    VkBufferImageCopy region = {};
+    region.bufferOffset = 0;
+    region.bufferRowLength = 0;
+    region.bufferImageHeight = 0;
+    region.imageSubresource.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
+    region.imageSubresource.mipLevel = 0;
+    region.imageSubresource.baseArrayLayer = 0;
+    region.imageSubresource.layerCount = 1;
+    region.imageOffset = {0, 0, 0};
+    region.imageExtent = {width, height, 1};
+
+    vkCmdCopyBufferToImage(commandBuffer, buffer, image,
+                           VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, 1, &region);
+
+    endSingleTimeCommands(commandBuffer);
+  }
+
+  void createVertexBuffer() {
+    VkDeviceSize bufferSize = sizeof(vertices[0]) * vertices.size();
+
+    VkBuffer stagingBuffer;
+    VkDeviceMemory stagingBufferMemory;
+    createBuffer(bufferSize, VK_BUFFER_USAGE_TRANSFER_SRC_BIT,
+                 VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT |
+                     VK_MEMORY_PROPERTY_HOST_COHERENT_BIT,
+                 stagingBuffer, stagingBufferMemory);
+
+    void* data;
+    vkMapMemory(device, stagingBufferMemory, 0, bufferSize, 0, &data);
+    memcpy(data, vertices.data(), (size_t)bufferSize);
+    vkUnmapMemory(device, stagingBufferMemory);
+
+    createBuffer(
+        bufferSize,
+        VK_BUFFER_USAGE_TRANSFER_DST_BIT | VK_BUFFER_USAGE_VERTEX_BUFFER_BIT,
+        VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT, vertexBuffer, vertexBufferMemory);
+
+    copyBuffer(stagingBuffer, vertexBuffer, bufferSize);
+
+    vkDestroyBuffer(device, stagingBuffer, nullptr);
+    vkFreeMemory(device, stagingBufferMemory, nullptr);
+  }
+
+  void createIndexBuffer() {
+    VkDeviceSize bufferSize = sizeof(indices[0]) * indices.size();
+
+    VkBuffer stagingBuffer;
+    VkDeviceMemory stagingBufferMemory;
+    createBuffer(bufferSize, VK_BUFFER_USAGE_TRANSFER_SRC_BIT,
+                 VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT |
+                     VK_MEMORY_PROPERTY_HOST_COHERENT_BIT,
+                 stagingBuffer, stagingBufferMemory);
+
+    void* data;
+    vkMapMemory(device, stagingBufferMemory, 0, bufferSize, 0, &data);
+    memcpy(data, indices.data(), (size_t)bufferSize);
+    vkUnmapMemory(device, stagingBufferMemory);
+
+    createBuffer(
+        bufferSize,
+        VK_BUFFER_USAGE_TRANSFER_DST_BIT | VK_BUFFER_USAGE_INDEX_BUFFER_BIT,
+        VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT, indexBuffer, indexBufferMemory);
+
+    copyBuffer(stagingBuffer, indexBuffer, bufferSize);
+
+    vkDestroyBuffer(device, stagingBuffer, nullptr);
+    vkFreeMemory(device, stagingBufferMemory, nullptr);
+  }
+
+  void createUniformBuffers() {
+    VkDeviceSize bufferSize = sizeof(UniformBufferObject);
+
+    uniformBuffers.resize(swapChainImages.size());
+    uniformBuffersMemory.resize(swapChainImages.size());
+
+    for (size_t i = 0; i < swapChainImages.size(); i++) {
+      createBuffer(bufferSize, VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT,
+                   VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT |
+                       VK_MEMORY_PROPERTY_HOST_COHERENT_BIT,
+                   uniformBuffers[i], uniformBuffersMemory[i]);
+    }
+  }
+
+  void createDescriptorPool() {
+    std::array<VkDescriptorPoolSize, 2> poolSizes = {};
+    poolSizes[0].type = VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER;
+    poolSizes[0].descriptorCount =
+        static_cast<uint32_t>(swapChainImages.size());
+    poolSizes[1].type = VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER;
+    poolSizes[1].descriptorCount =
+        static_cast<uint32_t>(swapChainImages.size());
+
+    VkDescriptorPoolCreateInfo poolInfo = {};
+    poolInfo.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_POOL_CREATE_INFO;
+    poolInfo.poolSizeCount = static_cast<uint32_t>(poolSizes.size());
+    poolInfo.pPoolSizes = poolSizes.data();
+    poolInfo.maxSets = static_cast<uint32_t>(swapChainImages.size());
+
+    if (vkCreateDescriptorPool(device, &poolInfo, nullptr, &descriptorPool) !=
+        VK_SUCCESS) {
+      throw std::runtime_error("failed to create descriptor pool!");
+    }
+  }
+
+  void createDescriptorSets() {
+    std::vector<VkDescriptorSetLayout> layouts(swapChainImages.size(),
+                                               descriptorSetLayout);
+    VkDescriptorSetAllocateInfo allocInfo = {};
+    allocInfo.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_ALLOCATE_INFO;
+    allocInfo.descriptorPool = descriptorPool;
+    allocInfo.descriptorSetCount =
+        static_cast<uint32_t>(swapChainImages.size());
+    allocInfo.pSetLayouts = layouts.data();
+
+    descriptorSets.resize(swapChainImages.size());
+    if (vkAllocateDescriptorSets(device, &allocInfo, descriptorSets.data()) !=
+        VK_SUCCESS) {
+      throw std::runtime_error("failed to allocate descriptor sets!");
+    }
+
+    for (size_t i = 0; i < swapChainImages.size(); i++) {
+      VkDescriptorBufferInfo bufferInfo = {};
+      bufferInfo.buffer = uniformBuffers[i];
+      bufferInfo.offset = 0;
+      bufferInfo.range = sizeof(UniformBufferObject);
+
+      VkDescriptorImageInfo imageInfo = {};
+      imageInfo.imageLayout = VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL;
+      imageInfo.imageView = textureImageView;
+      imageInfo.sampler = textureSampler;
+
+      std::array<VkWriteDescriptorSet, 2> descriptorWrites = {};
+
+      descriptorWrites[0].sType = VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET;
+      descriptorWrites[0].dstSet = descriptorSets[i];
+      descriptorWrites[0].dstBinding = 0;
+      descriptorWrites[0].dstArrayElement = 0;
+      descriptorWrites[0].descriptorType = VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER;
+      descriptorWrites[0].descriptorCount = 1;
+      descriptorWrites[0].pBufferInfo = &bufferInfo;
+
+      descriptorWrites[1].sType = VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET;
+      descriptorWrites[1].dstSet = descriptorSets[i];
+      descriptorWrites[1].dstBinding = 1;
+      descriptorWrites[1].dstArrayElement = 0;
+      descriptorWrites[1].descriptorType =
+          VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER;
+      descriptorWrites[1].descriptorCount = 1;
+      descriptorWrites[1].pImageInfo = &imageInfo;
+
+      vkUpdateDescriptorSets(device,
+                             static_cast<uint32_t>(descriptorWrites.size()),
+                             descriptorWrites.data(), 0, nullptr);
+    }
+  }
+
+  void createBuffer(VkDeviceSize size, VkBufferUsageFlags usage,
+                    VkMemoryPropertyFlags properties, VkBuffer& buffer,
+                    VkDeviceMemory& bufferMemory) {
+    VkBufferCreateInfo bufferInfo = {};
+    bufferInfo.sType = VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO;
+    bufferInfo.size = size;
+    bufferInfo.usage = usage;
+    bufferInfo.sharingMode = VK_SHARING_MODE_EXCLUSIVE;
+
+    if (vkCreateBuffer(device, &bufferInfo, nullptr, &buffer) != VK_SUCCESS) {
+      throw std::runtime_error("failed to create buffer!");
+    }
+
+    VkMemoryRequirements memRequirements;
+    vkGetBufferMemoryRequirements(device, buffer, &memRequirements);
+
+    VkMemoryAllocateInfo allocInfo = {};
+    allocInfo.sType = VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_INFO;
+    allocInfo.allocationSize = memRequirements.size;
+    allocInfo.memoryTypeIndex =
+        findMemoryType(memRequirements.memoryTypeBits, properties);
+
+    if (vkAllocateMemory(device, &allocInfo, nullptr, &bufferMemory) !=
+        VK_SUCCESS) {
+      throw std::runtime_error("failed to allocate buffer memory!");
+    }
+
+    vkBindBufferMemory(device, buffer, bufferMemory, 0);
+  }
+
+  VkCommandBuffer beginSingleTimeCommands() {
+    VkCommandBufferAllocateInfo allocInfo = {};
+    allocInfo.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO;
+    allocInfo.level = VK_COMMAND_BUFFER_LEVEL_PRIMARY;
+    allocInfo.commandPool = commandPool;
+    allocInfo.commandBufferCount = 1;
+
+    VkCommandBuffer commandBuffer;
+    vkAllocateCommandBuffers(device, &allocInfo, &commandBuffer);
+
+    VkCommandBufferBeginInfo beginInfo = {};
+    beginInfo.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO;
+    beginInfo.flags = VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT;
+
+    vkBeginCommandBuffer(commandBuffer, &beginInfo);
+
+    return commandBuffer;
+  }
+
+  void endSingleTimeCommands(VkCommandBuffer commandBuffer) {
+    vkEndCommandBuffer(commandBuffer);
+
+    VkSubmitInfo submitInfo = {};
+    submitInfo.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
+    submitInfo.commandBufferCount = 1;
+    submitInfo.pCommandBuffers = &commandBuffer;
+
+    vkQueueSubmit(graphicsQueue, 1, &submitInfo, VK_NULL_HANDLE);
+    vkQueueWaitIdle(graphicsQueue);
+
+    vkFreeCommandBuffers(device, commandPool, 1, &commandBuffer);
+  }
+
+  void copyBuffer(VkBuffer srcBuffer, VkBuffer dstBuffer, VkDeviceSize size) {
+    VkCommandBuffer commandBuffer = beginSingleTimeCommands();
+
+    VkBufferCopy copyRegion = {};
+    copyRegion.size = size;
+    vkCmdCopyBuffer(commandBuffer, srcBuffer, dstBuffer, 1, &copyRegion);
+
+    endSingleTimeCommands(commandBuffer);
+  }
+
+  uint32_t findMemoryType(uint32_t typeFilter,
+                          VkMemoryPropertyFlags properties) {
+    VkPhysicalDeviceMemoryProperties memProperties;
+    vkGetPhysicalDeviceMemoryProperties(physicalDevice, &memProperties);
+
+    for (uint32_t i = 0; i < memProperties.memoryTypeCount; i++) {
+      if ((typeFilter & (1 << i)) &&
+          (memProperties.memoryTypes[i].propertyFlags & properties) ==
+              properties) {
+        return i;
+      }
+    }
+
+    throw std::runtime_error("failed to find suitable memory type!");
+  }
+
+  void createCommandBuffers() {
+    commandBuffers.resize(swapChainFramebuffers.size());
+
+    VkCommandBufferAllocateInfo allocInfo = {};
+    allocInfo.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO;
+    allocInfo.commandPool = commandPool;
+    allocInfo.level = VK_COMMAND_BUFFER_LEVEL_PRIMARY;
+    allocInfo.commandBufferCount = (uint32_t)commandBuffers.size();
+
+    if (vkAllocateCommandBuffers(device, &allocInfo, commandBuffers.data()) !=
+        VK_SUCCESS) {
+      throw std::runtime_error("failed to allocate command buffers!");
+    }
+
+    for (size_t i = 0; i < commandBuffers.size(); i++) {
+      VkCommandBufferBeginInfo beginInfo = {};
+      beginInfo.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO;
+      beginInfo.flags = VK_COMMAND_BUFFER_USAGE_SIMULTANEOUS_USE_BIT;
+
+      if (vkBeginCommandBuffer(commandBuffers[i], &beginInfo) != VK_SUCCESS) {
+        throw std::runtime_error("failed to begin recording command buffer!");
+      }
+
+      VkRenderPassBeginInfo renderPassInfo = {};
+      renderPassInfo.sType = VK_STRUCTURE_TYPE_RENDER_PASS_BEGIN_INFO;
+      renderPassInfo.renderPass = renderPass;
+      renderPassInfo.framebuffer = swapChainFramebuffers[i];
+      renderPassInfo.renderArea.offset = {0, 0};
+      renderPassInfo.renderArea.extent = swapChainExtent;
+
+      VkClearValue clearColor = {0.0f, 0.0f, 0.0f, 1.0f};
+      renderPassInfo.clearValueCount = 1;
+      renderPassInfo.pClearValues = &clearColor;
+
+      vkCmdBeginRenderPass(commandBuffers[i], &renderPassInfo,
+                           VK_SUBPASS_CONTENTS_INLINE);
+
+      vkCmdBindPipeline(commandBuffers[i], VK_PIPELINE_BIND_POINT_GRAPHICS,
+                        graphicsPipeline);
+
+      VkBuffer vertexBuffers[] = {vertexBuffer};
+      VkDeviceSize offsets[] = {0};
+      vkCmdBindVertexBuffers(commandBuffers[i], 0, 1, vertexBuffers, offsets);
+
+      vkCmdBindIndexBuffer(commandBuffers[i], indexBuffer, 0,
+                           VK_INDEX_TYPE_UINT16);
+
+      vkCmdBindDescriptorSets(commandBuffers[i],
+                              VK_PIPELINE_BIND_POINT_GRAPHICS, pipelineLayout,
+                              0, 1, &descriptorSets[i], 0, nullptr);
+
+      vkCmdDrawIndexed(commandBuffers[i], static_cast<uint32_t>(indices.size()),
+                       1, 0, 0, 0);
+      // vkCmdDraw(commandBuffers[i], static_cast<uint32_t>(vertices.size()), 1,
+      // 0, 0);
+
+      vkCmdEndRenderPass(commandBuffers[i]);
+
+      if (vkEndCommandBuffer(commandBuffers[i]) != VK_SUCCESS) {
+        throw std::runtime_error("failed to record command buffer!");
+      }
+    }
+  }
+
+  void createSyncObjects() {
+    imageAvailableSemaphores.resize(MAX_FRAMES);
+    renderFinishedSemaphores.resize(MAX_FRAMES);
+    inFlightFences.resize(MAX_FRAMES);
+
+    VkSemaphoreCreateInfo semaphoreInfo = {};
+    semaphoreInfo.sType = VK_STRUCTURE_TYPE_SEMAPHORE_CREATE_INFO;
+
+    VkFenceCreateInfo fenceInfo = {};
+    fenceInfo.sType = VK_STRUCTURE_TYPE_FENCE_CREATE_INFO;
+    fenceInfo.flags = VK_FENCE_CREATE_SIGNALED_BIT;
+
+    for (size_t i = 0; i < MAX_FRAMES; i++) {
+      if (vkCreateSemaphore(device, &semaphoreInfo, nullptr,
+                            &imageAvailableSemaphores[i]) != VK_SUCCESS ||
+          vkCreateSemaphore(device, &semaphoreInfo, nullptr,
+                            &renderFinishedSemaphores[i]) != VK_SUCCESS ||
+          vkCreateFence(device, &fenceInfo, nullptr, &inFlightFences[i]) !=
+              VK_SUCCESS) {
+        throw std::runtime_error(
+            "failed to create synchronization objects for a frame!");
+      }
+    }
+  }
+
+  void createSyncObjectsExt() {
+    VkSemaphoreCreateInfo semaphoreInfo = {};
+    semaphoreInfo.sType = VK_STRUCTURE_TYPE_SEMAPHORE_CREATE_INFO;
+
+    memset(&semaphoreInfo, 0, sizeof(semaphoreInfo));
+    semaphoreInfo.sType = VK_STRUCTURE_TYPE_SEMAPHORE_CREATE_INFO;
+
+#ifdef _WIN64
+    WindowsSecurityAttributes winSecurityAttributes;
+
+    VkExportSemaphoreWin32HandleInfoKHR
+        vulkanExportSemaphoreWin32HandleInfoKHR = {};
+    vulkanExportSemaphoreWin32HandleInfoKHR.sType =
+        VK_STRUCTURE_TYPE_EXPORT_SEMAPHORE_WIN32_HANDLE_INFO_KHR;
+    vulkanExportSemaphoreWin32HandleInfoKHR.pNext = NULL;
+    vulkanExportSemaphoreWin32HandleInfoKHR.pAttributes =
+        &winSecurityAttributes;
+    vulkanExportSemaphoreWin32HandleInfoKHR.dwAccess =
+        DXGI_SHARED_RESOURCE_READ | DXGI_SHARED_RESOURCE_WRITE;
+    vulkanExportSemaphoreWin32HandleInfoKHR.name = (LPCWSTR)NULL;
+#endif
+    VkExportSemaphoreCreateInfoKHR vulkanExportSemaphoreCreateInfo = {};
+    vulkanExportSemaphoreCreateInfo.sType =
+        VK_STRUCTURE_TYPE_EXPORT_SEMAPHORE_CREATE_INFO_KHR;
+#ifdef _WIN64
+    vulkanExportSemaphoreCreateInfo.pNext =
+        IsWindows8OrGreater() ? &vulkanExportSemaphoreWin32HandleInfoKHR : NULL;
+    vulkanExportSemaphoreCreateInfo.handleTypes =
+        IsWindows8OrGreater()
+            ? VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32_BIT
+            : VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_WIN32_KMT_BIT;
+#else
+    vulkanExportSemaphoreCreateInfo.pNext = NULL;
+    vulkanExportSemaphoreCreateInfo.handleTypes =
+        VK_EXTERNAL_SEMAPHORE_HANDLE_TYPE_OPAQUE_FD_BIT;
+#endif
+    semaphoreInfo.pNext = &vulkanExportSemaphoreCreateInfo;
+
+    if (vkCreateSemaphore(device, &semaphoreInfo, nullptr,
+                          &cudaUpdateVkSemaphore) != VK_SUCCESS ||
+        vkCreateSemaphore(device, &semaphoreInfo, nullptr,
+                          &vkUpdateCudaSemaphore) != VK_SUCCESS) {
+      throw std::runtime_error(
+          "failed to create synchronization objects for a CUDA-Vulkan!");
+    }
+  }
+
+  void updateUniformBuffer() {
+    UniformBufferObject ubo = {};
+
+    mat4x4_identity(ubo.model);
+    mat4x4 Model;
+    mat4x4_dup(Model, ubo.model);
+    mat4x4_rotate(ubo.model, Model, 0.0f, 0.0f, 1.0f, degreesToRadians(135.0f));
+
+    vec3 eye = {2.0f, 2.0f, 2.0f};
+    vec3 center = {0.0f, 0.0f, 0.0f};
+    vec3 up = {0.0f, 0.0f, 1.0f};
+    mat4x4_look_at(ubo.view, eye, center, up);
+
+    mat4x4_perspective(ubo.proj, degreesToRadians(45.0f),
+                       swapChainExtent.width / (float)swapChainExtent.height,
+                       0.1f, 10.0f);
+    ubo.proj[1][1] *= -1;
+
+    for (size_t i = 0; i < swapChainImages.size(); i++) {
+      void* data;
+      vkMapMemory(device, uniformBuffersMemory[i], 0, sizeof(ubo), 0, &data);
+      memcpy(data, &ubo, sizeof(ubo));
+      vkUnmapMemory(device, uniformBuffersMemory[i]);
+    }
+  }
+
+  void drawFrame() {
+    static int startSubmit = 0;
+
+    vkWaitForFences(device, 1, &inFlightFences[currentFrame], VK_TRUE,
+                    std::numeric_limits<uint64_t>::max());
+
+    uint32_t imageIndex;
+    VkResult result = vkAcquireNextImageKHR(
+        device, swapChain, std::numeric_limits<uint64_t>::max(),
+        imageAvailableSemaphores[currentFrame], VK_NULL_HANDLE, &imageIndex);
+
+    if (result == VK_ERROR_OUT_OF_DATE_KHR) {
+      recreateSwapChain();
+      return;
+    } else if (result != VK_SUCCESS && result != VK_SUBOPTIMAL_KHR) {
+      throw std::runtime_error("failed to acquire swap chain image!");
+    }
+
+    vkResetFences(device, 1, &inFlightFences[currentFrame]);
+
+    if (!startSubmit) {
+      submitVulkan(imageIndex);
+      startSubmit = 1;
+    } else {
+      submitVulkanCuda(imageIndex);
+    }
+
+    VkPresentInfoKHR presentInfo = {};
+    presentInfo.sType = VK_STRUCTURE_TYPE_PRESENT_INFO_KHR;
+
+    VkSemaphore signalSemaphores[] = {renderFinishedSemaphores[currentFrame]};
+
+    presentInfo.waitSemaphoreCount = 1;
+    presentInfo.pWaitSemaphores = signalSemaphores;
+
+    VkSwapchainKHR swapChains[] = {swapChain};
+    presentInfo.swapchainCount = 1;
+    presentInfo.pSwapchains = swapChains;
+    presentInfo.pImageIndices = &imageIndex;
+    presentInfo.pResults = nullptr;  // Optional
+
+    result = vkQueuePresentKHR(presentQueue, &presentInfo);
+
+    if (result == VK_ERROR_OUT_OF_DATE_KHR || result == VK_SUBOPTIMAL_KHR ||
+        framebufferResized) {
+      framebufferResized = false;
+      recreateSwapChain();
+    } else if (result != VK_SUCCESS) {
+      throw std::runtime_error("failed to present swap chain image!");
+    }
+
+    cudaUpdateVkImage();
+
+    currentFrame = (currentFrame + 1) % MAX_FRAMES;
+    // Added sleep of 10 millisecs so that CPU does not submit too much work to
+    // GPU
+    std::this_thread::sleep_for(std::chrono::microseconds(10000));
+    char title[256];
+    sprintf(title, "Vulkan Image CUDA Box Filter (radius=%d)", filter_radius);
+    glfwSetWindowTitle(window, title);
+  }
+
+  void cudaVkSemaphoreSignal(hipExternalSemaphore_t& extSemaphore) {
+    hipExternalSemaphoreSignalParams extSemaphoreSignalParams;
+    memset(&extSemaphoreSignalParams, 0, sizeof(extSemaphoreSignalParams));
+
+    extSemaphoreSignalParams.params.fence.value = 0;
+    extSemaphoreSignalParams.flags = 0;
+    HIPCHECK(hipSignalExternalSemaphoresAsync(
+        &extSemaphore, &extSemaphoreSignalParams, 1, streamToRun));
+  }
+
+  void cudaVkSemaphoreWait(hipExternalSemaphore_t& extSemaphore) {
+    hipExternalSemaphoreWaitParams extSemaphoreWaitParams;
+
+    memset(&extSemaphoreWaitParams, 0, sizeof(extSemaphoreWaitParams));
+
+    extSemaphoreWaitParams.params.fence.value = 0;
+    extSemaphoreWaitParams.flags = 0;
+
+    HIPCHECK(hipWaitExternalSemaphoresAsync(
+        &extSemaphore, &extSemaphoreWaitParams, 1, streamToRun));
+  }
+
+  void submitVulkan(uint32_t imageIndex) {
+    VkSubmitInfo submitInfo = {};
+    submitInfo.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
+
+    VkSemaphore waitSemaphores[] = {imageAvailableSemaphores[currentFrame]};
+    VkPipelineStageFlags waitStages[] = {
+        VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT};
+    submitInfo.waitSemaphoreCount = 1;
+    submitInfo.pWaitSemaphores = waitSemaphores;
+    submitInfo.pWaitDstStageMask = waitStages;
+    submitInfo.commandBufferCount = 1;
+    submitInfo.pCommandBuffers = &commandBuffers[imageIndex];
+
+    VkSemaphore signalSemaphores[] = {renderFinishedSemaphores[currentFrame],
+                                      vkUpdateCudaSemaphore};
+
+    submitInfo.signalSemaphoreCount = 2;
+    submitInfo.pSignalSemaphores = signalSemaphores;
+
+    if (vkQueueSubmit(graphicsQueue, 1, &submitInfo,
+                      inFlightFences[currentFrame]) != VK_SUCCESS) {
+      throw std::runtime_error("failed to submit draw command buffer!");
+    }
+  }
+
+  void submitVulkanCuda(uint32_t imageIndex) {
+    VkSubmitInfo submitInfo = {};
+    submitInfo.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
+
+    VkSemaphore waitSemaphores[] = {imageAvailableSemaphores[currentFrame],
+                                    cudaUpdateVkSemaphore};
+    VkPipelineStageFlags waitStages[] = {
+        VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT,
+        VK_PIPELINE_STAGE_ALL_COMMANDS_BIT};
+    submitInfo.waitSemaphoreCount = 2;
+    submitInfo.pWaitSemaphores = waitSemaphores;
+    submitInfo.pWaitDstStageMask = waitStages;
+    submitInfo.commandBufferCount = 1;
+    submitInfo.pCommandBuffers = &commandBuffers[imageIndex];
+
+    VkSemaphore signalSemaphores[] = {renderFinishedSemaphores[currentFrame],
+                                      vkUpdateCudaSemaphore};
+
+    submitInfo.signalSemaphoreCount = 2;
+    submitInfo.pSignalSemaphores = signalSemaphores;
+
+    if (vkQueueSubmit(graphicsQueue, 1, &submitInfo,
+                      inFlightFences[currentFrame]) != VK_SUCCESS) {
+      throw std::runtime_error("failed to submit draw command buffer!");
+    }
+  }
+
+  VkShaderModule createShaderModule(const std::vector<char>& code) {
+    VkShaderModuleCreateInfo createInfo = {};
+    createInfo.sType = VK_STRUCTURE_TYPE_SHADER_MODULE_CREATE_INFO;
+    createInfo.codeSize = code.size();
+    createInfo.pCode = reinterpret_cast<const uint32_t*>(code.data());
+
+    VkShaderModule shaderModule;
+    if (vkCreateShaderModule(device, &createInfo, nullptr, &shaderModule) !=
+        VK_SUCCESS) {
+      throw std::runtime_error("failed to create shader module!");
+    }
+
+    return shaderModule;
+  }
+
+  VkSurfaceFormatKHR chooseSwapSurfaceFormat(
+      const std::vector<VkSurfaceFormatKHR>& availableFormats) {
+    if (availableFormats.size() == 1 &&
+        availableFormats[0].format == VK_FORMAT_UNDEFINED) {
+      return {VK_FORMAT_B8G8R8A8_UNORM, VK_COLOR_SPACE_SRGB_NONLINEAR_KHR};
+    }
+
+    for (const auto& availableFormat : availableFormats) {
+      if (availableFormat.format == VK_FORMAT_B8G8R8A8_UNORM &&
+          availableFormat.colorSpace == VK_COLOR_SPACE_SRGB_NONLINEAR_KHR) {
+        return availableFormat;
+      }
+    }
+
+    return availableFormats[0];
+  }
+
+  VkPresentModeKHR chooseSwapPresentMode(
+      const std::vector<VkPresentModeKHR>& availablePresentModes) {
+    VkPresentModeKHR bestMode = VK_PRESENT_MODE_FIFO_KHR;
+
+    for (const auto& availablePresentMode : availablePresentModes) {
+      if (availablePresentMode == VK_PRESENT_MODE_MAILBOX_KHR) {
+        return availablePresentMode;
+      } else if (availablePresentMode == VK_PRESENT_MODE_IMMEDIATE_KHR) {
+        bestMode = availablePresentMode;
+      }
+    }
+
+    return bestMode;
+  }
+
+  VkExtent2D chooseSwapExtent(const VkSurfaceCapabilitiesKHR& capabilities) {
+    if (capabilities.currentExtent.width !=
+        std::numeric_limits<uint32_t>::max()) {
+      return capabilities.currentExtent;
+    } else {
+      int width, height;
+      glfwGetFramebufferSize(window, &width, &height);
+
+      VkExtent2D actualExtent = {static_cast<uint32_t>(width),
+                                 static_cast<uint32_t>(height)};
+
+      actualExtent.width = std::max(
+          capabilities.minImageExtent.width,
+          std::min(capabilities.maxImageExtent.width, actualExtent.width));
+      actualExtent.height = std::max(
+          capabilities.minImageExtent.height,
+          std::min(capabilities.maxImageExtent.height, actualExtent.height));
+
+      return actualExtent;
+    }
+  }
+
+  SwapChainSupportDetails querySwapChainSupport(VkPhysicalDevice device) {
+    SwapChainSupportDetails details;
+
+    vkGetPhysicalDeviceSurfaceCapabilitiesKHR(device, surface,
+                                              &details.capabilities);
+
+    uint32_t formatCount;
+    vkGetPhysicalDeviceSurfaceFormatsKHR(device, surface, &formatCount,
+                                         nullptr);
+
+    if (formatCount != 0) {
+      details.formats.resize(formatCount);
+      vkGetPhysicalDeviceSurfaceFormatsKHR(device, surface, &formatCount,
+                                           details.formats.data());
+    }
+
+    uint32_t presentModeCount;
+    vkGetPhysicalDeviceSurfacePresentModesKHR(device, surface,
+                                              &presentModeCount, nullptr);
+
+    if (presentModeCount != 0) {
+      details.presentModes.resize(presentModeCount);
+      vkGetPhysicalDeviceSurfacePresentModesKHR(
+          device, surface, &presentModeCount, details.presentModes.data());
+    }
+
+    return details;
+  }
+
+  bool isDeviceSuitable(VkPhysicalDevice device) {
+    QueueFamilyIndices indices = findQueueFamilies(device);
+
+    bool extensionsSupported = checkDeviceExtensionSupport(device);
+
+    bool swapChainAdequate = false;
+    if (extensionsSupported) {
+      SwapChainSupportDetails swapChainSupport = querySwapChainSupport(device);
+      swapChainAdequate = !swapChainSupport.formats.empty() &&
+                          !swapChainSupport.presentModes.empty();
+    }
+
+    VkPhysicalDeviceFeatures supportedFeatures;
+    vkGetPhysicalDeviceFeatures(device, &supportedFeatures);
+
+    return indices.isComplete() && extensionsSupported && swapChainAdequate &&
+           supportedFeatures.samplerAnisotropy;
+  }
+
+  bool checkDeviceExtensionSupport(VkPhysicalDevice device) {
+    uint32_t extensionCount;
+    vkEnumerateDeviceExtensionProperties(device, nullptr, &extensionCount,
+                                         nullptr);
+
+    std::vector<VkExtensionProperties> availableExtensions(extensionCount);
+    vkEnumerateDeviceExtensionProperties(device, nullptr, &extensionCount,
+                                         availableExtensions.data());
+
+    std::set<std::string> requiredExtensions(deviceExtensions.begin(),
+                                             deviceExtensions.end());
+
+    for (const auto& extension : availableExtensions) {
+      requiredExtensions.erase(extension.extensionName);
+    }
+
+    return requiredExtensions.empty();
+  }
+
+  QueueFamilyIndices findQueueFamilies(VkPhysicalDevice device) {
+    QueueFamilyIndices indices;
+
+    uint32_t queueFamilyCount = 0;
+    vkGetPhysicalDeviceQueueFamilyProperties(device, &queueFamilyCount,
+                                             nullptr);
+
+    std::vector<VkQueueFamilyProperties> queueFamilies(queueFamilyCount);
+    vkGetPhysicalDeviceQueueFamilyProperties(device, &queueFamilyCount,
+                                             queueFamilies.data());
+
+    int i = 0;
+    for (const auto& queueFamily : queueFamilies) {
+      if (queueFamily.queueCount > 0 &&
+          queueFamily.queueFlags & VK_QUEUE_GRAPHICS_BIT) {
+        indices.graphicsFamily = i;
+      }
+
+      VkBool32 presentSupport = false;
+      vkGetPhysicalDeviceSurfaceSupportKHR(device, i, surface, &presentSupport);
+
+      if (queueFamily.queueCount > 0 && presentSupport) {
+        indices.presentFamily = i;
+      }
+
+      if (indices.isComplete()) {
+        break;
+      }
+
+      i++;
+    }
+
+    return indices;
+  }
+
+  std::vector<const char*> getRequiredExtensions() {
+    uint32_t glfwExtensionCount = 0;
+    const char** glfwExtensions;
+    glfwExtensions = glfwGetRequiredInstanceExtensions(&glfwExtensionCount);
+
+    std::vector<const char*> extensions(glfwExtensions,
+                                        glfwExtensions + glfwExtensionCount);
+
+    if (enableValidationLayers) {
+      extensions.push_back(VK_EXT_DEBUG_UTILS_EXTENSION_NAME);
+    }
+
+    return extensions;
+  }
+
+  bool checkValidationLayerSupport() {
+    uint32_t layerCount;
+    vkEnumerateInstanceLayerProperties(&layerCount, nullptr);
+
+    std::vector<VkLayerProperties> availableLayers(layerCount);
+    vkEnumerateInstanceLayerProperties(&layerCount, availableLayers.data());
+
+    for (const char* layerName : validationLayers) {
+      bool layerFound = false;
+
+      for (const auto& layerProperties : availableLayers) {
+        if (strcmp(layerName, layerProperties.layerName) == 0) {
+          layerFound = true;
+          break;
+        }
+      }
+
+      if (!layerFound) {
+        return false;
+      }
+    }
+
+    return true;
+  }
+
+  static std::vector<char> readFile(const std::string& filename) {
+    char* file_path = sdkFindFilePath(filename.c_str(), execution_path.c_str());
+    std::ifstream file(file_path, std::ios::ate | std::ios::binary);
+
+    if (!file.is_open()) {
+      throw std::runtime_error("failed to open file!");
+    }
+
+    size_t fileSize = (size_t)file.tellg();
+    std::vector<char> buffer(fileSize);
+
+    file.seekg(0);
+    file.read(buffer.data(), fileSize);
+
+    file.close();
+
+    return buffer;
+  }
+
+  static VKAPI_ATTR VkBool32 VKAPI_CALL
+  debugCallback(VkDebugUtilsMessageSeverityFlagBitsEXT messageSeverity,
+                VkDebugUtilsMessageTypeFlagsEXT messageType,
+                const VkDebugUtilsMessengerCallbackDataEXT* pCallbackData,
+                void* pUserData) {
+    std::cerr << "validation layer: " << pCallbackData->pMessage << std::endl;
+
+    return VK_FALSE;
+  }
+};
+
+int main(int argc, char** argv) {
+  execution_path = argv[0];
+  std::string image_filename = "teapot1024.ppm";
+
+  if (checkCmdLineFlag(argc, (const char**)argv, "file")) {
+    getCmdLineArgumentString(argc, (const char**)argv, "file",
+                             (char**)&image_filename);
+  }
+
+  vulkanImageCUDA app;
+
+  try {
+    // This app only works on ppm images
+    app.loadImageData(image_filename);
+    app.run();
+  } catch (const std::exception& e) {
+    std::cerr << e.what() << std::endl;
+    return EXIT_FAILURE;
+  }
+
+  return EXIT_SUCCESS;
+}
+
+
+  try {
+    // This app only works on ppm images
+    app.loadImageData(image_filename);
+    app.run();
+  } catch (const std::exception& e) {
+    std::cerr << e.what() << std::endl;
+    return EXIT_FAILURE;
+  }
+
+  return EXIT_SUCCESS;
+}
