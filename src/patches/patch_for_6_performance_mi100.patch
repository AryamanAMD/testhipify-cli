diff --git a/__pycache__/patch_gen.cpython-38.pyc b/__pycache__/patch_gen.cpython-38.pyc
index 5f42675..09f4bfb 100644
Binary files a/__pycache__/patch_gen.cpython-38.pyc and b/__pycache__/patch_gen.cpython-38.pyc differ
diff --git a/__pycache__/patch_gen2.cpython-38.pyc b/__pycache__/patch_gen2.cpython-38.pyc
index 3f37ba1..5c82b28 100644
Binary files a/__pycache__/patch_gen2.cpython-38.pyc and b/__pycache__/patch_gen2.cpython-38.pyc differ
diff --git a/__pycache__/patch_gen3.cpython-38.pyc b/__pycache__/patch_gen3.cpython-38.pyc
index 1eb8a53..7513c2a 100644
Binary files a/__pycache__/patch_gen3.cpython-38.pyc and b/__pycache__/patch_gen3.cpython-38.pyc differ
diff --git a/src/samples/Samples/0_Introduction/matrixMul_nvrtc/matrixMul_kernel.cu.hip b/src/samples/Samples/0_Introduction/matrixMul_nvrtc/matrixMul_kernel.cu.hip
index b365b74..e69de29 100755
--- a/src/samples/Samples/0_Introduction/matrixMul_nvrtc/matrixMul_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/matrixMul_nvrtc/matrixMul_kernel.cu.hip
@@ -1,132 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/**
- * Matrix multiplication: C = A * B.
- * Host code.
- *
- * This sample implements matrix multiplication as described in Chapter 3
- * of the programming guide.
- * It has been written for clarity of exposition to illustrate various CUDA
- * programming principles, not with the goal of providing the most
- * performant generic kernel for matrix multiplication.
- *
- * See also:
- * V. Volkov and J. Demmel, "Benchmarking GPUs to tune dense linear algebra,"
- * in Proc. 2008 ACM/IEEE Conf. on Supercomputing (SC '08),
- * Piscataway, NJ: IEEE Press, 2008, pp. Art. 31:1-11.
- */
-
-/**
- * Matrix multiplication (CUDA Kernel) on the device: C = A * B
- * wA is A's width and wB is B's width
- */
-
-#include <hip/hip_cooperative_groups.h>
-
-template <int BLOCK_SIZE>
-__device__ void matrixMulCUDA(float *C, float *A, float *B, int wA, int wB) {
-  // Handle to thread block group
-  cooperative_groups::thread_block cta =
-      cooperative_groups::this_thread_block();
-  // Block index
-  int bx = blockIdx.x;
-  int by = blockIdx.y;
-
-  // Thread index
-  int tx = threadIdx.x;
-  int ty = threadIdx.y;
-
-  // Index of the first sub-matrix of A processed by the block
-  int aBegin = wA * BLOCK_SIZE * by;
-
-  // Index of the last sub-matrix of A processed by the block
-  int aEnd = aBegin + wA - 1;
-
-  // Step size used to iterate through the sub-matrices of A
-  int aStep = BLOCK_SIZE;
-
-  // Index of the first sub-matrix of B processed by the block
-  int bBegin = BLOCK_SIZE * bx;
-
-  // Step size used to iterate through the sub-matrices of B
-  int bStep = BLOCK_SIZE * wB;
-
-  // Csub is used to store the element of the block sub-matrix
-  // that is computed by the thread
-  float Csub = 0;
-
-  // Loop over all the sub-matrices of A and B
-  // required to compute the block sub-matrix
-  for (int a = aBegin, b = bBegin; a <= aEnd; a += aStep, b += bStep) {
-    // Declaration of the shared memory array As used to
-    // store the sub-matrix of A
-    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
-
-    // Declaration of the shared memory array Bs used to
-    // store the sub-matrix of B
-    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];
-
-    // Load the matrices from device memory
-    // to shared memory; each thread loads
-    // one element of each matrix
-    As[ty][tx] = A[a + wA * ty + tx];
-    Bs[ty][tx] = B[b + wB * ty + tx];
-
-    // Synchronize to make sure the matrices are loaded
-    cooperative_groups::sync(cta);
-
-// Multiply the two matrices together;
-// each thread computes one element
-// of the block sub-matrix
-#pragma unroll
-    for (int k = 0; k < BLOCK_SIZE; ++k) {
-      Csub += As[ty][k] * Bs[k][tx];
-    }
-
-    // Synchronize to make sure that the preceding
-    // computation is done before loading two new
-    // sub-matrices of A and B in the next iteration
-    cooperative_groups::sync(cta);
-  }
-
-  // Write the block sub-matrix to device memory;
-  // each thread writes one element
-  int c = wB * BLOCK_SIZE * by + BLOCK_SIZE * bx;
-  C[c + wB * ty + tx] = Csub;
-}
-
-extern "C" __global__ void matrixMulCUDA_block16(float *C, float *A, float *B,
-                                                 int wA, int wB) {
-  matrixMulCUDA<16>(C, A, B, wA, wB);
-}
-
-extern "C" __global__ void matrixMulCUDA_block32(float *C, float *A, float *B,
-                                                 int wA, int wB) {
-  matrixMulCUDA<32>(C, A, B, wA, wB);
-}
diff --git a/src/samples/Samples/0_Introduction/simpleAWBarrier/simpleAWBarrier.cu.hip b/src/samples/Samples/0_Introduction/simpleAWBarrier/simpleAWBarrier.cu.hip
index 37e8e1b..bc7610f 100755
--- a/src/samples/Samples/0_Introduction/simpleAWBarrier/simpleAWBarrier.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleAWBarrier/simpleAWBarrier.cu.hip
@@ -35,10 +35,10 @@
 #include <hip/hip_cooperative_groups.h>
 
 // Utilities and timing functions
-#include "helper_functions.h"  // includes hip/hip_runtime.h and hip/hip_runtime_api.h
+#include <helper_functions.h>  // includes hip/hip_runtime.h and hip/hip_runtime_api.h
 
 // CUDA helper functions
-#include "helper_cuda_hipified.h"  // helper functions for CUDA error check
+#include <helper_cuda.h>  // helper functions for CUDA error check
 
 namespace cg = cooperative_groups;
 
@@ -145,7 +145,7 @@ int main(int argc, char **argv) {
   int dev = findCudaDevice(argc, (const char **)argv);
 
   int major = 0;
-  HIPCHECK(
+  checkCudaErrors(
       hipDeviceGetAttribute(&major, hipDeviceAttributeComputeCapabilityMajor, dev));
 
   // Arrive-Wait Barrier require a GPU of Volta (SM7X) architecture or higher.
@@ -155,7 +155,7 @@ int main(int argc, char **argv) {
   }
 
   int supportsCooperativeLaunch = 0;
-  HIPCHECK(hipDeviceGetAttribute(&supportsCooperativeLaunch,
+  checkCudaErrors(hipDeviceGetAttribute(&supportsCooperativeLaunch,
                                          hipDeviceAttributeCooperativeLaunch, dev));
 
   if (!supportsCooperativeLaunch) {
@@ -178,11 +178,11 @@ int runNormVecByDotProductAWBarrier(int argc, char **argv, int deviceId) {
   double *d_partialResults;
   int size = 10000000;
 
-  HIPCHECK(hipHostMalloc(&vecA, sizeof(float) * size));
-  HIPCHECK(hipHostMalloc(&vecB, sizeof(float) * size));
+  checkCudaErrors(hipHostMalloc(&vecA, sizeof(float) * size));
+  checkCudaErrors(hipHostMalloc(&vecB, sizeof(float) * size));
 
-  HIPCHECK(hipMalloc(&d_vecA, sizeof(float) * size));
-  HIPCHECK(hipMalloc(&d_vecB, sizeof(float) * size));
+  checkCudaErrors(hipMalloc(&d_vecA, sizeof(float) * size));
+  checkCudaErrors(hipMalloc(&d_vecB, sizeof(float) * size));
 
   float baseVal = 2.0;
   for (int i = 0; i < size; i++) {
@@ -190,31 +190,31 @@ int runNormVecByDotProductAWBarrier(int argc, char **argv, int deviceId) {
   }
 
   hipStream_t stream;
-  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
+  checkCudaErrors(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
 
-  HIPCHECK(hipMemcpyAsync(d_vecA, vecA, sizeof(float) * size,
+  checkCudaErrors(hipMemcpyAsync(d_vecA, vecA, sizeof(float) * size,
                                   hipMemcpyHostToDevice, stream));
-  HIPCHECK(hipMemcpyAsync(d_vecB, vecB, sizeof(float) * size,
+  checkCudaErrors(hipMemcpyAsync(d_vecB, vecB, sizeof(float) * size,
                                   hipMemcpyHostToDevice, stream));
 
   // Kernel configuration, where a one-dimensional
   // grid and one-dimensional blocks are configured.
   int minGridSize = 0, blockSize = 0;
-  HIPCHECK(hipOccupancyMaxPotentialBlockSize(
+  checkCudaErrors(hipOccupancyMaxPotentialBlockSize(
       &minGridSize, &blockSize, (void *)normVecByDotProductAWBarrier, 0, size));
 
   int smemSize = ((blockSize / 32) + 1) * sizeof(double);
 
   int numBlocksPerSm = 0;
-  HIPCHECK(hipOccupancyMaxActiveBlocksPerMultiprocessor(
+  checkCudaErrors(hipOccupancyMaxActiveBlocksPerMultiprocessor(
       &numBlocksPerSm, normVecByDotProductAWBarrier, blockSize, smemSize));
 
   int multiProcessorCount = 0;
-  HIPCHECK(hipDeviceGetAttribute(
+  checkCudaErrors(hipDeviceGetAttribute(
       &multiProcessorCount, hipDeviceAttributeMultiprocessorCount, deviceId));
 
   minGridSize = multiProcessorCount * numBlocksPerSm;
-  HIPCHECK(hipMalloc(&d_partialResults, minGridSize * sizeof(double)));
+  checkCudaErrors(hipMalloc(&d_partialResults, minGridSize * sizeof(double)));
 
   printf(
       "Launching normVecByDotProductAWBarrier kernel with numBlocks = %d "
@@ -226,13 +226,13 @@ int runNormVecByDotProductAWBarrier(int argc, char **argv, int deviceId) {
   void *kernelArgs[] = {(void *)&d_vecA, (void *)&d_vecB,
                         (void *)&d_partialResults, (void *)&size};
 
-  HIPCHECK(
+  checkCudaErrors(
       hipLaunchCooperativeKernel((void *)normVecByDotProductAWBarrier, dimGrid,
                                   dimBlock, kernelArgs, smemSize, stream));
 
-  HIPCHECK(hipMemcpyAsync(vecA, d_vecA, sizeof(float) * size,
+  checkCudaErrors(hipMemcpyAsync(vecA, d_vecA, sizeof(float) * size,
                                   hipMemcpyDeviceToHost, stream));
-  HIPCHECK(hipStreamSynchronize(stream));
+  checkCudaErrors(hipStreamSynchronize(stream));
 
   float expectedResult = (baseVal / sqrt(size * baseVal * baseVal));
   unsigned int matches = 0;
@@ -246,12 +246,12 @@ int runNormVecByDotProductAWBarrier(int argc, char **argv, int deviceId) {
   }
 
   printf("Result = %s\n", matches == size ? "PASSED" : "FAILED");
-  HIPCHECK(hipFree(d_vecA));
-  HIPCHECK(hipFree(d_vecB));
-  HIPCHECK(hipFree(d_partialResults));
+  checkCudaErrors(hipFree(d_vecA));
+  checkCudaErrors(hipFree(d_vecB));
+  checkCudaErrors(hipFree(d_partialResults));
 
-  HIPCHECK(hipHostFree(vecA));
-  HIPCHECK(hipHostFree(vecB));
+  checkCudaErrors(hipHostFree(vecA));
+  checkCudaErrors(hipHostFree(vecB));
   return matches == size;
 }
 eckCudaErrors(hipFree(d_partialResults));
diff --git a/src/samples/Samples/0_Introduction/simpleCUDA2GL/simpleCUDA2GL.cu.hip b/src/samples/Samples/0_Introduction/simpleCUDA2GL/simpleCUDA2GL.cu.hip
index 7f0c62c..e69de29 100755
--- a/src/samples/Samples/0_Introduction/simpleCUDA2GL/simpleCUDA2GL.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleCUDA2GL/simpleCUDA2GL.cu.hip
@@ -1,63 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-// Utilities and system includes
-
-#include "helper_cuda_hipified.h"
-
-// clamp x to range [a, b]
-__device__ float clamp(float x, float a, float b) { return max(a, min(b, x)); }
-
-__device__ int clamp(int x, int a, int b) { return max(a, min(b, x)); }
-
-// convert floating point rgb color to 8-bit integer
-__device__ int rgbToInt(float r, float g, float b) {
-  r = clamp(r, 0.0f, 255.0f);
-  g = clamp(g, 0.0f, 255.0f);
-  b = clamp(b, 0.0f, 255.0f);
-  return (int(b) << 16) | (int(g) << 8) | int(r);
-}
-
-__global__ void cudaProcess(unsigned int *g_odata, int imgw) {
-  extern __shared__ uchar4 sdata[];
-
-  int tx = threadIdx.x;
-  int ty = threadIdx.y;
-  int bw = blockDim.x;
-  int bh = blockDim.y;
-  int x = blockIdx.x * bw + tx;
-  int y = blockIdx.y * bh + ty;
-
-  uchar4 c4 = make_uchar4((x & 0x20) ? 100 : 0, 0, (y & 0x20) ? 100 : 0, 0);
-  g_odata[y * imgw + x] = rgbToInt(c4.z, c4.y, c4.x);
-}
-
-extern "C" void launch_cudaProcess(dim3 grid, dim3 block, int sbytes,
-                                   unsigned int *g_odata, int imgw) {
-  cudaProcess<<<grid, block, sbytes>>>(g_odata, imgw);
-}
diff --git a/src/samples/Samples/0_Introduction/simpleDrvRuntime/vectorAdd_kernel.cu.hip b/src/samples/Samples/0_Introduction/simpleDrvRuntime/vectorAdd_kernel.cu.hip
index 72e20f6..e69de29 100755
--- a/src/samples/Samples/0_Introduction/simpleDrvRuntime/vectorAdd_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleDrvRuntime/vectorAdd_kernel.cu.hip
@@ -1,43 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/* Vector addition: C = A + B.
- *
- * This sample is a very basic sample that implements element by element
- * vector addition. It is the same as the sample illustrating Chapter 3
- * of the programming guide with some additions like error checking.
- *
- */
-
-// Device code
-extern "C" __global__ void VecAdd_kernel(const float *A, const float *B,
-                                         float *C, int N) {
-  int i = blockDim.x * blockIdx.x + threadIdx.x;
-
-  if (i < N) C[i] = A[i] + B[i];
-}
diff --git a/src/samples/Samples/0_Introduction/simpleMultiGPU/simpleMultiGPU.cu.hip b/src/samples/Samples/0_Introduction/simpleMultiGPU/simpleMultiGPU.cu.hip
index cc5376e..e69de29 100755
--- a/src/samples/Samples/0_Introduction/simpleMultiGPU/simpleMultiGPU.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleMultiGPU/simpleMultiGPU.cu.hip
@@ -1,239 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * This application demonstrates how to use the CUDA API to use multiple GPUs,
- * with an emphasis on simple illustration of the techniques (not on
- * performance).
- *
- * Note that in order to detect multiple GPUs in your system you have to disable
- * SLI in the nvidia control panel. Otherwise only one GPU is visible to the
- * application. On the other side, you can still extend your desktop to screens
- * attached to both GPUs.
- */
-
-// System includes
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <assert.h>
-
-// CUDA runtime
-#include <hip/hip_runtime.h>
-
-// helper functions and utilities to work with CUDA
-#include "helper_functions.h"
-#include "helper_cuda_hipified.h"
-
-#ifndef MAX
-#define MAX(a, b) (a > b ? a : b)
-#endif
-
-#include "simpleMultiGPU.h"
-
-////////////////////////////////////////////////////////////////////////////////
-// Data configuration
-////////////////////////////////////////////////////////////////////////////////
-const int MAX_GPU_COUNT = 32;
-const int DATA_N = 1048576 * 32;
-
-////////////////////////////////////////////////////////////////////////////////
-// Simple reduction kernel.
-// Refer to the 'reduction' CUDA Sample describing
-// reduction optimization strategies
-////////////////////////////////////////////////////////////////////////////////
-__global__ static void reduceKernel(float *d_Result, float *d_Input, int N) {
-  const int tid = blockIdx.x * blockDim.x + threadIdx.x;
-  const int threadN = gridDim.x * blockDim.x;
-  float sum = 0;
-
-  for (int pos = tid; pos < N; pos += threadN) sum += d_Input[pos];
-
-  d_Result[tid] = sum;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Program main
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) {
-  // Solver config
-  TGPUplan plan[MAX_GPU_COUNT];
-
-  // GPU reduction results
-  float h_SumGPU[MAX_GPU_COUNT];
-
-  float sumGPU;
-  double sumCPU, diff;
-
-  int i, j, gpuBase, GPU_N;
-
-  const int BLOCK_N = 32;
-  const int THREAD_N = 256;
-  const int ACCUM_N = BLOCK_N * THREAD_N;
-
-  printf("Starting simpleMultiGPU\n");
-  HIPCHECK(hipGetDeviceCount(&GPU_N));
-
-  if (GPU_N > MAX_GPU_COUNT) {
-    GPU_N = MAX_GPU_COUNT;
-  }
-
-  printf("CUDA-capable device count: %i\n", GPU_N);
-
-  printf("Generating input data...\n\n");
-
-  // Subdividing input data across GPUs
-  // Get data sizes for each GPU
-  for (i = 0; i < GPU_N; i++) {
-    plan[i].dataN = DATA_N / GPU_N;
-  }
-
-  // Take into account "odd" data sizes
-  for (i = 0; i < DATA_N % GPU_N; i++) {
-    plan[i].dataN++;
-  }
-
-  // Assign data ranges to GPUs
-  gpuBase = 0;
-
-  for (i = 0; i < GPU_N; i++) {
-    plan[i].h_Sum = h_SumGPU + i;
-    gpuBase += plan[i].dataN;
-  }
-
-  // Create streams for issuing GPU command asynchronously and allocate memory
-  // (GPU and System page-locked)
-  for (i = 0; i < GPU_N; i++) {
-    HIPCHECK(hipSetDevice(i));
-    HIPCHECK(hipStreamCreate(&plan[i].stream));
-    // Allocate memory
-    HIPCHECK(
-        hipMalloc((void **)&plan[i].d_Data, plan[i].dataN * sizeof(float)));
-    HIPCHECK(
-        hipMalloc((void **)&plan[i].d_Sum, ACCUM_N * sizeof(float)));
-    HIPCHECK(hipHostMalloc((void **)&plan[i].h_Sum_from_device,
-                                   ACCUM_N * sizeof(float)));
-    HIPCHECK(hipHostMalloc((void **)&plan[i].h_Data,
-                                   plan[i].dataN * sizeof(float)));
-
-    for (j = 0; j < plan[i].dataN; j++) {
-      plan[i].h_Data[j] = (float)rand() / (float)RAND_MAX;
-    }
-  }
-
-  // Start timing and compute on GPU(s)
-  printf("Computing with %d GPUs...\n", GPU_N);
-  // create and start timer
-  StopWatchInterface *timer = NULL;
-  sdkCreateTimer(&timer);
-
-  // start the timer
-  sdkStartTimer(&timer);
-
-  // Copy data to GPU, launch the kernel and copy data back. All asynchronously
-  for (i = 0; i < GPU_N; i++) {
-    // Set device
-    HIPCHECK(hipSetDevice(i));
-
-    // Copy input data from CPU
-    HIPCHECK(hipMemcpyAsync(plan[i].d_Data, plan[i].h_Data,
-                                    plan[i].dataN * sizeof(float),
-                                    hipMemcpyHostToDevice, plan[i].stream));
-
-    // Perform GPU computations
-    reduceKernel<<<BLOCK_N, THREAD_N, 0, plan[i].stream>>>(
-        plan[i].d_Sum, plan[i].d_Data, plan[i].dataN);
-    getLastCudaError("reduceKernel() execution failed.\n");
-
-    // Read back GPU results
-    HIPCHECK(hipMemcpyAsync(plan[i].h_Sum_from_device, plan[i].d_Sum,
-                                    ACCUM_N * sizeof(float),
-                                    hipMemcpyDeviceToHost, plan[i].stream));
-  }
-
-  // Process GPU results
-  for (i = 0; i < GPU_N; i++) {
-    float sum;
-
-    // Set device
-    HIPCHECK(hipSetDevice(i));
-
-    // Wait for all operations to finish
-    hipStreamSynchronize(plan[i].stream);
-
-    // Finalize GPU reduction for current subvector
-    sum = 0;
-
-    for (j = 0; j < ACCUM_N; j++) {
-      sum += plan[i].h_Sum_from_device[j];
-    }
-
-    *(plan[i].h_Sum) = (float)sum;
-
-    // Shut down this GPU
-    HIPCHECK(hipHostFree(plan[i].h_Sum_from_device));
-    HIPCHECK(hipFree(plan[i].d_Sum));
-    HIPCHECK(hipFree(plan[i].d_Data));
-    HIPCHECK(hipStreamDestroy(plan[i].stream));
-  }
-
-  sumGPU = 0;
-
-  for (i = 0; i < GPU_N; i++) {
-    sumGPU += h_SumGPU[i];
-  }
-
-  sdkStopTimer(&timer);
-  printf("  GPU Processing time: %f (ms)\n\n", sdkGetTimerValue(&timer));
-  sdkDeleteTimer(&timer);
-
-  // Compute on Host CPU
-  printf("Computing with Host CPU...\n\n");
-
-  sumCPU = 0;
-
-  for (i = 0; i < GPU_N; i++) {
-    for (j = 0; j < plan[i].dataN; j++) {
-      sumCPU += plan[i].h_Data[j];
-    }
-  }
-
-  // Compare GPU and CPU results
-  printf("Comparing GPU and Host CPU results...\n");
-  diff = fabs(sumCPU - sumGPU) / fabs(sumCPU);
-  printf("  GPU sum: %f\n  CPU sum: %f\n", sumGPU, sumCPU);
-  printf("  Relative difference: %E \n\n", diff);
-
-  // Cleanup and shutdown
-  for (i = 0; i < GPU_N; i++) {
-    HIPCHECK(hipSetDevice(i));
-    HIPCHECK(hipHostFree(plan[i].h_Data));
-  }
-
-  exit((diff < 1e-5) ? EXIT_SUCCESS : EXIT_FAILURE);
-}
diff --git a/src/samples/Samples/0_Introduction/simpleTemplates_nvrtc/simpleTemplates_kernel.cu.hip b/src/samples/Samples/0_Introduction/simpleTemplates_nvrtc/simpleTemplates_kernel.cu.hip
index 89eed5d..e69de29 100755
--- a/src/samples/Samples/0_Introduction/simpleTemplates_nvrtc/simpleTemplates_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleTemplates_nvrtc/simpleTemplates_kernel.cu.hip
@@ -1,71 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-// includes, kernels
-#include "sharedmem.cuh"
-
-////////////////////////////////////////////////////////////////////////////////
-//! Simple test kernel for device functionality
-//! @param g_idata  input data in global memory
-//! @param g_odata  output data in global memory
-////////////////////////////////////////////////////////////////////////////////
-
-template <class T>
-__device__ void testKernel(T *g_idata, T *g_odata) {
-  // Shared mem size is determined by the host app at run time
-  SharedMemory<T> smem;
-
-  T *sdata = smem.getPointer();
-
-  // access thread id
-  const unsigned int tid = threadIdx.x;
-
-  // access number of threads in this block
-  const unsigned int num_threads = blockDim.x;
-
-  // read in input data from global memory
-  sdata[tid] = g_idata[tid];
-
-  __syncthreads();
-
-  // perform some computations
-  sdata[tid] = (T)num_threads * sdata[tid];
-
-  __syncthreads();
-
-  // write data to global memory
-  g_odata[tid] = sdata[tid];
-}
-
-extern "C" __global__ void testFloat(float *p1, float *p2) {
-  testKernel<float>(p1, p2);
-}
-
-extern "C" __global__ void testInt(int *p1, int *p2) {
-  testKernel<int>(p1, p2);
-}
diff --git a/src/samples/Samples/0_Introduction/simpleTextureDrv/simpleTexture_kernel.cu.hip b/src/samples/Samples/0_Introduction/simpleTextureDrv/simpleTexture_kernel.cu.hip
index d878b25..e69de29 100755
--- a/src/samples/Samples/0_Introduction/simpleTextureDrv/simpleTexture_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleTextureDrv/simpleTexture_kernel.cu.hip
@@ -1,56 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _SIMPLETEXTURE_KERNEL_H_
-#define _SIMPLETEXTURE_KERNEL_H_
-#include <hip/hip_runtime.h>
-
-////////////////////////////////////////////////////////////////////////////////
-//! Transform an image using texture lookups
-//! @param g_odata  output data in global memory
-////////////////////////////////////////////////////////////////////////////////
-extern "C" __global__ void transformKernel(float *g_odata, int width,
-                                           int height, float theta,
-                                           hipTextureObject_t tex) {
-  // calculate normalized texture coordinates
-  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
-  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
-
-  float u = (float)x - (float)width / 2;
-  float v = (float)y - (float)height / 2;
-  float tu = u * cosf(theta) - v * sinf(theta);
-  float tv = v * cosf(theta) + u * sinf(theta);
-
-  tu /= (float)width;
-  tv /= (float)height;
-
-  // read from texture and write to global memory
-  g_odata[y * width + x] = tex2D<float>(tex, tu + 0.5f, tv + 0.5f);
-}
-
-#endif  // #ifndef _SIMPLETEXTURE_KERNEL_H_
diff --git a/src/samples/Samples/0_Introduction/vectorAddDrv/vectorAdd_kernel.cu.hip b/src/samples/Samples/0_Introduction/vectorAddDrv/vectorAdd_kernel.cu.hip
index 8c2afb9..e69de29 100755
--- a/src/samples/Samples/0_Introduction/vectorAddDrv/vectorAdd_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/vectorAddDrv/vectorAdd_kernel.cu.hip
@@ -1,42 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-/* Vector addition: C = A + B.
- *
- * This sample is a very basic sample that implements element by element
- * vector addition. It is the same as the sample illustrating Chapter 3
- * of the programming guide with some additions like error checking.
- *
- */
-
-// Device code
-extern "C" __global__ void VecAdd_kernel(const float *A, const float *B,
-                                         float *C, int N) {
-  int i = blockDim.x * blockIdx.x + threadIdx.x;
-
-  if (i < N) C[i] = A[i] + B[i];
-}
diff --git a/src/samples/Samples/0_Introduction/vectorAddMMAP/vectorAdd_kernel.cu.hip b/src/samples/Samples/0_Introduction/vectorAddMMAP/vectorAdd_kernel.cu.hip
index 72e20f6..e69de29 100755
--- a/src/samples/Samples/0_Introduction/vectorAddMMAP/vectorAdd_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/vectorAddMMAP/vectorAdd_kernel.cu.hip
@@ -1,43 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/* Vector addition: C = A + B.
- *
- * This sample is a very basic sample that implements element by element
- * vector addition. It is the same as the sample illustrating Chapter 3
- * of the programming guide with some additions like error checking.
- *
- */
-
-// Device code
-extern "C" __global__ void VecAdd_kernel(const float *A, const float *B,
-                                         float *C, int N) {
-  int i = blockDim.x * blockIdx.x + threadIdx.x;
-
-  if (i < N) C[i] = A[i] + B[i];
-}
diff --git a/src/samples/Samples/0_Introduction/vectorAdd_nvrtc/vectorAdd_kernel.cu.hip b/src/samples/Samples/0_Introduction/vectorAdd_nvrtc/vectorAdd_kernel.cu.hip
index bb459dd..e69de29 100755
--- a/src/samples/Samples/0_Introduction/vectorAdd_nvrtc/vectorAdd_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/vectorAdd_nvrtc/vectorAdd_kernel.cu.hip
@@ -1,43 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/**
- * CUDA Kernel Device code
- *
- * Computes the vector addition of A and B into C. The 3 vectors have the same
- * number of elements numElements.
- */
-
-extern "C" __global__ void vectorAdd(const float *A, const float *B, float *C,
-                                     int numElements) {
-  int i = blockDim.x * blockIdx.x + threadIdx.x;
-
-  if (i < numElements) {
-    C[i] = A[i] + B[i];
-  }
-}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiInlineP/src/piestimator.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiInlineP/src/piestimator.cu.hip
index c56c703..e69de29 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiInlineP/src/piestimator.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiInlineP/src/piestimator.cu.hip
@@ -1,284 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "../inc/piestimator.h"
-
-#include <string>
-#include <vector>
-#include <numeric>
-#include <stdexcept>
-#include <typeinfo>
-#include <hip/hip_runtime.h>
-#include <hip/hip_cooperative_groups.h>
-
-namespace cg = cooperative_groups;
-#include <hiprand_kernel.h>
-
-using std::string;
-using std::vector;
-
-// RNG init kernel
-__global__ void initRNG(hiprandState *const rngStates, const unsigned int seed) {
-  // Determine thread ID
-  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
-
-  // Initialise the RNG
-  hiprand_init(seed, tid, 0, &rngStates[tid]);
-}
-
-__device__ unsigned int reduce_sum(unsigned int in, cg::thread_block cta) {
-  extern __shared__ unsigned int sdata[];
-
-  // Perform first level of reduction:
-  // - Write to shared memory
-  unsigned int ltid = threadIdx.x;
-
-  sdata[ltid] = in;
-  cg::sync(cta);
-
-  // Do reduction in shared mem
-  for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {
-    if (ltid < s) {
-      sdata[ltid] += sdata[ltid + s];
-    }
-
-    cg::sync(cta);
-  }
-
-  return sdata[0];
-}
-
-__device__ inline void getPoint(float &x, float &y, hiprandState &state) {
-  x = hiprand_uniform(&state);
-  y = hiprand_uniform(&state);
-}
-__device__ inline void getPoint(double &x, double &y, hiprandState &state) {
-  x = hiprand_uniform_double(&state);
-  y = hiprand_uniform_double(&state);
-}
-
-// Estimator kernel
-template <typename Real>
-__global__ void computeValue(unsigned int *const results,
-                             hiprandState *const rngStates,
-                             const unsigned int numSims) {
-  // Handle to thread block group
-  cg::thread_block cta = cg::this_thread_block();
-  // Determine thread ID
-  unsigned int bid = blockIdx.x;
-  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
-  unsigned int step = gridDim.x * blockDim.x;
-
-  // Initialise the RNG
-  hiprandState localState = rngStates[tid];
-
-  // Count the number of points which lie inside the unit quarter-circle
-  unsigned int pointsInside = 0;
-
-  for (unsigned int i = tid; i < numSims; i += step) {
-    Real x;
-    Real y;
-    getPoint(x, y, localState);
-    Real l2norm2 = x * x + y * y;
-
-    if (l2norm2 < static_cast<Real>(1)) {
-      pointsInside++;
-    }
-  }
-
-  // Reduce within the block
-  pointsInside = reduce_sum(pointsInside, cta);
-
-  // Store the result
-  if (threadIdx.x == 0) {
-    results[bid] = pointsInside;
-  }
-}
-
-template <typename Real>
-PiEstimator<Real>::PiEstimator(unsigned int numSims, unsigned int device,
-                               unsigned int threadBlockSize, unsigned int seed)
-    : m_numSims(numSims),
-      m_device(device),
-      m_threadBlockSize(threadBlockSize),
-      m_seed(seed) {}
-
-template <typename Real>
-Real PiEstimator<Real>::operator()() {
-  hipError_t cudaResult = hipSuccess;
-  struct hipDeviceProp_t deviceProperties;
-  struct hipFuncAttributes funcAttributes;
-
-  // Get device properties
-  cudaResult = hipGetDeviceProperties(&deviceProperties, m_device);
-
-  if (cudaResult != hipSuccess) {
-    string msg("Could not get device properties: ");
-    msg += hipGetErrorString(cudaResult);
-    throw std::runtime_error(msg);
-  }
-
-  // Check precision is valid
-  if (typeid(Real) == typeid(double) &&
-      (deviceProperties.major < 1 ||
-       (deviceProperties.major == 1 && deviceProperties.minor < 3))) {
-    throw std::runtime_error("Device does not have double precision support");
-  }
-
-  // Attach to GPU
-  cudaResult = hipSetDevice(m_device);
-
-  if (cudaResult != hipSuccess) {
-    string msg("Could not set CUDA device: ");
-    msg += hipGetErrorString(cudaResult);
-    throw std::runtime_error(msg);
-  }
-
-  // Determine how to divide the work between cores
-  dim3 block;
-  dim3 grid;
-  block.x = m_threadBlockSize;
-  grid.x = (m_numSims + m_threadBlockSize - 1) / m_threadBlockSize;
-
-  // Aim to launch around ten or more times as many blocks as there
-  // are multiprocessors on the target device.
-  unsigned int blocksPerSM = 10;
-  unsigned int numSMs = deviceProperties.multiProcessorCount;
-
-  while (grid.x > 2 * blocksPerSM * numSMs) {
-    grid.x >>= 1;
-  }
-
-  // Get initRNG function properties and check the maximum block size
-  cudaResult = hipFuncGetAttributes(&funcAttributes, initRNG);
-
-  if (cudaResult != hipSuccess) {
-    string msg("Could not get function attributes: ");
-    msg += hipGetErrorString(cudaResult);
-    throw std::runtime_error(msg);
-  }
-
-  if (block.x > (unsigned int)funcAttributes.maxThreadsPerBlock) {
-    throw std::runtime_error(
-        "Block X dimension is too large for initRNG kernel");
-  }
-
-  // Get computeValue function properties and check the maximum block size
-  cudaResult = hipFuncGetAttributes(&funcAttributes, computeValue<Real>);
-
-  if (cudaResult != hipSuccess) {
-    string msg("Could not get function attributes: ");
-    msg += hipGetErrorString(cudaResult);
-    throw std::runtime_error(msg);
-  }
-
-  if (block.x > (unsigned int)funcAttributes.maxThreadsPerBlock) {
-    throw std::runtime_error(
-        "Block X dimension is too large for computeValue kernel");
-  }
-
-  // Check the dimensions are valid
-  if (block.x > (unsigned int)deviceProperties.maxThreadsDim[0]) {
-    throw std::runtime_error("Block X dimension is too large for device");
-  }
-
-  if (grid.x > (unsigned int)deviceProperties.maxGridSize[0]) {
-    throw std::runtime_error("Grid X dimension is too large for device");
-  }
-
-  // Allocate memory for RNG states
-  hiprandState *d_rngStates = 0;
-  cudaResult =
-      hipMalloc((void **)&d_rngStates, grid.x * block.x * sizeof(hiprandState));
-
-  if (cudaResult != hipSuccess) {
-    string msg("Could not allocate memory on device for RNG states: ");
-    msg += hipGetErrorString(cudaResult);
-    throw std::runtime_error(msg);
-  }
-
-  // Allocate memory for result
-  // Each thread block will produce one result
-  unsigned int *d_results = 0;
-  cudaResult = hipMalloc((void **)&d_results, grid.x * sizeof(unsigned int));
-
-  if (cudaResult != hipSuccess) {
-    string msg("Could not allocate memory on device for partial results: ");
-    msg += hipGetErrorString(cudaResult);
-    throw std::runtime_error(msg);
-  }
-
-  // Initialise RNG
-  initRNG<<<grid, block>>>(d_rngStates, m_seed);
-
-  // Count the points inside unit quarter-circle
-  computeValue<Real><<<grid, block, block.x * sizeof(unsigned int)>>>(
-      d_results, d_rngStates, m_numSims);
-
-  // Copy partial results back
-  vector<unsigned int> results(grid.x);
-  cudaResult = hipMemcpy(&results[0], d_results, grid.x * sizeof(unsigned int),
-                          hipMemcpyDeviceToHost);
-
-  if (cudaResult != hipSuccess) {
-    string msg("Could not copy partial results to host: ");
-    msg += hipGetErrorString(cudaResult);
-    throw std::runtime_error(msg);
-  }
-
-  // Complete sum-reduction on host
-  Real value =
-      static_cast<Real>(std::accumulate(results.begin(), results.end(), 0));
-
-  // Determine the proportion of points inside the quarter-circle,
-  // i.e. the area of the unit quarter-circle
-  value /= m_numSims;
-
-  // Value is currently an estimate of the area of a unit quarter-circle, so we
-  // can scale to a full circle by multiplying by four. Now since the area of a
-  // circle is pi * r^2, and r is one, the value will be an estimate for the
-  // value of pi.
-  value *= 4;
-
-  // Cleanup
-  if (d_rngStates) {
-    hipFree(d_rngStates);
-    d_rngStates = 0;
-  }
-
-  if (d_results) {
-    hipFree(d_results);
-    d_results = 0;
-  }
-
-  return value;
-}
-
-// Explicit template instantiation
-template class PiEstimator<float>;
-template class PiEstimator<double>;
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiQ/src/piestimator.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiQ/src/piestimator.cu.hip
index 581aa2c..e69de29 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiQ/src/piestimator.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiQ/src/piestimator.cu.hip
@@ -1,312 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "../inc/piestimator.h"
-
-#include <string>
-#include <vector>
-#include <numeric>
-#include <stdexcept>
-#include <typeinfo>
-#include <hip/hip_runtime.h>
-#include <hip/hip_cooperative_groups.h>
-
-namespace cg = cooperative_groups;
-#include <hiprand.h>
-
-using std::string;
-using std::vector;
-
-__device__ unsigned int reduce_sum(unsigned int in, cg::thread_block cta) {
-  extern __shared__ unsigned int sdata[];
-
-  // Perform first level of reduction:
-  // - Write to shared memory
-  unsigned int ltid = threadIdx.x;
-
-  sdata[ltid] = in;
-  cg::sync(cta);
-
-  // Do reduction in shared mem
-  for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {
-    if (ltid < s) {
-      sdata[ltid] += sdata[ltid + s];
-    }
-
-    cg::sync(cta);
-  }
-
-  return sdata[0];
-}
-
-// Estimator kernel
-template <typename Real>
-__global__ void computeValue(unsigned int *const results,
-                             const Real *const points,
-                             const unsigned int numSims) {
-  // Handle to thread block group
-  cg::thread_block cta = cg::this_thread_block();
-  // Determine thread ID
-  unsigned int bid = blockIdx.x;
-  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
-  unsigned int step = gridDim.x * blockDim.x;
-
-  // Shift the input/output pointers
-  const Real *pointx = points + tid;
-  const Real *pointy = pointx + numSims;
-
-  // Count the number of points which lie inside the unit quarter-circle
-  unsigned int pointsInside = 0;
-
-  for (unsigned int i = tid; i < numSims;
-       i += step, pointx += step, pointy += step) {
-    Real x = *pointx;
-    Real y = *pointy;
-    Real l2norm2 = x * x + y * y;
-
-    if (l2norm2 < static_cast<Real>(1)) {
-      pointsInside++;
-    }
-  }
-
-  // Reduce within the block
-  pointsInside = reduce_sum(pointsInside, cta);
-
-  // Store the result
-  if (threadIdx.x == 0) {
-    results[bid] = pointsInside;
-  }
-}
-
-template <typename Real>
-PiEstimator<Real>::PiEstimator(unsigned int numSims, unsigned int device,
-                               unsigned int threadBlockSize)
-    : m_numSims(numSims),
-      m_device(device),
-      m_threadBlockSize(threadBlockSize) {}
-
-template <typename Real>
-Real PiEstimator<Real>::operator()() {
-  hipError_t cudaResult = hipSuccess;
-  struct hipDeviceProp_t deviceProperties;
-  struct hipFuncAttributes funcAttributes;
-
-  // Get device properties
-  cudaResult = hipGetDeviceProperties(&deviceProperties, m_device);
-
-  if (cudaResult != hipSuccess) {
-    string msg("Could not get device properties: ");
-    msg += hipGetErrorString(cudaResult);
-    throw std::runtime_error(msg);
-  }
-
-  // Check precision is valid
-  if (typeid(Real) == typeid(double) &&
-      (deviceProperties.major < 1 ||
-       (deviceProperties.major == 1 && deviceProperties.minor < 3))) {
-    throw std::runtime_error("Device does not have double precision support");
-  }
-
-  // Attach to GPU
-  cudaResult = hipSetDevice(m_device);
-
-  if (cudaResult != hipSuccess) {
-    string msg("Could not set CUDA device: ");
-    msg += hipGetErrorString(cudaResult);
-    throw std::runtime_error(msg);
-  }
-
-  // Determine how to divide the work between cores
-  dim3 block;
-  dim3 grid;
-  block.x = m_threadBlockSize;
-  grid.x = (m_numSims + m_threadBlockSize - 1) / m_threadBlockSize;
-
-  // Aim to launch around ten or more times as many blocks as there
-  // are multiprocessors on the target device.
-  unsigned int blocksPerSM = 10;
-  unsigned int numSMs = deviceProperties.multiProcessorCount;
-
-  while (grid.x > 2 * blocksPerSM * numSMs) {
-    grid.x >>= 1;
-  }
-
-  // Get computeValue function properties and check the maximum block size
-  cudaResult = hipFuncGetAttributes(&funcAttributes, computeValue<Real>);
-
-  if (cudaResult != hipSuccess) {
-    string msg("Could not get function attributes: ");
-    msg += hipGetErrorString(cudaResult);
-    throw std::runtime_error(msg);
-  }
-
-  if (block.x > (unsigned int)funcAttributes.maxThreadsPerBlock) {
-    throw std::runtime_error(
-        "Block X dimension is too large for computeValue kernel");
-  }
-
-  // Check the dimensions are valid
-  if (block.x > (unsigned int)deviceProperties.maxThreadsDim[0]) {
-    throw std::runtime_error("Block X dimension is too large for device");
-  }
-
-  if (grid.x > (unsigned int)deviceProperties.maxGridSize[0]) {
-    throw std::runtime_error("Grid X dimension is too large for device");
-  }
-
-  // Allocate memory for points
-  // Each simulation has two random numbers to give X and Y coordinate
-  Real *d_points = 0;
-  cudaResult = hipMalloc((void **)&d_points, 2 * m_numSims * sizeof(Real));
-
-  if (cudaResult != hipSuccess) {
-    string msg("Could not allocate memory on device for random numbers: ");
-    msg += hipGetErrorString(cudaResult);
-    throw std::runtime_error(msg);
-  }
-
-  // Allocate memory for result
-  // Each thread block will produce one result
-  unsigned int *d_results = 0;
-  cudaResult = hipMalloc((void **)&d_results, grid.x * sizeof(unsigned int));
-
-  if (cudaResult != hipSuccess) {
-    string msg("Could not allocate memory on device for partial results: ");
-    msg += hipGetErrorString(cudaResult);
-    throw std::runtime_error(msg);
-  }
-
-  // Generate random points in unit square
-  hiprandStatus_t curandResult;
-  hiprandGenerator_t qrng;
-
-  if (typeid(Real) == typeid(float)) {
-    curandResult = hiprandCreateGenerator(&qrng, HIPRAND_RNG_QUASI_SOBOL32);
-  } else if (typeid(Real) == typeid(double)) {
-    curandResult = hiprandCreateGenerator(&qrng, HIPRAND_RNG_QUASI_SOBOL64);
-  } else {
-    string msg("Could not create random number generator of specified type");
-    throw std::runtime_error(msg);
-  }
-
-  if (curandResult != HIPRAND_STATUS_SUCCESS) {
-    string msg("Could not create quasi-random number generator: ");
-    msg += curandResult;
-    throw std::runtime_error(msg);
-  }
-
-  curandResult = hiprandSetQuasiRandomGeneratorDimensions(qrng, 2);
-
-  if (curandResult != HIPRAND_STATUS_SUCCESS) {
-    string msg(
-        "Could not set number of dimensions for quasi-random number "
-        "generator: ");
-    msg += curandResult;
-    throw std::runtime_error(msg);
-  }
-
-  curandResult =
-      curandSetGeneratorOrdering(qrng, CURAND_ORDERING_QUASI_DEFAULT);
-
-  if (curandResult != HIPRAND_STATUS_SUCCESS) {
-    string msg("Could not set order for quasi-random number generator: ");
-    msg += curandResult;
-    throw std::runtime_error(msg);
-  }
-
-  if (typeid(Real) == typeid(float)) {
-    curandResult =
-        hiprandGenerateUniform(qrng, (float *)d_points, 2 * m_numSims);
-  } else if (typeid(Real) == typeid(double)) {
-    curandResult =
-        hiprandGenerateUniformDouble(qrng, (double *)d_points, 2 * m_numSims);
-  } else {
-    string msg("Could not generate random numbers of specified type");
-    throw std::runtime_error(msg);
-  }
-
-  if (curandResult != HIPRAND_STATUS_SUCCESS) {
-    string msg("Could not generate quasi-random numbers: ");
-    msg += curandResult;
-    throw std::runtime_error(msg);
-  }
-
-  curandResult = hiprandDestroyGenerator(qrng);
-
-  if (curandResult != HIPRAND_STATUS_SUCCESS) {
-    string msg("Could not destroy quasi-random number generator: ");
-    msg += curandResult;
-    throw std::runtime_error(msg);
-  }
-
-  // Count the points inside unit quarter-circle
-  computeValue<Real><<<grid, block, block.x * sizeof(unsigned int)>>>(
-      d_results, d_points, m_numSims);
-
-  // Copy partial results back
-  vector<unsigned int> results(grid.x);
-  cudaResult = hipMemcpy(&results[0], d_results, grid.x * sizeof(unsigned int),
-                          hipMemcpyDeviceToHost);
-
-  if (cudaResult != hipSuccess) {
-    string msg("Could not copy partial results to host: ");
-    msg += hipGetErrorString(cudaResult);
-    throw std::runtime_error(msg);
-  }
-
-  // Complete sum-reduction on host
-  Real value =
-      static_cast<Real>(std::accumulate(results.begin(), results.end(), 0));
-
-  // Determine the proportion of points inside the quarter-circle,
-  // i.e. the area of the unit quarter-circle
-  value /= m_numSims;
-
-  // Value is currently an estimate of the area of a unit quarter-circle, so we
-  // can scale to a full circle by multiplying by four. Now since the area of a
-  // circle is pi * r^2, and r is one, the value will be an estimate for the
-  // value of pi.
-  value *= 4;
-
-  // Cleanup
-  if (d_points) {
-    hipFree(d_points);
-    d_points = 0;
-  }
-
-  if (d_results) {
-    hipFree(d_results);
-    d_results = 0;
-  }
-
-  return value;
-}
-
-// Explicit template instantiation
-template class PiEstimator<float>;
-template class PiEstimator<double>;
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX_nvrtc/inlinePTX_kernel.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX_nvrtc/inlinePTX_kernel.cu.hip
index 9c714f1..e69de29 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX_nvrtc/inlinePTX_kernel.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX_nvrtc/inlinePTX_kernel.cu.hip
@@ -1,40 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-extern "C" __global__ void sequence_gpu(int *d_ptr, int length) {
-  int elemID = blockIdx.x * blockDim.x + threadIdx.x;
-
-  if (elemID < length) {
-    unsigned int laneid;
-
-    // This command gets the lane ID within the current warp
-    asm("mov.u32 %0, %%laneid;" : "=r"(laneid));
-
-    d_ptr[elemID] = laneid;
-  }
-}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.cu.hip
index 8b9ef76..42fc161 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.cu.hip
@@ -34,7 +34,7 @@
 #include <thrust/generate.h>
 #include <thrust/detail/type_traits.h>
 
-#include "helper_cuda_hipified.h"
+#include <helper_cuda.h>
 
 #include <algorithm>
 #include <time.h>
@@ -142,8 +142,8 @@ bool testSort(int argc, char **argv) {
 
   // run multiple iterations to compute an average sort time
   hipEvent_t start_event, stop_event;
-  HIPCHECK(hipEventCreate(&start_event));
-  HIPCHECK(hipEventCreate(&stop_event));
+  checkCudaErrors(hipEventCreate(&start_event));
+  checkCudaErrors(hipEventCreate(&stop_event));
 
   float totalTime = 0;
 
@@ -153,18 +153,18 @@ bool testSort(int argc, char **argv) {
 
     if (!keysOnly) d_values = h_values;
 
-    HIPCHECK(hipEventRecord(start_event, 0));
+    checkCudaErrors(hipEventRecord(start_event, 0));
 
     if (keysOnly)
       thrust::sort(d_keys.begin(), d_keys.end());
     else
       thrust::sort_by_key(d_keys.begin(), d_keys.end(), d_values.begin());
 
-    HIPCHECK(hipEventRecord(stop_event, 0));
-    HIPCHECK(hipEventSynchronize(stop_event));
+    checkCudaErrors(hipEventRecord(stop_event, 0));
+    checkCudaErrors(hipEventSynchronize(stop_event));
 
     float time = 0;
-    HIPCHECK(hipEventElapsedTime(&time, start_event, stop_event));
+    checkCudaErrors(hipEventElapsedTime(&time, start_event, stop_event));
     totalTime += time;
   }
 
@@ -188,8 +188,8 @@ bool testSort(int argc, char **argv) {
   bool bTestResult =
       thrust::is_sorted(h_keysSorted.begin(), h_keysSorted.end());
 
-  HIPCHECK(hipEventDestroy(start_event));
-  HIPCHECK(hipEventDestroy(stop_event));
+  checkCudaErrors(hipEventDestroy(start_event));
+  checkCudaErrors(hipEventDestroy(stop_event));
 
   if (!bTestResult && !quiet) {
     return false;
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/reductionMultiBlockCG/reductionMultiBlockCG.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/reductionMultiBlockCG/reductionMultiBlockCG.cu.hip
index dfe7459..82853ad 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/reductionMultiBlockCG/reductionMultiBlockCG.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/reductionMultiBlockCG/reductionMultiBlockCG.cu.hip
@@ -177,7 +177,7 @@ int main(int argc, char **argv) {
   printf("%s Starting...\n\n", sSDKsample);
 
   dev = findCudaDevice(argc, (const char **)argv);
-  HIPCHECK(hipGetDeviceProperties(&deviceProp, dev));
+  checkCudaErrors(hipGetDeviceProperties(&deviceProp, dev));
   if (!deviceProp.cooperativeLaunch) {
     printf(
         "\nSelected GPU (%d) does not support Cooperative Kernel Launch, "
@@ -235,7 +235,7 @@ void getNumBlocksAndThreads(int n, int maxBlocks, int maxThreads, int &blocks,
     threads = 1;
     blocks = 1;
   } else {
-    HIPCHECK(hipOccupancyMaxPotentialBlockSize(
+    checkCudaErrors(hipOccupancyMaxPotentialBlockSize(
         &blocks, &threads, reduceSinglePassMultiBlockCG));
   }
 
@@ -267,7 +267,7 @@ float benchmarkReduce(int n, int numThreads, int numBlocks, int maxThreads,
   // copy final sum from device to host
   error =
       hipMemcpy(&gpu_result, d_odata, sizeof(float), hipMemcpyDeviceToHost);
-  HIPCHECK(error);
+  checkCudaErrors(error);
 
   return gpu_result;
 }
@@ -287,8 +287,8 @@ bool runTest(int argc, char **argv, int device) {
 
   // Set the device to be used
   hipDeviceProp_t prop = {0};
-  HIPCHECK(hipSetDevice(device));
-  HIPCHECK(hipGetDeviceProperties(&prop, device));
+  checkCudaErrors(hipSetDevice(device));
+  checkCudaErrors(hipGetDeviceProperties(&prop, device));
 
   // create random input data on CPU
   unsigned int bytes = size * sizeof(float);
@@ -324,7 +324,7 @@ bool runTest(int argc, char **argv, int device) {
   // We calculate the occupancy to know how many block can actually fit on the
   // GPU
   int numBlocksPerSm = 0;
-  HIPCHECK(hipOccupancyMaxActiveBlocksPerMultiprocessor(
+  checkCudaErrors(hipOccupancyMaxActiveBlocksPerMultiprocessor(
       &numBlocksPerSm, reduceSinglePassMultiBlockCG, numThreads,
       numThreads * sizeof(double)));
 
@@ -342,12 +342,12 @@ bool runTest(int argc, char **argv, int device) {
   float *d_idata = NULL;
   float *d_odata = NULL;
 
-  HIPCHECK(hipMalloc((void **)&d_idata, bytes));
-  HIPCHECK(hipMalloc((void **)&d_odata, numBlocks * sizeof(float)));
+  checkCudaErrors(hipMalloc((void **)&d_idata, bytes));
+  checkCudaErrors(hipMalloc((void **)&d_odata, numBlocks * sizeof(float)));
 
   // copy data directly to device memory
-  HIPCHECK(hipMemcpy(d_idata, h_idata, bytes, hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(d_odata, h_idata, numBlocks * sizeof(float),
+  checkCudaErrors(hipMemcpy(d_idata, h_idata, bytes, hipMemcpyHostToDevice));
+  checkCudaErrors(hipMemcpy(d_odata, h_idata, numBlocks * sizeof(float),
                              hipMemcpyHostToDevice));
 
   int testIterations = 100;
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/shfl_scan/shfl_scan.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/shfl_scan/shfl_scan.cu.hip
index 83b26fa..05c1860 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/shfl_scan/shfl_scan.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/shfl_scan/shfl_scan.cu.hip
@@ -213,9 +213,9 @@ bool shuffle_simple_test(int argc, char **argv) {
   cuda_device = findCudaDevice(argc, (const char **)argv);
 
   hipDeviceProp_t deviceProp;
-  HIPCHECK(hipGetDevice(&cuda_device));
+  checkCudaErrors(hipGetDevice(&cuda_device));
 
-  HIPCHECK(hipGetDeviceProperties(&deviceProp, cuda_device));
+  checkCudaErrors(hipGetDeviceProperties(&deviceProp, cuda_device));
 
   printf("> Detected Compute SM %d.%d hardware with %d multi-processors\n",
          deviceProp.major, deviceProp.minor, deviceProp.multiProcessorCount);
@@ -227,9 +227,9 @@ bool shuffle_simple_test(int argc, char **argv) {
     exit(EXIT_WAIVED);
   }
 
-  HIPCHECK(hipHostMalloc(reinterpret_cast<void **>(&h_data),
+  checkCudaErrors(hipHostMalloc(reinterpret_cast<void **>(&h_data),
                                  sizeof(int) * n_elements));
-  HIPCHECK(hipHostMalloc(reinterpret_cast<void **>(&h_result),
+  checkCudaErrors(hipHostMalloc(reinterpret_cast<void **>(&h_result),
                                  sizeof(int) * n_elements));
 
   // initialize data:
@@ -259,32 +259,32 @@ bool shuffle_simple_test(int argc, char **argv) {
 
   // initialize a timer
   hipEvent_t start, stop;
-  HIPCHECK(hipEventCreate(&start));
-  HIPCHECK(hipEventCreate(&stop));
+  checkCudaErrors(hipEventCreate(&start));
+  checkCudaErrors(hipEventCreate(&stop));
   float et = 0;
   float inc = 0;
 
-  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_data), sz));
-  HIPCHECK(
+  checkCudaErrors(hipMalloc(reinterpret_cast<void **>(&d_data), sz));
+  checkCudaErrors(
       hipMalloc(reinterpret_cast<void **>(&d_partial_sums), partial_sz));
-  HIPCHECK(hipMemset(d_partial_sums, 0, partial_sz));
+  checkCudaErrors(hipMemset(d_partial_sums, 0, partial_sz));
 
-  HIPCHECK(
+  checkCudaErrors(
       hipHostMalloc(reinterpret_cast<void **>(&h_partial_sums), partial_sz));
-  HIPCHECK(hipMemcpy(d_data, h_data, sz, hipMemcpyHostToDevice));
+  checkCudaErrors(hipMemcpy(d_data, h_data, sz, hipMemcpyHostToDevice));
 
-  HIPCHECK(hipEventRecord(start, 0));
+  checkCudaErrors(hipEventRecord(start, 0));
   shfl_scan_test<<<gridSize, blockSize, shmem_sz>>>(d_data, 32, d_partial_sums);
   shfl_scan_test<<<p_gridSize, p_blockSize, shmem_sz>>>(d_partial_sums, 32);
   uniform_add<<<gridSize - 1, blockSize>>>(d_data + blockSize, d_partial_sums,
                                            n_elements);
-  HIPCHECK(hipEventRecord(stop, 0));
-  HIPCHECK(hipEventSynchronize(stop));
-  HIPCHECK(hipEventElapsedTime(&inc, start, stop));
+  checkCudaErrors(hipEventRecord(stop, 0));
+  checkCudaErrors(hipEventSynchronize(stop));
+  checkCudaErrors(hipEventElapsedTime(&inc, start, stop));
   et += inc;
 
-  HIPCHECK(hipMemcpy(h_result, d_data, sz, hipMemcpyDeviceToHost));
-  HIPCHECK(hipMemcpy(h_partial_sums, d_partial_sums, partial_sz,
+  checkCudaErrors(hipMemcpy(h_result, d_data, sz, hipMemcpyDeviceToHost));
+  checkCudaErrors(hipMemcpy(h_partial_sums, d_partial_sums, partial_sz,
                              hipMemcpyDeviceToHost));
 
   printf("Test Sum: %d\n", h_partial_sums[n_partialSums - 1]);
@@ -294,11 +294,11 @@ bool shuffle_simple_test(int argc, char **argv) {
 
   bool bTestResult = CPUverify(h_data, h_result, n_elements);
 
-  HIPCHECK(hipHostFree(h_data));
-  HIPCHECK(hipHostFree(h_result));
-  HIPCHECK(hipHostFree(h_partial_sums));
-  HIPCHECK(hipFree(d_data));
-  HIPCHECK(hipFree(d_partial_sums));
+  checkCudaErrors(hipHostFree(h_data));
+  checkCudaErrors(hipHostFree(h_result));
+  checkCudaErrors(hipHostFree(h_partial_sums));
+  checkCudaErrors(hipFree(d_data));
+  checkCudaErrors(hipFree(d_partial_sums));
 
   return bTestResult;
 }
@@ -317,7 +317,7 @@ bool shuffle_integral_image_test() {
   printf("\nComputing Integral Image Test on size %d x %d synthetic data\n", w,
          h);
   printf("---------------------------------------------------\n");
-  HIPCHECK(hipHostMalloc(reinterpret_cast<void **>(&h_image), sz));
+  checkCudaErrors(hipHostMalloc(reinterpret_cast<void **>(&h_image), sz));
   // fill test "image" with synthetic 1's data
   memset(h_image, 0, sz);
 
@@ -327,11 +327,11 @@ bool shuffle_integral_image_test() {
   int gridSize = h;
 
   // Create a synthetic image for testing
-  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_data), sz));
-  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_integral_image),
+  checkCudaErrors(hipMalloc(reinterpret_cast<void **>(&d_data), sz));
+  checkCudaErrors(hipMalloc(reinterpret_cast<void **>(&d_integral_image),
                              n_elements * sizeof(int) * 4));
-  HIPCHECK(hipMemset(d_data, 1, sz));
-  HIPCHECK(hipMemset(d_integral_image, 0, sz));
+  checkCudaErrors(hipMemset(d_data, 1, sz));
+  checkCudaErrors(hipMemset(d_integral_image, 0, sz));
 
   hipEvent_t start, stop;
   hipEventCreate(&start);
@@ -345,12 +345,12 @@ bool shuffle_integral_image_test() {
       reinterpret_cast<uint4 *>(d_data),
       reinterpret_cast<uint4 *>(d_integral_image));
   hipEventRecord(stop);
-  HIPCHECK(hipEventSynchronize(stop));
-  HIPCHECK(hipEventElapsedTime(&et, start, stop));
+  checkCudaErrors(hipEventSynchronize(stop));
+  checkCudaErrors(hipEventElapsedTime(&et, start, stop));
   printf("Method: Fast  Time (GPU Timer): %f ms ", et);
 
   // verify the scan line results
-  HIPCHECK(
+  checkCudaErrors(
       hipMemcpy(h_image, d_integral_image, sz, hipMemcpyDeviceToHost));
   err = verifyDataRowSums(h_image, w, h);
   printf("Diff = %d\n", err);
@@ -363,21 +363,21 @@ bool shuffle_integral_image_test() {
   shfl_vertical_shfl<<<testGrid, blockSz>>>((unsigned int *)d_integral_image, w,
                                             h);
   hipEventRecord(stop);
-  HIPCHECK(hipEventSynchronize(stop));
-  HIPCHECK(hipEventElapsedTime(&et, start, stop));
+  checkCudaErrors(hipEventSynchronize(stop));
+  checkCudaErrors(hipEventElapsedTime(&et, start, stop));
   printf("Method: Vertical Scan  Time (GPU Timer): %f ms ", et);
 
   // Verify the column results
-  HIPCHECK(
+  checkCudaErrors(
       hipMemcpy(h_image, d_integral_image, sz, hipMemcpyDeviceToHost));
   printf("\n");
 
   int finalSum = h_image[w * h - 1];
   printf("CheckSum: %d, (expect %dx%d=%d)\n", finalSum, w, h, w * h);
 
-  HIPCHECK(hipFree(d_data));
-  HIPCHECK(hipFree(d_integral_image));
-  HIPCHECK(hipHostFree(h_image));
+  checkCudaErrors(hipFree(d_data));
+  checkCudaErrors(hipFree(d_integral_image));
+  checkCudaErrors(hipHostFree(h_image));
   // verify final sum: if the final value in the corner is the same as the size
   // of the buffer (all 1's) then the integral image was generated successfully
   return (finalSum == w * h) ? true : false;
@@ -395,9 +395,9 @@ int main(int argc, char *argv[]) {
   cuda_device = findCudaDevice(argc, (const char **)argv);
 
   hipDeviceProp_t deviceProp;
-  HIPCHECK(hipGetDevice(&cuda_device));
+  checkCudaErrors(hipGetDevice(&cuda_device));
 
-  HIPCHECK(hipGetDeviceProperties(&deviceProp, cuda_device));
+  checkCudaErrors(hipGetDeviceProperties(&deviceProp, cuda_device));
 
   printf("> Detected Compute SM %d.%d hardware with %d multi-processors\n",
          deviceProp.major, deviceProp.minor, deviceProp.multiProcessorCount);
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationP2P/streamOrderedAllocationP2P.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationP2P/streamOrderedAllocationP2P.cu.hip
index 7e94df3..e69de29 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationP2P/streamOrderedAllocationP2P.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationP2P/streamOrderedAllocationP2P.cu.hip
@@ -1,255 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * This sample demonstrates peer-to-peer access of stream ordered memory
- * allocated with hipMallocAsync and cudaMemPool family of APIs through simple
- * kernel which does peer-to-peer to access & scales vector elements.
- */
-
-// System includes
-#include <assert.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <iostream>
-#include <map>
-#include <set>
-#include <utility>
-
-// CUDA runtime
-#include <hip/hip_runtime.h>
-
-// helper functions and utilities to work with CUDA
-#include "helper_cuda_hipified.h"
-#include "helper_functions.h"
-
-// Simple kernel to demonstrate copying hipMallocAsync memory via P2P to peer
-// device
-__global__ void copyP2PAndScale(const int *src, int *dst, int N) {
-  int idx = blockIdx.x * blockDim.x + threadIdx.x;
-
-  if (idx < N) {
-    // scale & store src vector.
-    dst[idx] = 2 * src[idx];
-  }
-}
-
-// Map of device version to device number
-std::multimap<std::pair<int, int>, int> getIdenticalGPUs() {
-  int numGpus = 0;
-  HIPCHECK(hipGetDeviceCount(&numGpus));
-
-  std::multimap<std::pair<int, int>, int> identicalGpus;
-
-  for (int i = 0; i < numGpus; i++) {
-    int isMemPoolSupported = 0;
-    HIPCHECK(hipDeviceGetAttribute(&isMemPoolSupported,
-                                           hipDeviceAttributeMemoryPoolsSupported, i));
-
-    // Filter unsupported devices
-    if (isMemPoolSupported) {
-      int major = 0, minor = 0;
-      HIPCHECK(
-          hipDeviceGetAttribute(&major, hipDeviceAttributeComputeCapabilityMajor, i));
-      HIPCHECK(
-          hipDeviceGetAttribute(&minor, hipDeviceAttributeComputeCapabilityMinor, i));
-      identicalGpus.emplace(std::make_pair(major, minor), i);
-    }
-  }
-
-  return identicalGpus;
-}
-
-std::pair<int, int> getP2PCapableGpuPair() {
-  constexpr size_t kNumGpusRequired = 2;
-
-  auto gpusByArch = getIdenticalGPUs();
-
-  auto it = gpusByArch.begin();
-  auto end = gpusByArch.end();
-
-  auto bestFit = std::make_pair(it, it);
-  // use std::distance to find the largest number of GPUs amongst architectures
-  auto distance = [](decltype(bestFit) p) {
-    return std::distance(p.first, p.second);
-  };
-
-  // Read each unique key/pair element in order
-  for (; it != end; it = gpusByArch.upper_bound(it->first)) {
-    // first and second are iterators bounded within the architecture group
-    auto testFit = gpusByArch.equal_range(it->first);
-    // Always use devices with highest architecture version or whichever has the
-    // most devices available
-    if (distance(bestFit) <= distance(testFit)) bestFit = testFit;
-  }
-
-  if (distance(bestFit) < kNumGpusRequired) {
-    printf(
-        "No Two or more GPUs with same architecture capable of cuda Memory "
-        "Pools found."
-        "\nWaiving the sample\n");
-    exit(EXIT_WAIVED);
-  }
-
-  std::set<int> bestFitDeviceIds;
-
-  // check & select peer-to-peer access capable GPU devices.
-  int devIds[2];
-  for (auto itr = bestFit.first; itr != bestFit.second; itr++) {
-    int deviceId = itr->second;
-    HIPCHECK(hipSetDevice(deviceId));
-
-    std::for_each(itr, bestFit.second, [&deviceId, &bestFitDeviceIds,
-                                        &kNumGpusRequired](
-                                           decltype(*itr) mapPair) {
-      if (deviceId != mapPair.second) {
-        int access = 0;
-        HIPCHECK(
-            hipDeviceCanAccessPeer(&access, deviceId, mapPair.second));
-        printf("Device=%d %s Access Peer Device=%d\n", deviceId,
-               access ? "CAN" : "CANNOT", mapPair.second);
-        if (access && bestFitDeviceIds.size() < kNumGpusRequired) {
-          bestFitDeviceIds.emplace(deviceId);
-          bestFitDeviceIds.emplace(mapPair.second);
-        } else {
-          printf("Ignoring device %i (max devices exceeded)\n", mapPair.second);
-        }
-      }
-    });
-
-    if (bestFitDeviceIds.size() >= kNumGpusRequired) {
-      printf("Selected p2p capable devices - ");
-      int i = 0;
-      for (auto devicesItr = bestFitDeviceIds.begin();
-           devicesItr != bestFitDeviceIds.end(); devicesItr++) {
-        devIds[i++] = *devicesItr;
-        printf("deviceId = %d  ", *devicesItr);
-      }
-      printf("\n");
-      break;
-    }
-  }
-
-  // if bestFitDeviceIds.size() == 0 it means the GPUs in system are not p2p
-  // capable, hence we add it without p2p capability check.
-  if (!bestFitDeviceIds.size()) {
-    printf("No Two or more Devices p2p capable found.. exiting..\n");
-    exit(EXIT_WAIVED);
-  }
-
-  auto p2pGpuPair = std::make_pair(devIds[0], devIds[1]);
-
-  return p2pGpuPair;
-}
-
-int memPoolP2PCopy() {
-  int *dev0_srcVec, *dev1_dstVec;  // Device buffers
-  hipStream_t stream1, stream2;
-  hipMemPool_t memPool;
-  hipEvent_t waitOnStream1;
-
-  // Allocate CPU memory.
-  size_t nelem = 1048576;
-  size_t bytes = nelem * sizeof(int);
-
-  int *a = (int *)malloc(bytes);
-  int *output = (int *)malloc(bytes);
-
-  /* Initialize the vectors. */
-  for (int n = 0; n < nelem; n++) {
-    a[n] = rand() / (int)RAND_MAX;
-  }
-
-  auto p2pDevices = getP2PCapableGpuPair();
-  printf("selected devices = %d & %d\n", p2pDevices.first, p2pDevices.second);
-  HIPCHECK(hipSetDevice(p2pDevices.first));
-  HIPCHECK(hipEventCreate(&waitOnStream1));
-
-  HIPCHECK(hipStreamCreateWithFlags(&stream1, hipStreamNonBlocking));
-
-  // Get the default mempool for device p2pDevices.first from the pair
-  HIPCHECK(hipDeviceGetDefaultMemPool(&memPool, p2pDevices.first));
-
-  // Allocate memory in a stream from the pool set above.
-  HIPCHECK(hipMallocAsync(&dev0_srcVec, bytes, stream1));
-
-  HIPCHECK(
-      hipMemcpyAsync(dev0_srcVec, a, bytes, hipMemcpyHostToDevice, stream1));
-  HIPCHECK(hipEventRecord(waitOnStream1, stream1));
-
-  HIPCHECK(hipSetDevice(p2pDevices.second));
-  HIPCHECK(hipStreamCreateWithFlags(&stream2, hipStreamNonBlocking));
-
-  // Allocate memory in p2pDevices.second device
-  HIPCHECK(hipMallocAsync(&dev1_dstVec, bytes, stream2));
-
-  // Setup peer mappings for p2pDevices.second device
-  hipMemAccessDesc desc;
-  memset(&desc, 0, sizeof(hipMemAccessDesc));
-  desc.location.type = hipMemLocationTypeDevice;
-  desc.location.id = p2pDevices.second;
-  desc.flags = hipMemAccessFlagsProtReadWrite;
-  HIPCHECK(hipMemPoolSetAccess(memPool, &desc, 1));
-
-  printf("> copyP2PAndScale kernel running ...\n");
-  dim3 block(256);
-  dim3 grid((unsigned int)ceil(nelem / (int)block.x));
-  HIPCHECK(hipStreamWaitEvent(stream2, waitOnStream1,3));
-  copyP2PAndScale<<<grid, block, 0, stream2>>>(dev0_srcVec, dev1_dstVec, nelem);
-
-  HIPCHECK(hipMemcpyAsync(output, dev1_dstVec, bytes,
-                                  hipMemcpyDeviceToHost, stream2));
-  HIPCHECK(hipFreeAsync(dev0_srcVec, stream2));
-  HIPCHECK(hipFreeAsync(dev1_dstVec, stream2));
-  HIPCHECK(hipStreamSynchronize(stream2));
-
-  /* Compare the results */
-  printf("> Checking the results from copyP2PAndScale() ...\n");
-
-  for (int n = 0; n < nelem; n++) {
-    if ((2 * a[n]) != output[n]) {
-      printf("mismatch i = %d expected = %d val = %d\n", n, 2 * a[n],
-             output[n]);
-      return EXIT_FAILURE;
-    }
-  }
-
-  free(a);
-  free(output);
-  HIPCHECK(hipStreamDestroy(stream1));
-  HIPCHECK(hipStreamDestroy(stream2));
-  printf("PASSED\n");
-
-  return EXIT_SUCCESS;
-}
-
-int main(int argc, char **argv) {
-  int ret = memPoolP2PCopy();
-  return ret;
-}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/threadMigration/threadMigration_kernel.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/threadMigration/threadMigration_kernel.cu.hip
index 42cc050..e69de29 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/threadMigration/threadMigration_kernel.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/threadMigration/threadMigration_kernel.cu.hip
@@ -1,31 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-extern "C" __global__ void kernelFunction(int *input) {
-  input[threadIdx.x] = 32 - threadIdx.x;
-}
diff --git a/src/samples/Samples/3_CUDA_Features/bf16TensorCoreGemm/bf16TensorCoreGemm.cu.hip b/src/samples/Samples/3_CUDA_Features/bf16TensorCoreGemm/bf16TensorCoreGemm.cu.hip
index a387e2e..5719083 100755
--- a/src/samples/Samples/3_CUDA_Features/bf16TensorCoreGemm/bf16TensorCoreGemm.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/bf16TensorCoreGemm/bf16TensorCoreGemm.cu.hip
@@ -651,7 +651,7 @@ int main(int argc, char **argv)
     int dev = findCudaDevice(argc, (const char **)argv);
 
     hipDeviceProp_t deviceProp;
-    HIPCHECK(hipGetDeviceProperties(&deviceProp, dev));
+    checkCudaErrors(hipGetDeviceProperties(&deviceProp, dev));
 
     // Tensor cores require a GPU of Volta (SM8X) architecture or higher.
     if (deviceProp.major < 8) {
@@ -684,10 +684,10 @@ int main(int argc, char **argv)
     float *C = NULL;
     float *D = NULL;
 
-    HIPCHECK(hipMalloc((void**)&A, sizeof(__nv_bfloat16) * M_GLOBAL * K_GLOBAL));
-    HIPCHECK(hipMalloc((void**)&B, sizeof(__nv_bfloat16) * N_GLOBAL * K_GLOBAL));
-    HIPCHECK(hipMalloc((void**)&C, sizeof(float) * M_GLOBAL * N_GLOBAL));
-    HIPCHECK(hipMalloc((void**)&D, sizeof(float) * M_GLOBAL * N_GLOBAL));
+    checkCudaErrors(hipMalloc((void**)&A, sizeof(__nv_bfloat16) * M_GLOBAL * K_GLOBAL));
+    checkCudaErrors(hipMalloc((void**)&B, sizeof(__nv_bfloat16) * N_GLOBAL * K_GLOBAL));
+    checkCudaErrors(hipMalloc((void**)&C, sizeof(float) * M_GLOBAL * N_GLOBAL));
+    checkCudaErrors(hipMalloc((void**)&D, sizeof(float) * M_GLOBAL * N_GLOBAL));
 
     assert(((unsigned long long)A) % 128 == 0);
     assert(((unsigned long long)B) % 128 == 0);
@@ -698,10 +698,10 @@ int main(int argc, char **argv)
 
     printf("Preparing data for GPU...\n");
 
-    HIPCHECK(hipMemcpy(A, A_h, sizeof(__nv_bfloat16) * M_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
-    HIPCHECK(hipMemcpy(B, B_h, sizeof(__nv_bfloat16) * N_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
-    HIPCHECK(hipMemcpy(C, C_h, sizeof(float) * M_GLOBAL * N_GLOBAL, hipMemcpyHostToDevice));
-    HIPCHECK(hipMemset(D, 0, sizeof(float) * M_GLOBAL * N_GLOBAL));
+    checkCudaErrors(hipMemcpy(A, A_h, sizeof(__nv_bfloat16) * M_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
+    checkCudaErrors(hipMemcpy(B, B_h, sizeof(__nv_bfloat16) * N_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
+    checkCudaErrors(hipMemcpy(C, C_h, sizeof(float) * M_GLOBAL * N_GLOBAL, hipMemcpyHostToDevice));
+    checkCudaErrors(hipMemset(D, 0, sizeof(float) * M_GLOBAL * N_GLOBAL));
 
     enum {
         // Compute the right amount of shared memory to request.
@@ -719,9 +719,9 @@ int main(int argc, char **argv)
 
     hipEvent_t start, stop;
 
-    HIPCHECK(hipEventCreate(&start));
-    HIPCHECK(hipEventCreate(&stop));
-    HIPCHECK(hipEventRecord(start));
+    checkCudaErrors(hipEventCreate(&start));
+    checkCudaErrors(hipEventCreate(&stop));
+    checkCudaErrors(hipEventRecord(start));
 
     // kernel to run - default (b16mma_shmem_gemm_async_copy == 0)
     kernels selected_kernel = bf16mma_shmem_gemm_async_copy;
@@ -745,16 +745,16 @@ int main(int argc, char **argv)
         {
             case bf16mma_shmem_gemm_async_copy :
             default:
-                HIPCHECK(hipFuncSetAttribute(compute_bf16gemm_async_copy, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
+                checkCudaErrors(hipFuncSetAttribute(compute_bf16gemm_async_copy, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
                 checkKernelErrors((compute_bf16gemm_async_copy<<<deviceProp.multiProcessorCount*2, THREADS_PER_BLOCK, SHMEM_SZ>>>(A, B, C, D, alpha, beta)));
                 break;
             case bf16mma_shmem_gemm :
-                HIPCHECK(hipFuncSetAttribute(compute_bf16gemm, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
+                checkCudaErrors(hipFuncSetAttribute(compute_bf16gemm, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
                 checkKernelErrors((compute_bf16gemm<<<deviceProp.multiProcessorCount*2, THREADS_PER_BLOCK, SHMEM_SZ>>>(A, B, C, D, alpha, beta)));
                 break;
         }
 #if CPU_DEBUG
-        HIPCHECK(hipMemcpy(result_hD, D, sizeof(float)*M_GLOBAL*N_GLOBAL, hipMemcpyDeviceToHost));
+        checkCudaErrors(hipMemcpy(result_hD, D, sizeof(float)*M_GLOBAL*N_GLOBAL, hipMemcpyDeviceToHost));
 #endif
     }
     else {
@@ -772,12 +772,12 @@ int main(int argc, char **argv)
         printf("Computing... using simple_wmma_gemm kernel\n");
         simple_wmma_bf16gemm<<<gridDim, blockDim>>>(A, B, C, D, M_GLOBAL, N_GLOBAL, K_GLOBAL, alpha, beta);
 #if CPU_DEBUG
-        HIPCHECK(hipMemcpy(result_hD, D, sizeof(float) * M_GLOBAL * N_GLOBAL, hipMemcpyDeviceToHost));
+        checkCudaErrors(hipMemcpy(result_hD, D, sizeof(float) * M_GLOBAL * N_GLOBAL, hipMemcpyDeviceToHost));
 #endif
     }
 
-    HIPCHECK(hipEventRecord(stop));
-    HIPCHECK(hipEventSynchronize(stop));
+    checkCudaErrors(hipEventRecord(stop));
+    checkCudaErrors(hipEventSynchronize(stop));
 
 #if CPU_DEBUG
     printf("Verifying correctness of the computations...\n");
@@ -801,7 +801,7 @@ int main(int argc, char **argv)
 
     float milliseconds = 0;
 
-    HIPCHECK(hipEventElapsedTime(&milliseconds, start, stop));
+    checkCudaErrors(hipEventElapsedTime(&milliseconds, start, stop));
 
     printf("Time: %f ms\n", milliseconds);
     printf("TFLOPS: %.2f\n", (((double)M_GLOBAL * N_GLOBAL * K_GLOBAL * 2)/(milliseconds/1000.)) / 1e12);
diff --git a/src/samples/Samples/3_CUDA_Features/cdpQuadtree/cdpQuadtree.cu.hip b/src/samples/Samples/3_CUDA_Features/cdpQuadtree/cdpQuadtree.cu.hip
index 0bdd297..a4ffd7e 100755
--- a/src/samples/Samples/3_CUDA_Features/cdpQuadtree/cdpQuadtree.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cdpQuadtree/cdpQuadtree.cu.hip
@@ -32,7 +32,7 @@
 #include <hip/hip_cooperative_groups.h>
 
 namespace cg = cooperative_groups;
-#include "helper_cuda_hipified.h"
+#include <helper_cuda.h>
 
 ////////////////////////////////////////////////////////////////////////////////
 // A structure of 2D points (structure of arrays).
@@ -654,8 +654,8 @@ bool cdpQuadtree(int warp_size) {
 
   // Allocate memory to store points.
   Points *points;
-  HIPCHECK(hipMalloc((void **)&points, 2 * sizeof(Points)));
-  HIPCHECK(hipMemcpy(points, points_init, 2 * sizeof(Points),
+  checkCudaErrors(hipMalloc((void **)&points, 2 * sizeof(Points)));
+  checkCudaErrors(hipMemcpy(points, points_init, 2 * sizeof(Points),
                              hipMemcpyHostToDevice));
 
   // We could use a close form...
@@ -669,11 +669,14 @@ bool cdpQuadtree(int warp_size) {
   Quadtree_node root;
   root.set_range(0, num_points);
   Quadtree_node *nodes;
-  HIPCHECK(
+  checkCudaErrors(
       hipMalloc((void **)&nodes, max_nodes * sizeof(Quadtree_node)));
-  HIPCHECK(
+  checkCudaErrors(
       hipMemcpy(nodes, &root, sizeof(Quadtree_node), hipMemcpyHostToDevice));
 
+  // We set the recursion limit for CDP to max_depth.
+  hipDeviceSetLimit(cudaLimitDevRuntimeSyncDepth, max_depth);
+
   // Build the quadtree.
   Parameters params(max_depth, min_points_per_node);
   std::cout << "Launching CDP kernel to build the quadtree" << std::endl;
@@ -683,7 +686,7 @@ bool cdpQuadtree(int warp_size) {
   build_quadtree_kernel<
       NUM_THREADS_PER_BLOCK><<<1, NUM_THREADS_PER_BLOCK, smem_size>>>(
       nodes, points, params);
-  HIPCHECK(hipGetLastError());
+  checkCudaErrors(hipGetLastError());
 
   // Copy points to CPU.
   thrust::host_vector<float> x_h(x_d0);
@@ -694,7 +697,7 @@ bool cdpQuadtree(int warp_size) {
 
   // Copy nodes to CPU.
   Quadtree_node *host_nodes = new Quadtree_node[max_nodes];
-  HIPCHECK(hipMemcpy(host_nodes, nodes,
+  checkCudaErrors(hipMemcpy(host_nodes, nodes,
                              max_nodes * sizeof(Quadtree_node),
                              hipMemcpyDeviceToHost));
 
@@ -706,8 +709,8 @@ bool cdpQuadtree(int warp_size) {
   delete[] host_nodes;
 
   // Free memory.
-  HIPCHECK(hipFree(nodes));
-  HIPCHECK(hipFree(points));
+  checkCudaErrors(hipFree(nodes));
+  checkCudaErrors(hipFree(points));
 
   return ok;
 }
@@ -720,7 +723,7 @@ int main(int argc, char **argv) {
   // The test requires an architecture SM35 or greater (CDP capable).
   int cuda_device = findCudaDevice(argc, (const char **)argv);
   hipDeviceProp_t deviceProps;
-  HIPCHECK(hipGetDeviceProperties(&deviceProps, cuda_device));
+  checkCudaErrors(hipGetDeviceProperties(&deviceProps, cuda_device));
   int cdpCapable = (deviceProps.major == 3 && deviceProps.minor >= 5) ||
                    deviceProps.major >= 4;
 
diff --git a/src/samples/Samples/3_CUDA_Features/globalToShmemAsyncCopy/globalToShmemAsyncCopy.cu.hip b/src/samples/Samples/3_CUDA_Features/globalToShmemAsyncCopy/globalToShmemAsyncCopy.cu.hip
index 01b203f..e50044a 100755
--- a/src/samples/Samples/3_CUDA_Features/globalToShmemAsyncCopy/globalToShmemAsyncCopy.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/globalToShmemAsyncCopy/globalToShmemAsyncCopy.cu.hip
@@ -55,8 +55,8 @@
 namespace cg = cooperative_groups;
 
 // Helper functions and utilities to work with CUDA
-#include "helper_functions.h"
-#include "helper_cuda_hipified.h"
+#include <helper_functions.h>
+#include <helper_cuda.h>
 
 enum kernels {
   AsyncCopyMultiStageLargeChunk = 0,
@@ -762,11 +762,11 @@ int MatrixMultiply(int argc, char **argv, const dim3 &dimsA, const dim3 &dimsB,
   unsigned int size_A = dimsA.x * dimsA.y;
   unsigned int mem_size_A = sizeof(float) * size_A;
   float *h_A;
-  HIPCHECK(hipHostMalloc(&h_A, mem_size_A));
+  checkCudaErrors(hipHostMalloc(&h_A, mem_size_A));
   unsigned int size_B = dimsB.x * dimsB.y;
   unsigned int mem_size_B = sizeof(float) * size_B;
   float *h_B;
-  HIPCHECK(hipHostMalloc(&h_B, mem_size_B));
+  checkCudaErrors(hipHostMalloc(&h_B, mem_size_B));
   hipStream_t stream;
 
   // Initialize host memory
@@ -781,29 +781,29 @@ int MatrixMultiply(int argc, char **argv, const dim3 &dimsA, const dim3 &dimsB,
   dim3 dimsC(dimsB.x, dimsA.y, 1);
   unsigned int mem_size_C = dimsC.x * dimsC.y * sizeof(float);
   float *h_C;
-  HIPCHECK(hipHostMalloc(&h_C, mem_size_C));
+  checkCudaErrors(hipHostMalloc(&h_C, mem_size_C));
 
   if (h_C == NULL) {
     fprintf(stderr, "Failed to allocate host matrix C!\n");
     exit(EXIT_FAILURE);
   }
 
-  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_A), mem_size_A));
-  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_B), mem_size_B));
-  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_C), mem_size_C));
+  checkCudaErrors(hipMalloc(reinterpret_cast<void **>(&d_A), mem_size_A));
+  checkCudaErrors(hipMalloc(reinterpret_cast<void **>(&d_B), mem_size_B));
+  checkCudaErrors(hipMalloc(reinterpret_cast<void **>(&d_C), mem_size_C));
   // Allocate CUDA events that we'll use for timing
   hipEvent_t start, stop;
-  HIPCHECK(hipEventCreate(&start));
-  HIPCHECK(hipEventCreate(&stop));
+  checkCudaErrors(hipEventCreate(&start));
+  checkCudaErrors(hipEventCreate(&stop));
 
-  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
+  checkCudaErrors(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
 
   // copy host memory to device
-  HIPCHECK(
+  checkCudaErrors(
       hipMemcpyAsync(d_A, h_A, mem_size_A, hipMemcpyHostToDevice, stream));
-  HIPCHECK(
+  checkCudaErrors(
       hipMemcpyAsync(d_B, h_B, mem_size_B, hipMemcpyHostToDevice, stream));
-  HIPCHECK(hipMemsetAsync(d_C, 0, mem_size_C, stream));
+  checkCudaErrors(hipMemsetAsync(d_C, 0, mem_size_C, stream));
 
   // Setup execution parameters
   dim3 threads(blockSize, blockSize);
@@ -861,13 +861,13 @@ int MatrixMultiply(int argc, char **argv, const dim3 &dimsA, const dim3 &dimsB,
   }
 
   printf("done\n");
-  HIPCHECK(hipStreamSynchronize(stream));
+  checkCudaErrors(hipStreamSynchronize(stream));
 
   // Execute the kernel
   int nIter = 100;
 
   // Record the start event
-  HIPCHECK(hipEventRecord(start, stream));
+  checkCudaErrors(hipEventRecord(start, stream));
 
   for (int j = 0; j < nIter; j++) {
     switch (kernel_number) {
@@ -911,13 +911,13 @@ int MatrixMultiply(int argc, char **argv, const dim3 &dimsA, const dim3 &dimsB,
   }
 
   // Record the stop event
-  HIPCHECK(hipEventRecord(stop, stream));
+  checkCudaErrors(hipEventRecord(stop, stream));
 
   // Wait for the stop event to complete
-  HIPCHECK(hipEventSynchronize(stop));
+  checkCudaErrors(hipEventSynchronize(stop));
 
   float msecTotal = 0.0f;
-  HIPCHECK(hipEventElapsedTime(&msecTotal, start, stop));
+  checkCudaErrors(hipEventElapsedTime(&msecTotal, start, stop));
 
   // Compute and print the performance
   float msecPerMatrixMul = msecTotal / nIter;
@@ -932,9 +932,9 @@ int MatrixMultiply(int argc, char **argv, const dim3 &dimsA, const dim3 &dimsB,
       gigaFlops, msecPerMatrixMul, flopsPerMatrixMul, threads.x * threads.y);
 
   // Copy result from device to host
-  HIPCHECK(
+  checkCudaErrors(
       hipMemcpyAsync(h_C, d_C, mem_size_C, hipMemcpyDeviceToHost, stream));
-  HIPCHECK(hipStreamSynchronize(stream));
+  checkCudaErrors(hipStreamSynchronize(stream));
 
   printf("Checking computed result for correctness: ");
   bool correct = true;
@@ -959,14 +959,14 @@ int MatrixMultiply(int argc, char **argv, const dim3 &dimsA, const dim3 &dimsB,
   printf("%s\n", correct ? "Result = PASS" : "Result = FAIL");
 
   // Clean up memory
-  HIPCHECK(hipHostFree(h_A));
-  HIPCHECK(hipHostFree(h_B));
-  HIPCHECK(hipHostFree(h_C));
-  HIPCHECK(hipFree(d_A));
-  HIPCHECK(hipFree(d_B));
-  HIPCHECK(hipFree(d_C));
-  HIPCHECK(hipEventDestroy(start));
-  HIPCHECK(hipEventDestroy(stop));
+  checkCudaErrors(hipHostFree(h_A));
+  checkCudaErrors(hipHostFree(h_B));
+  checkCudaErrors(hipHostFree(h_C));
+  checkCudaErrors(hipFree(d_A));
+  checkCudaErrors(hipFree(d_B));
+  checkCudaErrors(hipFree(d_C));
+  checkCudaErrors(hipEventDestroy(start));
+  checkCudaErrors(hipEventDestroy(stop));
   printf(
       "\nNOTE: The CUDA Samples are not meant for performance "
       "measurements. Results may vary when GPU Boost is enabled.\n");
@@ -1056,7 +1056,7 @@ int main(int argc, char **argv) {
   }
 
   int major = 0;
-  HIPCHECK(
+  checkCudaErrors(
       hipDeviceGetAttribute(&major, hipDeviceAttributeComputeCapabilityMajor, dev));
   if (major < 7) {
     printf("globalToShmemAsyncCopy requires SM 7.0 or higher.  Exiting...\n");
@@ -1084,10 +1084,3 @@ rintf("MatrixA(%d,%d), MatrixB(%d,%d)\n", dimsA.x, dimsA.y, dimsB.x,
 
   exit(matrix_result);
 }
-rintf("MatrixA(%d,%d), MatrixB(%d,%d)\n", dimsA.x, dimsA.y, dimsB.x,
-         dimsB.y);
-
-  int matrix_result = MatrixMultiply(argc, argv, dimsA, dimsB, selected_kernel);
-
-  exit(matrix_result);
-}
diff --git a/src/samples/Samples/3_CUDA_Features/graphMemoryNodes/graphMemoryNodes.cu.hip b/src/samples/Samples/3_CUDA_Features/graphMemoryNodes/graphMemoryNodes.cu.hip
index d226ff8..e69de29 100755
--- a/src/samples/Samples/3_CUDA_Features/graphMemoryNodes/graphMemoryNodes.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/graphMemoryNodes/graphMemoryNodes.cu.hip
@@ -1,568 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-// System includes
-#include <assert.h>
-#include <stdio.h>
-
-#include <climits>
-#include <vector>
-
-// CUDA runtime
-#include <hip/hip_runtime.h>
-
-// helper functions and utilities to work with CUDA
-#include "helper_cuda_hipified.h"
-#include "helper_functions.h"
-
-#define THREADS_PER_BLOCK 512
-#define ALLOWABLE_VARIANCE 1.e-6f
-#define NUM_ELEMENTS 8000000
-
-// Stores the square of each input element in output array
-__global__ void squareArray(const float *input, float *output,
-                            int numElements) {
-  int idx = blockIdx.x * blockDim.x + threadIdx.x;
-
-  if (idx < numElements) {
-    output[idx] = input[idx] * input[idx];
-  }
-}
-
-// Stores the negative of each input element in output array
-__global__ void negateArray(const float *input, float *output,
-                            int numElements) {
-  int idx = blockIdx.x * blockDim.x + threadIdx.x;
-
-  if (idx < numElements) {
-    output[idx] = input[idx] * -1;
-  }
-}
-
-struct negSquareArrays {
-  float *input;
-  float *square;
-  float *negSquare;
-  int numElements;
-  size_t bytes;
-  size_t numBlocks;
-};
-
-void fillRandomly(float *array, int numElements) {
-  for (int n = 0; n < numElements; n++) {
-    array[n] = rand() / (float)RAND_MAX;
-  }
-}
-
-void resetOutputArrays(negSquareArrays *hostArrays) {
-  fillRandomly(hostArrays->square, hostArrays->numElements);
-  fillRandomly(hostArrays->negSquare, hostArrays->numElements);
-}
-
-void prepareHostArrays(negSquareArrays *hostArrays) {
-  hostArrays->numElements = NUM_ELEMENTS;
-  size_t bytes = hostArrays->numElements * sizeof(float);
-
-  size_t numBlocks = hostArrays->numElements / (size_t)THREADS_PER_BLOCK;
-  if ((numBlocks % (size_t)THREADS_PER_BLOCK) != 0) {
-    numBlocks++;
-  }
-
-  hostArrays->input = (float *)malloc(bytes);
-  hostArrays->square = (float *)malloc(bytes);
-  hostArrays->negSquare = (float *)malloc(bytes);
-  hostArrays->bytes = bytes;
-  hostArrays->numBlocks = numBlocks;
-
-  fillRandomly(hostArrays->input, hostArrays->numElements);
-  fillRandomly(hostArrays->square, hostArrays->numElements);
-  fillRandomly(hostArrays->negSquare, hostArrays->numElements);
-}
-
-void createFreeGraph(hipGraphExec_t *graphExec, float *dPtr) {
-  hipGraph_t graph;
-  hipGraphNode_t freeNode;
-
-  HIPCHECK(hipGraphCreate(&graph, 0));
-
-  HIPCHECK(
-      cudaGraphAddMemFreeNode(&freeNode, graph, NULL, 0, (void *)dPtr));
-
-  HIPCHECK(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
-  HIPCHECK(hipGraphDestroy(graph));
-}
-
-/**
- * Demonstrates explicitly creating a CUDA graph including memory nodes.
- * createNegateSquaresGraphWithStreamCapture constructs an equivalent graph
- * using stream capture.
- *
- * If d_negSquare_out is non null, then:
- * 1) d_negSquare will not be freed;
- * 2) the value of d_negSquare_out will be set to d_negSquare.
- *
- * Diagram of the graph constructed by createNegateSquaresGraphExplicitly:
- *
- * alloc d_input
- *       |
- * alloc d_square
- *       |
- * Memcpy a to device
- *       |
- * launch kernel squareArray ------->---- Memcpy d_square to host
- *       |                                      |
- * free d_input                                 |
- *       |                                      |
- * allocate d_negSquare                         |
- *       |                                      |
- * launch kernel negateArray -------->--- free d_square
- *       |
- * Memcpy d_negSquare to host
- *       |
- * free d_negSquare
- */
-void createNegateSquaresGraphExplicitly(hipGraphExec_t *graphExec, int device,
-                                        negSquareArrays *hostArrays,
-                                        float **d_negSquare_out = NULL) {
-  // Array buffers on device
-  float *d_input, *d_square, *d_negSquare;
-
-  // Memory allocation parameters
-  cudaMemAllocNodeParams allocParams;
-  memset(&allocParams, 0, sizeof(allocParams));
-  allocParams.bytesize = hostArrays->bytes;
-  allocParams.poolProps.allocType = hipMemAllocationTypePinned;
-  allocParams.poolProps.location.id = device;
-  allocParams.poolProps.location.type = hipMemLocationTypeDevice;
-
-  // Kernel launch parameters
-  hipKernelNodeParams kernelNodeParams = {0};
-  kernelNodeParams.gridDim = dim3(hostArrays->numBlocks, 1, 1);
-  kernelNodeParams.blockDim = dim3(THREADS_PER_BLOCK, 1, 1);
-  kernelNodeParams.sharedMemBytes = 0;
-  kernelNodeParams.extra = NULL;
-
-  hipGraph_t graph;
-  hipGraphNode_t allocNodeInput, allocNodeSquare, allocNodeNegSquare;
-  hipGraphNode_t copyNodeInput, copyNodeSquare, copyNodeNegSquare;
-  hipGraphNode_t squareKernelNode, negateKernelNode;
-  hipGraphNode_t freeNodeInput, freeNodeSquare;
-
-  // Buffer for storing graph node dependencies
-  std::vector<hipGraphNode_t> nodeDependencies;
-
-  HIPCHECK(hipGraphCreate(&graph, 0));
-
-  HIPCHECK(
-      cudaGraphAddMemAllocNode(&allocNodeInput, graph, NULL, 0, &allocParams));
-  d_input = (float *)allocParams.dptr;
-
-  // To keep the graph structure simple (fewer branching dependencies),
-  // allocNodeSquare should depend on allocNodeInput
-  HIPCHECK(cudaGraphAddMemAllocNode(&allocNodeSquare, graph,
-                                           &allocNodeInput, 1, &allocParams));
-  d_square = (float *)allocParams.dptr;
-
-  // copyNodeInput needs to depend on allocNodeInput because copyNodeInput
-  // writes to d_input. It does so here indirectly through allocNodeSquare.
-  HIPCHECK(hipGraphAddMemcpyNode1D(
-      &copyNodeInput, graph, &allocNodeSquare, 1, d_input, hostArrays->input,
-      hostArrays->bytes, hipMemcpyHostToDevice));
-
-  void *squareKernelArgs[3] = {(void *)&d_input, (void *)&d_square,
-                               (void *)&(hostArrays->numElements)};
-  kernelNodeParams.func = (void *)squareArray;
-  kernelNodeParams.kernelParams = (void **)squareKernelArgs;
-
-  // Square kernel depends on copyNodeInput to ensure all data is on the device
-  // before kernel launch.
-  HIPCHECK(hipGraphAddKernelNode(&squareKernelNode, graph,
-                                         &copyNodeInput, 1, &kernelNodeParams));
-
-  HIPCHECK(hipGraphAddMemcpyNode1D(
-      &copyNodeSquare, graph, &squareKernelNode, 1, hostArrays->square,
-      d_square, hostArrays->bytes, hipMemcpyDeviceToHost));
-
-  // Free of d_input depends on the square kernel to ensure that d_input is not
-  // freed while being read by the kernel. It also depends on the alloc of
-  // d_input via squareKernelNode > copyNodeInput > allocNodeSquare >
-  // allocNodeInput.
-  HIPCHECK(cudaGraphAddMemFreeNode(&freeNodeInput, graph,
-                                          &squareKernelNode, 1, d_input));
-
-  // Allocation of C depends on free of A so CUDA can reuse the virtual address.
-  HIPCHECK(cudaGraphAddMemAllocNode(&allocNodeNegSquare, graph,
-                                           &freeNodeInput, 1, &allocParams));
-  d_negSquare = (float *)allocParams.dptr;
-
-  if (d_negSquare == d_input) {
-    printf(
-        "Check verified that d_negSquare and d_input share a virtual "
-        "address.\n");
-  }
-
-  void *negateKernelArgs[3] = {(void *)&d_square, (void *)&d_negSquare,
-                               (void *)&(hostArrays->numElements)};
-  kernelNodeParams.func = (void *)negateArray;
-  kernelNodeParams.kernelParams = (void **)negateKernelArgs;
-
-  HIPCHECK(hipGraphAddKernelNode(
-      &negateKernelNode, graph, &allocNodeNegSquare, 1, &kernelNodeParams));
-
-  nodeDependencies.push_back(copyNodeSquare);
-  nodeDependencies.push_back(negateKernelNode);
-  HIPCHECK(cudaGraphAddMemFreeNode(&freeNodeSquare, graph,
-                                          nodeDependencies.data(),
-                                          nodeDependencies.size(), d_square));
-  nodeDependencies.clear();
-
-  HIPCHECK(hipGraphAddMemcpyNode1D(
-      &copyNodeNegSquare, graph, &negateKernelNode, 1, hostArrays->negSquare,
-      d_negSquare, hostArrays->bytes, hipMemcpyDeviceToHost));
-
-  if (d_negSquare_out == NULL) {
-    hipGraphNode_t freeNodeNegSquare;
-    HIPCHECK(cudaGraphAddMemFreeNode(
-        &freeNodeNegSquare, graph, &copyNodeNegSquare, 1, d_negSquare));
-  } else {
-    *d_negSquare_out = d_negSquare;
-  }
-
-  HIPCHECK(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
-  HIPCHECK(hipGraphDestroy(graph));
-}
-
-/**
- * Adds work to a CUDA stream which negates the square of values in the input
- * array.
- *
- * If d_negSquare_out is non null, then:
- * 1) d_negSquare will not be freed;
- * 2) the value of d_negSquare_out will be set to d_negSquare.
- *
- * Diagram of the stream operations in doNegateSquaresInStream
- * ---------------------------------------------------------------------
- * | STREAM                             | STREAM2                      |
- * ---------------------------------------------------------------------
- *
- * alloc d_input
- *       |
- * alloc d_square
- *       |
- * Memcpy a to device
- *       |
- * launch kernel squareArray
- *       |
- * record squareKernelCompleteEvent -->-- wait squareKernelCompleteEvent
- *       |                                      |
- * free d_input                                 |
- *       |                                      |
- * allocate d_negSquare                   Memcpy d_square to host
- *       |                                      |
- * launch kernel negateArray                    |
- *       |                                      |
- * record negateKernelCompleteEvent -->-- wait negateKernelCompleteEvent
- *       |                                      |
- * Memcpy d_negSquare to host                   |
- *       |                                free d_square
- * free d_negSquare                             |
- *       |                                      |
- * wait squareFreeEvent --------------<---- record squareFreeEvent
- */
-void doNegateSquaresInStream(hipStream_t stream1, negSquareArrays *hostArrays,
-                             float **d_negSquare_out = NULL) {
-  float *d_input, *d_square, *d_negSquare;
-  hipStream_t stream2;
-  hipEvent_t squareKernelCompleteEvent, negateKernelCompleteEvent,
-      squareFreeEvent;
-
-  HIPCHECK(hipStreamCreateWithFlags(&stream2, hipStreamNonBlocking));
-
-  HIPCHECK(hipEventCreate(&squareKernelCompleteEvent));
-  HIPCHECK(hipEventCreate(&negateKernelCompleteEvent));
-  HIPCHECK(hipEventCreate(&squareFreeEvent));
-
-  // Virtual addresses are assigned synchronously when hipMallocAsync is
-  // called, thus there is no performace benefit gained by separating the
-  // allocations into two streams.
-  HIPCHECK(hipMallocAsync(&d_input, hostArrays->bytes, stream1));
-  HIPCHECK(hipMallocAsync(&d_square, hostArrays->bytes, stream1));
-
-  HIPCHECK(hipMemcpyAsync(d_input, hostArrays->input, hostArrays->bytes,
-                                  hipMemcpyHostToDevice, stream1));
-  squareArray<<<hostArrays->numBlocks, THREADS_PER_BLOCK, 0, stream1>>>(
-      d_input, d_square, hostArrays->numElements);
-  HIPCHECK(hipEventRecord(squareKernelCompleteEvent, stream1));
-
-  HIPCHECK(hipStreamWaitEvent(stream2, squareKernelCompleteEvent, 0));
-  HIPCHECK(hipMemcpyAsync(hostArrays->square, d_square,
-                                  hostArrays->bytes, hipMemcpyDeviceToHost,
-                                  stream2));
-
-  HIPCHECK(hipFreeAsync(d_input, stream1));
-  HIPCHECK(hipMallocAsync(&d_negSquare, hostArrays->bytes, stream1));
-  negateArray<<<hostArrays->numBlocks, THREADS_PER_BLOCK, 0, stream1>>>(
-      d_square, d_negSquare, hostArrays->numElements);
-  HIPCHECK(hipEventRecord(negateKernelCompleteEvent, stream1));
-  HIPCHECK(hipMemcpyAsync(hostArrays->negSquare, d_negSquare,
-                                  hostArrays->bytes, hipMemcpyDeviceToHost,
-                                  stream1));
-  if (d_negSquare_out == NULL) {
-    HIPCHECK(hipFreeAsync(d_negSquare, stream1));
-  } else {
-    *d_negSquare_out = d_negSquare;
-  }
-
-  HIPCHECK(hipStreamWaitEvent(stream2, negateKernelCompleteEvent, 0));
-  HIPCHECK(hipFreeAsync(d_square, stream2));
-  HIPCHECK(hipEventRecord(squareFreeEvent, stream2));
-
-  HIPCHECK(hipStreamWaitEvent(stream1, squareFreeEvent, 0));
-
-  HIPCHECK(hipStreamDestroy(stream2));
-  HIPCHECK(hipEventDestroy(squareKernelCompleteEvent));
-  HIPCHECK(hipEventDestroy(negateKernelCompleteEvent));
-  HIPCHECK(hipEventDestroy(squareFreeEvent));
-}
-
-/**
- * Demonstrates creating a CUDA graph including memory nodes using stream
- * capture. createNegateSquaresGraphExplicitly constructs an equivalent graph
- * without stream capture.
- */
-void createNegateSquaresGraphWithStreamCapture(hipGraphExec_t *graphExec,
-                                               negSquareArrays *hostArrays,
-                                               float **d_negSquare_out = NULL) {
-  hipGraph_t graph;
-  hipStream_t stream;
-
-  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
-
-  HIPCHECK(hipStreamBeginCapture(stream, hipStreamCaptureModeGlobal));
-  doNegateSquaresInStream(stream, hostArrays, d_negSquare_out);
-  HIPCHECK(hipStreamEndCapture(stream, &graph));
-
-  HIPCHECK(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
-  HIPCHECK(hipStreamDestroy(stream));
-  HIPCHECK(hipGraphDestroy(graph));
-}
-
-void prepareRefArrays(negSquareArrays *hostArrays,
-                      negSquareArrays *deviceRefArrays,
-                      bool **foundValidationFailure) {
-  deviceRefArrays->bytes = hostArrays->bytes;
-  deviceRefArrays->numElements = hostArrays->numElements;
-
-  for (int i = 0; i < hostArrays->numElements; i++) {
-    hostArrays->square[i] = hostArrays->input[i] * hostArrays->input[i];
-    hostArrays->negSquare[i] = hostArrays->square[i] * -1;
-  }
-
-  HIPCHECK(
-      hipMalloc((void **)&deviceRefArrays->negSquare, deviceRefArrays->bytes));
-  HIPCHECK(hipMemcpy(deviceRefArrays->negSquare, hostArrays->negSquare,
-                             hostArrays->bytes, hipMemcpyHostToDevice));
-
-  HIPCHECK(
-      hipMallocManaged((void **)foundValidationFailure, sizeof(bool)));
-}
-
-int checkValidationFailure(bool *foundValidationFailure) {
-  if (*foundValidationFailure) {
-    printf("Validation FAILURE!\n\n");
-    *foundValidationFailure = false;
-    return EXIT_FAILURE;
-  } else {
-    printf("Validation PASSED!\n\n");
-    return EXIT_SUCCESS;
-  }
-}
-
-__global__ void validateGPU(float *d_negSquare, negSquareArrays devRefArrays,
-                            bool *foundValidationFailure) {
-  int idx = blockIdx.x * blockDim.x + threadIdx.x;
-  float ref, diff;
-
-  if (idx < devRefArrays.numElements) {
-    ref = devRefArrays.negSquare[idx];
-    diff = d_negSquare[idx] - ref;
-    diff *= diff;
-    ref *= ref;
-    if (diff / ref > ALLOWABLE_VARIANCE) {
-      *foundValidationFailure = true;
-    }
-  }
-}
-
-void validateHost(negSquareArrays *hostArrays, bool *foundValidationFailure) {
-  float ref, diff;
-
-  for (int i = 0; i < hostArrays->numElements; i++) {
-    ref = hostArrays->input[i] * hostArrays->input[i] * -1;
-    diff = hostArrays->negSquare[i] - ref;
-    diff *= diff;
-    ref *= ref;
-    if (diff / ref > ALLOWABLE_VARIANCE) {
-      *foundValidationFailure = true;
-    }
-  }
-}
-
-int main(int argc, char **argv) {
-  negSquareArrays hostArrays, deviceRefArrays;
-  hipStream_t stream;
-  hipGraphExec_t graphExec, graphExecFreeC;
-
-  // Declare pointers for GPU buffers
-  float *d_negSquare = NULL;
-  bool *foundValidationFailure = NULL;
-
-  srand(time(0));
-  int device = findCudaDevice(argc, (const char **)argv);
-
-  int driverVersion = 0;
-  int deviceSupportsMemoryPools = 0;
-
-  hipDriverGetVersion(&driverVersion);
-  printf("Driver version is: %d.%d\n", driverVersion / 1000,
-         (driverVersion % 100) / 10);
-
-  if (driverVersion < 11040) {
-    printf("Waiving execution as driver does not support Graph Memory Nodes\n");
-    exit(EXIT_WAIVED);
-  }
-
-  hipDeviceGetAttribute(&deviceSupportsMemoryPools,
-                         hipDeviceAttributeMemoryPoolsSupported, device);
-  if (!deviceSupportsMemoryPools) {
-    printf("Waiving execution as device does not support Memory Pools\n");
-    exit(EXIT_WAIVED);
-  } else {
-    printf("Setting up sample.\n");
-  }
-
-  prepareHostArrays(&hostArrays);
-  prepareRefArrays(&hostArrays, &deviceRefArrays, &foundValidationFailure);
-  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
-  printf("Setup complete.\n\n");
-
-  printf("Running negateSquares in a stream.\n");
-  doNegateSquaresInStream(stream, &hostArrays);
-  HIPCHECK(hipStreamSynchronize(stream));
-  printf("Validating negateSquares in a stream...\n");
-  validateHost(&hostArrays, foundValidationFailure);
-  checkValidationFailure(foundValidationFailure);
-  resetOutputArrays(&hostArrays);
-
-  printf("Running negateSquares in a stream-captured graph.\n");
-  createNegateSquaresGraphWithStreamCapture(&graphExec, &hostArrays);
-  HIPCHECK(hipGraphLaunch(graphExec, stream));
-  HIPCHECK(hipStreamSynchronize(stream));
-  printf("Validating negateSquares in a stream-captured graph...\n");
-  validateHost(&hostArrays, foundValidationFailure);
-  checkValidationFailure(foundValidationFailure);
-  resetOutputArrays(&hostArrays);
-
-  printf("Running negateSquares in an explicitly constructed graph.\n");
-  createNegateSquaresGraphExplicitly(&graphExec, device, &hostArrays);
-  HIPCHECK(hipGraphLaunch(graphExec, stream));
-  HIPCHECK(hipStreamSynchronize(stream));
-  printf("Validating negateSquares in an explicitly constructed graph...\n");
-  validateHost(&hostArrays, foundValidationFailure);
-  checkValidationFailure(foundValidationFailure);
-  resetOutputArrays(&hostArrays);
-
-  // Each of the three examples below free d_negSquare outside the graph. As
-  // demonstrated by validateGPU, d_negSquare can be accessed by outside the
-  // graph before d_negSquare is freed.
-
-  printf("Running negateSquares with d_negSquare freed outside the stream.\n");
-  createNegateSquaresGraphExplicitly(&graphExec, device, &hostArrays,
-                                     &d_negSquare);
-  HIPCHECK(hipGraphLaunch(graphExec, stream));
-  validateGPU<<<hostArrays.numBlocks, THREADS_PER_BLOCK, 0, stream>>>(
-      d_negSquare, deviceRefArrays, foundValidationFailure);
-  // Since hipFree is synchronous, the stream must synchronize before freeing
-  // d_negSquare to ensure d_negSquare no longer being accessed.
-  HIPCHECK(hipStreamSynchronize(stream));
-  HIPCHECK(hipFree(d_negSquare));
-  printf(
-      "Validating negateSquares with d_negSquare freed outside the "
-      "stream...\n");
-  validateHost(&hostArrays, foundValidationFailure);
-  checkValidationFailure(foundValidationFailure);
-  resetOutputArrays(&hostArrays);
-
-  printf("Running negateSquares with d_negSquare freed outside the graph.\n");
-  HIPCHECK(hipGraphLaunch(graphExec, stream));
-  validateGPU<<<hostArrays.numBlocks, THREADS_PER_BLOCK, 0, stream>>>(
-      d_negSquare, deviceRefArrays, foundValidationFailure);
-  HIPCHECK(hipFreeAsync(d_negSquare, stream));
-  HIPCHECK(hipStreamSynchronize(stream));
-  printf(
-      "Validating negateSquares with d_negSquare freed outside the graph...\n");
-  checkValidationFailure(foundValidationFailure);
-  resetOutputArrays(&hostArrays);
-
-  printf(
-      "Running negateSquares with d_negSquare freed in a different graph.\n");
-  createFreeGraph(&graphExecFreeC, d_negSquare);
-  HIPCHECK(hipGraphLaunch(graphExec, stream));
-  validateGPU<<<hostArrays.numBlocks, THREADS_PER_BLOCK, 0, stream>>>(
-      d_negSquare, deviceRefArrays, foundValidationFailure);
-  HIPCHECK(hipGraphLaunch(graphExecFreeC, stream));
-  HIPCHECK(hipStreamSynchronize(stream));
-  printf(
-      "Validating negateSquares with d_negSquare freed in a different "
-      "graph...\n");
-  checkValidationFailure(foundValidationFailure);
-
-  printf("Cleaning up sample.\n");
-  HIPCHECK(hipGraphExecDestroy(graphExec));
-  HIPCHECK(hipGraphExecDestroy(graphExecFreeC));
-  HIPCHECK(hipStreamDestroy(stream));
-  HIPCHECK(hipFree(foundValidationFailure));
-  HIPCHECK(hipFree(deviceRefArrays.negSquare));
-  free(hostArrays.input);
-  free(hostArrays.square);
-  free(hostArrays.negSquare);
-  printf("Cleanup complete. Exiting sample.\n");
-}aph...\n");
-  checkValidationFailure(foundValidationFailure);
-
-  printf("Cleaning up sample.\n");
-  checkCudaErrors(hipGraphExecDestroy(graphExec));
-  checkCudaErrors(hipGraphExecDestroy(graphExecFreeC));
-  checkCudaErrors(hipStreamDestroy(stream));
-  checkCudaErrors(hipFree(foundValidationFailure));
-  checkCudaErrors(hipFree(deviceRefArrays.negSquare));
-  free(hostArrays.input);
-  free(hostArrays.square);
-  free(hostArrays.negSquare);
-  printf("Cleanup complete. Exiting sample.\n");
-}
\ No newline at end of file
diff --git a/src/samples/Samples/3_CUDA_Features/memMapIPCDrv/memMapIpc_kernel.cu.hip b/src/samples/Samples/3_CUDA_Features/memMapIPCDrv/memMapIpc_kernel.cu.hip
index 41b1784..e69de29 100755
--- a/src/samples/Samples/3_CUDA_Features/memMapIPCDrv/memMapIpc_kernel.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/memMapIPCDrv/memMapIpc_kernel.cu.hip
@@ -1,38 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-// Device code
-extern "C" __global__ void memMapIpc_kernel(char *ptr, int sz, char val)
-{
-    // Dummy kernel
-    int idx = blockIdx.x * blockDim.x + threadIdx.x;
-    for (; idx < sz; idx += (gridDim.x * blockDim.x)) {
-        ptr[idx] = val;
-    }
-}
diff --git a/src/samples/Samples/3_CUDA_Features/tf32TensorCoreGemm/tf32TensorCoreGemm.cu.hip b/src/samples/Samples/3_CUDA_Features/tf32TensorCoreGemm/tf32TensorCoreGemm.cu.hip
index c0bb314..0f654a9 100755
--- a/src/samples/Samples/3_CUDA_Features/tf32TensorCoreGemm/tf32TensorCoreGemm.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/tf32TensorCoreGemm/tf32TensorCoreGemm.cu.hip
@@ -836,10 +836,3 @@ int main(int argc, char **argv)
 
     return 0;
 }
-ors(hipFree((void*)A));
-    checkCudaErrors(hipFree((void*)B));
-    checkCudaErrors(hipFree((void*)C));
-    checkCudaErrors(hipFree((void*)D));
-
-    return 0;
-}
diff --git a/src/samples/Samples/3_CUDA_Features/warpAggregatedAtomicsCG/warpAggregatedAtomicsCG.cu.hip b/src/samples/Samples/3_CUDA_Features/warpAggregatedAtomicsCG/warpAggregatedAtomicsCG.cu.hip
index 6f02e39..c3ab431 100755
--- a/src/samples/Samples/3_CUDA_Features/warpAggregatedAtomicsCG/warpAggregatedAtomicsCG.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/warpAggregatedAtomicsCG/warpAggregatedAtomicsCG.cu.hip
@@ -28,8 +28,8 @@
 
 #include <stdio.h>
 // includes, project
-#include "helper_cuda_hipified.h"
-#include "helper_functions.h"
+#include <helper_cuda.h>
+#include <helper_functions.h>
 
 #include <hip/hip_runtime.h>
 
@@ -118,17 +118,17 @@ int mapIndicesToBuckets(int *h_srcArr, int *d_srcArr, int numOfBuckets)
     h_bucketCounters[i] = i*NUM_ELEMS;
   }
 
-  HIPCHECK(hipMalloc(&d_indicesBuckets, sizeof(int) * NUM_ELEMS * numOfBuckets));
-  HIPCHECK(hipMalloc(&d_bucketCounters, sizeof(int) * numOfBuckets));
+  checkCudaErrors(hipMalloc(&d_indicesBuckets, sizeof(int) * NUM_ELEMS * numOfBuckets));
+  checkCudaErrors(hipMalloc(&d_bucketCounters, sizeof(int) * numOfBuckets));
 
-  HIPCHECK(hipMemcpy(d_bucketCounters, h_bucketCounters, sizeof(int)*numOfBuckets, hipMemcpyHostToDevice));
+  checkCudaErrors(hipMemcpy(d_bucketCounters, h_bucketCounters, sizeof(int)*numOfBuckets, hipMemcpyHostToDevice));
 
   dim3 dimBlock(NUM_THREADS_PER_BLOCK, 1, 1);
   dim3 dimGrid((NUM_ELEMS / NUM_THREADS_PER_BLOCK), 1, 1);
 
   mapToBuckets<<<dimGrid, dimBlock>>>(d_srcArr, d_indicesBuckets, d_bucketCounters, NUM_ELEMS, numOfBuckets);
 
-  HIPCHECK(hipMemcpy(h_bucketCounters, d_bucketCounters, sizeof(int)*numOfBuckets, hipMemcpyDeviceToHost));
+  checkCudaErrors(hipMemcpy(h_bucketCounters, d_bucketCounters, sizeof(int)*numOfBuckets, hipMemcpyDeviceToHost));
 
   for (int i=0; i < NUM_ELEMS; i++)
   {
@@ -204,18 +204,18 @@ int calculateMaxInBuckets(int *h_srcArr, int *d_srcArr, int numOfBuckets)
     h_valueInBuckets[i] = rand();
   }
 
-  HIPCHECK(hipMalloc(&d_valueInBuckets, sizeof(int) * NUM_ELEMS));
-  HIPCHECK(hipMalloc(&d_bucketsMax, sizeof(int) * numOfBuckets));
+  checkCudaErrors(hipMalloc(&d_valueInBuckets, sizeof(int) * NUM_ELEMS));
+  checkCudaErrors(hipMalloc(&d_bucketsMax, sizeof(int) * numOfBuckets));
 
-  HIPCHECK(hipMemset(d_bucketsMax, 0, sizeof(int) * numOfBuckets));
-  HIPCHECK(hipMemcpy(d_valueInBuckets, h_valueInBuckets, sizeof(int) * NUM_ELEMS, hipMemcpyHostToDevice));
+  checkCudaErrors(hipMemset(d_bucketsMax, 0, sizeof(int) * numOfBuckets));
+  checkCudaErrors(hipMemcpy(d_valueInBuckets, h_valueInBuckets, sizeof(int) * NUM_ELEMS, hipMemcpyHostToDevice));
 
   dim3 dimBlock(NUM_THREADS_PER_BLOCK, 1, 1);
   dim3 dimGrid((NUM_ELEMS / NUM_THREADS_PER_BLOCK), 1, 1);
 
   calculateMaxInEachBuckets<<<dimGrid, dimBlock>>>(d_srcArr, d_valueInBuckets, d_bucketsMax, NUM_ELEMS, numOfBuckets);
 
-  HIPCHECK(hipMemcpy(h_bucketsMax, d_bucketsMax, sizeof(int) * numOfBuckets, hipMemcpyDeviceToHost));
+  checkCudaErrors(hipMemcpy(h_bucketsMax, d_bucketsMax, sizeof(int) * numOfBuckets, hipMemcpyDeviceToHost));
 
   for (int i = 0; i < NUM_ELEMS; i++)
   {
@@ -244,8 +244,8 @@ int calculateMaxInBuckets(int *h_srcArr, int *d_srcArr, int numOfBuckets)
   delete[] h_valueInBuckets;
   delete[] cpuBucketsMax;
   delete[] h_bucketsMax;
-  HIPCHECK(hipFree(d_valueInBuckets));
-  HIPCHECK(hipFree(d_bucketsMax));
+  checkCudaErrors(hipFree(d_valueInBuckets));
+  checkCudaErrors(hipFree(d_bucketsMax));
 
   if (!allMatch && finalElems != NUM_ELEMS)
   {
@@ -270,13 +270,13 @@ int main(int argc, char **argv) {
 
   int devId = findCudaDevice(argc, (const char **)argv);
 
-  HIPCHECK(hipMalloc(&d_data_to_filter, sizeof(int) * NUM_ELEMS));
-  HIPCHECK(hipMalloc(&d_filtered_data, sizeof(int) * NUM_ELEMS));
-  HIPCHECK(hipMalloc(&d_nres, sizeof(int)));
+  checkCudaErrors(hipMalloc(&d_data_to_filter, sizeof(int) * NUM_ELEMS));
+  checkCudaErrors(hipMalloc(&d_filtered_data, sizeof(int) * NUM_ELEMS));
+  checkCudaErrors(hipMalloc(&d_nres, sizeof(int)));
 
-  HIPCHECK(hipMemcpy(d_data_to_filter, data_to_filter,
+  checkCudaErrors(hipMemcpy(d_data_to_filter, data_to_filter,
                              sizeof(int) * NUM_ELEMS, hipMemcpyHostToDevice));
-  HIPCHECK(hipMemset(d_nres, 0, sizeof(int)));
+  checkCudaErrors(hipMemset(d_nres, 0, sizeof(int)));
 
   dim3 dimBlock(NUM_THREADS_PER_BLOCK, 1, 1);
   dim3 dimGrid((NUM_ELEMS / NUM_THREADS_PER_BLOCK) + 1, 1, 1);
@@ -284,12 +284,12 @@ int main(int argc, char **argv) {
   filter_arr<<<dimGrid, dimBlock>>>(d_filtered_data, d_nres, d_data_to_filter,
                                     NUM_ELEMS);
 
-  HIPCHECK(
+  checkCudaErrors(
       hipMemcpy(&nres, d_nres, sizeof(int), hipMemcpyDeviceToHost));
 
   filtered_data = reinterpret_cast<int *>(malloc(sizeof(int) * nres));
 
-  HIPCHECK(hipMemcpy(filtered_data, d_filtered_data, sizeof(int) * nres,
+  checkCudaErrors(hipMemcpy(filtered_data, d_filtered_data, sizeof(int) * nres,
                              hipMemcpyDeviceToHost));
 
   int *host_filtered_data =
@@ -304,7 +304,7 @@ int main(int argc, char **argv) {
   }
 
   int major = 0;
-  HIPCHECK(hipDeviceGetAttribute(&major, hipDeviceAttributeComputeCapabilityMajor, devId));
+  checkCudaErrors(hipDeviceGetAttribute(&major, hipDeviceAttributeComputeCapabilityMajor, devId));
 
   int mapIndicesToBucketsStatus = EXIT_SUCCESS;
   int calculateMaxInBucketsStatus = EXIT_SUCCESS;
@@ -319,14 +319,8 @@ int main(int argc, char **argv) {
          (host_flt_count == nres) && (mapIndicesToBucketsStatus == EXIT_SUCCESS) &&
          (calculateMaxInBucketsStatus == EXIT_SUCCESS) ? "PASSED" : "FAILED");
 
-  HIPCHECK(hipFree(d_data_to_filter));
-  HIPCHECK(hipFree(d_filtered_data));
-  HIPCHECK(hipFree(d_nres));
-  free(data_to_filter);
-  free(filtered_data);
-  free(host_filtered_data);
-}
-eckCudaErrors(hipFree(d_filtered_data));
+  checkCudaErrors(hipFree(d_data_to_filter));
+  checkCudaErrors(hipFree(d_filtered_data));
   checkCudaErrors(hipFree(d_nres));
   free(data_to_filter);
   free(filtered_data);
diff --git a/src/samples/Samples/4_CUDA_Libraries/conjugateGradientMultiDeviceCG/conjugateGradientMultiDeviceCG.cu.hip b/src/samples/Samples/4_CUDA_Libraries/conjugateGradientMultiDeviceCG/conjugateGradientMultiDeviceCG.cu.hip
index 36080e3..2751c9b 100755
--- a/src/samples/Samples/4_CUDA_Libraries/conjugateGradientMultiDeviceCG/conjugateGradientMultiDeviceCG.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/conjugateGradientMultiDeviceCG/conjugateGradientMultiDeviceCG.cu.hip
@@ -432,13 +432,13 @@ extern "C" __global__ void multiGpuConjugateGradient(
 // Map of device version to device number
 std::multimap<std::pair<int, int>, int> getIdenticalGPUs() {
   int numGpus = 0;
-  HIPCHECK(hipGetDeviceCount(&numGpus));
+  checkCudaErrors(hipGetDeviceCount(&numGpus));
 
   std::multimap<std::pair<int, int>, int> identicalGpus;
 
   for (int i = 0; i < numGpus; i++) {
     hipDeviceProp_t deviceProp;
-    HIPCHECK(hipGetDeviceProperties(&deviceProp, i));
+    checkCudaErrors(hipGetDeviceProperties(&deviceProp, i));
 
     // Filter unsupported devices
     if (deviceProp.cooperativeLaunch && deviceProp.concurrentManagedAccess) {
@@ -497,7 +497,7 @@ int main(int argc, char **argv) {
   // access between participating GPUs gives better performance.
   for (auto itr = bestFit.first; itr != bestFit.second; itr++) {
     int deviceId = itr->second;
-    HIPCHECK(hipSetDevice(deviceId));
+    checkCudaErrors(hipSetDevice(deviceId));
 
     std::for_each(
         itr, bestFit.second,
@@ -505,7 +505,7 @@ int main(int argc, char **argv) {
          &kNumGpusRequired](decltype(*itr) mapPair) {
           if (deviceId != mapPair.second) {
             int access = 0;
-            HIPCHECK(
+            checkCudaErrors(
                 hipDeviceCanAccessPeer(&access, deviceId, mapPair.second));
             printf("Device=%d %s Access Peer Device=%d\n", deviceId,
                    access ? "CAN" : "CANNOT", mapPair.second);
@@ -551,12 +551,12 @@ int main(int argc, char **argv) {
     // participating devices.
     for (auto p1_itr = bestFitDeviceIds.begin();
          p1_itr != bestFitDeviceIds.end(); p1_itr++) {
-      HIPCHECK(hipSetDevice(*p1_itr));
+      checkCudaErrors(hipSetDevice(*p1_itr));
       for (auto p2_itr = bestFitDeviceIds.begin();
            p2_itr != bestFitDeviceIds.end(); p2_itr++) {
         if (*p1_itr != *p2_itr) {
-          HIPCHECK(hipDeviceEnablePeerAccess(*p2_itr, 0));
-          HIPCHECK(hipSetDevice(*p1_itr));
+          checkCudaErrors(hipDeviceEnablePeerAccess(*p2_itr, 0));
+          checkCudaErrors(hipSetDevice(*p1_itr));
         }
       }
     }
@@ -566,33 +566,33 @@ int main(int argc, char **argv) {
   N = 10485760 * 2;
   nz = (N - 2) * 3 + 4;
 
-  HIPCHECK(hipMallocManaged((void **)&I, sizeof(int) * (N + 1)));
-  HIPCHECK(hipMallocManaged((void **)&J, sizeof(int) * nz));
-  HIPCHECK(hipMallocManaged((void **)&val, sizeof(float) * nz));
+  checkCudaErrors(hipMallocManaged((void **)&I, sizeof(int) * (N + 1)));
+  checkCudaErrors(hipMallocManaged((void **)&J, sizeof(int) * nz));
+  checkCudaErrors(hipMallocManaged((void **)&val, sizeof(float) * nz));
 
   float *val_cpu = (float *)malloc(sizeof(float) * nz);
 
   genTridiag(I, J, val_cpu, N, nz);
 
   memcpy(val, val_cpu, sizeof(float) * nz);
-  HIPCHECK(
+  checkCudaErrors(
       hipMemAdvise(I, sizeof(int) * (N + 1), hipMemAdviseSetReadMostly, 0));
-  HIPCHECK(
+  checkCudaErrors(
       hipMemAdvise(J, sizeof(int) * nz, hipMemAdviseSetReadMostly, 0));
-  HIPCHECK(
+  checkCudaErrors(
       hipMemAdvise(val, sizeof(float) * nz, hipMemAdviseSetReadMostly, 0));
 
-  HIPCHECK(hipMallocManaged((void **)&x, sizeof(float) * N));
+  checkCudaErrors(hipMallocManaged((void **)&x, sizeof(float) * N));
 
   double *dot_result;
-  HIPCHECK(hipMallocManaged((void **)&dot_result, sizeof(double)));
+  checkCudaErrors(hipMallocManaged((void **)&dot_result, sizeof(double)));
 
-  HIPCHECK(hipMemset(dot_result, 0, sizeof(double)));
+  checkCudaErrors(hipMemset(dot_result, 0, sizeof(double)));
 
   // temp memory for ConjugateGradient
-  HIPCHECK(hipMallocManaged((void **)&r, N * sizeof(float)));
-  HIPCHECK(hipMallocManaged((void **)&p, N * sizeof(float)));
-  HIPCHECK(hipMallocManaged((void **)&Ax, N * sizeof(float)));
+  checkCudaErrors(hipMallocManaged((void **)&r, N * sizeof(float)));
+  checkCudaErrors(hipMallocManaged((void **)&p, N * sizeof(float)));
+  checkCudaErrors(hipMallocManaged((void **)&Ax, N * sizeof(float)));
 
   std::cout << "\nRunning on GPUs = " << kNumGpusRequired << std::endl;
   hipStream_t nStreams[kNumGpusRequired];
@@ -606,11 +606,11 @@ int main(int argc, char **argv) {
   // set numSms & numBlocksPerSm to be lowest of 2 devices
   while (deviceId != bestFitDeviceIds.end()) {
     hipDeviceProp_t deviceProp;
-    HIPCHECK(hipSetDevice(*deviceId));
-    HIPCHECK(hipGetDeviceProperties(&deviceProp, *deviceId));
+    checkCudaErrors(hipSetDevice(*deviceId));
+    checkCudaErrors(hipGetDeviceProperties(&deviceProp, *deviceId));
 
     int numBlocksPerSm_current = 0;
-    HIPCHECK(hipOccupancyMaxActiveBlocksPerMultiprocessor(
+    checkCudaErrors(hipOccupancyMaxActiveBlocksPerMultiprocessor(
         &numBlocksPerSm_current, multiGpuConjugateGradient, numThreads,
         sMemSize));
 
@@ -634,8 +634,8 @@ int main(int argc, char **argv) {
   int totalThreadsPerGPU = numSms * numBlocksPerSm * THREADS_PER_BLOCK;
   deviceId = bestFitDeviceIds.begin();
   while (deviceId != bestFitDeviceIds.end()) {
-    HIPCHECK(hipSetDevice(*deviceId));
-    HIPCHECK(hipStreamCreate(&nStreams[device_count]));
+    checkCudaErrors(hipSetDevice(*deviceId));
+    checkCudaErrors(hipStreamCreate(&nStreams[device_count]));
 
     int perGPUIter = N / (totalThreadsPerGPU * kNumGpusRequired);
     int offset_Ax = device_count * totalThreadsPerGPU;
@@ -643,11 +643,11 @@ int main(int argc, char **argv) {
     int offset_p = device_count * totalThreadsPerGPU;
     int offset_x = device_count * totalThreadsPerGPU;
 
-    HIPCHECK(hipMemPrefetchAsync(I, sizeof(int) * N, *deviceId,
+    checkCudaErrors(hipMemPrefetchAsync(I, sizeof(int) * N, *deviceId,
                                          nStreams[device_count]));
-    HIPCHECK(hipMemPrefetchAsync(val, sizeof(float) * nz, *deviceId,
+    checkCudaErrors(hipMemPrefetchAsync(val, sizeof(float) * nz, *deviceId,
                                          nStreams[device_count]));
-    HIPCHECK(hipMemPrefetchAsync(J, sizeof(float) * nz, *deviceId,
+    checkCudaErrors(hipMemPrefetchAsync(J, sizeof(float) * nz, *deviceId,
                                          nStreams[device_count]));
 
     if (offset_Ax <= N) {
@@ -704,7 +704,7 @@ int main(int argc, char **argv) {
 
   // Structure used for cross-grid synchronization.
   MultiDeviceData multi_device_data;
-  HIPCHECK(hipHostAlloc(
+  checkCudaErrors(hipHostAlloc(
       &multi_device_data.hostMemoryArrivedList,
       (kNumGpusRequired - 1) * sizeof(*multi_device_data.hostMemoryArrivedList),
       hipHostMallocPortable));
@@ -725,23 +725,23 @@ int main(int argc, char **argv) {
   deviceId = bestFitDeviceIds.begin();
   device_count = 0;
   while (deviceId != bestFitDeviceIds.end()) {
-    HIPCHECK(hipSetDevice(*deviceId));
-    HIPCHECK(hipLaunchCooperativeKernel(
+    checkCudaErrors(hipSetDevice(*deviceId));
+    checkCudaErrors(hipLaunchCooperativeKernel(
         (void *)multiGpuConjugateGradient, dimGrid, dimBlock, kernelArgs,
         sMemSize, nStreams[device_count++]));
     multi_device_data.deviceRank++;
     deviceId++;
   }
 
-  HIPCHECK(hipMemPrefetchAsync(x, sizeof(float) * N, hipCpuDeviceId));
-  HIPCHECK(
+  checkCudaErrors(hipMemPrefetchAsync(x, sizeof(float) * N, hipCpuDeviceId));
+  checkCudaErrors(
       hipMemPrefetchAsync(dot_result, sizeof(double), hipCpuDeviceId));
 
   deviceId = bestFitDeviceIds.begin();
   device_count = 0;
   while (deviceId != bestFitDeviceIds.end()) {
-    HIPCHECK(hipSetDevice(*deviceId));
-    HIPCHECK(hipStreamSynchronize(nStreams[device_count++]));
+    checkCudaErrors(hipSetDevice(*deviceId));
+    checkCudaErrors(hipStreamSynchronize(nStreams[device_count++]));
     deviceId++;
   }
 
@@ -769,15 +769,15 @@ int main(int argc, char **argv) {
     }
   }
 
-  HIPCHECK(hipHostFree(multi_device_data.hostMemoryArrivedList));
-  HIPCHECK(hipFree(I));
-  HIPCHECK(hipFree(J));
-  HIPCHECK(hipFree(val));
-  HIPCHECK(hipFree(x));
-  HIPCHECK(hipFree(r));
-  HIPCHECK(hipFree(p));
-  HIPCHECK(hipFree(Ax));
-  HIPCHECK(hipFree(dot_result));
+  checkCudaErrors(hipHostFree(multi_device_data.hostMemoryArrivedList));
+  checkCudaErrors(hipFree(I));
+  checkCudaErrors(hipFree(J));
+  checkCudaErrors(hipFree(val));
+  checkCudaErrors(hipFree(x));
+  checkCudaErrors(hipFree(r));
+  checkCudaErrors(hipFree(p));
+  checkCudaErrors(hipFree(Ax));
+  checkCudaErrors(hipFree(dot_result));
   free(val_cpu);
 
 #if ENABLE_CPU_DEBUG_CODE
diff --git a/src/samples/Samples/4_CUDA_Libraries/cuDLAErrorReporting/main.cu.hip b/src/samples/Samples/4_CUDA_Libraries/cuDLAErrorReporting/main.cu.hip
index c38223b..e69de29 100755
--- a/src/samples/Samples/4_CUDA_Libraries/cuDLAErrorReporting/main.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/cuDLAErrorReporting/main.cu.hip
@@ -1,431 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "cudla.h"
-#include "hip/hip_runtime.h"
-
-#include <cstdio>
-#include <cstdlib>
-#include <cstring>
-#include <sys/stat.h>
-#include <fstream>
-#include <sstream>
-
-#define DPRINTF(...) printf(__VA_ARGS__)
-
-static void printTensorDesc(cudlaModuleTensorDescriptor* tensorDesc) {
-  DPRINTF("\tTENSOR NAME : %s\n", tensorDesc->name);
-  DPRINTF("\tsize: %lu\n", tensorDesc->size);
-
-  DPRINTF("\tdims: [%lu, %lu, %lu, %lu]\n", tensorDesc->n, tensorDesc->c,
-          tensorDesc->h, tensorDesc->w);
-
-  DPRINTF("\tdata fmt: %d\n", tensorDesc->dataFormat);
-  DPRINTF("\tdata type: %d\n", tensorDesc->dataType);
-  DPRINTF("\tdata category: %d\n", tensorDesc->dataCategory);
-  DPRINTF("\tpixel fmt: %d\n", tensorDesc->pixelFormat);
-  DPRINTF("\tpixel mapping: %d\n", tensorDesc->pixelMapping);
-  DPRINTF("\tstride[0]: %d\n", tensorDesc->stride[0]);
-  DPRINTF("\tstride[1]: %d\n", tensorDesc->stride[1]);
-  DPRINTF("\tstride[2]: %d\n", tensorDesc->stride[2]);
-  DPRINTF("\tstride[3]: %d\n", tensorDesc->stride[3]);
-}
-
-typedef struct {
-  cudlaDevHandle devHandle;
-  cudlaModule moduleHandle;
-  unsigned char* loadableData;
-  hipStream_t stream;
-  unsigned char* inputBuffer;
-  unsigned char* outputBuffer;
-  void* inputBufferGPU;
-  void* outputBufferGPU;
-  cudlaModuleTensorDescriptor* inputTensorDesc;
-  cudlaModuleTensorDescriptor* outputTensorDesc;
-} ResourceList;
-
-void cleanUp(ResourceList* resourceList);
-
-void cleanUp(ResourceList* resourceList) {
-  if (resourceList->inputTensorDesc != NULL) {
-    free(resourceList->inputTensorDesc);
-    resourceList->inputTensorDesc = NULL;
-  }
-  if (resourceList->outputTensorDesc != NULL) {
-    free(resourceList->outputTensorDesc);
-    resourceList->outputTensorDesc = NULL;
-  }
-
-  if (resourceList->loadableData != NULL) {
-    free(resourceList->loadableData);
-    resourceList->loadableData = NULL;
-  }
-
-  if (resourceList->moduleHandle != NULL) {
-    cudlaModuleUnload(resourceList->moduleHandle, 0);
-    resourceList->moduleHandle = NULL;
-  }
-
-  if (resourceList->devHandle != NULL) {
-    cudlaDestroyDevice(resourceList->devHandle);
-    resourceList->devHandle = NULL;
-  }
-
-  if (resourceList->inputBufferGPU != 0) {
-    hipFree(resourceList->inputBufferGPU);
-    resourceList->inputBufferGPU = 0;
-  }
-  if (resourceList->outputBufferGPU != 0) {
-    hipFree(resourceList->outputBufferGPU);
-    resourceList->outputBufferGPU = 0;
-  }
-
-  if (resourceList->inputBuffer != NULL) {
-    free(resourceList->inputBuffer);
-    resourceList->inputBuffer = NULL;
-  }
-  if (resourceList->outputBuffer != NULL) {
-    free(resourceList->outputBuffer);
-    resourceList->outputBuffer = NULL;
-  }
-
-  if (resourceList->stream != NULL) {
-    hipStreamDestroy(resourceList->stream);
-    resourceList->stream = NULL;
-  }
-}
-
-int main(int argc, char** argv) {
-  cudlaDevHandle devHandle;
-  cudlaModule moduleHandle;
-  cudlaStatus err;
-  FILE* fp = NULL;
-  struct stat st;
-  size_t file_size;
-  size_t actually_read = 0;
-  unsigned char* loadableData = NULL;
-
-  hipStream_t stream;
-  hipError_t result;
-  const char* errPtr = NULL;
-
-  ResourceList resourceList;
-
-  memset(&resourceList, 0x00, sizeof(ResourceList));
-
-  if (argc != 2) {
-    DPRINTF("Usage : ./cuDLAErrorReporting <loadable>\n");
-    return 1;
-  }
-
-  // Read loadable into buffer.
-  fp = fopen(argv[1], "rb");
-  if (fp == NULL) {
-    DPRINTF("Cannot open file %s\n", argv[1]);
-    return 1;
-  }
-
-  if (stat(argv[1], &st) != 0) {
-    DPRINTF("Cannot stat file\n");
-    return 1;
-  }
-
-  file_size = st.st_size;
-  DPRINTF("The file size = %ld\n", file_size);
-
-  loadableData = (unsigned char*)malloc(file_size);
-  if (loadableData == NULL) {
-    DPRINTF("Cannot Allocate memory for loadable\n");
-    return 1;
-  }
-
-  actually_read = fread(loadableData, 1, file_size, fp);
-  if (actually_read != file_size) {
-    free(loadableData);
-    DPRINTF("Read wrong size\n");
-    return 1;
-  }
-  fclose(fp);
-
-  resourceList.loadableData = loadableData;
-
-  // Initialize CUDA.
-  result = hipFree(0);
-  if (result != hipSuccess) {
-    errPtr = hipGetErrorName(result);
-    DPRINTF("Error in creating hipFree = %s\n", errPtr);
-    cleanUp(&resourceList);
-    return 1;
-  }
-  result = hipSetDevice(0);
-  if (result != hipSuccess) {
-    errPtr = hipGetErrorName(result);
-    DPRINTF("Error in creating hipSetDevice = %s\n", errPtr);
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  err = cudlaCreateDevice(0, &devHandle, CUDLA_CUDA_DLA);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in cuDLA create device = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  DPRINTF("Device created successfully\n");
-  resourceList.devHandle = devHandle;
-
-  err = cudlaModuleLoadFromMemory(devHandle, loadableData, file_size,
-                                  &moduleHandle, 0);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in cudlaModuleLoadFromMemory = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  } else {
-    DPRINTF("Successfully loaded module\n");
-  }
-
-  resourceList.moduleHandle = moduleHandle;
-
-  // Create CUDA stream.
-  result = hipStreamCreateWithFlags(&stream, hipStreamNonBlocking);
-
-  if (result != hipSuccess) {
-    errPtr = hipGetErrorName(result);
-    DPRINTF("Error in creating cuda stream = %s\n", errPtr);
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  resourceList.stream = stream;
-
-  // Get tensor attributes.
-  uint32_t numInputTensors = 0;
-  uint32_t numOutputTensors = 0;
-  cudlaModuleAttribute attribute;
-
-  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_NUM_INPUT_TENSORS,
-                                 &attribute);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in getting numInputTensors = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  }
-  numInputTensors = attribute.numInputTensors;
-  DPRINTF("numInputTensors = %d\n", numInputTensors);
-
-  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_NUM_OUTPUT_TENSORS,
-                                 &attribute);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in getting numOutputTensors = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  }
-  numOutputTensors = attribute.numOutputTensors;
-  DPRINTF("numOutputTensors = %d\n", numOutputTensors);
-
-  cudlaModuleTensorDescriptor* inputTensorDesc =
-      (cudlaModuleTensorDescriptor*)malloc(sizeof(cudlaModuleTensorDescriptor) *
-                                           numInputTensors);
-  cudlaModuleTensorDescriptor* outputTensorDesc =
-      (cudlaModuleTensorDescriptor*)malloc(sizeof(cudlaModuleTensorDescriptor) *
-                                           numOutputTensors);
-
-  if ((inputTensorDesc == NULL) || (outputTensorDesc == NULL)) {
-    if (inputTensorDesc != NULL) {
-      free(inputTensorDesc);
-      inputTensorDesc = NULL;
-    }
-
-    if (outputTensorDesc != NULL) {
-      free(outputTensorDesc);
-      outputTensorDesc = NULL;
-    }
-
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  resourceList.inputTensorDesc = inputTensorDesc;
-  resourceList.outputTensorDesc = outputTensorDesc;
-
-  attribute.inputTensorDesc = inputTensorDesc;
-  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_INPUT_TENSOR_DESCRIPTORS,
-                                 &attribute);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in getting input tensor descriptor = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  }
-  DPRINTF("Printing input tensor descriptor\n");
-  printTensorDesc(inputTensorDesc);
-
-  attribute.outputTensorDesc = outputTensorDesc;
-  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_OUTPUT_TENSOR_DESCRIPTORS,
-                                 &attribute);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in getting output tensor descriptor = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  }
-  DPRINTF("Printing output tensor descriptor\n");
-  printTensorDesc(outputTensorDesc);
-
-  // Setup the input and output buffers which will be used as an input to CUDA.
-  unsigned char* inputBuffer = (unsigned char*)malloc(inputTensorDesc[0].size);
-  if (inputBuffer == NULL) {
-    DPRINTF("Error in allocating input memory\n");
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  resourceList.inputBuffer = inputBuffer;
-
-  unsigned char* outputBuffer =
-      (unsigned char*)malloc(outputTensorDesc[0].size);
-  if (outputBuffer == NULL) {
-    DPRINTF("Error in allocating output memory\n");
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  resourceList.outputBuffer = outputBuffer;
-
-  memset(inputBuffer, 0x01, inputTensorDesc[0].size);
-  memset(outputBuffer, 0x00, outputTensorDesc[0].size);
-
-  // Allocate memory on GPU.
-  void* inputBufferGPU;
-  void* outputBufferGPU;
-  result = hipMalloc(&inputBufferGPU, inputTensorDesc[0].size);
-  if (result != hipSuccess) {
-    DPRINTF("Error in allocating input memory on GPU\n");
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  resourceList.inputBufferGPU = inputBufferGPU;
-
-  result = hipMalloc(&outputBufferGPU, outputTensorDesc[0].size);
-  if (result != hipSuccess) {
-    DPRINTF("Error in allocating output memory on GPU\n");
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  resourceList.outputBufferGPU = outputBufferGPU;
-
-  // Register the CUDA-allocated buffers.
-  uint64_t* inputBufferRegisteredPtr = NULL;
-  uint64_t* outputBufferRegisteredPtr = NULL;
-
-  err = cudlaMemRegister(devHandle, (uint64_t*)inputBufferGPU,
-                         inputTensorDesc[0].size, &inputBufferRegisteredPtr, 0);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in registering input memory = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  err =
-      cudlaMemRegister(devHandle, (uint64_t*)outputBufferGPU,
-                       outputTensorDesc[0].size, &outputBufferRegisteredPtr, 0);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in registering output memory = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  }
-  DPRINTF("ALL MEMORY REGISTERED SUCCESSFULLY\n");
-
-  // Copy data from CPU buffers to GPU buffers.
-  result = hipMemcpyAsync(inputBufferGPU, inputBuffer, inputTensorDesc[0].size,
-                           hipMemcpyHostToDevice, stream);
-  if (result != hipSuccess) {
-    DPRINTF("Error in enqueueing memcpy for input\n");
-    cleanUp(&resourceList);
-    return 1;
-  }
-  result =
-      hipMemsetAsync(outputBufferGPU, 0, outputTensorDesc[0].size, stream);
-  if (result != hipSuccess) {
-    DPRINTF("Error in enqueueing memset for output\n");
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  // Enqueue a cuDLA task.
-  cudlaTask task;
-  task.moduleHandle = moduleHandle;
-  task.outputTensor = &outputBufferRegisteredPtr;
-  task.numOutputTensors = 1;
-  task.numInputTensors = 1;
-  task.inputTensor = &inputBufferRegisteredPtr;
-  task.waitEvents = NULL;
-  task.signalEvents = NULL;
-  err = cudlaSubmitTask(devHandle, &task, 1, stream, 0);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in submitting task\n");
-    cleanUp(&resourceList);
-    return 1;
-  }
-  DPRINTF("SUBMIT IS DONE !!!\n");
-
-  // Wait for stream operations to finish and bring output buffer to CPU.
-  result =
-      hipMemcpyAsync(outputBuffer, outputBufferGPU, outputTensorDesc[0].size,
-                      hipMemcpyDeviceToHost, stream);
-  if (result != hipSuccess) {
-    if (result != cudaErrorExternalDevice) {
-      DPRINTF("Error in bringing result back to CPU\n");
-      cleanUp(&resourceList);
-      return 1;
-    } else {
-      cudlaStatus hwStatus = cudlaGetLastError(devHandle);
-      if (hwStatus != cudlaSuccess) {
-        DPRINTF("Asynchronous error in HW = %u\n", hwStatus);
-      }
-    }
-  }
-
-  result = hipStreamSynchronize(stream);
-  if (result != hipSuccess) {
-    DPRINTF("Error in synchronizing stream = %s\n", hipGetErrorName(result));
-
-    if (result == cudaErrorExternalDevice) {
-      cudlaStatus hwStatus = cudlaGetLastError(devHandle);
-      if (hwStatus != cudlaSuccess) {
-        DPRINTF("Asynchronous error in HW = %u\n", hwStatus);
-      }
-    }
-  }
-
-  cleanUp(&resourceList);
-
-  DPRINTF("cuDLAErrorReporting DONE !!!\n");
-
-  return 0;
-}
diff --git a/src/samples/Samples/4_CUDA_Libraries/cuDLAHybridMode/main.cu.hip b/src/samples/Samples/4_CUDA_Libraries/cuDLAHybridMode/main.cu.hip
index ecaf605..e69de29 100755
--- a/src/samples/Samples/4_CUDA_Libraries/cuDLAHybridMode/main.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/cuDLAHybridMode/main.cu.hip
@@ -1,496 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "cudla.h"
-#include "hip/hip_runtime.h"
-
-#include <cstdio>
-#include <cstdlib>
-#include <cstring>
-#include <sys/stat.h>
-#include <fstream>
-#include <sstream>
-
-#define DPRINTF(...) printf(__VA_ARGS__)
-
-static void printTensorDesc(cudlaModuleTensorDescriptor* tensorDesc) {
-  DPRINTF("\tTENSOR NAME : %s\n", tensorDesc->name);
-  DPRINTF("\tsize: %lu\n", tensorDesc->size);
-
-  DPRINTF("\tdims: [%lu, %lu, %lu, %lu]\n", tensorDesc->n, tensorDesc->c,
-          tensorDesc->h, tensorDesc->w);
-
-  DPRINTF("\tdata fmt: %d\n", tensorDesc->dataFormat);
-  DPRINTF("\tdata type: %d\n", tensorDesc->dataType);
-  DPRINTF("\tdata category: %d\n", tensorDesc->dataCategory);
-  DPRINTF("\tpixel fmt: %d\n", tensorDesc->pixelFormat);
-  DPRINTF("\tpixel mapping: %d\n", tensorDesc->pixelMapping);
-  DPRINTF("\tstride[0]: %d\n", tensorDesc->stride[0]);
-  DPRINTF("\tstride[1]: %d\n", tensorDesc->stride[1]);
-  DPRINTF("\tstride[2]: %d\n", tensorDesc->stride[2]);
-  DPRINTF("\tstride[3]: %d\n", tensorDesc->stride[3]);
-}
-
-static int initializeInputBuffers(char* filePath,
-                                  cudlaModuleTensorDescriptor* tensorDesc,
-                                  unsigned char* buf) {
-  // Read the file in filePath and fill up 'buf' according to format
-  // specified by the user.
-
-  return 0;
-}
-
-typedef struct {
-  cudlaDevHandle devHandle;
-  cudlaModule moduleHandle;
-  unsigned char* loadableData;
-  hipStream_t stream;
-  unsigned char* inputBuffer;
-  unsigned char* outputBuffer;
-  void* inputBufferGPU;
-  void* outputBufferGPU;
-  cudlaModuleTensorDescriptor* inputTensorDesc;
-  cudlaModuleTensorDescriptor* outputTensorDesc;
-} ResourceList;
-
-void cleanUp(ResourceList* resourceList);
-
-void cleanUp(ResourceList* resourceList) {
-  if (resourceList->inputTensorDesc != NULL) {
-    free(resourceList->inputTensorDesc);
-    resourceList->inputTensorDesc = NULL;
-  }
-  if (resourceList->outputTensorDesc != NULL) {
-    free(resourceList->outputTensorDesc);
-    resourceList->outputTensorDesc = NULL;
-  }
-
-  if (resourceList->loadableData != NULL) {
-    free(resourceList->loadableData);
-    resourceList->loadableData = NULL;
-  }
-
-  if (resourceList->moduleHandle != NULL) {
-    cudlaModuleUnload(resourceList->moduleHandle, 0);
-    resourceList->moduleHandle = NULL;
-  }
-
-  if (resourceList->devHandle != NULL) {
-    cudlaDestroyDevice(resourceList->devHandle);
-    resourceList->devHandle = NULL;
-  }
-
-  if (resourceList->inputBufferGPU != 0) {
-    hipFree(resourceList->inputBufferGPU);
-    resourceList->inputBufferGPU = 0;
-  }
-  if (resourceList->outputBufferGPU != 0) {
-    hipFree(resourceList->outputBufferGPU);
-    resourceList->outputBufferGPU = 0;
-  }
-
-  if (resourceList->inputBuffer != NULL) {
-    free(resourceList->inputBuffer);
-    resourceList->inputBuffer = NULL;
-  }
-  if (resourceList->outputBuffer != NULL) {
-    free(resourceList->outputBuffer);
-    resourceList->outputBuffer = NULL;
-  }
-
-  if (resourceList->stream != NULL) {
-    hipStreamDestroy(resourceList->stream);
-    resourceList->stream = NULL;
-  }
-}
-
-int main(int argc, char** argv) {
-  cudlaDevHandle devHandle;
-  cudlaModule moduleHandle;
-  cudlaStatus err;
-  FILE* fp = NULL;
-  struct stat st;
-  size_t file_size;
-  size_t actually_read = 0;
-  unsigned char* loadableData = NULL;
-
-  hipStream_t stream;
-  hipError_t result;
-  const char* errPtr = NULL;
-
-  ResourceList resourceList;
-
-  memset(&resourceList, 0x00, sizeof(ResourceList));
-
-  if (argc != 3) {
-    DPRINTF("Usage : ./cuDLAHybridMode <loadable> <imageFile>\n");
-    return 1;
-  }
-
-  // Read loadable into buffer.
-  fp = fopen(argv[1], "rb");
-  if (fp == NULL) {
-    DPRINTF("Cannot open file %s\n", argv[1]);
-    return 1;
-  }
-
-  if (stat(argv[1], &st) != 0) {
-    DPRINTF("Cannot stat file\n");
-    return 1;
-  }
-
-  file_size = st.st_size;
-  DPRINTF("The file size = %ld\n", file_size);
-
-  loadableData = (unsigned char*)malloc(file_size);
-  if (loadableData == NULL) {
-    DPRINTF("Cannot Allocate memory for loadable\n");
-    return 1;
-  }
-
-  actually_read = fread(loadableData, 1, file_size, fp);
-  if (actually_read != file_size) {
-    free(loadableData);
-    DPRINTF("Read wrong size\n");
-    return 1;
-  }
-  fclose(fp);
-
-  resourceList.loadableData = loadableData;
-
-  // Initialize CUDA.
-  result = hipFree(0);
-  if (result != hipSuccess) {
-    errPtr = hipGetErrorName(result);
-    DPRINTF("Error in creating hipFree = %s\n", errPtr);
-    cleanUp(&resourceList);
-    return 1;
-  }
-  result = hipSetDevice(0);
-  if (result != hipSuccess) {
-    errPtr = hipGetErrorName(result);
-    DPRINTF("Error in creating hipSetDevice = %s\n", errPtr);
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  err = cudlaCreateDevice(0, &devHandle, CUDLA_CUDA_DLA);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in cuDLA create device = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  DPRINTF("Device created successfully\n");
-  resourceList.devHandle = devHandle;
-
-  err = cudlaModuleLoadFromMemory(devHandle, loadableData, file_size,
-                                  &moduleHandle, 0);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in cudlaModuleLoadFromMemory = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  } else {
-    DPRINTF("Successfully loaded module\n");
-  }
-
-  resourceList.moduleHandle = moduleHandle;
-
-  // Create CUDA stream.
-  result = hipStreamCreateWithFlags(&stream, hipStreamNonBlocking);
-
-  if (result != hipSuccess) {
-    errPtr = hipGetErrorName(result);
-    DPRINTF("Error in creating cuda stream = %s\n", errPtr);
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  resourceList.stream = stream;
-
-  // Get tensor attributes.
-  uint32_t numInputTensors = 0;
-  uint32_t numOutputTensors = 0;
-  cudlaModuleAttribute attribute;
-
-  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_NUM_INPUT_TENSORS,
-                                 &attribute);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in getting numInputTensors = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  }
-  numInputTensors = attribute.numInputTensors;
-  DPRINTF("numInputTensors = %d\n", numInputTensors);
-
-  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_NUM_OUTPUT_TENSORS,
-                                 &attribute);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in getting numOutputTensors = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  }
-  numOutputTensors = attribute.numOutputTensors;
-  DPRINTF("numOutputTensors = %d\n", numOutputTensors);
-
-  cudlaModuleTensorDescriptor* inputTensorDesc =
-      (cudlaModuleTensorDescriptor*)malloc(sizeof(cudlaModuleTensorDescriptor) *
-                                           numInputTensors);
-  cudlaModuleTensorDescriptor* outputTensorDesc =
-      (cudlaModuleTensorDescriptor*)malloc(sizeof(cudlaModuleTensorDescriptor) *
-                                           numOutputTensors);
-
-  if ((inputTensorDesc == NULL) || (outputTensorDesc == NULL)) {
-    if (inputTensorDesc != NULL) {
-      free(inputTensorDesc);
-      inputTensorDesc = NULL;
-    }
-
-    if (outputTensorDesc != NULL) {
-      free(outputTensorDesc);
-      outputTensorDesc = NULL;
-    }
-
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  resourceList.inputTensorDesc = inputTensorDesc;
-  resourceList.outputTensorDesc = outputTensorDesc;
-
-  attribute.inputTensorDesc = inputTensorDesc;
-  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_INPUT_TENSOR_DESCRIPTORS,
-                                 &attribute);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in getting input tensor descriptor = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  }
-  DPRINTF("Printing input tensor descriptor\n");
-  printTensorDesc(inputTensorDesc);
-
-  attribute.outputTensorDesc = outputTensorDesc;
-  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_OUTPUT_TENSOR_DESCRIPTORS,
-                                 &attribute);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in getting output tensor descriptor = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  }
-  DPRINTF("Printing output tensor descriptor\n");
-  printTensorDesc(outputTensorDesc);
-
-  // Setup the input and output buffers which will be used as an input to CUDA.
-  unsigned char* inputBuffer = (unsigned char*)malloc(inputTensorDesc[0].size);
-  if (inputBuffer == NULL) {
-    DPRINTF("Error in allocating input memory\n");
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  resourceList.inputBuffer = inputBuffer;
-
-  unsigned char* outputBuffer =
-      (unsigned char*)malloc(outputTensorDesc[0].size);
-  if (outputBuffer == NULL) {
-    DPRINTF("Error in allocating output memory\n");
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  resourceList.outputBuffer = outputBuffer;
-
-  memset(inputBuffer, 0x00, inputTensorDesc[0].size);
-  memset(outputBuffer, 0x00, outputTensorDesc[0].size);
-
-  // Fill up the buffers with data.
-  if (initializeInputBuffers(argv[2], inputTensorDesc, inputBuffer) != 0) {
-    DPRINTF("Error in initializing input buffer\n");
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  // Allocate memory on GPU.
-  void* inputBufferGPU;
-  void* outputBufferGPU;
-  result = hipMalloc(&inputBufferGPU, inputTensorDesc[0].size);
-  if (result != hipSuccess) {
-    DPRINTF("Error in allocating input memory on GPU\n");
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  resourceList.inputBufferGPU = inputBufferGPU;
-
-  result = hipMalloc(&outputBufferGPU, outputTensorDesc[0].size);
-  if (result != hipSuccess) {
-    DPRINTF("Error in allocating output memory on GPU\n");
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  resourceList.outputBufferGPU = outputBufferGPU;
-
-  // Register the CUDA-allocated buffers.
-  uint64_t* inputBufferRegisteredPtr = NULL;
-  uint64_t* outputBufferRegisteredPtr = NULL;
-
-  err = cudlaMemRegister(devHandle, (uint64_t*)inputBufferGPU,
-                         inputTensorDesc[0].size, &inputBufferRegisteredPtr, 0);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in registering input memory = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  err =
-      cudlaMemRegister(devHandle, (uint64_t*)outputBufferGPU,
-                       outputTensorDesc[0].size, &outputBufferRegisteredPtr, 0);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in registering output memory = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  }
-  DPRINTF("ALL MEMORY REGISTERED SUCCESSFULLY\n");
-
-  // Copy data from CPU buffers to GPU buffers.
-  result = hipMemcpyAsync(inputBufferGPU, inputBuffer, inputTensorDesc[0].size,
-                           hipMemcpyHostToDevice, stream);
-  if (result != hipSuccess) {
-    DPRINTF("Error in enqueueing memcpy for input\n");
-    cleanUp(&resourceList);
-    return 1;
-  }
-  result =
-      hipMemsetAsync(outputBufferGPU, 0, outputTensorDesc[0].size, stream);
-  if (result != hipSuccess) {
-    DPRINTF("Error in enqueueing memset for output\n");
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  // Enqueue a cuDLA task.
-  cudlaTask task;
-  task.moduleHandle = moduleHandle;
-  task.outputTensor = &outputBufferRegisteredPtr;
-  task.numOutputTensors = 1;
-  task.numInputTensors = 1;
-  task.inputTensor = &inputBufferRegisteredPtr;
-  task.waitEvents = NULL;
-  task.signalEvents = NULL;
-  err = cudlaSubmitTask(devHandle, &task, 1, stream, 0);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in submitting task\n");
-    cleanUp(&resourceList);
-    return 1;
-  }
-  DPRINTF("SUBMIT IS DONE !!!\n");
-
-  // Wait for stream operations to finish and bring output buffer to CPU.
-  result =
-      hipMemcpyAsync(outputBuffer, outputBufferGPU, outputTensorDesc[0].size,
-                      hipMemcpyDeviceToHost, stream);
-  if (result != hipSuccess) {
-    DPRINTF("Error in bringing result back to CPU\n");
-    cleanUp(&resourceList);
-    return 1;
-  }
-  result = hipStreamSynchronize(stream);
-  if (result != hipSuccess) {
-    DPRINTF("Error in synchronizing stream\n");
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  // Output is available in outputBuffer.
-
-  // Teardown.
-  err = cudlaMemUnregister(devHandle, inputBufferRegisteredPtr);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in unregistering input memory = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  err = cudlaMemUnregister(devHandle, outputBufferRegisteredPtr);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in registering output memory = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  }
-  DPRINTF("ALL MEMORY UNREGISTERED SUCCESSFULLY\n");
-
-  free(inputTensorDesc);
-  free(outputTensorDesc);
-  free(loadableData);
-  free(inputBuffer);
-  free(outputBuffer);
-  hipFree(inputBufferGPU);
-  hipFree(outputBufferGPU);
-
-  resourceList.inputTensorDesc = NULL;
-  resourceList.outputTensorDesc = NULL;
-  resourceList.loadableData = NULL;
-  resourceList.inputBuffer = NULL;
-  resourceList.outputBuffer = NULL;
-  resourceList.inputBufferGPU = 0;
-  resourceList.outputBufferGPU = 0;
-
-  result = hipStreamDestroy(stream);
-  if (result != hipSuccess) {
-    errPtr = hipGetErrorName(result);
-    DPRINTF("Error in destroying cuda stream = %s\n", errPtr);
-    cleanUp(&resourceList);
-    return 1;
-  }
-
-  resourceList.stream = NULL;
-
-  err = cudlaModuleUnload(moduleHandle, 0);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in cudlaModuleUnload = %d\n", err);
-    cleanUp(&resourceList);
-    return 1;
-  } else {
-    DPRINTF("Successfully unloaded module\n");
-  }
-
-  resourceList.moduleHandle = NULL;
-
-  err = cudlaDestroyDevice(devHandle);
-  if (err != cudlaSuccess) {
-    DPRINTF("Error in cuDLA destroy device = %d\n", err);
-    return 1;
-  }
-  DPRINTF("Device destroyed successfully\n");
-
-  resourceList.devHandle = NULL;
-
-  DPRINTF("cuDLAHybridMode DONE !!!\n");
-
-  return 0;
-}
diff --git a/src/samples/Samples/4_CUDA_Libraries/lineOfSight/lineOfSight.cu.hip b/src/samples/Samples/4_CUDA_Libraries/lineOfSight/lineOfSight.cu.hip
index 7edb61c..c4dd461 100755
--- a/src/samples/Samples/4_CUDA_Libraries/lineOfSight/lineOfSight.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/lineOfSight/lineOfSight.cu.hip
@@ -152,11 +152,11 @@ int runTest(int argc, char **argv) {
   hipChannelFormatDesc channelDesc =
       hipCreateChannelDesc(32, 0, 0, 0, hipChannelFormatKindFloat);
   hipArray *heightFieldArray;
-  HIPCHECK(
+  checkCudaErrors(
       hipMallocArray(&heightFieldArray, &channelDesc, dim.x, dim.y));
 
   // Initialize device memory
-  HIPCHECK(hipMemcpy2DToArray(
+  checkCudaErrors(hipMemcpy2DToArray(
       heightFieldArray, 0, 0, heightField.height, dim.x * sizeof(float),
       dim.x * sizeof(float), dim.y, hipMemcpyHostToDevice));
 
@@ -175,7 +175,7 @@ int runTest(int argc, char **argv) {
   texDescr.addressMode[1] = hipAddressModeClamp;
   texDescr.readMode = hipReadModeElementType;
 
-  HIPCHECK(
+  checkCudaErrors(
       hipCreateTextureObject(&heightFieldTex, &texRes, &texDescr, NULL));
 
   //////////////////////////////////////////////////////////////////////////////
@@ -256,7 +256,7 @@ int runTest(int argc, char **argv) {
   sdkResetTimer(&timer);
 
   // Cleanup memory
-  HIPCHECK(hipFreeArray(heightFieldArray));
+  checkCudaErrors(hipFreeArray(heightFieldArray));
   return res;
 }
 
diff --git a/src/samples/Samples/4_CUDA_Libraries/oceanFFT/oceanFFT_kernel.cu.hip b/src/samples/Samples/4_CUDA_Libraries/oceanFFT/oceanFFT_kernel.cu.hip
index 16d309d..e69de29 100755
--- a/src/samples/Samples/4_CUDA_Libraries/oceanFFT/oceanFFT_kernel.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/oceanFFT/oceanFFT_kernel.cu.hip
@@ -1,160 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-///////////////////////////////////////////////////////////////////////////////
-#include <hipfft.h>
-#include <math_constants.h>
-
-// Round a / b to nearest higher integer value
-int cuda_iDivUp(int a, int b) { return (a + (b - 1)) / b; }
-
-// complex math functions
-__device__ float2 conjugate(float2 arg) { return make_float2(arg.x, -arg.y); }
-
-__device__ float2 complex_exp(float arg) {
-  return make_float2(cosf(arg), sinf(arg));
-}
-
-__device__ float2 complex_add(float2 a, float2 b) {
-  return make_float2(a.x + b.x, a.y + b.y);
-}
-
-__device__ float2 complex_mult(float2 ab, float2 cd) {
-  return make_float2(ab.x * cd.x - ab.y * cd.y, ab.x * cd.y + ab.y * cd.x);
-}
-
-// generate wave heightfield at time t based on initial heightfield and
-// dispersion relationship
-__global__ void generateSpectrumKernel(float2 *h0, float2 *ht,
-                                       unsigned int in_width,
-                                       unsigned int out_width,
-                                       unsigned int out_height, float t,
-                                       float patchSize) {
-  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
-  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
-  unsigned int in_index = y * in_width + x;
-  unsigned int in_mindex =
-      (out_height - y) * in_width + (out_width - x);  // mirrored
-  unsigned int out_index = y * out_width + x;
-
-  // calculate wave vector
-  float2 k;
-  k.x = (-(int)out_width / 2.0f + x) * (2.0f * CUDART_PI_F / patchSize);
-  k.y = (-(int)out_width / 2.0f + y) * (2.0f * CUDART_PI_F / patchSize);
-
-  // calculate dispersion w(k)
-  float k_len = sqrtf(k.x * k.x + k.y * k.y);
-  float w = sqrtf(9.81f * k_len);
-
-  if ((x < out_width) && (y < out_height)) {
-    float2 h0_k = h0[in_index];
-    float2 h0_mk = h0[in_mindex];
-
-    // output frequency-space complex values
-    ht[out_index] =
-        complex_add(complex_mult(h0_k, complex_exp(w * t)),
-                    complex_mult(conjugate(h0_mk), complex_exp(-w * t)));
-    // ht[out_index] = h0_k;
-  }
-}
-
-// update height map values based on output of FFT
-__global__ void updateHeightmapKernel(float *heightMap, float2 *ht,
-                                      unsigned int width) {
-  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
-  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
-  unsigned int i = y * width + x;
-
-  // cos(pi * (m1 + m2))
-  float sign_correction = ((x + y) & 0x01) ? -1.0f : 1.0f;
-
-  heightMap[i] = ht[i].x * sign_correction;
-}
-
-// update height map values based on output of FFT
-__global__ void updateHeightmapKernel_y(float *heightMap, float2 *ht,
-                                        unsigned int width) {
-  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
-  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
-  unsigned int i = y * width + x;
-
-  // cos(pi * (m1 + m2))
-  float sign_correction = ((x + y) & 0x01) ? -1.0f : 1.0f;
-
-  heightMap[i] = ht[i].y * sign_correction;
-}
-
-// generate slope by partial differences in spatial domain
-__global__ void calculateSlopeKernel(float *h, float2 *slopeOut,
-                                     unsigned int width, unsigned int height) {
-  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
-  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
-  unsigned int i = y * width + x;
-
-  float2 slope = make_float2(0.0f, 0.0f);
-
-  if ((x > 0) && (y > 0) && (x < width - 1) && (y < height - 1)) {
-    slope.x = h[i + 1] - h[i - 1];
-    slope.y = h[i + width] - h[i - width];
-  }
-
-  slopeOut[i] = slope;
-}
-
-// wrapper functions
-extern "C" void cudaGenerateSpectrumKernel(float2 *d_h0, float2 *d_ht,
-                                           unsigned int in_width,
-                                           unsigned int out_width,
-                                           unsigned int out_height,
-                                           float animTime, float patchSize) {
-  dim3 block(8, 8, 1);
-  dim3 grid(cuda_iDivUp(out_width, block.x), cuda_iDivUp(out_height, block.y),
-            1);
-  generateSpectrumKernel<<<grid, block>>>(d_h0, d_ht, in_width, out_width,
-                                          out_height, animTime, patchSize);
-}
-
-extern "C" void cudaUpdateHeightmapKernel(float *d_heightMap, float2 *d_ht,
-                                          unsigned int width,
-                                          unsigned int height, bool autoTest) {
-  dim3 block(8, 8, 1);
-  dim3 grid(cuda_iDivUp(width, block.x), cuda_iDivUp(height, block.y), 1);
-  if (autoTest) {
-    updateHeightmapKernel_y<<<grid, block>>>(d_heightMap, d_ht, width);
-  } else {
-    updateHeightmapKernel<<<grid, block>>>(d_heightMap, d_ht, width);
-  }
-}
-
-extern "C" void cudaCalculateSlopeKernel(float *hptr, float2 *slopeOut,
-                                         unsigned int width,
-                                         unsigned int height) {
-  dim3 block(8, 8, 1);
-  dim3 grid2(cuda_iDivUp(width, block.x), cuda_iDivUp(height, block.y), 1);
-  calculateSlopeKernel<<<grid2, block>>>(hptr, slopeOut, width, height);
-}
diff --git a/src/samples/Samples/5_Domain_Specific/SobelFilter/SobelFilter_kernels.cu.hip b/src/samples/Samples/5_Domain_Specific/SobelFilter/SobelFilter_kernels.cu.hip
index b76fae3..1a9e3b2 100755
--- a/src/samples/Samples/5_Domain_Specific/SobelFilter/SobelFilter_kernels.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/SobelFilter/SobelFilter_kernels.cu.hip
@@ -299,3 +299,6 @@ extern "C" void sobelFilter(Pixel *odata, int iw, int ih,
     } break;
   }
 }
+         iw, ih, fScale, texObject);
+    } break;
+  }
diff --git a/src/samples/Samples/5_Domain_Specific/binomialOptions_nvrtc/binomialOptions_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/binomialOptions_nvrtc/binomialOptions_kernel.cu.hip
index 24fd6b5..e69de29 100755
--- a/src/samples/Samples/5_Domain_Specific/binomialOptions_nvrtc/binomialOptions_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/binomialOptions_nvrtc/binomialOptions_kernel.cu.hip
@@ -1,108 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "common_gpu_header.h"
-#include "binomialOptions_common.h"
-#include "realtype.h"
-
-// Preprocessed input option data
-typedef struct {
-  real S;
-  real X;
-  real vDt;
-  real puByDf;
-  real pdByDf;
-} __TOptionData;
-static __constant__ __TOptionData d_OptionData[MAX_OPTIONS];
-__device__ real d_CallValue[MAX_OPTIONS];
-
-#define THREADBLOCK_SIZE 128
-#define ELEMS_PER_THREAD (NUM_STEPS / THREADBLOCK_SIZE)
-#if NUM_STEPS % THREADBLOCK_SIZE
-#error Bad constants
-#endif
-
-////////////////////////////////////////////////////////////////////////////////
-// Overloaded shortcut functions for different precision modes
-////////////////////////////////////////////////////////////////////////////////
-
-#ifndef DOUBLE_PRECISION
-__device__ inline float expiryCallValue(float S, float X, float vDt, int i) {
-  float d = S * __expf(vDt * (2.0f * i - NUM_STEPS)) - X;
-  return (d > 0.0F) ? d : 0.0F;
-}
-
-#else
-__device__ inline double expiryCallValue(double S, double X, double vDt,
-                                         int i) {
-  double d = S * exp(vDt * (2.0 * i - NUM_STEPS)) - X;
-  return (d > 0.0) ? d : 0.0;
-}
-#endif
-
-////////////////////////////////////////////////////////////////////////////////
-// GPU kernel
-////////////////////////////////////////////////////////////////////////////////
-extern "C" __global__ void binomialOptionsKernel() {
-  __shared__ real call_exchange[THREADBLOCK_SIZE + 1];
-
-  const int tid = threadIdx.x;
-  const real S = d_OptionData[blockIdx.x].S;
-  const real X = d_OptionData[blockIdx.x].X;
-  const real vDt = d_OptionData[blockIdx.x].vDt;
-  const real puByDf = d_OptionData[blockIdx.x].puByDf;
-  const real pdByDf = d_OptionData[blockIdx.x].pdByDf;
-
-  real call[ELEMS_PER_THREAD + 1];
-#pragma unroll
-  for (int i = 0; i < ELEMS_PER_THREAD; ++i)
-    call[i] = expiryCallValue(S, X, vDt, tid * ELEMS_PER_THREAD + i);
-
-  if (tid == 0)
-    call_exchange[THREADBLOCK_SIZE] = expiryCallValue(S, X, vDt, NUM_STEPS);
-
-  int final_it = max(0, tid * ELEMS_PER_THREAD - 1);
-
-#pragma unroll 16
-  for (int i = NUM_STEPS; i > 0; --i) {
-    call_exchange[tid] = call[0];
-    __syncthreads();
-    call[ELEMS_PER_THREAD] = call_exchange[tid + 1];
-    __syncthreads();
-
-    if (i > final_it) {
-#pragma unroll
-      for (int j = 0; j < ELEMS_PER_THREAD; ++j)
-        call[j] = puByDf * call[j + 1] + pdByDf * call[j];
-    }
-  }
-
-  if (tid == 0) {
-    d_CallValue[blockIdx.x] = call[0];
-  }
-}
diff --git a/src/samples/Samples/5_Domain_Specific/dwtHaar1D/dwtHaar1D.cu.hip b/src/samples/Samples/5_Domain_Specific/dwtHaar1D/dwtHaar1D.cu.hip
index 79d9421..e69de29 100755
--- a/src/samples/Samples/5_Domain_Specific/dwtHaar1D/dwtHaar1D.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/dwtHaar1D/dwtHaar1D.cu.hip
@@ -1,391 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
-* 1D DWT for Haar wavelet and signals with a length which is a power of 2.
-* The code reduces bank conflicts and non-coalesced reads / writes as
-* appropriate but does not fully remove them because the computational
-* overhead to achieve this would outweighs the benefit (see inline comments
-* for more details).
-* Large signals are subdivided into sub-signals with 512 elements and the
-* wavelet transform for these is computed with one block over 10 decomposition
-* levels. The resulting signal consisting of the approximation coefficients at
-* level X is then processed in a subsequent step on the device. This requires
-* interblock synchronization which is only possible on host side.
-* Detail coefficients which have been computed are not further referenced
-* during the decomposition so that they can be stored directly in their final
-* position in global memory. The transform and its storing scheme preserve
-* locality in the coefficients so that these writes are coalesced.
-* Approximation coefficients are stored in shared memory because they are
-* needed to compute the subsequent decomposition step. The top most
-* approximation coefficient for a sub-signal processed by one block is stored
-* in a special global memory location to simplify the processing after the
-* interblock synchronization.
-* Most books on wavelets explain the Haar wavelet decomposition. A good freely
-* available resource is the Wavelet primer by Stollnitz et al.
-* http://grail.cs.washington.edu/projects/wavelets/article/wavelet1.pdf
-* http://grail.cs.washington.edu/projects/wavelets/article/wavelet2.pdf
-* The basic of all Wavelet transforms is to decompose a signal into
-* approximation (a) and detail (d) coefficients where the detail tends to be
-* small or zero which allows / simplifies compression. The following "graphs"
-* demonstrate the transform for a signal
-* of length eight. The index always describes the decomposition level where
-* a coefficient arises. The input signal is interpreted as approximation signal
-* at level 0. The coefficients computed on the device are stored in the same
-* scheme as in the example. This data structure is particularly well suited for
-* compression and also preserves the hierarchical structure of the
-decomposition.
-
--------------------------------------------------
-| a_0 | a_0 | a_0 | a_0 | a_0 | a_0 | a_0 | a_0 |
--------------------------------------------------
-
--------------------------------------------------
-| a_1 | a_1 | a_1 | a_1 | d_1 | d_1 | d_1 | d_1 |
--------------------------------------------------
-
--------------------------------------------------
-| a_2 | a_2 | d_2 | d_2 | d_1 | d_1 | d_1 | d_1 |
--------------------------------------------------
-
--------------------------------------------------
-| a_3 | d_3 | d_2 | d_2 | d_1 | d_1 | d_1 | d_1 |
--------------------------------------------------
-
-* Host code.
-*/
-
-#ifdef _WIN32
-#define NOMINMAX
-#endif
-
-// includes, system
-#include <stdlib.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <string.h>
-#include <math.h>
-#include <assert.h>
-
-// includes, project
-#include "helper_functions.h"
-#include "helper_cuda_hipified.h"
-
-// constants which are used in host and device code
-#define INV_SQRT_2 0.70710678118654752440f;
-const unsigned int LOG_NUM_BANKS = 4;
-const unsigned int NUM_BANKS = 16;
-
-////////////////////////////////////////////////////////////////////////////////
-// includes, kernels
-#include "dwtHaar1D_kernel.cuh"
-
-////////////////////////////////////////////////////////////////////////////////
-// declaration, forward
-void runTest(int argc, char **argv);
-bool getLevels(unsigned int len, unsigned int *levels);
-
-////////////////////////////////////////////////////////////////////////////////
-// Program main
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) {
-  // run test
-  runTest(argc, argv);
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Perform the wavelet decomposition
-////////////////////////////////////////////////////////////////////////////////
-void runTest(int argc, char **argv) {
-  bool bResult = false;  // flag for final validation of the results
-
-  char *s_fname = NULL, *r_gold_fname = NULL;
-  char r_fname[256];
-  const char usage[] = {
-      "\nUsage:\n"
-      "  dwtHaar1D --signal=<signal_file> --result=<result_file> "
-      "--gold=<gold_file>\n\n"
-      "  <signal_file> Input file containing the signal\n"
-      "  <result_file> Output file storing the result of the wavelet "
-      "decomposition\n"
-      "  <gold_file>   Input file containing the reference result of the "
-      "wavelet decomposition\n"
-      "\nExample:\n"
-      "  ./dwtHaar1D\n"
-      "       --signal=signal.dat\n"
-      "       --result=result.dat\n"
-      "       --gold=regression.gold.dat\n"};
-
-  printf("%s Starting...\n\n", argv[0]);
-
-  // use command-line specified CUDA device, otherwise use device with highest
-  // Gflops/s
-  findCudaDevice(argc, (const char **)argv);
-
-  // file names, either specified as cmd line args or use default
-  if (argc == 4) {
-    char *tmp_sfname, *tmp_rfname, *tmp_goldfname;
-
-    if ((getCmdLineArgumentString(argc, (const char **)argv, "signal",
-                                  &tmp_sfname) != true) ||
-        (getCmdLineArgumentString(argc, (const char **)argv, "result",
-                                  &tmp_rfname) != true) ||
-        (getCmdLineArgumentString(argc, (const char **)argv, "gold",
-                                  &tmp_goldfname) != true)) {
-      fprintf(stderr, "Invalid input syntax.\n%s", usage);
-      exit(EXIT_FAILURE);
-    }
-
-    s_fname = sdkFindFilePath(tmp_sfname, argv[0]);
-    r_gold_fname = sdkFindFilePath(tmp_goldfname, argv[0]);
-    strcpy(r_fname, tmp_rfname);
-  } else {
-    s_fname = sdkFindFilePath("signal.dat", argv[0]);
-    r_gold_fname = sdkFindFilePath("regression.gold.dat", argv[0]);
-    strcpy(r_fname, "result.dat");
-  }
-
-  printf("source file    = \"%s\"\n", s_fname);
-  printf("reference file = \"%s\"\n", r_fname);
-  printf("gold file      = \"%s\"\n", r_gold_fname);
-
-  // read in signal
-  unsigned int slength = 0;
-  float *signal = NULL;
-
-  if (s_fname == NULL) {
-    fprintf(stderr, "Cannot find the file containing the signal.\n%s", usage);
-
-    exit(EXIT_FAILURE);
-  }
-
-  if (sdkReadFile(s_fname, &signal, &slength, false) == true) {
-    printf("Reading signal from \"%s\"\n", s_fname);
-  } else {
-    exit(EXIT_FAILURE);
-  }
-
-  // get the number of decompositions necessary to perform a full decomposition
-  unsigned int dlevels_complete = 0;
-
-  if (true != getLevels(slength, &dlevels_complete)) {
-    // error message
-    fprintf(stderr, "Signal length not supported.\n");
-    // cleanup and abort
-    free(signal);
-    exit(EXIT_FAILURE);
-  }
-
-  // device in data
-  float *d_idata = NULL;
-  // device out data
-  float *d_odata = NULL;
-  // device approx_final data
-  float *approx_final = NULL;
-  // The very final approximation coefficient has to be written to the output
-  // data, all others are reused as input data in the next global step and
-  // therefore have to be written to the input data again.
-  // The following flag indicates where to copy approx_final data
-  //   - 0 is input, 1 is output
-  int approx_is_input;
-
-  // allocate device mem
-  const unsigned int smem_size = sizeof(float) * slength;
-  HIPCHECK(hipMalloc((void **)&d_idata, smem_size));
-  HIPCHECK(hipMalloc((void **)&d_odata, smem_size));
-  HIPCHECK(hipMalloc((void **)&approx_final, smem_size));
-  // copy input data to device
-  HIPCHECK(
-      hipMemcpy(d_idata, signal, smem_size, hipMemcpyHostToDevice));
-
-  // total number of threads
-  // in the first decomposition step always one thread computes the average and
-  // detail signal for one pair of adjacent values
-  unsigned int num_threads_total_left = slength / 2;
-  // decomposition levels performed in the current / next step
-  unsigned int dlevels_step = dlevels_complete;
-
-  // 1D signal so the arrangement of elements is also 1D
-  dim3 block_size;
-  dim3 grid_size;
-
-  // number of decomposition levels left after one iteration on the device
-  unsigned int dlevels_left = dlevels_complete;
-
-  // if less or equal 1k elements, then the data can be processed in one block,
-  // this avoids the Wait-For-Idle (WFI) on host side which is necessary if the
-  // computation is split across multiple SM's if enough input data
-  if (dlevels_complete <= 10) {
-    // decomposition can be performed at once
-    block_size.x = num_threads_total_left;
-    approx_is_input = 0;
-  } else {
-    // 512 threads per block
-    grid_size.x = (num_threads_total_left / 512);
-    block_size.x = 512;
-
-    // 512 threads corresponds to 10 decomposition steps
-    dlevels_step = 10;
-    dlevels_left -= 10;
-
-    approx_is_input = 1;
-  }
-
-  // Initialize d_odata to 0.0f
-  initValue<<<grid_size, block_size>>>(d_odata, 0.0f);
-
-  // do until full decomposition is accomplished
-  while (0 != num_threads_total_left) {
-    // double the number of threads as bytes
-    unsigned int mem_shared = (2 * block_size.x) * sizeof(float);
-    // extra memory requirements to avoid bank conflicts
-    mem_shared += ((2 * block_size.x) / NUM_BANKS) * sizeof(float);
-
-    // run kernel
-    dwtHaar1D<<<grid_size, block_size, mem_shared>>>(
-        d_idata, d_odata, approx_final, dlevels_step, num_threads_total_left,
-        block_size.x);
-
-    // Copy approx_final to appropriate location
-    if (approx_is_input) {
-      HIPCHECK(hipMemcpy(d_idata, approx_final, grid_size.x * 4,
-                                 hipMemcpyDeviceToDevice));
-    } else {
-      HIPCHECK(hipMemcpy(d_odata, approx_final, grid_size.x * 4,
-                                 hipMemcpyDeviceToDevice));
-    }
-
-    // update level variables
-    if (dlevels_left < 10) {
-      // approx_final = d_odata;
-      approx_is_input = 0;
-    }
-
-    // more global steps necessary
-    dlevels_step = (dlevels_left > 10) ? dlevels_left - 10 : dlevels_left;
-    dlevels_left -= 10;
-
-    // after each step only half the threads are used any longer
-    // therefore after 10 steps 2^10 less threads
-    num_threads_total_left = num_threads_total_left >> 10;
-
-    // update block and grid size
-    grid_size.x =
-        (num_threads_total_left / 512) + (0 != (num_threads_total_left % 512))
-            ? 1
-            : 0;
-
-    if (grid_size.x <= 1) {
-      block_size.x = num_threads_total_left;
-    }
-  }
-
-  // get the result back from the server
-  // allocate mem for the result
-  float *odata = (float *)malloc(smem_size);
-  HIPCHECK(
-      hipMemcpy(odata, d_odata, smem_size, hipMemcpyDeviceToHost));
-
-  // post processing
-  // write file for regression test
-  if (r_fname == NULL) {
-    fprintf(stderr,
-            "Cannot write the output file storing the result of the wavelet "
-            "decomposition.\n%s",
-            usage);
-    exit(EXIT_FAILURE);
-  }
-
-  if (sdkWriteFile(r_fname, odata, slength, 0.001f, false) == true) {
-    printf("Writing result to \"%s\"\n", r_fname);
-  } else {
-    exit(EXIT_FAILURE);
-  }
-
-  // load the reference solution
-  unsigned int len_reference = 0;
-  float *reference = NULL;
-
-  if (r_gold_fname == NULL) {
-    fprintf(stderr,
-            "Cannot read the file containing the reference result of the "
-            "wavelet decomposition.\n%s",
-            usage);
-
-    exit(EXIT_FAILURE);
-  }
-
-  if (sdkReadFile(r_gold_fname, &reference, &len_reference, false) == true) {
-    printf("Reading reference result from \"%s\"\n", r_gold_fname);
-  } else {
-    exit(EXIT_FAILURE);
-  }
-
-  assert(slength == len_reference);
-
-  // compare the computed solution and the reference
-  bResult = (bool)sdkCompareL2fe(reference, odata, slength, 0.001f);
-  free(reference);
-
-  // free allocated host and device memory
-  HIPCHECK(hipFree(d_odata));
-  HIPCHECK(hipFree(d_idata));
-  HIPCHECK(hipFree(approx_final));
-
-  free(signal);
-  free(odata);
-  free(s_fname);
-  free(r_gold_fname);
-
-  printf(bResult ? "Test success!\n" : "Test failure!\n");
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Get number of decomposition levels to perform a full decomposition
-//! Also check if the input signal size is suitable
-//! @return  true if the number of decomposition levels could be determined
-//!          and the signal length is supported by the implementation,
-//!          otherwise false
-//! @param   len  length of input signal
-//! @param   levels  number of decomposition levels necessary to perform a full
-//!           decomposition
-////////////////////////////////////////////////////////////////////////////////
-bool getLevels(unsigned int len, unsigned int *levels) {
-  bool retval = false;
-
-  // currently signals up to a length of 2^20 supported
-  for (unsigned int i = 0; i < 20; ++i) {
-    if (len == (1 << i)) {
-      *levels = i;
-      retval = true;
-      break;
-    }
-  }
-
-  return retval;
-}
diff --git a/src/samples/Samples/5_Domain_Specific/dxtc/dxtc.cu.hip b/src/samples/Samples/5_Domain_Specific/dxtc/dxtc.cu.hip
index ffc0b68..662ba28 100755
--- a/src/samples/Samples/5_Domain_Specific/dxtc/dxtc.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/dxtc/dxtc.cu.hip
@@ -37,9 +37,9 @@ namespace cg = cooperative_groups;
 #include <helper_math.h>
 #include <float.h>  // for FLT_MAX
 
-#include "CudaMath.h"
-#include "dds.h"
-#include "permutations.h"
+#include "CudaMath_hipified.h"
+#include "dds_hipified.h"
+#include "permutations_hipified.h"
 
 // Definitions
 #define INPUT_IMAGE "teapot512_std.ppm"
@@ -611,12 +611,12 @@ int main(int argc, char **argv) {
 
   // copy into global mem
   uint *d_data = NULL;
-  HIPCHECK(hipMalloc((void **)&d_data, memSize));
+  checkCudaErrors(hipMalloc((void **)&d_data, memSize));
 
   // Result
   uint *d_result = NULL;
   const uint compressedSize = (w / 4) * (h / 4) * 8;
-  HIPCHECK(hipMalloc((void **)&d_result, compressedSize));
+  checkCudaErrors(hipMalloc((void **)&d_result, compressedSize));
   uint *h_result = (uint *)malloc(compressedSize);
 
   // Compute permutations.
@@ -625,8 +625,8 @@ int main(int argc, char **argv) {
 
   // Copy permutations host to devie.
   uint *d_permutations = NULL;
-  HIPCHECK(hipMalloc((void **)&d_permutations, 1024 * sizeof(uint)));
-  HIPCHECK(hipMemcpy(d_permutations, permutations, 1024 * sizeof(uint),
+  checkCudaErrors(hipMalloc((void **)&d_permutations, 1024 * sizeof(uint)));
+  checkCudaErrors(hipMemcpy(d_permutations, permutations, 1024 * sizeof(uint),
                              hipMemcpyHostToDevice));
 
   // create a timer
@@ -634,7 +634,7 @@ int main(int argc, char **argv) {
   sdkCreateTimer(&timer);
 
   // Copy image from host to device
-  HIPCHECK(
+  checkCudaErrors(
       hipMemcpy(d_data, block_image, memSize, hipMemcpyHostToDevice));
 
   // Determine launch configuration and run timed computation numIterations
@@ -646,8 +646,8 @@ int main(int argc, char **argv) {
   hipDeviceProp_t deviceProp;
 
   // get number of SMs on this GPU
-  HIPCHECK(hipGetDevice(&devID));
-  HIPCHECK(hipGetDeviceProperties(&deviceProp, devID));
+  checkCudaErrors(hipGetDevice(&devID));
+  checkCudaErrors(hipGetDeviceProperties(&deviceProp, devID));
 
   // Restrict the numbers of blocks to launch on low end GPUs to avoid kernel
   // timeout
@@ -660,7 +660,7 @@ int main(int argc, char **argv) {
 
   for (int i = -1; i < numIterations; ++i) {
     if (i == 0) {
-      HIPCHECK(hipDeviceSynchronize());
+      checkCudaErrors(hipDeviceSynchronize());
       sdkStartTimer(&timer);
     }
 
@@ -673,7 +673,7 @@ int main(int argc, char **argv) {
   getLastCudaError("compress");
 
   // sync to host, stop timer, record perf
-  HIPCHECK(hipDeviceSynchronize());
+  checkCudaErrors(hipDeviceSynchronize());
   sdkStopTimer(&timer);
   double dAvgTime = 1.0e-3 * sdkGetTimerValue(&timer) / (double)numIterations;
   printf(
@@ -682,7 +682,7 @@ int main(int argc, char **argv) {
       (1.0e-6 * (double)(W * H) / dAvgTime), dAvgTime, (W * H), 1, NUM_THREADS);
 
   // copy result data from device to host
-  HIPCHECK(
+  checkCudaErrors(
       hipMemcpy(h_result, d_result, compressedSize, hipMemcpyDeviceToHost));
 
   // Write out result data to DDS file
@@ -770,9 +770,9 @@ int main(int argc, char **argv) {
   rms /= w * h * 3;
 
   // Free allocated resources and exit
-  HIPCHECK(hipFree(d_permutations));
-  HIPCHECK(hipFree(d_data));
-  HIPCHECK(hipFree(d_result));
+  checkCudaErrors(hipFree(d_permutations));
+  checkCudaErrors(hipFree(d_data));
+  checkCudaErrors(hipFree(d_result));
   free(image_path);
   free(data);
   free(block_image);
diff --git a/src/samples/Samples/5_Domain_Specific/nbody/bodysystemcuda.cu.hip b/src/samples/Samples/5_Domain_Specific/nbody/bodysystemcuda.cu.hip
index 8823fdd..e69de29 100755
--- a/src/samples/Samples/5_Domain_Specific/nbody/bodysystemcuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/nbody/bodysystemcuda.cu.hip
@@ -1,288 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "helper_cuda_hipified.h"
-#include <math.h>
-
-#if defined(__APPLE__) || defined(MACOSX)
-#pragma clang diagnostic ignored "-Wdeprecated-declarations"
-#include <GLUT/glut.h>
-#else
-#include <GL/freeglut.h>
-#endif
-
-// CUDA standard includes
-#include <hip/hip_runtime.h>
-#include <cuda_gl_interop.h>
-
-#include <hip/hip_cooperative_groups.h>
-
-namespace cg = cooperative_groups;
-
-#include "bodysystem.h"
-
-__constant__ float softeningSquared;
-__constant__ double softeningSquared_fp64;
-
-hipError_t setSofteningSquared(float softeningSq) {
-  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared), &softeningSq, sizeof(float), 0,
-                            hipMemcpyHostToDevice);
-}
-
-hipError_t setSofteningSquared(double softeningSq) {
-  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared_fp64), &softeningSq, sizeof(double),
-                            0, hipMemcpyHostToDevice);
-}
-
-template <class T>
-struct SharedMemory {
-  __device__ inline operator T *() {
-    extern __shared__ int __smem[];
-    return (T *)__smem;
-  }
-
-  __device__ inline operator const T *() const {
-    extern __shared__ int __smem[];
-    return (T *)__smem;
-  }
-};
-
-template <typename T>
-__device__ T rsqrt_T(T x) {
-  return rsqrt(x);
-}
-
-template <>
-__device__ float rsqrt_T<float>(float x) {
-  return rsqrtf(x);
-}
-
-template <>
-__device__ double rsqrt_T<double>(double x) {
-  return rsqrt(x);
-}
-
-// Macros to simplify shared memory addressing
-#define SX(i) sharedPos[i + blockDim.x * threadIdx.y]
-// This macro is only used when multithreadBodies is true (below)
-#define SX_SUM(i, j) sharedPos[i + blockDim.x * j]
-
-template <typename T>
-__device__ T getSofteningSquared() {
-  return softeningSquared;
-}
-template <>
-__device__ double getSofteningSquared<double>() {
-  return softeningSquared_fp64;
-}
-
-template <typename T>
-struct DeviceData {
-  T *dPos[2];  // mapped host pointers
-  T *dVel;
-  hipEvent_t event;
-  unsigned int offset;
-  unsigned int numBodies;
-};
-
-template <typename T>
-__device__ typename vec3<T>::Type bodyBodyInteraction(
-    typename vec3<T>::Type ai, typename vec4<T>::Type bi,
-    typename vec4<T>::Type bj) {
-  typename vec3<T>::Type r;
-
-  // r_ij  [3 FLOPS]
-  r.x = bj.x - bi.x;
-  r.y = bj.y - bi.y;
-  r.z = bj.z - bi.z;
-
-  // distSqr = dot(r_ij, r_ij) + EPS^2  [6 FLOPS]
-  T distSqr = r.x * r.x + r.y * r.y + r.z * r.z;
-  distSqr += getSofteningSquared<T>();
-
-  // invDistCube =1/distSqr^(3/2)  [4 FLOPS (2 mul, 1 sqrt, 1 inv)]
-  T invDist = rsqrt_T(distSqr);
-  T invDistCube = invDist * invDist * invDist;
-
-  // s = m_j * invDistCube [1 FLOP]
-  T s = bj.w * invDistCube;
-
-  // a_i =  a_i + s * r_ij [6 FLOPS]
-  ai.x += r.x * s;
-  ai.y += r.y * s;
-  ai.z += r.z * s;
-
-  return ai;
-}
-
-template <typename T>
-__device__ typename vec3<T>::Type computeBodyAccel(
-    typename vec4<T>::Type bodyPos, typename vec4<T>::Type *positions,
-    int numTiles, cg::thread_block cta) {
-  typename vec4<T>::Type *sharedPos = SharedMemory<typename vec4<T>::Type>();
-
-  typename vec3<T>::Type acc = {0.0f, 0.0f, 0.0f};
-
-  for (int tile = 0; tile < numTiles; tile++) {
-    sharedPos[threadIdx.x] = positions[tile * blockDim.x + threadIdx.x];
-
-    cg::sync(cta);
-
-// This is the "tile_calculation" from the GPUG3 article.
-#pragma unroll 128
-
-    for (unsigned int counter = 0; counter < blockDim.x; counter++) {
-      acc = bodyBodyInteraction<T>(acc, bodyPos, sharedPos[counter]);
-    }
-
-    cg::sync(cta);
-  }
-
-  return acc;
-}
-
-template <typename T>
-__global__ void integrateBodies(typename vec4<T>::Type *__restrict__ newPos,
-                                typename vec4<T>::Type *__restrict__ oldPos,
-                                typename vec4<T>::Type *vel,
-                                unsigned int deviceOffset,
-                                unsigned int deviceNumBodies, float deltaTime,
-                                float damping, int numTiles) {
-  // Handle to thread block group
-  cg::thread_block cta = cg::this_thread_block();
-  int index = blockIdx.x * blockDim.x + threadIdx.x;
-
-  if (index >= deviceNumBodies) {
-    return;
-  }
-
-  typename vec4<T>::Type position = oldPos[deviceOffset + index];
-
-  typename vec3<T>::Type accel =
-      computeBodyAccel<T>(position, oldPos, numTiles, cta);
-
-  // acceleration = force / mass;
-  // new velocity = old velocity + acceleration * deltaTime
-  // note we factor out the body's mass from the equation, here and in
-  // bodyBodyInteraction
-  // (because they cancel out).  Thus here force == acceleration
-  typename vec4<T>::Type velocity = vel[deviceOffset + index];
-
-  velocity.x += accel.x * deltaTime;
-  velocity.y += accel.y * deltaTime;
-  velocity.z += accel.z * deltaTime;
-
-  velocity.x *= damping;
-  velocity.y *= damping;
-  velocity.z *= damping;
-
-  // new position = old position + velocity * deltaTime
-  position.x += velocity.x * deltaTime;
-  position.y += velocity.y * deltaTime;
-  position.z += velocity.z * deltaTime;
-
-  // store new position and velocity
-  newPos[deviceOffset + index] = position;
-  vel[deviceOffset + index] = velocity;
-}
-
-template <typename T>
-void integrateNbodySystem(DeviceData<T> *deviceData,
-                          hipGraphicsResource **pgres,
-                          unsigned int currentRead, float deltaTime,
-                          float damping, unsigned int numBodies,
-                          unsigned int numDevices, int blockSize,
-                          bool bUsePBO) {
-  if (bUsePBO) {
-    HIPCHECK(cudaGraphicsResourceSetMapFlags(
-        pgres[currentRead], cudaGraphicsMapFlagsReadOnly));
-    HIPCHECK(cudaGraphicsResourceSetMapFlags(
-        pgres[1 - currentRead], cudaGraphicsMapFlagsWriteDiscard));
-    HIPCHECK(hipGraphicsMapResources(2, pgres, 0));
-    size_t bytes;
-    HIPCHECK(hipGraphicsResourceGetMappedPointer(
-        (void **)&(deviceData[0].dPos[currentRead]), &bytes,
-        pgres[currentRead]));
-    HIPCHECK(hipGraphicsResourceGetMappedPointer(
-        (void **)&(deviceData[0].dPos[1 - currentRead]), &bytes,
-        pgres[1 - currentRead]));
-  }
-
-  for (unsigned int dev = 0; dev != numDevices; dev++) {
-    if (numDevices > 1) {
-      hipSetDevice(dev);
-    }
-
-    int numBlocks = (deviceData[dev].numBodies + blockSize - 1) / blockSize;
-    int numTiles = (numBodies + blockSize - 1) / blockSize;
-    int sharedMemSize = blockSize * 4 * sizeof(T);  // 4 floats for pos
-
-    integrateBodies<T><<<numBlocks, blockSize, sharedMemSize>>>(
-        (typename vec4<T>::Type *)deviceData[dev].dPos[1 - currentRead],
-        (typename vec4<T>::Type *)deviceData[dev].dPos[currentRead],
-        (typename vec4<T>::Type *)deviceData[dev].dVel, deviceData[dev].offset,
-        deviceData[dev].numBodies, deltaTime, damping, numTiles);
-
-    if (numDevices > 1) {
-      HIPCHECK(hipEventRecord(deviceData[dev].event));
-      // MJH: Hack on older driver versions to force kernel launches to flush!
-      hipStreamQuery(0);
-    }
-
-    // check if kernel invocation generated an error
-    getLastCudaError("Kernel execution failed");
-  }
-
-  if (numDevices > 1) {
-    for (unsigned int dev = 0; dev < numDevices; dev++) {
-      HIPCHECK(hipEventSynchronize(deviceData[dev].event));
-    }
-  }
-
-  if (bUsePBO) {
-    HIPCHECK(hipGraphicsUnmapResources(2, pgres, 0));
-  }
-}
-
-// Explicit specializations needed to generate code
-template void integrateNbodySystem<float>(DeviceData<float> *deviceData,
-                                          hipGraphicsResource **pgres,
-                                          unsigned int currentRead,
-                                          float deltaTime, float damping,
-                                          unsigned int numBodies,
-                                          unsigned int numDevices,
-                                          int blockSize, bool bUsePBO);
-
-template void integrateNbodySystem<double>(DeviceData<double> *deviceData,
-                                           hipGraphicsResource **pgres,
-                                           unsigned int currentRead,
-                                           float deltaTime, float damping,
-                                           unsigned int numBodies,
-                                           unsigned int numDevices,
-                                           int blockSize, bool bUsePBO);
-                          int blockSize, bool bUsePBO);
diff --git a/src/samples/Samples/5_Domain_Specific/nbody_opengles/bodysystemcuda.cu.hip b/src/samples/Samples/5_Domain_Specific/nbody_opengles/bodysystemcuda.cu.hip
index 986ac9a..e69de29 100755
--- a/src/samples/Samples/5_Domain_Specific/nbody_opengles/bodysystemcuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/nbody_opengles/bodysystemcuda.cu.hip
@@ -1,278 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "helper_cuda_hipified.h"
-#include <math.h>
-
-//#include <GL/glew.h>
-//#include <GL/freeglut.h>
-
-// CUDA standard includes
-#include <hip/hip_runtime.h>
-//#include <cuda_gl_interop.h>
-
-#include "bodysystem.h"
-
-__constant__ float softeningSquared;
-__constant__ double softeningSquared_fp64;
-
-hipError_t setSofteningSquared(float softeningSq) {
-  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared), &softeningSq, sizeof(float), 0,
-                            hipMemcpyHostToDevice);
-}
-
-hipError_t setSofteningSquared(double softeningSq) {
-  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared_fp64), &softeningSq, sizeof(double),
-                            0, hipMemcpyHostToDevice);
-}
-
-template <class T>
-struct SharedMemory {
-  __device__ inline operator T *() {
-    extern __shared__ int __smem[];
-    return (T *)__smem;
-  }
-
-  __device__ inline operator const T *() const {
-    extern __shared__ int __smem[];
-    return (T *)__smem;
-  }
-};
-
-template <typename T>
-__device__ T rsqrt_T(T x) {
-  return rsqrt(x);
-}
-
-template <>
-__device__ float rsqrt_T<float>(float x) {
-  return rsqrtf(x);
-}
-
-template <>
-__device__ double rsqrt_T<double>(double x) {
-  return rsqrt(x);
-}
-
-// Macros to simplify shared memory addressing
-#define SX(i) sharedPos[i + blockDim.x * threadIdx.y]
-// This macro is only used when multithreadBodies is true (below)
-#define SX_SUM(i, j) sharedPos[i + blockDim.x * j]
-
-template <typename T>
-__device__ T getSofteningSquared() {
-  return softeningSquared;
-}
-template <>
-__device__ double getSofteningSquared<double>() {
-  return softeningSquared_fp64;
-}
-
-template <typename T>
-struct DeviceData {
-  T *dPos[2];  // mapped host pointers
-  T *dVel;
-  hipEvent_t event;
-  unsigned int offset;
-  unsigned int numBodies;
-};
-
-template <typename T>
-__device__ typename vec3<T>::Type bodyBodyInteraction(
-    typename vec3<T>::Type ai, typename vec4<T>::Type bi,
-    typename vec4<T>::Type bj) {
-  typename vec3<T>::Type r;
-
-  // r_ij  [3 FLOPS]
-  r.x = bj.x - bi.x;
-  r.y = bj.y - bi.y;
-  r.z = bj.z - bi.z;
-
-  // distSqr = dot(r_ij, r_ij) + EPS^2  [6 FLOPS]
-  T distSqr = r.x * r.x + r.y * r.y + r.z * r.z;
-  distSqr += getSofteningSquared<T>();
-
-  // invDistCube =1/distSqr^(3/2)  [4 FLOPS (2 mul, 1 sqrt, 1 inv)]
-  T invDist = rsqrt_T(distSqr);
-  T invDistCube = invDist * invDist * invDist;
-
-  // s = m_j * invDistCube [1 FLOP]
-  T s = bj.w * invDistCube;
-
-  // a_i =  a_i + s * r_ij [6 FLOPS]
-  ai.x += r.x * s;
-  ai.y += r.y * s;
-  ai.z += r.z * s;
-
-  return ai;
-}
-
-template <typename T>
-__device__ typename vec3<T>::Type computeBodyAccel(
-    typename vec4<T>::Type bodyPos, typename vec4<T>::Type *positions,
-    int numTiles) {
-  typename vec4<T>::Type *sharedPos = SharedMemory<typename vec4<T>::Type>();
-
-  typename vec3<T>::Type acc = {0.0f, 0.0f, 0.0f};
-
-  for (int tile = 0; tile < numTiles; tile++) {
-    sharedPos[threadIdx.x] = positions[tile * blockDim.x + threadIdx.x];
-
-    __syncthreads();
-
-    // This is the "tile_calculation" from the GPUG3 article.
-#pragma unroll 128
-
-    for (unsigned int counter = 0; counter < blockDim.x; counter++) {
-      acc = bodyBodyInteraction<T>(acc, bodyPos, sharedPos[counter]);
-    }
-
-    __syncthreads();
-  }
-
-  return acc;
-}
-
-template <typename T>
-__global__ void integrateBodies(typename vec4<T>::Type *__restrict__ newPos,
-                                typename vec4<T>::Type *__restrict__ oldPos,
-                                typename vec4<T>::Type *vel,
-                                unsigned int deviceOffset,
-                                unsigned int deviceNumBodies, float deltaTime,
-                                float damping, int numTiles) {
-  int index = blockIdx.x * blockDim.x + threadIdx.x;
-
-  if (index >= deviceNumBodies) {
-    return;
-  }
-
-  typename vec4<T>::Type position = oldPos[deviceOffset + index];
-
-  typename vec3<T>::Type accel =
-      computeBodyAccel<T>(position, oldPos, numTiles);
-
-  // acceleration = force / mass;
-  // new velocity = old velocity + acceleration * deltaTime
-  // note we factor out the body's mass from the equation, here and in
-  // bodyBodyInteraction (because they cancel out).  Thus here force ==
-  // acceleration
-  typename vec4<T>::Type velocity = vel[deviceOffset + index];
-
-  velocity.x += accel.x * deltaTime;
-  velocity.y += accel.y * deltaTime;
-  velocity.z += accel.z * deltaTime;
-
-  velocity.x *= damping;
-  velocity.y *= damping;
-  velocity.z *= damping;
-
-  // new position = old position + velocity * deltaTime
-  position.x += velocity.x * deltaTime;
-  position.y += velocity.y * deltaTime;
-  position.z += velocity.z * deltaTime;
-
-  // store new position and velocity
-  newPos[deviceOffset + index] = position;
-  vel[deviceOffset + index] = velocity;
-}
-
-template <typename T>
-void integrateNbodySystem(DeviceData<T> *deviceData,
-                          hipGraphicsResource **pgres,
-                          unsigned int currentRead, float deltaTime,
-                          float damping, unsigned int numBodies,
-                          unsigned int numDevices, int blockSize,
-                          bool bUsePBO) {
-  if (bUsePBO) {
-    HIPCHECK(cudaGraphicsResourceSetMapFlags(
-        pgres[currentRead], cudaGraphicsMapFlagsReadOnly));
-    HIPCHECK(cudaGraphicsResourceSetMapFlags(
-        pgres[1 - currentRead], cudaGraphicsMapFlagsWriteDiscard));
-    HIPCHECK(hipGraphicsMapResources(2, pgres, 0));
-    size_t bytes;
-    HIPCHECK(hipGraphicsResourceGetMappedPointer(
-        (void **)&(deviceData[0].dPos[currentRead]), &bytes,
-        pgres[currentRead]));
-    HIPCHECK(hipGraphicsResourceGetMappedPointer(
-        (void **)&(deviceData[0].dPos[1 - currentRead]), &bytes,
-        pgres[1 - currentRead]));
-  }
-
-  for (unsigned int dev = 0; dev != numDevices; dev++) {
-    if (numDevices > 1) {
-      hipSetDevice(dev);
-    }
-
-    int numBlocks = (deviceData[dev].numBodies + blockSize - 1) / blockSize;
-    int numTiles = (numBodies + blockSize - 1) / blockSize;
-    int sharedMemSize = blockSize * 4 * sizeof(T);  // 4 floats for pos
-
-    integrateBodies<T><<<numBlocks, blockSize, sharedMemSize>>>(
-        (typename vec4<T>::Type *)deviceData[dev].dPos[1 - currentRead],
-        (typename vec4<T>::Type *)deviceData[dev].dPos[currentRead],
-        (typename vec4<T>::Type *)deviceData[dev].dVel, deviceData[dev].offset,
-        deviceData[dev].numBodies, deltaTime, damping, numTiles);
-
-    if (numDevices > 1) {
-      HIPCHECK(hipEventRecord(deviceData[dev].event));
-      // MJH: Hack on older driver versions to force kernel launches to flush!
-      hipStreamQuery(0);
-    }
-
-    // check if kernel invocation generated an error
-    getLastCudaError("Kernel execution failed");
-  }
-
-  if (numDevices > 1) {
-    for (unsigned int dev = 0; dev < numDevices; dev++) {
-      HIPCHECK(hipEventSynchronize(deviceData[dev].event));
-    }
-  }
-
-  if (bUsePBO) {
-    HIPCHECK(hipGraphicsUnmapResources(2, pgres, 0));
-  }
-}
-
-// Explicit specializations needed to generate code
-template void integrateNbodySystem<float>(DeviceData<float> *deviceData,
-                                          hipGraphicsResource **pgres,
-                                          unsigned int currentRead,
-                                          float deltaTime, float damping,
-                                          unsigned int numBodies,
-                                          unsigned int numDevices,
-                                          int blockSize, bool bUsePBO);
-
-template void integrateNbodySystem<double>(DeviceData<double> *deviceData,
-                                           hipGraphicsResource **pgres,
-                                           unsigned int currentRead,
-                                           float deltaTime, float damping,
-                                           unsigned int numBodies,
-                                           unsigned int numDevices,
-                                           int blockSize, bool bUsePBO);
-                          int blockSize, bool bUsePBO);
diff --git a/src/samples/Samples/5_Domain_Specific/nbody_screen/bodysystemcuda.cu.hip b/src/samples/Samples/5_Domain_Specific/nbody_screen/bodysystemcuda.cu.hip
index 986ac9a..e69de29 100755
--- a/src/samples/Samples/5_Domain_Specific/nbody_screen/bodysystemcuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/nbody_screen/bodysystemcuda.cu.hip
@@ -1,278 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "helper_cuda_hipified.h"
-#include <math.h>
-
-//#include <GL/glew.h>
-//#include <GL/freeglut.h>
-
-// CUDA standard includes
-#include <hip/hip_runtime.h>
-//#include <cuda_gl_interop.h>
-
-#include "bodysystem.h"
-
-__constant__ float softeningSquared;
-__constant__ double softeningSquared_fp64;
-
-hipError_t setSofteningSquared(float softeningSq) {
-  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared), &softeningSq, sizeof(float), 0,
-                            hipMemcpyHostToDevice);
-}
-
-hipError_t setSofteningSquared(double softeningSq) {
-  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared_fp64), &softeningSq, sizeof(double),
-                            0, hipMemcpyHostToDevice);
-}
-
-template <class T>
-struct SharedMemory {
-  __device__ inline operator T *() {
-    extern __shared__ int __smem[];
-    return (T *)__smem;
-  }
-
-  __device__ inline operator const T *() const {
-    extern __shared__ int __smem[];
-    return (T *)__smem;
-  }
-};
-
-template <typename T>
-__device__ T rsqrt_T(T x) {
-  return rsqrt(x);
-}
-
-template <>
-__device__ float rsqrt_T<float>(float x) {
-  return rsqrtf(x);
-}
-
-template <>
-__device__ double rsqrt_T<double>(double x) {
-  return rsqrt(x);
-}
-
-// Macros to simplify shared memory addressing
-#define SX(i) sharedPos[i + blockDim.x * threadIdx.y]
-// This macro is only used when multithreadBodies is true (below)
-#define SX_SUM(i, j) sharedPos[i + blockDim.x * j]
-
-template <typename T>
-__device__ T getSofteningSquared() {
-  return softeningSquared;
-}
-template <>
-__device__ double getSofteningSquared<double>() {
-  return softeningSquared_fp64;
-}
-
-template <typename T>
-struct DeviceData {
-  T *dPos[2];  // mapped host pointers
-  T *dVel;
-  hipEvent_t event;
-  unsigned int offset;
-  unsigned int numBodies;
-};
-
-template <typename T>
-__device__ typename vec3<T>::Type bodyBodyInteraction(
-    typename vec3<T>::Type ai, typename vec4<T>::Type bi,
-    typename vec4<T>::Type bj) {
-  typename vec3<T>::Type r;
-
-  // r_ij  [3 FLOPS]
-  r.x = bj.x - bi.x;
-  r.y = bj.y - bi.y;
-  r.z = bj.z - bi.z;
-
-  // distSqr = dot(r_ij, r_ij) + EPS^2  [6 FLOPS]
-  T distSqr = r.x * r.x + r.y * r.y + r.z * r.z;
-  distSqr += getSofteningSquared<T>();
-
-  // invDistCube =1/distSqr^(3/2)  [4 FLOPS (2 mul, 1 sqrt, 1 inv)]
-  T invDist = rsqrt_T(distSqr);
-  T invDistCube = invDist * invDist * invDist;
-
-  // s = m_j * invDistCube [1 FLOP]
-  T s = bj.w * invDistCube;
-
-  // a_i =  a_i + s * r_ij [6 FLOPS]
-  ai.x += r.x * s;
-  ai.y += r.y * s;
-  ai.z += r.z * s;
-
-  return ai;
-}
-
-template <typename T>
-__device__ typename vec3<T>::Type computeBodyAccel(
-    typename vec4<T>::Type bodyPos, typename vec4<T>::Type *positions,
-    int numTiles) {
-  typename vec4<T>::Type *sharedPos = SharedMemory<typename vec4<T>::Type>();
-
-  typename vec3<T>::Type acc = {0.0f, 0.0f, 0.0f};
-
-  for (int tile = 0; tile < numTiles; tile++) {
-    sharedPos[threadIdx.x] = positions[tile * blockDim.x + threadIdx.x];
-
-    __syncthreads();
-
-    // This is the "tile_calculation" from the GPUG3 article.
-#pragma unroll 128
-
-    for (unsigned int counter = 0; counter < blockDim.x; counter++) {
-      acc = bodyBodyInteraction<T>(acc, bodyPos, sharedPos[counter]);
-    }
-
-    __syncthreads();
-  }
-
-  return acc;
-}
-
-template <typename T>
-__global__ void integrateBodies(typename vec4<T>::Type *__restrict__ newPos,
-                                typename vec4<T>::Type *__restrict__ oldPos,
-                                typename vec4<T>::Type *vel,
-                                unsigned int deviceOffset,
-                                unsigned int deviceNumBodies, float deltaTime,
-                                float damping, int numTiles) {
-  int index = blockIdx.x * blockDim.x + threadIdx.x;
-
-  if (index >= deviceNumBodies) {
-    return;
-  }
-
-  typename vec4<T>::Type position = oldPos[deviceOffset + index];
-
-  typename vec3<T>::Type accel =
-      computeBodyAccel<T>(position, oldPos, numTiles);
-
-  // acceleration = force / mass;
-  // new velocity = old velocity + acceleration * deltaTime
-  // note we factor out the body's mass from the equation, here and in
-  // bodyBodyInteraction (because they cancel out).  Thus here force ==
-  // acceleration
-  typename vec4<T>::Type velocity = vel[deviceOffset + index];
-
-  velocity.x += accel.x * deltaTime;
-  velocity.y += accel.y * deltaTime;
-  velocity.z += accel.z * deltaTime;
-
-  velocity.x *= damping;
-  velocity.y *= damping;
-  velocity.z *= damping;
-
-  // new position = old position + velocity * deltaTime
-  position.x += velocity.x * deltaTime;
-  position.y += velocity.y * deltaTime;
-  position.z += velocity.z * deltaTime;
-
-  // store new position and velocity
-  newPos[deviceOffset + index] = position;
-  vel[deviceOffset + index] = velocity;
-}
-
-template <typename T>
-void integrateNbodySystem(DeviceData<T> *deviceData,
-                          hipGraphicsResource **pgres,
-                          unsigned int currentRead, float deltaTime,
-                          float damping, unsigned int numBodies,
-                          unsigned int numDevices, int blockSize,
-                          bool bUsePBO) {
-  if (bUsePBO) {
-    HIPCHECK(cudaGraphicsResourceSetMapFlags(
-        pgres[currentRead], cudaGraphicsMapFlagsReadOnly));
-    HIPCHECK(cudaGraphicsResourceSetMapFlags(
-        pgres[1 - currentRead], cudaGraphicsMapFlagsWriteDiscard));
-    HIPCHECK(hipGraphicsMapResources(2, pgres, 0));
-    size_t bytes;
-    HIPCHECK(hipGraphicsResourceGetMappedPointer(
-        (void **)&(deviceData[0].dPos[currentRead]), &bytes,
-        pgres[currentRead]));
-    HIPCHECK(hipGraphicsResourceGetMappedPointer(
-        (void **)&(deviceData[0].dPos[1 - currentRead]), &bytes,
-        pgres[1 - currentRead]));
-  }
-
-  for (unsigned int dev = 0; dev != numDevices; dev++) {
-    if (numDevices > 1) {
-      hipSetDevice(dev);
-    }
-
-    int numBlocks = (deviceData[dev].numBodies + blockSize - 1) / blockSize;
-    int numTiles = (numBodies + blockSize - 1) / blockSize;
-    int sharedMemSize = blockSize * 4 * sizeof(T);  // 4 floats for pos
-
-    integrateBodies<T><<<numBlocks, blockSize, sharedMemSize>>>(
-        (typename vec4<T>::Type *)deviceData[dev].dPos[1 - currentRead],
-        (typename vec4<T>::Type *)deviceData[dev].dPos[currentRead],
-        (typename vec4<T>::Type *)deviceData[dev].dVel, deviceData[dev].offset,
-        deviceData[dev].numBodies, deltaTime, damping, numTiles);
-
-    if (numDevices > 1) {
-      HIPCHECK(hipEventRecord(deviceData[dev].event));
-      // MJH: Hack on older driver versions to force kernel launches to flush!
-      hipStreamQuery(0);
-    }
-
-    // check if kernel invocation generated an error
-    getLastCudaError("Kernel execution failed");
-  }
-
-  if (numDevices > 1) {
-    for (unsigned int dev = 0; dev < numDevices; dev++) {
-      HIPCHECK(hipEventSynchronize(deviceData[dev].event));
-    }
-  }
-
-  if (bUsePBO) {
-    HIPCHECK(hipGraphicsUnmapResources(2, pgres, 0));
-  }
-}
-
-// Explicit specializations needed to generate code
-template void integrateNbodySystem<float>(DeviceData<float> *deviceData,
-                                          hipGraphicsResource **pgres,
-                                          unsigned int currentRead,
-                                          float deltaTime, float damping,
-                                          unsigned int numBodies,
-                                          unsigned int numDevices,
-                                          int blockSize, bool bUsePBO);
-
-template void integrateNbodySystem<double>(DeviceData<double> *deviceData,
-                                           hipGraphicsResource **pgres,
-                                           unsigned int currentRead,
-                                           float deltaTime, float damping,
-                                           unsigned int numBodies,
-                                           unsigned int numDevices,
-                                           int blockSize, bool bUsePBO);
-                          int blockSize, bool bUsePBO);
diff --git a/src/samples/Samples/5_Domain_Specific/quasirandomGenerator_nvrtc/quasirandomGenerator_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/quasirandomGenerator_nvrtc/quasirandomGenerator_kernel.cu.hip
index f825488..e69de29 100755
--- a/src/samples/Samples/5_Domain_Specific/quasirandomGenerator_nvrtc/quasirandomGenerator_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/quasirandomGenerator_nvrtc/quasirandomGenerator_kernel.cu.hip
@@ -1,160 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef QUASIRANDOMGENERATOR_KERNEL_CUH
-#define QUASIRANDOMGENERATOR_KERNEL_CUH
-
-#include "quasirandomGenerator_common.h"
-
-// Fast integer multiplication
-#define MUL(a, b) __umul24(a, b)
-
-////////////////////////////////////////////////////////////////////////////////
-// Niederreiter quasirandom number generation kernel
-////////////////////////////////////////////////////////////////////////////////
-__constant__ unsigned int c_Table[QRNG_DIMENSIONS][QRNG_RESOLUTION];
-
-extern "C" __global__ void quasirandomGeneratorKernel(float *d_Output,
-                                                      unsigned int seed,
-                                                      unsigned int N) {
-  unsigned int *dimBase = &c_Table[threadIdx.y][0];
-  unsigned int tid = MUL(blockDim.x, blockIdx.x) + threadIdx.x;
-  unsigned int threadN = MUL(blockDim.x, gridDim.x);
-
-  for (unsigned int pos = tid; pos < N; pos += threadN) {
-    unsigned int result = 0;
-    unsigned int data = seed + pos;
-
-    for (int bit = 0; bit < QRNG_RESOLUTION; bit++, data >>= 1)
-      if (data & 1) {
-        result ^= dimBase[bit];
-      }
-
-    d_Output[MUL(threadIdx.y, N) + pos] = (float)(result + 1) * INT_SCALE;
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Moro's Inverse Cumulative Normal Distribution function approximation
-////////////////////////////////////////////////////////////////////////////////
-__device__ inline float MoroInvCNDgpu(unsigned int x) {
-  const float a1 = 2.50662823884f;
-  const float a2 = -18.61500062529f;
-  const float a3 = 41.39119773534f;
-  const float a4 = -25.44106049637f;
-  const float b1 = -8.4735109309f;
-  const float b2 = 23.08336743743f;
-  const float b3 = -21.06224101826f;
-  const float b4 = 3.13082909833f;
-  const float c1 = 0.337475482272615f;
-  const float c2 = 0.976169019091719f;
-  const float c3 = 0.160797971491821f;
-  const float c4 = 2.76438810333863E-02f;
-  const float c5 = 3.8405729373609E-03f;
-  const float c6 = 3.951896511919E-04f;
-  const float c7 = 3.21767881768E-05f;
-  const float c8 = 2.888167364E-07f;
-  const float c9 = 3.960315187E-07f;
-
-  float z;
-
-  bool negate = false;
-
-  // Ensure the conversion to floating point will give a value in the
-  // range (0,0.5] by restricting the input to the bottom half of the
-  // input domain. We will later reflect the result if the input was
-  // originally in the top half of the input domain
-  if (x >= 0x80000000UL) {
-    x = 0xffffffffUL - x;
-    negate = true;
-  }
-
-  // x is now in the range [0,0x80000000) (i.e. [0,0x7fffffff])
-  // Convert to floating point in (0,0.5]
-  const float x1 = 1.0f / static_cast<float>(0xffffffffUL);
-  const float x2 = x1 / 2.0f;
-  float p1 = x * x1 + x2;
-  // Convert to floating point in (-0.5,0]
-  float p2 = p1 - 0.5f;
-
-  // The input to the Moro inversion is p2 which is in the range
-  // (-0.5,0]. This means that our output will be the negative side
-  // of the bell curve (which we will reflect if "negate" is true).
-
-  // Main body of the bell curve for |p| < 0.42
-  if (p2 > -0.42f) {
-    z = p2 * p2;
-    z = p2 * (((a4 * z + a3) * z + a2) * z + a1) /
-        ((((b4 * z + b3) * z + b2) * z + b1) * z + 1.0f);
-  }
-  // Special case (Chebychev) for tail
-  else {
-    z = __logf(-__logf(p1));
-    z = -(c1 + z * (c2 + z * (c3 + z * (c4 + z * (c5 + z * (c6 + z *
-        (c7 + z * (c8 + z * c9))))))));
-  }
-
-  // If the original input (x) was in the top half of the range, reflect
-  // to get the positive side of the bell curve
-
-  return negate ? -z : z;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Main kernel. Choose between transforming
-// input sequence and uniform ascending (0, 1) sequence
-////////////////////////////////////////////////////////////////////////////////
-
-extern "C" __global__ void inverseCNDKernel(float *d_Output,
-                                            unsigned int pathN) {
-  unsigned int distance = ((unsigned int)-1) / (pathN + 1);
-  unsigned int tid = MUL(blockDim.x, blockIdx.x) + threadIdx.x;
-  unsigned int threadN = MUL(blockDim.x, gridDim.x);
-
-  // Transform input number sequence if it's supplied
-  if (0)  // d_Input)
-  {
-    /*
-      for (unsigned int pos = tid; pos < pathN; pos += threadN)
-      {
-          unsigned int d = d_Input[pos];
-          d_Output[pos] = (float)MoroInvCNDgpu(d);
-      }
-      */
-  }
-  // Else generate input uniformly placed samples on the fly
-  // and write to destination
-  else {
-    for (unsigned int pos = tid; pos < pathN; pos += threadN) {
-      unsigned int d = (pos + 1) * distance;
-      d_Output[pos] = (float)MoroInvCNDgpu(d);
-    }
-  }
-}
-
-#endif
diff --git a/src/samples/Samples/5_Domain_Specific/simpleD3D12/sinewave_cuda.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleD3D12/sinewave_cuda.cu.hip
index 7220a2e..e69de29 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleD3D12/sinewave_cuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleD3D12/sinewave_cuda.cu.hip
@@ -1,70 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "ShaderStructs.h"
-
-__global__ void sinewave_gen_kernel(Vertex *vertices, unsigned int width,
-                                    unsigned int height, float time) {
-  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
-  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
-
-  // calculate uv coordinates
-  float u = x / (float)width;
-  float v = y / (float)height;
-  u = u * 2.0f - 1.0f;
-  v = v * 2.0f - 1.0f;
-
-  // calculate simple sine wave pattern
-  float freq = 4.0f;
-  float w = sinf(u * freq + time) * cosf(v * freq + time) * 0.5f;
-
-  if (y < height && x < width) {
-    // write output vertex
-    vertices[y * width + x].position.x = u;
-    vertices[y * width + x].position.y = w;
-    vertices[y * width + x].position.z = v;
-    // vertices[y*width+x].position[3] = 1.0f;
-    vertices[y * width + x].color.x = 1.0f;
-    vertices[y * width + x].color.y = 0.0f;
-    vertices[y * width + x].color.z = 0.0f;
-    vertices[y * width + x].color.w = 0.0f;
-  }
-}
-
-// The host CPU Sinewave thread spawner
-void RunSineWaveKernel(size_t mesh_width, size_t mesh_height,
-                       Vertex *cudaDevVertptr, hipStream_t streamToRun,
-                       float AnimTime) {
-  dim3 block(16, 16, 1);
-  dim3 grid(mesh_width / 16, mesh_height / 16, 1);
-  Vertex *vertices = (Vertex *)cudaDevVertptr;
-  sinewave_gen_kernel<<<grid, block, 0, streamToRun>>>(vertices, mesh_width,
-                                                       mesh_height, AnimTime);
-
-  getLastCudaError("sinewave_gen_kernel execution failed.\n");
-}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleVulkanMMAP/MonteCarloPi.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleVulkanMMAP/MonteCarloPi.cu.hip
index fa1dde3..7cc9033 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleVulkanMMAP/MonteCarloPi.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleVulkanMMAP/MonteCarloPi.cu.hip
@@ -314,3 +314,9 @@ void MonteCarloPiSimulation::cleanupSimulationAllocations() {
     m_pointsInsideCircle = nullptr;
   }
 }
+   checkCudaErrors(
+        hipMemAddressFree((hipDeviceptr_t)m_xyVector, m_totalAllocationSize));
+
+    m_xyVector = nullptr;
+    m_pointsInsideCircle = nullptr;
+  }
diff --git a/src/samples/Samples/5_Domain_Specific/volumeFiltering/volumeFilter_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/volumeFiltering/volumeFilter_kernel.cu.hip
index c397a84..e69de29 100755
--- a/src/samples/Samples/5_Domain_Specific/volumeFiltering/volumeFilter_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/volumeFiltering/volumeFilter_kernel.cu.hip
@@ -1,117 +0,0 @@
-#include "hip/hip_runtime.h"
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _VOLUMEFILTER_KERNEL_CU_
-#define _VOLUMEFILTER_KERNEL_CU_
-
-#include "helper_cuda_hipified.h"
-#include <helper_math.h>
-#include "volumeFilter.h"
-
-typedef unsigned int uint;
-typedef unsigned char uchar;
-typedef unsigned short ushort;
-
-__constant__ float4 c_filterData[VOLUMEFILTER_MAXWEIGHTS];
-
-__global__ void d_filter_surface3d(int filterSize, float filter_offset,
-                                   hipExtent volumeSize,
-                                   hipTextureObject_t volumeTexIn,
-                                   hipSurfaceObject_t volumeTexOut) {
-  int x = blockIdx.x * blockDim.x + threadIdx.x;
-  int y = blockIdx.y * blockDim.y + threadIdx.y;
-  int z = blockIdx.z * blockDim.z + threadIdx.z;
-
-  if (x >= volumeSize.width || y >= volumeSize.height ||
-      z >= volumeSize.depth) {
-    return;
-  }
-
-  float filtered = 0;
-  float4 basecoord = make_float4(x, y, z, 0);
-
-  for (int i = 0; i < filterSize; i++) {
-    float4 coord = basecoord + c_filterData[i];
-    filtered += tex3D<float>(volumeTexIn, coord.x, coord.y, coord.z) *
-                c_filterData[i].w;
-  }
-
-  filtered += filter_offset;
-
-  VolumeType output = VolumeTypeInfo<VolumeType>::convert(filtered);
-
-  // surface writes need byte offsets for x!
-  surf3Dwrite(output, volumeTexOut, x * sizeof(VolumeType), y, z);
-}
-
-static unsigned int iDivUp(size_t a, size_t b) {
-  size_t val = (a % b != 0) ? (a / b + 1) : (a / b);
-  if (val > UINT_MAX) {
-    fprintf(stderr, "\nUINT_MAX limit exceeded in iDivUp() exiting.....\n");
-    exit(EXIT_FAILURE);  // val exceeds limit
-  }
-
-  return static_cast<unsigned int>(val);
-}
-
-extern "C" Volume *VolumeFilter_runFilter(Volume *input, Volume *output0,
-                                          Volume *output1, int iterations,
-                                          int numWeights, float4 *weights,
-                                          float postWeightOffset) {
-  Volume *swap = 0;
-  hipExtent size = input->size;
-  unsigned int dim = 32 / sizeof(VolumeType);
-  dim3 blockSize(dim, dim, 1);
-  dim3 gridSize(iDivUp(size.width, blockSize.x),
-                iDivUp(size.height, blockSize.y),
-                iDivUp(size.depth, blockSize.z));
-
-  // set weights
-  HIPCHECK(
-      hipMemcpyToSymbol(HIP_SYMBOL(c_filterData), weights, sizeof(float4) * numWeights));
-
-  for (int i = 0; i < iterations; i++) {
-    d_filter_surface3d<<<gridSize, blockSize>>>(numWeights, postWeightOffset,
-                                                size, input->volumeTex,
-                                                output0->volumeSurf);
-
-    getLastCudaError("filter kernel failed");
-
-    swap = input;
-    input = output0;
-    output0 = swap;
-
-    if (i == 0) {
-      output0 = output1;
-    }
-  }
-
-  return input;
-}
-#endif
-#endif
diff --git a/src/samples/Samples/6_Performance/UnifiedMemoryPerf/UnifiedMemoryPerf.out b/src/samples/Samples/6_Performance/UnifiedMemoryPerf/UnifiedMemoryPerf.out
deleted file mode 100755
index bfd8b3c..0000000
Binary files a/src/samples/Samples/6_Performance/UnifiedMemoryPerf/UnifiedMemoryPerf.out and /dev/null differ
diff --git a/src/samples/Samples/6_Performance/UnifiedMemoryPerf/commonKernels.cu.cpp b/src/samples/Samples/6_Performance/UnifiedMemoryPerf/commonKernels.cu.cpp
index e69de29..ed8cd7e 100644
--- a/src/samples/Samples/6_Performance/UnifiedMemoryPerf/commonKernels.cu.cpp
+++ b/src/samples/Samples/6_Performance/UnifiedMemoryPerf/commonKernels.cu.cpp
@@ -0,0 +1,34 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "commonKernels.hpp"
+
+__global__ void spinWhileLessThanOne(volatile unsigned int *latch) {
+  while (latch[0] < 1)
+    ;
+}
diff --git a/src/samples/Samples/6_Performance/alignedTypes/alignedTypes.cu.cpp b/src/samples/Samples/6_Performance/alignedTypes/alignedTypes.cu.cpp
index 0626e0c..39223b9 100644
--- a/src/samples/Samples/6_Performance/alignedTypes/alignedTypes.cu.cpp
+++ b/src/samples/Samples/6_Performance/alignedTypes/alignedTypes.cu.cpp
@@ -1,4 +1,4 @@
-
+#include "hip/hip_runtime.h"
 /* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -42,11 +42,9 @@
 #include <string.h>
 
 // includes, project
-#include "helper_cuda_hipified.h"  // helper functions for CUDA error checking and initialization
-#include "helper_functions.h"  // helper utility functions
+#include <helper_cuda.h>  // helper functions for CUDA error checking and initialization
+#include <helper_functions.h>  // helper utility functions
 
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
 ////////////////////////////////////////////////////////////////////////////////
 // Misaligned types
 ////////////////////////////////////////////////////////////////////////////////
@@ -315,5 +313,3 @@ int main(int argc, char **argv) {
   printf("Test passed\n");
   exit(EXIT_SUCCESS);
 }
-XIT_FAILURE);
-  }
diff --git a/src/samples/Samples/6_Performance/alignedTypes/alignedTypes.cu.hip b/src/samples/Samples/6_Performance/alignedTypes/alignedTypes.cu.hip
index 6aeb070..b3e70e1 100755
--- a/src/samples/Samples/6_Performance/alignedTypes/alignedTypes.cu.hip
+++ b/src/samples/Samples/6_Performance/alignedTypes/alignedTypes.cu.hip
@@ -38,15 +38,13 @@
 // includes, system
 #include <math.h>
 #include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
 #include <stdlib.h>
 #include <string.h>
 
 // includes, project
-#include <helper_cuda.h>  // helper functions for CUDA error checking and initialization
+#include "helper_cuda_hipified.h"  // helper functions for CUDA error checking and initialization
 #include <helper_functions.h>  // helper utility functions
-
+#include "HIPCHECK.h"
 ////////////////////////////////////////////////////////////////////////////////
 // Misaligned types
 ////////////////////////////////////////////////////////////////////////////////
@@ -315,5 +313,3 @@ int main(int argc, char **argv) {
   printf("Test passed\n");
   exit(EXIT_SUCCESS);
 }
-XIT_FAILURE);
-  }
diff --git a/src/samples/Samples/6_Performance/alignedTypes/alignedTypes.out b/src/samples/Samples/6_Performance/alignedTypes/alignedTypes.out
index 5d04598..c506452 100755
Binary files a/src/samples/Samples/6_Performance/alignedTypes/alignedTypes.out and b/src/samples/Samples/6_Performance/alignedTypes/alignedTypes.out differ
diff --git a/src/samples/Samples/6_Performance/transpose/transpose.cu.hip b/src/samples/Samples/6_Performance/transpose/transpose.cu.hip
index b68100a..d707d4c 100755
--- a/src/samples/Samples/6_Performance/transpose/transpose.cu.hip
+++ b/src/samples/Samples/6_Performance/transpose/transpose.cu.hip
@@ -45,8 +45,8 @@ namespace cg = cooperative_groups;
 // Utilities and system includes
 #include <helper_string.h>    // helper for string parsing
 #include <helper_image.h>     // helper for image and data comparison
-#include <helper_cuda.h>      // helper for cuda error checking functions
-
+#include <helper_cuda_hipified.h>      // helper for cuda error checking functions
+#include "HIPCHECK.h"
 const char *sSDKsample = "Transpose";
 
 // Each block transposes/copies a tile of TILE_DIM x TILE_DIM elements
@@ -612,7 +612,3 @@ int main(int argc, char **argv) {
   printf("Test passed\n");
   exit(EXIT_SUCCESS);
 }
-ccess) {
-    printf("Test failed!\n");
-    exit(EXIT_FAILURE);
-  }
diff --git a/src/samples/Samples/6_Performance/transpose/transpose.out b/src/samples/Samples/6_Performance/transpose/transpose.out
index 99c6cdf..c0a5ce4 100755
Binary files a/src/samples/Samples/6_Performance/transpose/transpose.out and b/src/samples/Samples/6_Performance/transpose/transpose.out differ
