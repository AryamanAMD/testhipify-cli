diff --git a/src/samples/Samples/0_Introduction/simpleAssert_nvrtc/simpleAssert_hipified.cpp b/src/samples/Samples/0_Introduction/simpleAssert_nvrtc/simpleAssert_hipified.cpp
index 1567fe0..793bbc0 100644
--- a/src/samples/Samples/0_Introduction/simpleAssert_nvrtc/simpleAssert_hipified.cpp
+++ b/src/samples/Samples/0_Introduction/simpleAssert_nvrtc/simpleAssert_hipified.cpp
@@ -40,7 +40,7 @@
 // Includes CUDA
 #include <hip/hip_runtime.h>
 #include "nvrtc_helper.h"
-
+#include "HIPCHECK.h"
 // Utilities and timing functions
 #include "helper_functions.h"  // includes hip/hip_runtime.h and hip/hip_runtime_api.h
 
@@ -86,12 +86,12 @@ void runTest(int argc, char **argv) {
   hipModule_t module = loadCUBIN(cubin, argc, argv);
   hipFunction_t kernel_addr;
 
-  checkCudaErrors(hipModuleGetFunction(&kernel_addr, module, "testKernel"));
+  HIPCHECK(hipModuleGetFunction(&kernel_addr, module, "testKernel"));
 
   int count = 60;
   void *args[] = {(void *)&count};
 
-  checkCudaErrors(hipModuleLaunchKernel(
+  HIPCHECK(hipModuleLaunchKernel(
       kernel_addr, dimGrid.x, dimGrid.y, dimGrid.z, /* grid dim */
       dimBlock.x, dimBlock.y, dimBlock.z,           /* block dim */
       0, 0,                                         /* shared mem, stream */
diff --git a/src/samples/Samples/0_Introduction/simpleAssert_nvrtc/simpleAssert_kernel.cu.hip b/src/samples/Samples/0_Introduction/simpleAssert_nvrtc/simpleAssert_kernel.cu.hip
index e69de29..1206b41 100644
--- a/src/samples/Samples/0_Introduction/simpleAssert_nvrtc/simpleAssert_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleAssert_nvrtc/simpleAssert_kernel.cu.hip
@@ -0,0 +1,39 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+////////////////////////////////////////////////////////////////////////////////
+// Kernels
+////////////////////////////////////////////////////////////////////////////////
+//! Tests assert function.
+//! Thread whose id > N will print assertion failed error message.
+////////////////////////////////////////////////////////////////////////////////
+
+extern "C" __global__ void testKernel(int N) {
+  int gtid = blockIdx.x * blockDim.x + threadIdx.x;
+  assert(gtid < N);
+}
diff --git a/src/samples/Samples/0_Introduction/simpleCubemapTexture/simpleCubemapTexture.cu.hip b/src/samples/Samples/0_Introduction/simpleCubemapTexture/simpleCubemapTexture.cu.hip
index 5a730b6..791e52d 100644
--- a/src/samples/Samples/0_Introduction/simpleCubemapTexture/simpleCubemapTexture.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleCubemapTexture/simpleCubemapTexture.cu.hip
@@ -47,8 +47,8 @@
 #include <hip/hip_runtime.h>
 
 // helper functions and utilities to work with CUDA
-#include <helper_functions.h>
-#include <helper_cuda.h>
+#include "helper_functions.h"
+#include "helper_cuda_hipified.h"
 
 static const char *sSDKname = "simpleCubemapTexture";
 
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/FunctionPointers/FunctionPointers_kernels.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/FunctionPointers/FunctionPointers_kernels.cu.hip
index e69de29..57ad723 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/FunctionPointers/FunctionPointers_kernels.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/FunctionPointers/FunctionPointers_kernels.cu.hip
@@ -0,0 +1,403 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <hip/hip_cooperative_groups.h>
+
+namespace cg = cooperative_groups;
+
+#include "helper_cuda_hipified.h"
+#include "HIPCHECK.h"
+#include "FunctionPointers_kernels.h"
+
+// Texture object for reading image
+hipTextureObject_t tex;
+extern __shared__ unsigned char LocalBlock[];
+static hipArray *array = NULL;
+
+#define RADIUS 1
+
+// pixel value used for thresholding function,
+// works well with sample image 'teapot512'
+#define THRESHOLD 150.0f
+
+#ifdef FIXED_BLOCKWIDTH
+#define BlockWidth 80
+#define SharedPitch 384
+#endif
+
+// A function pointer can be declared explicitly like this line:
+//__device__ unsigned char (*pointFunction)(unsigned char, float ) = NULL;
+// or by using typedef's like below:
+
+typedef unsigned char (*blockFunction_t)(unsigned char, unsigned char,
+                                         unsigned char, unsigned char,
+                                         unsigned char, unsigned char,
+                                         unsigned char, unsigned char,
+                                         unsigned char, float);
+
+typedef unsigned char (*pointFunction_t)(unsigned char, float);
+
+__device__ blockFunction_t blockFunction;
+
+__device__ unsigned char ComputeSobel(unsigned char ul,  // upper left
+                                      unsigned char um,  // upper middle
+                                      unsigned char ur,  // upper right
+                                      unsigned char ml,  // middle left
+                                      unsigned char mm,  // middle (unused)
+                                      unsigned char mr,  // middle right
+                                      unsigned char ll,  // lower left
+                                      unsigned char lm,  // lower middle
+                                      unsigned char lr,  // lower right
+                                      float fScale) {
+  short Horz = ur + 2 * mr + lr - ul - 2 * ml - ll;
+  short Vert = ul + 2 * um + ur - ll - 2 * lm - lr;
+  short Sum = (short)(fScale * (abs((int)Horz) + abs((int)Vert)));
+  return (unsigned char)((Sum < 0) ? 0 : ((Sum > 255) ? 255 : Sum));
+}
+
+// define a function pointer and initialize to NULL
+__device__ unsigned char (*varFunction)(unsigned char, unsigned char,
+                                        unsigned char, unsigned char,
+                                        unsigned char, unsigned char,
+                                        unsigned char, unsigned char,
+                                        unsigned char, float x) = NULL;
+
+__device__ unsigned char ComputeBox(unsigned char ul,  // upper left
+                                    unsigned char um,  // upper middle
+                                    unsigned char ur,  // upper right
+                                    unsigned char ml,  // middle left
+                                    unsigned char mm,  // middle...middle
+                                    unsigned char mr,  // middle right
+                                    unsigned char ll,  // lower left
+                                    unsigned char lm,  // lower middle
+                                    unsigned char lr,  // lower right
+                                    float fscale) {
+  short Sum = (short)(ul + um + ur + ml + mm + mr + ll + lm + lr) / 9;
+  Sum *= fscale;
+  return (unsigned char)((Sum < 0) ? 0 : ((Sum > 255) ? 255 : Sum));
+}
+__device__ unsigned char Threshold(unsigned char in, float thresh) {
+  if (in > thresh) {
+    return 0xFF;
+  } else {
+    return 0;
+  }
+}
+
+// Declare function tables, one for the point function chosen, one for the
+// block function chosen.  The number of entries is determined by the
+// enum in FunctionPointers_kernels.h
+__device__ blockFunction_t blockFunction_table[LAST_BLOCK_FILTER];
+__device__ pointFunction_t pointFunction_table[LAST_POINT_FILTER];
+
+// Declare device side function pointers.  We retrieve them later with
+// hipMemcpyFromSymbol to set our function tables above in some
+// particular order specified at runtime.
+__device__ blockFunction_t pComputeSobel = ComputeSobel;
+__device__ blockFunction_t pComputeBox = ComputeBox;
+__device__ pointFunction_t pComputeThreshold = Threshold;
+
+// Allocate host side tables to mirror the device side, and later, we
+// fill these tables with the function pointers.  This lets us send
+// the pointers to the kernel on invocation, as a method of choosing
+// which function to run.
+blockFunction_t h_blockFunction_table[2];
+pointFunction_t h_pointFunction_table[2];
+
+// Perform a filter operation on the data, using shared memory
+// The actual operation performed is
+// determined by the function pointer "blockFunction" and selected
+// by the integer argument "blockOperation" and has access
+// to an apron around the current pixel being processed.
+// Following the block operation, a per-pixel operation,
+// pointed to by pPointFunction is performed before the final
+// pixel is produced.
+__global__ void SobelShared(uchar4 *pSobelOriginal, unsigned short SobelPitch,
+#ifndef FIXED_BLOCKWIDTH
+                            short BlockWidth, short SharedPitch,
+#endif
+                            short w, short h, float fScale, int blockOperation,
+                            pointFunction_t pPointFunction,
+                            hipTextureObject_t tex) {
+  // Handle to thread block group
+  cg::thread_block cta = cg::this_thread_block();
+  short u = 4 * blockIdx.x * BlockWidth;
+  short v = blockIdx.y * blockDim.y + threadIdx.y;
+  short ib;
+
+  int SharedIdx = threadIdx.y * SharedPitch;
+
+  for (ib = threadIdx.x; ib < BlockWidth + 2 * RADIUS; ib += blockDim.x) {
+    LocalBlock[SharedIdx + 4 * ib + 0] = tex2D<unsigned char>(
+        tex, (float)(u + 4 * ib - RADIUS + 0), (float)(v - RADIUS));
+    LocalBlock[SharedIdx + 4 * ib + 1] = tex2D<unsigned char>(
+        tex, (float)(u + 4 * ib - RADIUS + 1), (float)(v - RADIUS));
+    LocalBlock[SharedIdx + 4 * ib + 2] = tex2D<unsigned char>(
+        tex, (float)(u + 4 * ib - RADIUS + 2), (float)(v - RADIUS));
+    LocalBlock[SharedIdx + 4 * ib + 3] = tex2D<unsigned char>(
+        tex, (float)(u + 4 * ib - RADIUS + 3), (float)(v - RADIUS));
+  }
+
+  if (threadIdx.y < RADIUS * 2) {
+    //
+    // copy trailing RADIUS*2 rows of pixels into shared
+    //
+    SharedIdx = (blockDim.y + threadIdx.y) * SharedPitch;
+
+    for (ib = threadIdx.x; ib < BlockWidth + 2 * RADIUS; ib += blockDim.x) {
+      LocalBlock[SharedIdx + 4 * ib + 0] =
+          tex2D<unsigned char>(tex, (float)(u + 4 * ib - RADIUS + 0),
+                               (float)(v + blockDim.y - RADIUS));
+      LocalBlock[SharedIdx + 4 * ib + 1] =
+          tex2D<unsigned char>(tex, (float)(u + 4 * ib - RADIUS + 1),
+                               (float)(v + blockDim.y - RADIUS));
+      LocalBlock[SharedIdx + 4 * ib + 2] =
+          tex2D<unsigned char>(tex, (float)(u + 4 * ib - RADIUS + 2),
+                               (float)(v + blockDim.y - RADIUS));
+      LocalBlock[SharedIdx + 4 * ib + 3] =
+          tex2D<unsigned char>(tex, (float)(u + 4 * ib - RADIUS + 3),
+                               (float)(v + blockDim.y - RADIUS));
+    }
+  }
+
+  cg::sync(cta);
+
+  u >>= 2;  // index as uchar4 from here
+  uchar4 *pSobel = (uchar4 *)(((char *)pSobelOriginal) + v * SobelPitch);
+  SharedIdx = threadIdx.y * SharedPitch;
+
+  blockFunction = blockFunction_table[blockOperation];
+
+  for (ib = threadIdx.x; ib < BlockWidth; ib += blockDim.x) {
+    uchar4 out;
+
+    unsigned char pix00 = LocalBlock[SharedIdx + 4 * ib + 0 * SharedPitch + 0];
+    unsigned char pix01 = LocalBlock[SharedIdx + 4 * ib + 0 * SharedPitch + 1];
+    unsigned char pix02 = LocalBlock[SharedIdx + 4 * ib + 0 * SharedPitch + 2];
+    unsigned char pix10 = LocalBlock[SharedIdx + 4 * ib + 1 * SharedPitch + 0];
+    unsigned char pix11 = LocalBlock[SharedIdx + 4 * ib + 1 * SharedPitch + 1];
+    unsigned char pix12 = LocalBlock[SharedIdx + 4 * ib + 1 * SharedPitch + 2];
+    unsigned char pix20 = LocalBlock[SharedIdx + 4 * ib + 2 * SharedPitch + 0];
+    unsigned char pix21 = LocalBlock[SharedIdx + 4 * ib + 2 * SharedPitch + 1];
+    unsigned char pix22 = LocalBlock[SharedIdx + 4 * ib + 2 * SharedPitch + 2];
+
+    out.x = (*blockFunction)(pix00, pix01, pix02, pix10, pix11, pix12, pix20,
+                             pix21, pix22, fScale);
+
+    pix00 = LocalBlock[SharedIdx + 4 * ib + 0 * SharedPitch + 3];
+    pix10 = LocalBlock[SharedIdx + 4 * ib + 1 * SharedPitch + 3];
+    pix20 = LocalBlock[SharedIdx + 4 * ib + 2 * SharedPitch + 3];
+    out.y = (*blockFunction)(pix01, pix02, pix00, pix11, pix12, pix10, pix21,
+                             pix22, pix20, fScale);
+
+    pix01 = LocalBlock[SharedIdx + 4 * ib + 0 * SharedPitch + 4];
+    pix11 = LocalBlock[SharedIdx + 4 * ib + 1 * SharedPitch + 4];
+    pix21 = LocalBlock[SharedIdx + 4 * ib + 2 * SharedPitch + 4];
+    out.z = (*blockFunction)(pix02, pix00, pix01, pix12, pix10, pix11, pix22,
+                             pix20, pix21, fScale);
+
+    pix02 = LocalBlock[SharedIdx + 4 * ib + 0 * SharedPitch + 5];
+    pix12 = LocalBlock[SharedIdx + 4 * ib + 1 * SharedPitch + 5];
+    pix22 = LocalBlock[SharedIdx + 4 * ib + 2 * SharedPitch + 5];
+    out.w = (*blockFunction)(pix00, pix01, pix02, pix10, pix11, pix12, pix20,
+                             pix21, pix22, fScale);
+
+    if (pPointFunction != NULL) {
+      out.x = (*pPointFunction)(out.x, THRESHOLD);
+      out.y = (*pPointFunction)(out.y, THRESHOLD);
+      out.z = (*pPointFunction)(out.z, THRESHOLD);
+      out.w = (*pPointFunction)(out.w, THRESHOLD);
+    }
+
+    if (u + ib < w / 4 && v < h) {
+      pSobel[u + ib] = out;
+    }
+  }
+
+  cg::sync(cta);
+}
+
+__global__ void SobelCopyImage(Pixel *pSobelOriginal, unsigned int Pitch, int w,
+                               int h, float fscale, hipTextureObject_t tex) {
+  unsigned char *pSobel =
+      (unsigned char *)(((char *)pSobelOriginal) + blockIdx.x * Pitch);
+
+  for (int i = threadIdx.x; i < w; i += blockDim.x) {
+    pSobel[i] = min(
+        max((tex2D<unsigned char>(tex, (float)i, (float)blockIdx.x) * fscale),
+            0.f),
+        255.f);
+  }
+}
+
+// Perform block and pointer filtering using texture lookups.
+// The block and point operations are determined by the
+// input argument (see comment above for "SobelShared" function)
+__global__ void SobelTex(Pixel *pSobelOriginal, unsigned int Pitch, int w,
+                         int h, float fScale, int blockOperation,
+                         pointFunction_t pPointOperation,
+                         hipTextureObject_t tex) {
+  unsigned char *pSobel =
+      (unsigned char *)(((char *)pSobelOriginal) + blockIdx.x * Pitch);
+  unsigned char tmp = 0;
+
+  for (int i = threadIdx.x; i < w; i += blockDim.x) {
+    unsigned char pix00 =
+        tex2D<unsigned char>(tex, (float)i - 1, (float)blockIdx.x - 1);
+    unsigned char pix01 =
+        tex2D<unsigned char>(tex, (float)i + 0, (float)blockIdx.x - 1);
+    unsigned char pix02 =
+        tex2D<unsigned char>(tex, (float)i + 1, (float)blockIdx.x - 1);
+    unsigned char pix10 =
+        tex2D<unsigned char>(tex, (float)i - 1, (float)blockIdx.x + 0);
+    unsigned char pix11 =
+        tex2D<unsigned char>(tex, (float)i + 0, (float)blockIdx.x + 0);
+    unsigned char pix12 =
+        tex2D<unsigned char>(tex, (float)i + 1, (float)blockIdx.x + 0);
+    unsigned char pix20 =
+        tex2D<unsigned char>(tex, (float)i - 1, (float)blockIdx.x + 1);
+    unsigned char pix21 =
+        tex2D<unsigned char>(tex, (float)i + 0, (float)blockIdx.x + 1);
+    unsigned char pix22 =
+        tex2D<unsigned char>(tex, (float)i + 1, (float)blockIdx.x + 1);
+    tmp = (*(blockFunction_table[blockOperation]))(
+        pix00, pix01, pix02, pix10, pix11, pix12, pix20, pix21, pix22, fScale);
+
+    if (pPointOperation != NULL) {
+      tmp = (*pPointOperation)(tmp, 150.0);
+    }
+
+    pSobel[i] = tmp;
+  }
+}
+
+extern "C" void setupTexture(int iw, int ih, Pixel *data, int Bpp) {
+  hipChannelFormatDesc desc;
+
+  if (Bpp == 1) {
+    desc = hipCreateChannelDesc<unsigned char>();
+  } else {
+    desc = hipCreateChannelDesc<uchar4>();
+  }
+
+  HIPCHECK(hipMallocArray(&array, &desc, iw, ih));
+  HIPCHECK(hipMemcpy2DToArray(
+      array, 0, 0, data, iw * Bpp * sizeof(Pixel), iw * Bpp * sizeof(Pixel), ih,
+      hipMemcpyHostToDevice));
+
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeArray;
+  texRes.res.array.array = array;
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  HIPCHECK(hipCreateTextureObject(&tex, &texRes, &texDescr, NULL));
+}
+
+extern "C" void deleteTexture(void) {
+  HIPCHECK(hipFreeArray(array));
+  HIPCHECK(hipDestroyTextureObject(tex));
+}
+
+// Copy the pointers from the function tables to the host side
+void setupFunctionTables() {
+  // Dynamically assign the function table.
+  // Copy the function pointers to their appropriate locations according to the
+  // enum
+  HIPCHECK(hipMemcpyFromSymbol(&h_blockFunction_table[SOBEL_FILTER],
+                                       HIP_SYMBOL(pComputeSobel), sizeof(blockFunction_t)));
+  HIPCHECK(hipMemcpyFromSymbol(&h_blockFunction_table[BOX_FILTER],
+                                       HIP_SYMBOL(pComputeBox), sizeof(blockFunction_t)));
+
+  // do the same for the point function, where the 2nd function is NULL ("no-op"
+  // filter, skipped in kernel code)
+  HIPCHECK(hipMemcpyFromSymbol(&h_pointFunction_table[THRESHOLD_FILTER],
+                                       HIP_SYMBOL(pComputeThreshold),
+                                       sizeof(pointFunction_t)));
+  h_pointFunction_table[NULL_FILTER] = NULL;
+
+  // now copy the function tables back to the device, so if we wish we can use
+  // an index into the table to choose them
+  // We have now set the order in the function table according to our enum.
+  HIPCHECK(
+      hipMemcpyToSymbol(HIP_SYMBOL(blockFunction_table), h_blockFunction_table,
+                         sizeof(blockFunction_t) * LAST_BLOCK_FILTER));
+  HIPCHECK(
+      hipMemcpyToSymbol(HIP_SYMBOL(pointFunction_table), h_pointFunction_table,
+                         sizeof(pointFunction_t) * LAST_POINT_FILTER));
+}
+
+// Wrapper for the __global__ call that sets up the texture and threads
+// Below two methods for selecting the image processing function to run are
+// shown.
+// BlockOperation is an integer kernel argument used as an index into the
+// blockFunction_table on the device side
+// pPointOp is itself a function pointer passed as a kernel argument, retrieved
+// from a host side copy of the function table
+extern "C" void sobelFilter(Pixel *odata, int iw, int ih,
+                            enum SobelDisplayMode mode, float fScale,
+                            int blockOperation, int pointOperation) {
+  pointFunction_t pPointOp = h_pointFunction_table[pointOperation];
+
+  switch (mode) {
+    case SOBELDISPLAY_IMAGE:
+      SobelCopyImage<<<ih, 384>>>(odata, iw, iw, ih, fScale, tex);
+      break;
+
+    case SOBELDISPLAY_SOBELTEX:
+      SobelTex<<<ih, 384>>>(odata, iw, iw, ih, fScale, blockOperation, pPointOp,
+                            tex);
+      break;
+
+    case SOBELDISPLAY_SOBELSHARED: {
+      dim3 threads(16, 4);
+#ifndef FIXED_BLOCKWIDTH
+      int BlockWidth = 80;  // must be divisible by 16 for coalescing
+#endif
+      dim3 blocks = dim3(iw / (4 * BlockWidth) + (0 != iw % (4 * BlockWidth)),
+                         ih / threads.y + (0 != ih % threads.y));
+      int SharedPitch = ~0x3f & (4 * (BlockWidth + 2 * RADIUS) + 0x3f);
+      int sharedMem = SharedPitch * (threads.y + 2 * RADIUS);
+
+      // for the shared kernel, width must be divisible by 4
+      iw &= ~3;
+
+      SobelShared<<<blocks, threads, sharedMem>>>(
+          (uchar4 *)odata, iw,
+#ifndef FIXED_BLOCKWIDTH
+          BlockWidth, SharedPitch,
+#endif
+          iw, ih, fScale, blockOperation, pPointOp, tex);
+    } break;
+  }
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/boxFilter/boxFilter_kernel.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/boxFilter/boxFilter_kernel.cu.hip
index afcdd84..1ab72b1 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/boxFilter/boxFilter_kernel.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/boxFilter/boxFilter_kernel.cu.hip
@@ -27,9 +27,9 @@
 
 #ifndef _BOXFILTER_KERNEL_CH_
 #define _BOXFILTER_KERNEL_CH_
-
-#include <helper_math.h>
-#include <helper_functions.h>
+#include <hip/hip_cooperative_groups.h>
+//#include <helper_math.h>
+#include "helper_functions.h"
 
 hipTextureObject_t tex;
 hipTextureObject_t texTempArray;
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/interval/cuda_interval_rounded_arith.h b/src/samples/Samples/2_Concepts_and_Techniques/interval/cuda_interval_rounded_arith.h
index c6e0b1f..386f051 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/interval/cuda_interval_rounded_arith.h
+++ b/src/samples/Samples/2_Concepts_and_Techniques/interval/cuda_interval_rounded_arith.h
@@ -59,44 +59,44 @@ struct rounded_arith {
 template <>
 struct rounded_arith<float> {
   __device__ float add_down(const float &x, const float &y) {
-    return __fadd_rd(x, y);
+    return __fadd_rn(x, y);
   }
 
   __device__ float add_up(const float &x, const float &y) {
-    return __fadd_ru(x, y);
+    return __fadd_rn(x, y);
   }
 
   __device__ float sub_down(const float &x, const float &y) {
-    return __fadd_rd(x, -y);
+    return __fadd_rn(x, -y);
   }
 
   __device__ float sub_up(const float &x, const float &y) {
-    return __fadd_ru(x, -y);
+    return __fadd_rn(x, -y);
   }
 
   __device__ float mul_down(const float &x, const float &y) {
-    return __fmul_rd(x, y);
+    return __fmul_rn(x, y);
   }
 
   __device__ float mul_up(const float &x, const float &y) {
-    return __fmul_ru(x, y);
+    return __fmul_rn(x, y);
   }
 
   __device__ float div_down(const float &x, const float &y) {
-    return __fdiv_rd(x, y);
+    return __fdiv_rn(x, y);
   }
 
   __device__ float div_up(const float &x, const float &y) {
-    return __fdiv_ru(x, y);
+    return __fdiv_rn(x, y);
   }
 
   __device__ float median(const float &x, const float &y) {
     return (x + y) * .5f;
   }
 
-  __device__ float sqrt_down(const float &x) { return __fsqrt_rd(x); }
+  __device__ float sqrt_down(const float &x) { return __fsqrt_rn(x); }
 
-  __device__ float sqrt_up(const float &x) { return __fsqrt_ru(x); }
+  __device__ float sqrt_up(const float &x) { return __fsqrt_rn(x); }
 
   __device__ float int_down(const float &x) { return floorf(x); }
 
@@ -117,43 +117,43 @@ struct rounded_arith<float> {
 template <>
 struct rounded_arith<double> {
   __device__ double add_down(const double &x, const double &y) {
-    return __dadd_rd(x, y);
+    return __dadd_rn(x, y);
   }
 
   __device__ double add_up(const double &x, const double &y) {
-    return __dadd_ru(x, y);
+    return __dadd_rn(x, y);
   }
 
   __device__ double sub_down(const double &x, const double &y) {
-    return __dadd_rd(x, -y);
+    return __dadd_rn(x, -y);
   }
 
   __device__ double sub_up(const double &x, const double &y) {
-    return __dadd_ru(x, -y);
+    return __dadd_rn(x, -y);
   }
 
   __device__ double mul_down(const double &x, const double &y) {
-    return __dmul_rd(x, y);
+    return __dmul_rn(x, y);
   }
 
   __device__ double mul_up(const double &x, const double &y) {
-    return __dmul_ru(x, y);
+    return __dmul_rn(x, y);
   }
 
   __device__ double div_down(const double &x, const double &y) {
-    return __ddiv_rd(x, y);
+    return __ddiv_rn(x, y);
   }
 
   __device__ double div_up(const double &x, const double &y) {
-    return __ddiv_ru(x, y);
+    return __ddiv_rn(x, y);
   }
   __device__ double median(const double &x, const double &y) {
     return (x + y) * .5;
   }
 
-  __device__ double sqrt_down(const double &x) { return __dsqrt_rd(x); }
+  __device__ double sqrt_down(const double &x) { return __dsqrt_rn(x); }
 
-  __device__ double sqrt_up(const double &x) { return __dsqrt_ru(x); }
+  __device__ double sqrt_up(const double &x) { return __dsqrt_rn(x); }
 
   __device__ double int_down(const double &x) { return floor(x); }
 
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/interval/interval.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/interval/interval.cu.hip
index 4ca03d2..b75f49b 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/interval/interval.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/interval/interval.cu.hip
@@ -45,10 +45,10 @@ const static char *sSDKsample = "Interval Computing";
 #include <stdio.h>
 #include "rocprofiler.h"
 #include "HIPCHECK.h"
-#include "helper_cuda.h"
-#include "interval.h"
-#include "cuda_interval.h"
-#include "cpu_interval.h"
+#include "helper_cuda_hipified.h"
+#include "interval_hipified.h"
+#include "cuda_interval_hipified.h"
+#include "cpu_interval_hipified.h"
 
 int main(int argc, char *argv[]) {
   int implementation_choice = 0;
@@ -103,7 +103,7 @@ int main(int argc, char *argv[]) {
 
   // Increase the stack size large enough for the non-inlined and recursive
   // function calls (only applicable to sm_20 and higher)
-  CHECKED_CALL(cudaDeviceSetLimit(cudaLimitStackSize, 8192));
+  CHECKED_CALL(hipDeviceSetLimit(hipLimitStackSize, 8192));
 
   interval_gpu<T> i(0.01f, 4.0f);
   std::cout << "Searching for roots in [" << i.lower() << ", " << i.upper()
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/particles/particleSystem_cuda.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/particles/particleSystem_cuda.cu.hip
index 52452e9..fd47a66 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/particles/particleSystem_cuda.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/particles/particleSystem_cuda.cu.hip
@@ -44,9 +44,9 @@
 #include <hip/hip_runtime.h>
 #include <cuda_gl_interop.h>
 
-#include <helper_cuda.h>
+#include "helper_cuda_hipified.h"
 
-#include <helper_functions.h>
+#include "helper_functions.h"
 #include "thrust/device_ptr.h"
 #include "thrust/for_each.h"
 #include "thrust/iterator/zip_iterator.h"
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.cu.hip
index e69de29..8bd4923 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.cu.hip
@@ -0,0 +1,215 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <thrust/host_vector.h>
+#include <thrust/device_vector.h>
+#include <thrust/sort.h>
+#include <thrust/copy.h>
+#include <thrust/sequence.h>
+#include <thrust/random.h>
+#include <thrust/generate.h>
+#include <thrust/detail/type_traits.h>
+
+#include <helper_cuda.h>
+
+#include <algorithm>
+#include <time.h>
+#include <limits.h>
+
+template <typename T, bool floatKeys>
+bool testSort(int argc, char **argv) {
+  int cmdVal;
+  int keybits = 32;
+
+  unsigned int numElements = 1048576;
+  bool keysOnly = checkCmdLineFlag(argc, (const char **)argv, "keysonly");
+  bool quiet = checkCmdLineFlag(argc, (const char **)argv, "quiet");
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "n")) {
+    cmdVal = getCmdLineArgumentInt(argc, (const char **)argv, "n");
+    numElements = cmdVal;
+
+    if (cmdVal < 0) {
+      printf("Error: elements must be > 0, elements=%d is invalid\n", cmdVal);
+      exit(EXIT_SUCCESS);
+    }
+  }
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "keybits")) {
+    cmdVal = getCmdLineArgumentInt(argc, (const char **)argv, "keybits");
+    keybits = cmdVal;
+
+    if (keybits <= 0) {
+      printf("Error: keybits must be > 0, keybits=%d is invalid\n", keybits);
+      exit(EXIT_SUCCESS);
+    }
+  }
+
+  unsigned int numIterations = (numElements >= 16777216) ? 10 : 100;
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "iterations")) {
+    cmdVal = getCmdLineArgumentInt(argc, (const char **)argv, "iterations");
+    numIterations = cmdVal;
+  }
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "help")) {
+    printf("Command line:\nradixSortThrust [-option]\n");
+    printf("Valid options:\n");
+    printf("-n=<N>        : number of elements to sort\n");
+    printf("-keybits=bits : keybits must be > 0\n");
+    printf(
+        "-keysonly     : only sort an array of keys (default sorts key-value "
+        "pairs)\n");
+    printf(
+        "-float        : use 32-bit float keys (default is 32-bit unsigned "
+        "int)\n");
+    printf(
+        "-quiet        : Output only the number of elements and the time to "
+        "sort\n");
+    printf("-help         : Output a help message\n");
+    exit(EXIT_SUCCESS);
+  }
+
+  if (!quiet)
+    printf("\nSorting %d %d-bit %s keys %s\n\n", numElements, keybits,
+           floatKeys ? "float" : "unsigned int",
+           keysOnly ? "(only)" : "and values");
+
+  int deviceID = -1;
+
+  if (hipSuccess == hipGetDevice(&deviceID)) {
+    hipDeviceProp_t devprop;
+    hipGetDeviceProperties(&devprop, deviceID);
+    unsigned int totalMem = (keysOnly ? 2 : 4) * numElements * sizeof(T);
+
+    if (devprop.totalGlobalMem < totalMem) {
+      printf("Error: insufficient amount of memory to sort %d elements.\n",
+             numElements);
+      printf("%d bytes needed, %d bytes available\n", (int)totalMem,
+             (int)devprop.totalGlobalMem);
+      exit(EXIT_SUCCESS);
+    }
+  }
+
+  thrust::host_vector<T> h_keys(numElements);
+  thrust::host_vector<T> h_keysSorted(numElements);
+  thrust::host_vector<unsigned int> h_values;
+
+  if (!keysOnly) h_values = thrust::host_vector<unsigned int>(numElements);
+
+  // Fill up with some random data
+  thrust::default_random_engine rng(clock());
+
+  if (floatKeys) {
+    thrust::uniform_real_distribution<float> u01(0, 1);
+
+    for (int i = 0; i < (int)numElements; i++) h_keys[i] = u01(rng);
+  } else {
+    thrust::uniform_int_distribution<unsigned int> u(0, UINT_MAX);
+
+    for (int i = 0; i < (int)numElements; i++) h_keys[i] = u(rng);
+  }
+
+  if (!keysOnly) thrust::sequence(h_values.begin(), h_values.end());
+
+  // Copy data onto the GPU
+  thrust::device_vector<T> d_keys;
+  thrust::device_vector<unsigned int> d_values;
+
+  // run multiple iterations to compute an average sort time
+  hipEvent_t start_event, stop_event;
+  checkCudaErrors(hipEventCreate(&start_event));
+  checkCudaErrors(hipEventCreate(&stop_event));
+
+  float totalTime = 0;
+
+  for (unsigned int i = 0; i < numIterations; i++) {
+    // reset data before sort
+    d_keys = h_keys;
+
+    if (!keysOnly) d_values = h_values;
+
+    checkCudaErrors(hipEventRecord(start_event, 0));
+
+    if (keysOnly)
+      thrust::sort(d_keys.begin(), d_keys.end());
+    else
+      thrust::sort_by_key(d_keys.begin(), d_keys.end(), d_values.begin());
+
+    checkCudaErrors(hipEventRecord(stop_event, 0));
+    checkCudaErrors(hipEventSynchronize(stop_event));
+
+    float time = 0;
+    checkCudaErrors(hipEventElapsedTime(&time, start_event, stop_event));
+    totalTime += time;
+  }
+
+  totalTime /= (1.0e3f * numIterations);
+  printf(
+      "radixSortThrust, Throughput = %.4f MElements/s, Time = %.5f s, Size = "
+      "%u elements\n",
+      1.0e-6f * numElements / totalTime, totalTime, numElements);
+
+  getLastCudaError("after radixsort");
+
+  // Get results back to host for correctness checking
+  thrust::copy(d_keys.begin(), d_keys.end(), h_keysSorted.begin());
+
+  if (!keysOnly)
+    thrust::copy(d_values.begin(), d_values.end(), h_values.begin());
+
+  getLastCudaError("copying results to host memory");
+
+  // Check results
+  bool bTestResult =
+      thrust::is_sorted(h_keysSorted.begin(), h_keysSorted.end());
+
+  checkCudaErrors(hipEventDestroy(start_event));
+  checkCudaErrors(hipEventDestroy(stop_event));
+
+  if (!bTestResult && !quiet) {
+    return false;
+  }
+
+  return bTestResult;
+}
+
+int main(int argc, char **argv) {
+  // Start logs
+  printf("%s Starting...\n\n", argv[0]);
+
+  findCudaDevice(argc, (const char **)argv);
+
+  bool bTestResult = false;
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "float"))
+    bTestResult = testSort<float, true>(argc, argv);
+  else
+    bTestResult = testSort<unsigned int, false>(argc, argv);
+
+  printf(bTestResult ? "Test passed\n" : "Test failed!\n");
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/reductionMultiBlockCG/reductionMultiBlockCG.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/reductionMultiBlockCG/reductionMultiBlockCG.cu.hip
index e69de29..40f4bca 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/reductionMultiBlockCG/reductionMultiBlockCG.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/reductionMultiBlockCG/reductionMultiBlockCG.cu.hip
@@ -0,0 +1,387 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+  Parallel reduction
+
+  This sample shows how to perform a reduction operation on an array of values
+  to produce a single value in a single kernel (as opposed to two or more
+  kernel calls as shown in the "reduction" CUDA Sample).  Single-pass
+  reduction requires Cooperative Groups.
+
+  Reductions are a very common computation in parallel algorithms.  Any time
+  an array of values needs to be reduced to a single value using a binary
+  associative operator, a reduction can be used.  Example applications include
+  statistics computations such as mean and standard deviation, and image
+  processing applications such as finding the total luminance of an
+  image.
+
+  This code performs sum reductions, but any associative operator such as
+  min() or max() could also be used.
+
+  It assumes the input size is a power of 2.
+
+  COMMAND LINE ARGUMENTS
+
+  "--n=<N>"         :Specify the number of elements to reduce (default 33554432)
+  "--threads=<N>"   :Specify the number of threads per block (default 128)
+  "--maxblocks=<N>" :Specify the maximum number of thread blocks to launch
+ (kernel 6 only, default 64)
+*/
+
+// includes, system
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <math.h>
+
+// includes, project
+#include "helper_functions.h"
+#include "helper_cuda_hipified.h"
+//#include <hipify/__clang_cuda_intrinsics.h>
+#include <hip/hip_runtime.h>
+
+const char *sSDKsample = "reductionMultiBlockCG";
+
+#include <hip/hip_runtime_api.h>
+#include <hip/hip_cooperative_groups.h>
+#include <cooperative_groups/reduce.h>
+
+namespace cg = cooperative_groups;
+
+/*
+  Parallel sum reduction using shared memory
+  - takes log(n) steps for n input elements
+  - uses n/2 threads
+  - only works for power-of-2 arrays
+
+  This version adds multiple elements per thread sequentially. This reduces the
+  overall cost of the algorithm while keeping the work complexity O(n) and the
+  step complexity O(log n).
+  (Brent's Theorem optimization)
+
+  See the CUDA SDK "reduction" sample for more information.
+*/
+
+__device__ void reduceBlock(double *sdata, const cg::thread_block &cta) {
+  const unsigned int tid = cta.thread_rank();
+  cg::thread_block_tile<32> tile32 = cg::tiled_partition<32>(cta);
+
+  sdata[tid] = cg::reduce(tile32, sdata[tid], cg::plus<double>());
+  cg::sync(cta);
+
+  double beta = 0.0;
+  if (cta.thread_rank() == 0) {
+    beta = 0;
+    for (int i = 0; i < blockDim.x; i += tile32.size()) {
+      beta += sdata[i];
+    }
+    sdata[0] = beta;
+  }
+  cg::sync(cta);
+}
+
+// This reduction kernel reduces an arbitrary size array in a single kernel
+// invocation
+//
+// For more details on the reduction algorithm (notably the multi-pass
+// approach), see the "reduction" sample in the CUDA SDK.
+extern "C" __global__ void reduceSinglePassMultiBlockCG(const float *g_idata,
+                                                        float *g_odata,
+                                                        unsigned int n) {
+  // Handle to thread block group
+  cg::thread_block block = cg::this_thread_block();
+  cg::grid_group grid = cg::this_grid();
+
+  extern double __shared__ sdata[];
+
+  // Stride over grid and add the values to a shared memory buffer
+  sdata[block.thread_rank()] = 0;
+
+  for (int i = grid.thread_rank(); i < n; i += grid.size()) {
+    sdata[block.thread_rank()] += g_idata[i];
+  }
+
+  cg::sync(block);
+
+  // Reduce each block (called once per block)
+  reduceBlock(sdata, block);
+  // Write out the result to global memory
+  if (block.thread_rank() == 0) {
+    g_odata[blockIdx.x] = sdata[0];
+  }
+  cg::sync(grid);
+
+  if (grid.thread_rank() == 0) {
+    for (int block = 1; block < gridDim.x; block++) {
+      g_odata[0] += g_odata[block];
+    }
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Wrapper function for kernel launch
+////////////////////////////////////////////////////////////////////////////////
+void call_reduceSinglePassMultiBlockCG(int size, int threads, int numBlocks,
+                                       float *d_idata, float *d_odata) {
+  int smemSize = threads * sizeof(double);
+  void *kernelArgs[] = {
+      (void *)&d_idata, (void *)&d_odata, (void *)&size,
+  };
+
+  dim3 dimBlock(threads, 1, 1);
+  dim3 dimGrid(numBlocks, 1, 1);
+
+  hipLaunchCooperativeKernel((void *)reduceSinglePassMultiBlockCG, dimGrid,
+                              dimBlock, kernelArgs, smemSize, NULL);
+  // check if kernel execution generated an error
+  getLastCudaError("Kernel execution failed");
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// declaration, forward
+bool runTest(int argc, char **argv, int device);
+
+////////////////////////////////////////////////////////////////////////////////
+// Program main
+////////////////////////////////////////////////////////////////////////////////
+int main(int argc, char **argv) {
+  hipDeviceProp_t deviceProp = {0};
+  int dev;
+
+  printf("%s Starting...\n\n", sSDKsample);
+
+  dev = findCudaDevice(argc, (const char **)argv);
+  checkCudaErrors(hipGetDeviceProperties(&deviceProp, dev));
+  if (!deviceProp.cooperativeLaunch) {
+    printf(
+        "\nSelected GPU (%d) does not support Cooperative Kernel Launch, "
+        "Waiving the run\n",
+        dev);
+    exit(EXIT_WAIVED);
+  }
+
+  bool bTestPassed = false;
+  bTestPassed = runTest(argc, argv, dev);
+
+  exit(bTestPassed ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Compute sum reduction on CPU
+//! We use Kahan summation for an accurate sum of large arrays.
+//! http://en.wikipedia.org/wiki/Kahan_summation_algorithm
+//!
+//! @param data       pointer to input data
+//! @param size       number of input data elements
+////////////////////////////////////////////////////////////////////////////////
+template <class T>
+T reduceCPU(T *data, int size) {
+  T sum = data[0];
+  T c = (T)0.0;
+
+  for (int i = 1; i < size; i++) {
+    T y = data[i] - c;
+    T t = sum + y;
+    c = (t - sum) - y;
+    sum = t;
+  }
+
+  return sum;
+}
+
+unsigned int nextPow2(unsigned int x) {
+  --x;
+  x |= x >> 1;
+  x |= x >> 2;
+  x |= x >> 4;
+  x |= x >> 8;
+  x |= x >> 16;
+  return ++x;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Compute the number of threads and blocks to use for the reduction
+// We set threads / block to the minimum of maxThreads and n/2.
+////////////////////////////////////////////////////////////////////////////////
+void getNumBlocksAndThreads(int n, int maxBlocks, int maxThreads, int &blocks,
+                            int &threads) {
+  if (n == 1) {
+    threads = 1;
+    blocks = 1;
+  } else {
+    checkCudaErrors(hipOccupancyMaxPotentialBlockSize(
+        &blocks, &threads, reduceSinglePassMultiBlockCG));
+  }
+
+  blocks = min(maxBlocks, blocks);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// This function performs a reduction of the input data multiple times and
+// measures the average reduction time.
+////////////////////////////////////////////////////////////////////////////////
+float benchmarkReduce(int n, int numThreads, int numBlocks, int maxThreads,
+                      int maxBlocks, int testIterations,
+                      StopWatchInterface *timer, float *h_odata, float *d_idata,
+                      float *d_odata) {
+  float gpu_result = 0;
+  hipError_t error;
+
+  printf("\nLaunching %s kernel\n",
+         "SinglePass Multi Block Cooperative Groups");
+  for (int i = 0; i < testIterations; ++i) {
+    gpu_result = 0;
+    sdkStartTimer(&timer);
+    call_reduceSinglePassMultiBlockCG(n, numThreads, numBlocks, d_idata,
+                                      d_odata);
+    hipDeviceSynchronize();
+    sdkStopTimer(&timer);
+  }
+
+  // copy final sum from device to host
+  error =
+      hipMemcpy(&gpu_result, d_odata, sizeof(float), hipMemcpyDeviceToHost);
+  checkCudaErrors(error);
+
+  return gpu_result;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// The main function which runs the reduction test.
+////////////////////////////////////////////////////////////////////////////////
+bool runTest(int argc, char **argv, int device) {
+  int size = 1 << 25;  // number of elements to reduce
+  bool bTestPassed = false;
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "n")) {
+    size = getCmdLineArgumentInt(argc, (const char **)argv, "n");
+  }
+
+  printf("%d elements\n", size);
+
+  // Set the device to be used
+  hipDeviceProp_t prop = {0};
+  checkCudaErrors(hipSetDevice(device));
+  checkCudaErrors(hipGetDeviceProperties(&prop, device));
+
+  // create random input data on CPU
+  unsigned int bytes = size * sizeof(float);
+
+  float *h_idata = (float *)malloc(bytes);
+
+  for (int i = 0; i < size; i++) {
+    // Keep the numbers small so we don't get truncation error in the sum
+    h_idata[i] = (rand() & 0xFF) / (float)RAND_MAX;
+  }
+
+  // Determine the launch configuration (threads, blocks)
+  int maxThreads = 0;
+  int maxBlocks = 0;
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "threads")) {
+    maxThreads = getCmdLineArgumentInt(argc, (const char **)argv, "threads");
+  } else {
+    maxThreads = prop.maxThreadsPerBlock;
+  }
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "maxblocks")) {
+    maxBlocks = getCmdLineArgumentInt(argc, (const char **)argv, "maxblocks");
+  } else {
+    maxBlocks = prop.multiProcessorCount *
+                (prop.maxThreadsPerMultiProcessor / prop.maxThreadsPerBlock);
+  }
+
+  int numBlocks = 0;
+  int numThreads = 0;
+  getNumBlocksAndThreads(size, maxBlocks, maxThreads, numBlocks, numThreads);
+
+  // We calculate the occupancy to know how many block can actually fit on the
+  // GPU
+  int numBlocksPerSm = 0;
+  checkCudaErrors(hipOccupancyMaxActiveBlocksPerMultiprocessor(
+      &numBlocksPerSm, reduceSinglePassMultiBlockCG, numThreads,
+      numThreads * sizeof(double)));
+
+  int numSms = prop.multiProcessorCount;
+  if (numBlocks > numBlocksPerSm * numSms) {
+    numBlocks = numBlocksPerSm * numSms;
+  }
+  printf("numThreads: %d\n", numThreads);
+  printf("numBlocks: %d\n", numBlocks);
+
+  // allocate mem for the result on host side
+  float *h_odata = (float *)malloc(numBlocks * sizeof(float));
+
+  // allocate device memory and data
+  float *d_idata = NULL;
+  float *d_odata = NULL;
+
+  checkCudaErrors(hipMalloc((void **)&d_idata, bytes));
+  checkCudaErrors(hipMalloc((void **)&d_odata, numBlocks * sizeof(float)));
+
+  // copy data directly to device memory
+  checkCudaErrors(hipMemcpy(d_idata, h_idata, bytes, hipMemcpyHostToDevice));
+  checkCudaErrors(hipMemcpy(d_odata, h_idata, numBlocks * sizeof(float),
+                             hipMemcpyHostToDevice));
+
+  int testIterations = 100;
+
+  StopWatchInterface *timer = 0;
+  sdkCreateTimer(&timer);
+
+  float gpu_result = 0;
+
+  gpu_result =
+      benchmarkReduce(size, numThreads, numBlocks, maxThreads, maxBlocks,
+                      testIterations, timer, h_odata, d_idata, d_odata);
+
+  float reduceTime = sdkGetAverageTimerValue(&timer);
+  printf("Average time: %f ms\n", reduceTime);
+  printf("Bandwidth:    %f GB/s\n\n",
+         (size * sizeof(int)) / (reduceTime * 1.0e6));
+
+  // compute reference solution
+  float cpu_result = reduceCPU<float>(h_idata, size);
+  printf("GPU result = %0.12f\n", gpu_result);
+  printf("CPU result = %0.12f\n", cpu_result);
+
+  double threshold = 1e-8 * size;
+  double diff = abs((double)gpu_result - (double)cpu_result);
+  bTestPassed = (diff < threshold);
+
+  // cleanup
+  sdkDeleteTimer(&timer);
+
+  free(h_idata);
+  free(h_odata);
+  hipFree(d_idata);
+  hipFree(d_odata);
+
+  return bTestPassed;
+}
diff --git a/src/samples/Samples/3_CUDA_Features/bf16TensorCoreGemm/bf16TensorCoreGemm.cu.hip b/src/samples/Samples/3_CUDA_Features/bf16TensorCoreGemm/bf16TensorCoreGemm.cu.hip
index e69de29..de966f7 100644
--- a/src/samples/Samples/3_CUDA_Features/bf16TensorCoreGemm/bf16TensorCoreGemm.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/bf16TensorCoreGemm/bf16TensorCoreGemm.cu.hip
@@ -0,0 +1,818 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// CUDA sample demonstrating a __nv_bfloat16 (E8M7) GEMM computation using the Warp Matrix Multiply
+// and Accumulate API introduced in CUDA 11.0.
+
+// In this program, the compute_gemm kernel computes the result of a matrix multiplication
+// and addition: D = alpha * A * B + beta * C. The dimensions of both C and D matrices
+// are M_GLOBAL x N_GLOBAL. The A matrix is M_GLOBAL x K_GLOBAL (row-major), the B matrix
+// is K_GLOBAL x N_GLOBAL (column-major).
+// In that kernel, each CTA computes one 128 x 128 tile of the resulting matrix
+// per iteration. When the tile is computed, the CTA stores it to the global memory
+// and begins a new iteration, selecting a new 128 x 128 tile to compute.
+// Each CTA consists of eight warps. For the 128 x 128 tile, each warp computes eight
+// 16 x 16 subtiles, organized in a 2 x 4 two-dimensional array.
+// Warps compute the 16 x 16 subtiles using nvcuda::wmma::mma_sync operations by
+// moving through the K_GLOBAL dimension of the A and B matrices and accumulating
+// the intermediate result in the local thread state.
+
+// There are a number of simple optimizations used in the algorithm:
+// - The CTA copies the 128 x 128 tile of the C matrix from the global memory to
+//   shared memory. After that is done, each warp loads the C matrix fragments from
+//   shared memory, thus avoiding a random global memory access.
+// - On each internal iteration, the CTA copies a portion of the A and B matrices from
+//   global memory to shared memory. After that, all warps in the CTA reuse the A and B
+//   data from shared memory, thus reducing the number of data copies from global memory.
+// - The portions of the A and B matrices are stored in shared memory with an additional
+//   padding (skew) to reduce the number of shared memory access bank conflicts.
+//   (See a detailed explanation near the SKEW_BF16 macro definition.)
+// - When the CTA finishes computing the tiles of the resulting matrix, each warp stores
+//   its subtiles to shared memory. The CTA then copies the shared memory contents to
+//   global memory, again avoiding redundant random global memory accesses.
+// - Note that the CTA tile size is chosen to maximize the GPU register utilization,
+//   but carefully enough to avoid local memory use.
+
+#include <assert.h>
+#include <stdio.h>
+#include <hip/hip_runtime.h>
+#include <cuda_bf16.h>
+#include <mma.h>
+#include <cuda/pipeline>
+
+// helper functions and utilities to work with CUDA
+#include <helper_functions.h>
+#include <helper_cuda.h>
+
+// Externally configurable parameters.
+
+#ifndef CPU_DEBUG
+// Set this to 1 to verify the correctness of the GPU-computed matrix.
+#define CPU_DEBUG 0
+#endif
+
+#ifndef SHARED_MEMORY_LIMIT_64K
+// Set this to 0 to use more than 64 Kb of shared memory to cache data, to
+// improve the performance of the computations on GPU.
+// Note that you need a GPU that can have more than 64 Kb of shared memory
+// per multiprocessor.
+#define SHARED_MEMORY_LIMIT_64K 0
+#endif
+
+// GPU configuration.
+
+#define WARP_SIZE 32
+
+// MMA matrix tile dimensions.
+
+#define M 16
+#define N 16
+#define K 16
+
+// GEMM configuration.
+
+#define M_TILES 512
+#define N_TILES 512
+#define K_TILES 512
+
+#define M_GLOBAL (M * M_TILES)
+#define N_GLOBAL (N * N_TILES)
+#define K_GLOBAL (K * K_TILES)
+
+#define C_LAYOUT wmma::mem_row_major
+
+// Implementation constants.
+
+#define WARPS_PER_BLOCK 8
+#define THREADS_PER_BLOCK (WARP_SIZE * WARPS_PER_BLOCK)
+
+#if SHARED_MEMORY_LIMIT_64K
+// With only 64 Kb shared memory available, we can fit two 8-tile chunks of
+// the A and B matrix data, that is (M = 16) * (K = 16) * 8 * (CHUNK_K = 8)
+// * sizeof(__nv_bfloat16) = 32 Kb each.
+// (i.e. two 8x8 arrays of tiles of 16x16 __nv_bfloat16-typed elements per CTA).
+// But we cannot account the 8 Kb total skew overhead, without which the performance
+// would be severely impacted. So we choose to reduce the chunk size in half,
+// i.e. the amount of A and B matrix data we cache in shared memory.
+// Accordingly, this doubles the number of outer iterations across the global K
+// dimension, which only slightly impacts the performance.
+#define CHUNK_K 4
+#else
+#define CHUNK_K 8
+#endif
+
+#define CHUNK_LINE_BYTES (CHUNK_K * K * sizeof(__nv_bfloat16))
+#define WARP_COPY_BYTES (WARP_SIZE * sizeof(int4))
+#define CHUNK_COPY_LINES_PER_WARP (WARP_COPY_BYTES / CHUNK_LINE_BYTES)
+#define CHUNK_COPY_LINE_LANES (WARP_SIZE / CHUNK_COPY_LINES_PER_WARP)
+
+#define BLOCK_ROW_WARPS 2
+#define BLOCK_COL_WARPS 4
+
+#define WARP_ROW_TILES 4
+#define WARP_COL_TILES 2
+
+#define BLOCK_ROW_TILES (WARP_ROW_TILES * BLOCK_ROW_WARPS)
+#define BLOCK_COL_TILES (WARP_COL_TILES * BLOCK_COL_WARPS)
+
+#define GLOBAL_MEM_STRIDE N_GLOBAL
+
+#define SHMEM_STRIDE (N * BLOCK_ROW_TILES)
+#define SHMEM_OFFSET (N * WARP_ROW_TILES)
+
+// The macro below is used to shift rows of the A matrix and columns of the B matrix
+// in shared memory to minimize possible bank conflicts.
+// Before performing the nvcuda::wmma::mma_sync operation, the warp must load the matrix
+// data using the nvcuda::wmma::load_matrix_sync operation. Although the memory access pattern
+// is not specified for that function, each lane in the warp can read one or multiple matrix
+// elements from different matrix rows or columns.
+// For shared memory, such access can result in bank conflicts if different rows / columns
+// of the matrix map to the same bank. By shifting each row and column by a few bytes, we
+// make sure that they map to different banks, thus reducing the number of possible bank
+// conflicts.
+// The number of 16 two-byte "__nv_bfloat16" elements is chosen as the minimum possible shift because
+// we must keep each row and column 256-bit aligned, as required by nvcuda::wmma::load_matrix_sync.
+#define SKEW_BF16 16
+
+#define checkKernelErrors(expr) do {                                                        \
+    expr;                                                                                   \
+                                                                                            \
+    hipError_t __err = hipGetLastError();                                                 \
+    if (__err != hipSuccess) {                                                             \
+        printf("Line %d: '%s' failed: %s\n", __LINE__, # expr, hipGetErrorString(__err));  \
+        abort();                                                                            \
+    }                                                                                       \
+} while(0)
+
+enum kernels
+{
+    bf16mma_shmem_gemm_async_copy  = 0, // __nv_bfloat16 MMA shmem using kernel with async_copy 
+    bf16mma_shmem_gemm             = 1, // __nv_bfloat16 MMA shmem using kernel normal copy (without async_copy).
+    simple_bf16mma_gemm            = 2  // __nv_bfloat16 MMA non-shmem using simple kernel.
+};
+
+const char* kernelNames[] = {"compute_bf16gemm_async_copy", "compute_bf16gemm", 
+                            "simple_wmma_bf16gemm"};
+
+using namespace nvcuda;
+
+__host__ void init_host_matrices(__nv_bfloat16 *a, __nv_bfloat16 *b, float *c)
+{
+    for (int i = 0; i < M_GLOBAL; i++) {
+        for (int j = 0; j < K_GLOBAL; j++) {
+            a[i*K_GLOBAL+j] = (__nv_bfloat16)(float)(rand() % 3);
+        }
+    }
+
+    for (int i = 0; i < N_GLOBAL; i++) {
+        for (int j = 0; j < K_GLOBAL; j++) {
+            b[i*K_GLOBAL+j] = (__nv_bfloat16)(float)(rand() % 3);
+        }
+    }
+
+    for (int t = 0; t < M_GLOBAL * N_GLOBAL; t++) {
+        c[t] =  (float)(rand() % 3);
+    }
+}
+
+__global__ void compute_bf16gemm(const __nv_bfloat16 *A, const __nv_bfloat16 *B, const float *C, float *D, float alpha, float beta)
+{
+#if __CUDA_ARCH__ >= 800
+    extern __shared__ __nv_bfloat16 shmem[][CHUNK_K * K + SKEW_BF16];
+
+    // Warp and lane identification.
+    const unsigned int warpId = threadIdx.x / WARP_SIZE;
+    const unsigned int laneId = threadIdx.x % WARP_SIZE;
+
+    // Offset in shared memory from which the B matrix is stored.
+    const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;
+
+    // This pointer is used to access the C and D matrix tiles this warp computes.
+    float *shmem_warp_tile_ptr = (float*)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * SHMEM_STRIDE * N * BLOCK_ROW_WARPS + (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;
+
+    // This pointer is used to stream the C and D matrices block-wide tile to and from shared memory.
+    float *shmem_warp_stream_ptr = (float*)&shmem[0][0] + warpId * SHMEM_STRIDE * N;
+
+    // Adjust the beta scaler, as it'll be multiplied by alpha at the end of
+    // each tile computation. Technically this is not generally correct (may result
+    // in a loss of precision). Zero still needs to be specially handled though.
+    beta /= alpha;
+
+    // Each CTA slides along the 128 x 128 tiles from the top left corner of the matrix to the
+    // right and down, and selects the next tile to compute. Once there's no such tile,
+    // all warps in this CTA exit.
+    for(unsigned int block_pos = blockIdx.x;; block_pos += gridDim.x) {
+        const unsigned int block_tile_i = ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);
+        const unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % N_TILES;
+
+        // Stop when there are no more D matrix tiles to compute in this CTA.
+        if (block_tile_i >= M_TILES) {
+            break;
+        }
+
+        // This warp's pointer to the C matrix data to copy memory from to shared memory.
+        const size_t gmem_idx = (block_tile_i + warpId) * M * GLOBAL_MEM_STRIDE + block_tile_j * N;
+        const float *src_gmem_warp_stream_ptr = &C[gmem_idx];
+
+        // Stream multiple C tiles to shared memory.
+#pragma unroll
+        for (int i = 0; i < N; i++) {
+            *((int4*)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId) = 
+                *((int4*)(src_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) + laneId);
+        }
+
+        __syncthreads();
+
+        // These fragments will accumulate the result of A and B matrix fragment multiplications
+        // along the K_GLOBAL dimension.
+        wmma::fragment<wmma::accumulator, M, N, K, float> c[WARP_COL_TILES][WARP_ROW_TILES];
+
+        // Load the C matrix tiles into fragments from shared memory.
+#pragma unroll
+        for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+                const float *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * N + j * N;
+
+                wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, C_LAYOUT);
+            }
+        }
+
+        __syncthreads();
+
+        // Scale the C matrix.
+#pragma unroll
+       for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+#pragma unroll
+                for (int t = 0; t < c[i][j].num_elements; t++) {
+                    c[i][j].x[t] *= beta;
+                }
+            }
+        }
+
+        // Select what warp copies what matrix to shared memory.
+        // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.
+        const __nv_bfloat16 *warp_ptr = (warpId < (WARPS_PER_BLOCK/2)) ? (&A[block_tile_i * M * K_GLOBAL] + M * K_GLOBAL * (warpId % (WARPS_PER_BLOCK/2)) * 2) :
+                                              (&B[block_tile_j * N * K_GLOBAL] + N * K_GLOBAL * (warpId % (WARPS_PER_BLOCK/2)) * 2);
+
+        // Go through the global K dimension by a fixed step at a time.
+#pragma unroll
+        for (int tile_k = 0; tile_k < K_TILES; tile_k += CHUNK_K) {
+            // Copy slices of the A and B matrices to shared memory.
+            // The first half of the warps in the CTA copy the A matrix, the rest copy the B matrix.
+            size_t shmem_idx = warpId < (WARPS_PER_BLOCK/2) ? (M * (warpId % (WARPS_PER_BLOCK/2)) * 2) : 
+                                                              (N * (warpId % (WARPS_PER_BLOCK/2)) * 2 + shmem_idx_b_off);
+
+            // First half of the warp copies the first row / column of the matrix,
+            // the second half of the warp copies the next.
+            const __nv_bfloat16 *lane_ptr = (warp_ptr + tile_k * K + (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL);
+
+            // Shift the second half of the warp to the next row / column in the shared memory.
+            shmem_idx += laneId / CHUNK_COPY_LINE_LANES;
+
+#pragma unroll
+            for(int i = 0; i < ((WARP_SIZE/2) / CHUNK_COPY_LINES_PER_WARP) * 2; i++) {
+                // Copy 16 bytes at once in each lane.
+                *((int4*)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) = *((int4*)lane_ptr +  (laneId % CHUNK_COPY_LINE_LANES));
+
+                // Advance the global memory pointer and the shared memory index.
+                lane_ptr = lane_ptr + K_GLOBAL * CHUNK_COPY_LINES_PER_WARP;
+                shmem_idx += CHUNK_COPY_LINES_PER_WARP;
+            }
+
+            __syncthreads();
+
+            // Compute a grid of C matrix tiles in each warp.
+#pragma unroll
+            for (int k_step = 0; k_step < CHUNK_K; k_step++) {
+                wmma::fragment<wmma::matrix_a, M, N, K, __nv_bfloat16, wmma::row_major> a[WARP_COL_TILES];
+                wmma::fragment<wmma::matrix_b, M, N, K, __nv_bfloat16, wmma::col_major> b[WARP_ROW_TILES];
+
+#pragma unroll
+                for (int i = 0; i < WARP_COL_TILES; i++) {
+                    size_t shmem_idx_a = (warpId/BLOCK_ROW_WARPS) * M * BLOCK_ROW_WARPS + (i * M);
+                    const __nv_bfloat16 *tile_ptr = &shmem[shmem_idx_a][k_step * K];
+
+                    wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_BF16);
+
+#pragma unroll
+                    for (int j = 0; j < WARP_ROW_TILES; j++) {
+                        if (i == 0) {
+                            // Load the B matrix fragment once, because it is going to be reused
+                            // against the other A matrix fragments.
+                            size_t shmem_idx_b = shmem_idx_b_off + (WARP_ROW_TILES * N) * (warpId%2) + (j * N);
+                            const __nv_bfloat16 *tile_ptr = &shmem[shmem_idx_b][k_step * K];
+
+                            wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_BF16);
+                        }
+
+                        wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);
+                    }
+                }
+            }
+
+            __syncthreads();
+        }
+
+        // Store the D fragments to shared memory.
+#pragma unroll
+        for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+#pragma unroll
+                // Uniform, point-wise transformations of ALL fragment elements by ALL threads in the
+                // warp are well-defined even though element indices within fragment storage are not defined.
+                for (int t = 0; t < c[i][j].num_elements; t++)
+                    c[i][j].x[t] *= alpha;
+
+                float *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;
+
+                wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);
+            }
+        }
+
+        __syncthreads();
+
+        // Now that shared memory contains all the D tiles, stream them to global memory.
+        float *dst_gmem_warp_stream_ptr = &D[gmem_idx];
+
+#pragma unroll
+        for (int i = 0; i < N; i++) {
+            *((float4*)(dst_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) + laneId) =
+                *((float4*)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId);
+        }
+
+        __syncthreads();
+    }
+#endif
+}
+
+__global__ void compute_bf16gemm_async_copy(const __nv_bfloat16 *A, const __nv_bfloat16 *B, const float *C, float *D, float alpha, float beta)
+{
+#if __CUDA_ARCH__ >= 800
+    extern __shared__ __nv_bfloat16 shmem[][CHUNK_K * K + SKEW_BF16];
+
+    // Warp and lane identification.
+    const unsigned int warpId = threadIdx.x / WARP_SIZE;
+    const unsigned int laneId = threadIdx.x % WARP_SIZE;
+
+    // Offset in shared memory from which the B matrix is stored.
+    constexpr size_t shmem_idx_b_off = BLOCK_COL_TILES * M;
+
+    // This pointer is used to access the C and D matrix tiles this warp computes.
+    float *shmem_warp_tile_ptr = (float*)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * SHMEM_STRIDE * N * BLOCK_ROW_WARPS + (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;
+
+    // This pointer is used to stream the C and D matrices block-wide tile to and from shared memory.
+    float *shmem_warp_stream_ptr = (float*)&shmem[0][0] + warpId * SHMEM_STRIDE * N;
+
+    // Adjust the beta scaler, as it'll be multiplied by alpha at the end of
+    // each tile computation. Technically this is not generally correct (may result
+    // in a loss of precision). Zero still needs to be specially handled though.
+    beta /= alpha;
+
+    cuda::pipeline<cuda::thread_scope_thread> pipe = cuda::make_pipeline();
+    const auto shape4 = cuda::aligned_size_t<alignof(float4)>(sizeof(float4));
+    constexpr int loadStride = 2; // load 4 floats, left-shift by 2.
+
+    // Each CTA slides along the 128 x 128 tiles from the top left corner of the matrix to the
+    // right and down, and selects the next tile to compute. Once there's no such tile,
+    // all warps in this CTA exit.
+    for(unsigned int block_pos = blockIdx.x;; block_pos += gridDim.x) {
+        const unsigned int block_tile_i = ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);
+        const unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % N_TILES;
+
+        // Stop when there are no more D matrix tiles to compute in this CTA.
+        if (block_tile_i >= M_TILES) {
+            break;
+        }
+
+        // This warp's pointer to the C matrix data to copy memory from to shared memory.
+        const size_t gmem_idx = (block_tile_i + warpId) * M * GLOBAL_MEM_STRIDE + block_tile_j * N;
+        const float *src_gmem_warp_stream_ptr = &C[gmem_idx];
+
+        // Stream multiple C tiles to shared memory.
+#pragma unroll
+        for (int i = 0; i < N; i++) {
+            pipe.producer_acquire();
+
+            cuda::memcpy_async(&shmem_warp_stream_ptr[(SHMEM_STRIDE * i) + (laneId << loadStride)],
+                                &src_gmem_warp_stream_ptr[(GLOBAL_MEM_STRIDE * i) + (laneId << loadStride)],
+                                shape4, pipe);
+
+            pipe.producer_commit();
+        }
+
+        // Now wait for all the above issued 8 batches to complete.
+        cuda::pipeline_consumer_wait_prior<0>(pipe);
+        __syncthreads();
+
+        // These fragments will accumulate the result of A and B matrix fragment multiplications
+        // along the K_GLOBAL dimension.
+        wmma::fragment<wmma::accumulator, M, N, K, float> c[WARP_COL_TILES][WARP_ROW_TILES];
+
+        // Load the C matrix tiles into fragments from shared memory.
+#pragma unroll
+        for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+                const float *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * N + j * N;
+
+                wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, C_LAYOUT);
+                // Scale the C matrix.
+#pragma unroll
+                for (int t = 0; t < c[i][j].num_elements; t++) {
+                    c[i][j].x[t] *= beta;
+                }
+            }
+        }
+
+        pipe.consumer_release();
+
+        // sync here so that shared memory can then be used for loading A & B matrices.
+        __syncthreads();
+        // Select what warp copies what matrix to shared memory.
+        // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.
+        const __nv_bfloat16 *warp_ptr = (warpId < (WARPS_PER_BLOCK/2)) ? (&A[block_tile_i * M * K_GLOBAL] + M * K_GLOBAL * (warpId % (WARPS_PER_BLOCK/2)) * 2) :
+                                              (&B[block_tile_j * N * K_GLOBAL] + N * K_GLOBAL * (warpId % (WARPS_PER_BLOCK/2)) * 2);
+
+        constexpr int chunksPerLane = ((WARP_SIZE/2) / CHUNK_COPY_LINES_PER_WARP) * 2;
+        constexpr int loadStrideBfloat8 = 3; // load 8 bfloats, left-shift by 3.
+        const int laneLoadElem = (laneId % CHUNK_COPY_LINE_LANES) << loadStrideBfloat8;
+        const int stridePerLaneCopy = (laneId / CHUNK_COPY_LINE_LANES);
+
+        // Go through the global K dimension by a fixed step at a time.
+#pragma unroll
+        for (int tile_k = 0; tile_k < K_TILES; tile_k += CHUNK_K) {
+            // Copy slices of the A and B matrices to shared memory.
+            // The first half of the warps in the CTA copy the A matrix, the rest copy the B matrix.
+            // As for bf16 MMA  M == N we use M for warp 4-7 + shmem_idx_b_off.
+            size_t shmem_idx = (M * (warpId % (WARPS_PER_BLOCK/2)) * 2) + ((warpId / (WARPS_PER_BLOCK/2)) * shmem_idx_b_off);
+
+            // First half of the warp copies the first row / column of the matrix,
+            // the second half of the warp copies the next.
+            const __nv_bfloat16 *lane_ptr = warp_ptr + tile_k * K + stridePerLaneCopy * K_GLOBAL + laneLoadElem;
+
+            // Shift the second half of the warp to the next row / column in the shared memory.
+            shmem_idx += stridePerLaneCopy;
+
+#pragma unroll
+            for(int i = 0; i < chunksPerLane; i++) {
+                // Copy 16 bytes at once in each lane.
+                pipe.producer_acquire();
+                cuda::memcpy_async(&shmem[shmem_idx][laneLoadElem], lane_ptr, shape4, pipe);
+                pipe.producer_commit();
+                // Advance the global memory pointer and the shared memory index.
+                lane_ptr = lane_ptr + K_GLOBAL * CHUNK_COPY_LINES_PER_WARP;
+                shmem_idx += CHUNK_COPY_LINES_PER_WARP;
+            }
+            cuda::pipeline_consumer_wait_prior<0>(pipe);
+            __syncthreads();
+
+            // Compute a grid of C matrix tiles in each warp.
+#pragma unroll
+            for (int k_step = 0; k_step < CHUNK_K; k_step++) {
+                wmma::fragment<wmma::matrix_a, M, N, K, __nv_bfloat16, wmma::row_major> a[WARP_COL_TILES];
+                wmma::fragment<wmma::matrix_b, M, N, K, __nv_bfloat16, wmma::col_major> b[WARP_ROW_TILES];
+
+#pragma unroll
+                for (int i = 0; i < WARP_COL_TILES; i++) {
+                    size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * BLOCK_ROW_WARPS + (i * M);
+                    const __nv_bfloat16 *tile_ptr = &shmem[shmem_idx_a][k_step * K];
+
+                    wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_BF16);
+
+#pragma unroll
+                    for (int j = 0; j < WARP_ROW_TILES; j++) {
+                        if (i == 0) {
+                            // Load the B matrix fragment once, because it is going to be reused
+                            // against the other A matrix fragments.
+                            size_t shmem_idx_b = shmem_idx_b_off + (WARP_ROW_TILES * N) * (warpId%2) + (j * N);
+                            const __nv_bfloat16 *tile_ptr = &shmem[shmem_idx_b][k_step * K];
+
+                            wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_BF16);
+                        }
+
+                        wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);
+                    }
+                }
+            }
+
+            pipe.consumer_release();
+            __syncthreads();
+        }
+
+        // Store the D fragments to shared memory.
+#pragma unroll
+        for (int i = 0; i < WARP_COL_TILES; i++) {
+#pragma unroll
+            for (int j = 0; j < WARP_ROW_TILES; j++) {
+#pragma unroll
+                // Uniform, point-wise transformations of ALL fragment elements by ALL threads in the
+                // warp are well-defined even though element indices within fragment storage are not defined.
+                for (int t = 0; t < c[i][j].num_elements; t++)
+                    c[i][j].x[t] *= alpha;
+
+                float *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;
+
+                wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);
+            }
+        }
+
+        __syncthreads();
+
+        // Now that shared memory contains all the D tiles, stream them to global memory.
+        float *dst_gmem_warp_stream_ptr = &D[gmem_idx];
+
+#pragma unroll
+        for (int i = 0; i < N; i++) {
+            *((int4*)(dst_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) + laneId) =
+                *((int4*)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId);
+        }
+
+        __syncthreads();
+    }
+#endif
+}
+
+// Performs an MxNxK bf16 GEMM (C=alpha*A*B + beta*C) assuming:
+//  1) Matrices are packed in memory.
+//  2) M, N and K are multiples of 16, 16 and 16 respectively. 
+//  3) A is row major, B is column major matrix.
+// Note: This is a less performant version of the compute_bf16gemm kernel. It is designed for
+//       demonstration purposes only to show the CUDA WMMA API use without relying on
+//       availability of the shared memory.
+__global__ void simple_wmma_bf16gemm(__nv_bfloat16 *a, __nv_bfloat16 *b, float *c, float *d, int m_ld, int n_ld, int k_ld, float alpha, float beta)
+{
+#if __CUDA_ARCH__ >= 800
+   // Leading dimensions. Packed with no transpositions.
+    int lda = k_ld;
+    int ldb = k_ld;
+    int ldc = n_ld;
+
+   // Tile using a 2D grid
+   int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / warpSize;
+   int warpN = (blockIdx.y * blockDim.y + threadIdx.y);
+ 
+   // Declare the fragments
+   wmma::fragment<wmma::matrix_a, M, N, K, __nv_bfloat16, wmma::row_major> a_frag;
+   wmma::fragment<wmma::matrix_b, M, N, K, __nv_bfloat16, wmma::col_major> b_frag;
+   wmma::fragment<wmma::accumulator, M, N, K, float> acc_frag;
+   wmma::fragment<wmma::accumulator, M, N, K, float> c_frag;
+
+   wmma::fill_fragment(acc_frag, 0.0f);
+
+   // Loop over k
+   for (int i = 0; i < k_ld; i += K) {
+      int aCol = i; 
+      int aRow = warpM * M;
+
+      int bCol = i;
+      int bRow = warpN * N;
+
+      // Bounds checking
+      if (aRow < m_ld && aCol < k_ld && bRow < k_ld && bCol < n_ld) {
+         // Load the inputs
+         wmma::load_matrix_sync(a_frag, a + aCol + aRow * lda, lda);
+         wmma::load_matrix_sync(b_frag, b + bRow + bCol * ldb, ldb);
+ 
+         // Perform the matrix multiplication
+         wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag);
+
+      }
+   }
+
+   // Load in the current value of c, scale it by beta, and add this our result scaled by alpha
+   int cCol = warpN * N;
+   int cRow = warpM * M;
+
+   if (cRow < m_ld && cCol < n_ld) {
+      wmma::load_matrix_sync(c_frag, c + cCol + cRow * ldc, ldc, wmma::mem_row_major);
+
+      for(int i=0; i < c_frag.num_elements; i++) {
+         c_frag.x[i] = alpha * acc_frag.x[i] + beta * c_frag.x[i];
+      }
+
+      // Store the output
+      wmma::store_matrix_sync(d + cCol + cRow * ldc, c_frag, ldc, wmma::mem_row_major);
+   }
+#endif
+}
+
+__host__ void matMultiplyOnHost(__nv_bfloat16 *A, __nv_bfloat16 *B, float *C,
+                                float alpha, float beta,
+                                int numARows, int numAColumns,
+                                int numBRows, int numBColumns,
+                                int numCRows, int numCColumns)
+{
+    for (int i = 0; i < numCRows; i++) {
+        for (int j = 0; j < numCColumns; j++) {
+            float temp = 0.0;
+
+            for (int k = 0; k < numAColumns; k++) {
+                temp += (float)A[i * numAColumns + k] * (float)B[j * numBRows + k];
+            }
+
+            C[i*numCColumns + j] = temp * alpha + beta * C[i * numCColumns + j];
+        }
+    }
+}
+
+int main(int argc, char **argv)
+{
+    printf("Initializing...\n");
+
+    int dev = findCudaDevice(argc, (const char **)argv);
+
+    hipDeviceProp_t deviceProp;
+    checkCudaErrors(hipGetDeviceProperties(&deviceProp, dev));
+
+    // Tensor cores require a GPU of Volta (SM8X) architecture or higher.
+    if (deviceProp.major < 8) {
+        printf("bf16TensorCoreGemm requires requires SM 8.0 or higher to use Tensor Cores.  Exiting...\n");
+        exit(EXIT_WAIVED);
+    }
+
+    printf("M: %d (%d x %d)\n", M_GLOBAL, M, M_TILES);
+    printf("N: %d (%d x %d)\n", N_GLOBAL, N, N_TILES);
+    printf("K: %d (%d x %d)\n", K_GLOBAL, K, K_TILES);
+
+    __nv_bfloat16 *A_h = NULL;
+    __nv_bfloat16 *B_h = NULL;
+    float *C_h = NULL;
+#if CPU_DEBUG
+    float *result_hD = NULL;
+    float *result_host = NULL;
+#endif
+
+    A_h = (__nv_bfloat16*) malloc(sizeof(__nv_bfloat16) * M_GLOBAL * K_GLOBAL);
+    B_h = (__nv_bfloat16*) malloc(sizeof(__nv_bfloat16) * K_GLOBAL * N_GLOBAL);
+    C_h = (float*) malloc(sizeof(float) * M_GLOBAL * N_GLOBAL);
+#if CPU_DEBUG
+    result_hD   = (float*) malloc(sizeof(float) * M_GLOBAL * N_GLOBAL);
+    result_host = (float*) malloc(sizeof(float) * M_GLOBAL * N_GLOBAL);
+#endif
+
+    __nv_bfloat16 *A = NULL;
+    __nv_bfloat16 *B = NULL;
+    float *C = NULL;
+    float *D = NULL;
+
+    checkCudaErrors(hipMalloc((void**)&A, sizeof(__nv_bfloat16) * M_GLOBAL * K_GLOBAL));
+    checkCudaErrors(hipMalloc((void**)&B, sizeof(__nv_bfloat16) * N_GLOBAL * K_GLOBAL));
+    checkCudaErrors(hipMalloc((void**)&C, sizeof(float) * M_GLOBAL * N_GLOBAL));
+    checkCudaErrors(hipMalloc((void**)&D, sizeof(float) * M_GLOBAL * N_GLOBAL));
+
+    assert(((unsigned long long)A) % 128 == 0);
+    assert(((unsigned long long)B) % 128 == 0);
+    assert(((unsigned long long)C) % 128 == 0);
+    assert(((unsigned long long)D) % 128 == 0);
+
+    init_host_matrices(A_h, B_h, C_h);
+
+    printf("Preparing data for GPU...\n");
+
+    checkCudaErrors(hipMemcpy(A, A_h, sizeof(__nv_bfloat16) * M_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
+    checkCudaErrors(hipMemcpy(B, B_h, sizeof(__nv_bfloat16) * N_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
+    checkCudaErrors(hipMemcpy(C, C_h, sizeof(float) * M_GLOBAL * N_GLOBAL, hipMemcpyHostToDevice));
+    checkCudaErrors(hipMemset(D, 0, sizeof(float) * M_GLOBAL * N_GLOBAL));
+
+    enum {
+        // Compute the right amount of shared memory to request.
+        // We need shared memory to hold per-CTA C and D matrix tiles, and to cache per-CTA chunks
+        // of the A and B matrices. Therefore, the right amount to request is the maximum of those
+        // two numbers.
+        SHMEM_SZ = MAX(sizeof(__nv_bfloat16) * (BLOCK_COL_TILES * M) * (CHUNK_K * K + SKEW_BF16) * 2,
+                       M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N * (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(float))
+    };
+
+    printf("Required shared memory size: %lu Kb\n", SHMEM_SZ / 1024UL);
+
+    const float alpha = 1.1f;
+    const float beta = 1.2f;
+
+    hipEvent_t start, stop;
+
+    checkCudaErrors(hipEventCreate(&start));    
+    checkCudaErrors(hipEventCreate(&stop));
+    checkCudaErrors(hipEventRecord(start));
+
+    // kernel to run - default (b16mma_shmem_gemm_async_copy == 0)
+    kernels selected_kernel = bf16mma_shmem_gemm_async_copy;
+
+    if (checkCmdLineFlag(argc, (const char **)argv, "kernel")) {
+        int kernel_number = getCmdLineArgumentInt(argc, (const char **)argv, "kernel");
+        if (kernel_number < 3) {
+            selected_kernel = (kernels)kernel_number;
+        }
+        else {
+            printf("Error: kernel number should be between 0 to 2, you have entered %d\n", kernel_number);
+            exit(EXIT_FAILURE);
+        }
+    }
+
+    // If enough shared memory available on the GPU use high performant kernel
+    if ((deviceProp.sharedMemPerMultiprocessor >= SHMEM_SZ) && (selected_kernel != simple_bf16mma_gemm)) {
+        printf("Computing using high performance kernel = %d - %s\n", selected_kernel, kernelNames[selected_kernel]);
+
+        switch (selected_kernel)
+        {
+            case bf16mma_shmem_gemm_async_copy :
+            default:
+                checkCudaErrors(hipFuncSetAttribute(compute_bf16gemm_async_copy, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
+                checkKernelErrors((compute_bf16gemm_async_copy<<<deviceProp.multiProcessorCount*2, THREADS_PER_BLOCK, SHMEM_SZ>>>(A, B, C, D, alpha, beta)));
+                break;
+            case bf16mma_shmem_gemm :
+                checkCudaErrors(hipFuncSetAttribute(compute_bf16gemm, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
+                checkKernelErrors((compute_bf16gemm<<<deviceProp.multiProcessorCount*2, THREADS_PER_BLOCK, SHMEM_SZ>>>(A, B, C, D, alpha, beta)));
+                break;
+        }
+#if CPU_DEBUG
+        checkCudaErrors(hipMemcpy(result_hD, D, sizeof(float)*M_GLOBAL*N_GLOBAL, hipMemcpyDeviceToHost));
+#endif
+    }
+    else {
+        dim3 gridDim;
+        dim3 blockDim;
+     
+        // blockDim.x must be a multple of warpSize
+        // 128x4 means we have 16 warps and a block computes a 64x64 output tile
+        blockDim.x = 128;
+        blockDim.y = 4;
+
+        gridDim.x = (M_GLOBAL + (M * blockDim.x / 32 - 1)) / (M * blockDim.x / 32);
+        gridDim.y = (N_GLOBAL + N * blockDim.y - 1) / (N * blockDim.y);
+
+        printf("Computing... using simple_wmma_gemm kernel\n");
+        simple_wmma_bf16gemm<<<gridDim, blockDim>>>(A, B, C, D, M_GLOBAL, N_GLOBAL, K_GLOBAL, alpha, beta);
+#if CPU_DEBUG
+        checkCudaErrors(hipMemcpy(result_hD, D, sizeof(float) * M_GLOBAL * N_GLOBAL, hipMemcpyDeviceToHost));
+#endif
+    }
+
+    checkCudaErrors(hipEventRecord(stop));
+    checkCudaErrors(hipEventSynchronize(stop));
+
+#if CPU_DEBUG
+    printf("Verifying correctness of the computations...\n");
+
+    memcpy(result_host, C_h, sizeof(float) * M_GLOBAL * N_GLOBAL);
+
+    matMultiplyOnHost(A_h, B_h, result_host,
+                      alpha, beta,
+                      M_GLOBAL, K_GLOBAL,
+                      K_GLOBAL, N_GLOBAL,
+                      M_GLOBAL, N_GLOBAL);
+
+    for (int i = 0; i < N_GLOBAL * M_GLOBAL; i++) {
+        if (fabs(result_hD[i] - result_host[i]) > 0.1f) {
+            printf("mismatch i=%d result_hD=%f result_host=%f\n", i, result_hD[i], result_host[i]);
+        }
+    }
+    free(result_hD);
+    free(result_host);
+#endif
+
+    float milliseconds = 0;
+
+    checkCudaErrors(hipEventElapsedTime(&milliseconds, start, stop));
+
+    printf("Time: %f ms\n", milliseconds);
+    printf("TFLOPS: %.2f\n", (((double)M_GLOBAL * N_GLOBAL * K_GLOBAL * 2)/(milliseconds/1000.)) / 1e12);
+
+    free(A_h);
+    free(B_h);
+    free(C_h);
+    checkCudaErrors(hipFree((void*)A));
+    checkCudaErrors(hipFree((void*)B));
+    checkCudaErrors(hipFree((void*)C));
+    checkCudaErrors(hipFree((void*)D));
+
+    return 0;
+}
diff --git a/src/samples/Samples/3_CUDA_Features/cdpSimplePrint/cdpSimplePrint.cu b/src/samples/Samples/3_CUDA_Features/cdpSimplePrint/cdpSimplePrint.cu
index 1dabb14..f4acf13 100644
--- a/src/samples/Samples/3_CUDA_Features/cdpSimplePrint/cdpSimplePrint.cu
+++ b/src/samples/Samples/3_CUDA_Features/cdpSimplePrint/cdpSimplePrint.cu
@@ -25,8 +25,8 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  */
 
-#include <helper_cuda.h>
-#include <helper_string.h>
+//#include <helper_cuda.h>
+//#include <helper_string.h>
 
 #include <cstdio>
 #include <cstdlib>
diff --git a/src/samples/Samples/3_CUDA_Features/cdpSimplePrint/cdpSimplePrint.cu.hip b/src/samples/Samples/3_CUDA_Features/cdpSimplePrint/cdpSimplePrint.cu.hip
index 90f7d33..d363080 100644
--- a/src/samples/Samples/3_CUDA_Features/cdpSimplePrint/cdpSimplePrint.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cdpSimplePrint/cdpSimplePrint.cu.hip
@@ -1,3 +1,4 @@
+#include "hip/hip_runtime.h"
 /* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -25,11 +26,9 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  */
 
-
-#include <hip/hip_runtime.h>
-#include <helper_cuda.h>
+#include "helper_cuda_hipified.h"
 #include <helper_string.h>
-#include "HIPCHECK.h"
+
 #include <cstdio>
 #include <cstdlib>
 #include <iostream>
@@ -125,7 +124,7 @@ int main(int argc, char **argv) {
   int device = -1;
   hipDeviceProp_t deviceProp;
   device = findCudaDevice(argc, (const char **)argv);
-  HIPCHECK(hipGetDeviceProperties(&deviceProp, device));
+  checkCudaErrors(hipGetDeviceProperties(&deviceProp, device));
 
   if (!(deviceProp.major > 3 ||
         (deviceProp.major == 3 && deviceProp.minor >= 5))) {
@@ -160,15 +159,15 @@ int main(int argc, char **argv) {
       "***\n\n");
 
   // We set the recursion limit for CDP to max_depth.
-  cudaDeviceSetLimit(cudaLimitDevRuntimeSyncDepth, max_depth);
+  hipDeviceSetLimit(cudaLimitDevRuntimeSyncDepth, max_depth);
 
   // Launch the kernel from the CPU.
   printf("Launching cdp_kernel() with CUDA Dynamic Parallelism:\n\n");
   cdp_kernel<<<2, 2>>>(max_depth, 0, 0, -1);
-  HIPCHECK(hipGetLastError());
+  checkCudaErrors(hipGetLastError());
 
   // Finalize.
-  HIPCHECK(hipDeviceSynchronize());
+  checkCudaErrors(hipDeviceSynchronize());
 
   exit(EXIT_SUCCESS);
 }
diff --git a/src/samples/Samples/3_CUDA_Features/cdpSimpleQuicksort/cdpSimpleQuicksort.cu.hip b/src/samples/Samples/3_CUDA_Features/cdpSimpleQuicksort/cdpSimpleQuicksort.cu.hip
index 88766f9..735ac95 100644
--- a/src/samples/Samples/3_CUDA_Features/cdpSimpleQuicksort/cdpSimpleQuicksort.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cdpSimpleQuicksort/cdpSimpleQuicksort.cu.hip
@@ -32,6 +32,7 @@
 #include "helper_cuda_hipified.h"
 #include <helper_string.h>
 #include "HIPCHECK.h"
+#include <hip/hip_runtime_api.h>
 #define MAX_DEPTH 16
 #define INSERTION_SORT 32
 
