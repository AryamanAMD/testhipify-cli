diff --git a/src/samples/Common/HIPCHECK.h b/src/samples/Common/HIPCHECK.h
index 4297e27..12c7dca 100755
--- a/src/samples/Common/HIPCHECK.h
+++ b/src/samples/Common/HIPCHECK.h
@@ -6,8 +6,7 @@
 #define KMAG "\x1B[35m"
 #define KCYN "\x1B[36m"
 #define KWHT "\x1B[37m"
-
-
+#include "hip/hip_runtime_api.h"
 #define failed(...)                                                                                \
     printf("%serror: ", KRED);                                                                     \
     printf(__VA_ARGS__);                                                                           \
diff --git a/src/samples/Samples/0_Introduction/UnifiedMemoryStreams/UnifiedMemoryStreams.cu.hip b/src/samples/Samples/0_Introduction/UnifiedMemoryStreams/UnifiedMemoryStreams.cu.hip
old mode 100755
new mode 100644
index 84fab07..fa34be6
--- a/src/samples/Samples/0_Introduction/UnifiedMemoryStreams/UnifiedMemoryStreams.cu.hip
+++ b/src/samples/Samples/0_Introduction/UnifiedMemoryStreams/UnifiedMemoryStreams.cu.hip
@@ -213,7 +213,7 @@ void execute(Task<T> &t, hipblasHandle_t *handle, hipStream_t *stream,
     double zero = 0.0;
 
     // attach managed memory to my stream
-    //HIPCHECK(hipblasSetStream(handle[tid + 1], stream[tid + 1]));
+   // HIPCHECK(hipblasSetStream(handle[tid + 1], stream[tid + 1]));
     HIPCHECK(hipStreamAttachMemAsync(stream[tid + 1], t.data, 0,
                                              hipMemAttachSingle));
     HIPCHECK(hipStreamAttachMemAsync(stream[tid + 1], t.vector, 0,
@@ -343,8 +343,4 @@ int main(int argc, char **argv) {
   printf("All Done!\n");
   exit(EXIT_SUCCESS);
 }
-cuBlas handles
-  for (int i = 0; i < nthreads + 1; i++) {
-    hipStreamDestroy(streams[i]);
-    hipblasDestroy(handles[i]);
-  }
+
diff --git a/src/samples/Samples/0_Introduction/asyncAPI/asyncAPI.out b/src/samples/Samples/0_Introduction/asyncAPI/asyncAPI.out
index dd9b019..7febf5a 100755
Binary files a/src/samples/Samples/0_Introduction/asyncAPI/asyncAPI.out and b/src/samples/Samples/0_Introduction/asyncAPI/asyncAPI.out differ
diff --git a/src/samples/Samples/0_Introduction/clock/clock.cu.hip b/src/samples/Samples/0_Introduction/clock/clock.cu.hip
index e69de29..badf117 100755
--- a/src/samples/Samples/0_Introduction/clock/clock.cu.hip
+++ b/src/samples/Samples/0_Introduction/clock/clock.cu.hip
@@ -0,0 +1,154 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * This example shows how to use the clock function to measure the performance
+ * of block of threads of a kernel accurately. Blocks are executed in parallel
+ * and out of order. Since there's no synchronization mechanism between blocks,
+ * we measure the clock once for each block. The clock samples are written to
+ * device memory.
+ */
+
+// System includes
+#include <assert.h>
+#include <stdint.h>
+#include <stdio.h>
+
+// CUDA runtime
+#include <hip/hip_runtime.h>
+
+// helper functions and utilities to work with CUDA
+#include "helper_cuda_hipified.h"
+#include "helper_functions.h"
+#include "HIPCHECK.h"
+// This kernel computes a standard parallel reduction and evaluates the
+// time it takes to do that for each block. The timing results are stored
+// in device memory.
+__global__ static void timedReduction(const float *input, float *output,
+                                      clock_t *timer) {
+  // __shared__ float shared[2 * blockDim.x];
+  extern __shared__ float shared[];
+
+  const int tid = threadIdx.x;
+  const int bid = blockIdx.x;
+
+  if (tid == 0) timer[bid] = clock();
+
+  // Copy input.
+  shared[tid] = input[tid];
+  shared[tid + blockDim.x] = input[tid + blockDim.x];
+
+  // Perform reduction to find minimum.
+  for (int d = blockDim.x; d > 0; d /= 2) {
+    __syncthreads();
+
+    if (tid < d) {
+      float f0 = shared[tid];
+      float f1 = shared[tid + d];
+
+      if (f1 < f0) {
+        shared[tid] = f1;
+      }
+    }
+  }
+
+  // Write result.
+  if (tid == 0) output[bid] = shared[0];
+
+  __syncthreads();
+
+  if (tid == 0) timer[bid + gridDim.x] = clock();
+}
+
+#define NUM_BLOCKS 64
+#define NUM_THREADS 256
+
+// It's interesting to change the number of blocks and the number of threads to
+// understand how to keep the hardware busy.
+//
+// Here are some numbers I get on my G80:
+//    blocks - clocks
+//    1 - 3096
+//    8 - 3232
+//    16 - 3364
+//    32 - 4615
+//    64 - 9981
+//
+// With less than 16 blocks some of the multiprocessors of the device are idle.
+// With more than 16 you are using all the multiprocessors, but there's only one
+// block per multiprocessor and that doesn't allow you to hide the latency of
+// the memory. With more than 32 the speed scales linearly.
+
+// Start the main CUDA Sample here
+int main(int argc, char **argv) {
+  printf("CUDA Clock sample\n");
+
+  // This will pick the best possible CUDA capable device
+  int dev = findCudaDevice(argc, (const char **)argv);
+
+  float *dinput = NULL;
+  float *doutput = NULL;
+  clock_t *dtimer = NULL;
+
+  clock_t timer[NUM_BLOCKS * 2];
+  float input[NUM_THREADS * 2];
+
+  for (int i = 0; i < NUM_THREADS * 2; i++) {
+    input[i] = (float)i;
+  }
+
+  HIPCHECK(
+      hipMalloc((void **)&dinput, sizeof(float) * NUM_THREADS * 2));
+  HIPCHECK(hipMalloc((void **)&doutput, sizeof(float) * NUM_BLOCKS));
+  HIPCHECK(
+      hipMalloc((void **)&dtimer, sizeof(clock_t) * NUM_BLOCKS * 2));
+
+  HIPCHECK(hipMemcpy(dinput, input, sizeof(float) * NUM_THREADS * 2,
+                             hipMemcpyHostToDevice));
+
+  timedReduction<<<NUM_BLOCKS, NUM_THREADS, sizeof(float) * 2 * NUM_THREADS>>>(
+      dinput, doutput, dtimer);
+
+  HIPCHECK(hipMemcpy(timer, dtimer, sizeof(clock_t) * NUM_BLOCKS * 2,
+                             hipMemcpyDeviceToHost));
+
+  HIPCHECK(hipFree(dinput));
+  HIPCHECK(hipFree(doutput));
+  HIPCHECK(hipFree(dtimer));
+
+  long double avgElapsedClocks = 0;
+
+  for (int i = 0; i < NUM_BLOCKS; i++) {
+    avgElapsedClocks += (long double)(timer[i + NUM_BLOCKS] - timer[i]);
+  }
+
+  avgElapsedClocks = avgElapsedClocks / NUM_BLOCKS;
+  printf("Average clocks/block = %Lf\n", avgElapsedClocks);
+
+  return EXIT_SUCCESS;
+}
diff --git a/src/samples/Samples/0_Introduction/concurrentKernels/concurrentKernels.cu.hip b/src/samples/Samples/0_Introduction/concurrentKernels/concurrentKernels.cu.hip
index 93677a6..73b1cd7 100755
--- a/src/samples/Samples/0_Introduction/concurrentKernels/concurrentKernels.cu.hip
+++ b/src/samples/Samples/0_Introduction/concurrentKernels/concurrentKernels.cu.hip
@@ -229,6 +229,3 @@ int main(int argc, char **argv) {
   printf("Test passed\n");
   exit(EXIT_SUCCESS);
 }
-Test failed!\n");
-    exit(EXIT_FAILURE);
-  }
diff --git a/src/samples/Samples/0_Introduction/concurrentKernels/concurrentKernels.out b/src/samples/Samples/0_Introduction/concurrentKernels/concurrentKernels.out
index 972eea9..0a70d62 100755
Binary files a/src/samples/Samples/0_Introduction/concurrentKernels/concurrentKernels.out and b/src/samples/Samples/0_Introduction/concurrentKernels/concurrentKernels.out differ
diff --git a/src/samples/Samples/0_Introduction/cppIntegration/cppIntegration.cu.hip b/src/samples/Samples/0_Introduction/cppIntegration/cppIntegration.cu.hip
index e69de29..77d961d 100755
--- a/src/samples/Samples/0_Introduction/cppIntegration/cppIntegration.cu.hip
+++ b/src/samples/Samples/0_Introduction/cppIntegration/cppIntegration.cu.hip
@@ -0,0 +1,173 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* 
+ * Example of integrating CUDA functions into an existing
+ * application / framework.
+ * Host part of the device code.
+ * Compiled with Cuda compiler.
+ */
+
+// System includes
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <math.h>
+#include <assert.h>
+
+// CUDA runtime
+#include <hip/hip_runtime.h>
+
+// helper functions and utilities to work with CUDA
+#include "helper_cuda_hipified.h"
+#include "helper_functions.h"
+#include "HIPCHECK.h"
+#ifndef MAX
+#define MAX(a, b) (a > b ? a : b)
+#endif
+
+////////////////////////////////////////////////////////////////////////////////
+// declaration, forward
+
+extern "C" void computeGold(char *reference, char *idata,
+                            const unsigned int len);
+extern "C" void computeGold2(int2 *reference, int2 *idata,
+                             const unsigned int len);
+
+///////////////////////////////////////////////////////////////////////////////
+//! Simple test kernel for device functionality
+//! @param g_odata  memory to process (in and out)
+///////////////////////////////////////////////////////////////////////////////
+__global__ void kernel(int *g_data) {
+  // write data to global memory
+  const unsigned int tid = threadIdx.x;
+  int data = g_data[tid];
+
+  // use integer arithmetic to process all four bytes with one thread
+  // this serializes the execution, but is the simplest solutions to avoid
+  // bank conflicts for this very low number of threads
+  // in general it is more efficient to process each byte by a separate thread,
+  // to avoid bank conflicts the access pattern should be
+  // g_data[4 * wtid + wid], where wtid is the thread id within the half warp
+  // and wid is the warp id
+  // see also the programming guide for a more in depth discussion.
+  g_data[tid] =
+      ((((data << 0) >> 24) - 10) << 24) | ((((data << 8) >> 24) - 10) << 16) |
+      ((((data << 16) >> 24) - 10) << 8) | ((((data << 24) >> 24) - 10) << 0);
+}
+
+///////////////////////////////////////////////////////////////////////////////
+//! Demonstration that int2 data can be used in the cpp code
+//! @param g_odata  memory to process (in and out)
+///////////////////////////////////////////////////////////////////////////////
+__global__ void kernel2(int2 *g_data) {
+  // write data to global memory
+  const unsigned int tid = threadIdx.x;
+  int2 data = g_data[tid];
+
+  // use integer arithmetic to process all four bytes with one thread
+  // this serializes the execution, but is the simplest solutions to avoid
+  // bank conflicts for this very low number of threads
+  // in general it is more efficient to process each byte by a separate thread,
+  // to avoid bank conflicts the access pattern should be
+  // g_data[4 * wtid + wid], where wtid is the thread id within the half warp
+  // and wid is the warp id
+  // see also the programming guide for a more in depth discussion.
+  g_data[tid].x = data.x - data.y;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Entry point for Cuda functionality on host side
+//! @param argc  command line argument count
+//! @param argv  command line arguments
+//! @param data  data to process on the device
+//! @param len   len of \a data
+////////////////////////////////////////////////////////////////////////////////
+extern "C" bool runTest(const int argc, const char **argv, char *data,
+                        int2 *data_int2, unsigned int len) {
+  // use command-line specified CUDA device, otherwise use device with highest
+  // Gflops/s
+  findCudaDevice(argc, (const char **)argv);
+
+  const unsigned int num_threads = len / 4;
+  assert(0 == (len % 4));
+  const unsigned int mem_size = sizeof(char) * len;
+  const unsigned int mem_size_int2 = sizeof(int2) * len;
+
+  // allocate device memory
+  char *d_data;
+  HIPCHECK(hipMalloc((void **)&d_data, mem_size));
+  // copy host memory to device
+  HIPCHECK(hipMemcpy(d_data, data, mem_size, hipMemcpyHostToDevice));
+  // allocate device memory for int2 version
+  int2 *d_data_int2;
+  HIPCHECK(hipMalloc((void **)&d_data_int2, mem_size_int2));
+  // copy host memory to device
+  HIPCHECK(hipMemcpy(d_data_int2, data_int2, mem_size_int2,
+                             hipMemcpyHostToDevice));
+
+  // setup execution parameters
+  dim3 grid(1, 1, 1);
+  dim3 threads(num_threads, 1, 1);
+  dim3 threads2(len, 1, 1);  // more threads needed fir separate int2 version
+  // execute the kernel
+  kernel<<<grid, threads>>>((int *)d_data);
+  kernel2<<<grid, threads2>>>(d_data_int2);
+
+  // check if kernel execution generated and error
+  getLastCudaError("Kernel execution failed");
+
+  // compute reference solutions
+  char *reference = (char *)malloc(mem_size);
+  computeGold(reference, data, len);
+  int2 *reference2 = (int2 *)malloc(mem_size_int2);
+  computeGold2(reference2, data_int2, len);
+
+  // copy results from device to host
+  HIPCHECK(hipMemcpy(data, d_data, mem_size, hipMemcpyDeviceToHost));
+  HIPCHECK(hipMemcpy(data_int2, d_data_int2, mem_size_int2,
+                             hipMemcpyDeviceToHost));
+
+  // check result
+  bool success = true;
+
+  for (unsigned int i = 0; i < len; i++) {
+    if (reference[i] != data[i] || reference2[i].x != data_int2[i].x ||
+        reference2[i].y != data_int2[i].y) {
+      success = false;
+    }
+  }
+
+  // cleanup memory
+  HIPCHECK(hipFree(d_data));
+  HIPCHECK(hipFree(d_data_int2));
+  free(reference);
+  free(reference2);
+
+  return success;
+}
diff --git a/src/samples/Samples/0_Introduction/cudaOpenMP/cudaOpenMP.cu.hip b/src/samples/Samples/0_Introduction/cudaOpenMP/cudaOpenMP.cu.hip
index 7f4d166..1310094 100755
--- a/src/samples/Samples/0_Introduction/cudaOpenMP/cudaOpenMP.cu.hip
+++ b/src/samples/Samples/0_Introduction/cudaOpenMP/cudaOpenMP.cu.hip
@@ -34,7 +34,7 @@
 #include "helper_cuda_hipified.h"
 #include <omp.h>
 #include <stdio.h>  // stdio functions are used since C++ streams aren't necessarily thread safe
+#include "HIPCHECK.h"
+
 using namespace std;
 
 // a simple kernel that simply increments each array element by b
diff --git a/src/samples/Samples/0_Introduction/matrixMul/matrixMul.out b/src/samples/Samples/0_Introduction/matrixMul/matrixMul.out
index 7ee5535..ee7127c 100755
Binary files a/src/samples/Samples/0_Introduction/matrixMul/matrixMul.out and b/src/samples/Samples/0_Introduction/matrixMul/matrixMul.out differ
diff --git a/src/samples/Samples/0_Introduction/matrixMul_nvrtc/matrixMul_kernel.cu.hip b/src/samples/Samples/0_Introduction/matrixMul_nvrtc/matrixMul_kernel.cu.hip
index e69de29..b365b74 100755
--- a/src/samples/Samples/0_Introduction/matrixMul_nvrtc/matrixMul_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/matrixMul_nvrtc/matrixMul_kernel.cu.hip
@@ -0,0 +1,132 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/**
+ * Matrix multiplication: C = A * B.
+ * Host code.
+ *
+ * This sample implements matrix multiplication as described in Chapter 3
+ * of the programming guide.
+ * It has been written for clarity of exposition to illustrate various CUDA
+ * programming principles, not with the goal of providing the most
+ * performant generic kernel for matrix multiplication.
+ *
+ * See also:
+ * V. Volkov and J. Demmel, "Benchmarking GPUs to tune dense linear algebra,"
+ * in Proc. 2008 ACM/IEEE Conf. on Supercomputing (SC '08),
+ * Piscataway, NJ: IEEE Press, 2008, pp. Art. 31:1-11.
+ */
+
+/**
+ * Matrix multiplication (CUDA Kernel) on the device: C = A * B
+ * wA is A's width and wB is B's width
+ */
+
+#include <hip/hip_cooperative_groups.h>
+
+template <int BLOCK_SIZE>
+__device__ void matrixMulCUDA(float *C, float *A, float *B, int wA, int wB) {
+  // Handle to thread block group
+  cooperative_groups::thread_block cta =
+      cooperative_groups::this_thread_block();
+  // Block index
+  int bx = blockIdx.x;
+  int by = blockIdx.y;
+
+  // Thread index
+  int tx = threadIdx.x;
+  int ty = threadIdx.y;
+
+  // Index of the first sub-matrix of A processed by the block
+  int aBegin = wA * BLOCK_SIZE * by;
+
+  // Index of the last sub-matrix of A processed by the block
+  int aEnd = aBegin + wA - 1;
+
+  // Step size used to iterate through the sub-matrices of A
+  int aStep = BLOCK_SIZE;
+
+  // Index of the first sub-matrix of B processed by the block
+  int bBegin = BLOCK_SIZE * bx;
+
+  // Step size used to iterate through the sub-matrices of B
+  int bStep = BLOCK_SIZE * wB;
+
+  // Csub is used to store the element of the block sub-matrix
+  // that is computed by the thread
+  float Csub = 0;
+
+  // Loop over all the sub-matrices of A and B
+  // required to compute the block sub-matrix
+  for (int a = aBegin, b = bBegin; a <= aEnd; a += aStep, b += bStep) {
+    // Declaration of the shared memory array As used to
+    // store the sub-matrix of A
+    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
+
+    // Declaration of the shared memory array Bs used to
+    // store the sub-matrix of B
+    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];
+
+    // Load the matrices from device memory
+    // to shared memory; each thread loads
+    // one element of each matrix
+    As[ty][tx] = A[a + wA * ty + tx];
+    Bs[ty][tx] = B[b + wB * ty + tx];
+
+    // Synchronize to make sure the matrices are loaded
+    cooperative_groups::sync(cta);
+
+// Multiply the two matrices together;
+// each thread computes one element
+// of the block sub-matrix
+#pragma unroll
+    for (int k = 0; k < BLOCK_SIZE; ++k) {
+      Csub += As[ty][k] * Bs[k][tx];
+    }
+
+    // Synchronize to make sure that the preceding
+    // computation is done before loading two new
+    // sub-matrices of A and B in the next iteration
+    cooperative_groups::sync(cta);
+  }
+
+  // Write the block sub-matrix to device memory;
+  // each thread writes one element
+  int c = wB * BLOCK_SIZE * by + BLOCK_SIZE * bx;
+  C[c + wB * ty + tx] = Csub;
+}
+
+extern "C" __global__ void matrixMulCUDA_block16(float *C, float *A, float *B,
+                                                 int wA, int wB) {
+  matrixMulCUDA<16>(C, A, B, wA, wB);
+}
+
+extern "C" __global__ void matrixMulCUDA_block32(float *C, float *A, float *B,
+                                                 int wA, int wB) {
+  matrixMulCUDA<32>(C, A, B, wA, wB);
+}
diff --git a/src/samples/Samples/0_Introduction/mergeSort/bitonic.cu.hip b/src/samples/Samples/0_Introduction/mergeSort/bitonic.cu.hip
index e69de29..fc01b30 100755
--- a/src/samples/Samples/0_Introduction/mergeSort/bitonic.cu.hip
+++ b/src/samples/Samples/0_Introduction/mergeSort/bitonic.cu.hip
@@ -0,0 +1,279 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <hip/hip_cooperative_groups.h>
+
+namespace cg = cooperative_groups;
+#include "helper_cuda_hipified.h"
+#include <assert.h>
+#include "mergeSort_common.h"
+
+inline __device__ void Comparator(uint &keyA, uint &valA, uint &keyB,
+                                  uint &valB, uint arrowDir) {
+  uint t;
+
+  if ((keyA > keyB) == arrowDir) {
+    t = keyA;
+    keyA = keyB;
+    keyB = t;
+    t = valA;
+    valA = valB;
+    valB = t;
+  }
+}
+
+__global__ void bitonicSortSharedKernel(uint *d_DstKey, uint *d_DstVal,
+                                        uint *d_SrcKey, uint *d_SrcVal,
+                                        uint arrayLength, uint sortDir) {
+  // Handle to thread block group
+  cg::thread_block cta = cg::this_thread_block();
+  // Shared memory storage for one or more short vectors
+  __shared__ uint s_key[SHARED_SIZE_LIMIT];
+  __shared__ uint s_val[SHARED_SIZE_LIMIT];
+
+  // Offset to the beginning of subbatch and load data
+  d_SrcKey += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
+  d_SrcVal += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
+  d_DstKey += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
+  d_DstVal += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
+  s_key[threadIdx.x + 0] = d_SrcKey[0];
+  s_val[threadIdx.x + 0] = d_SrcVal[0];
+  s_key[threadIdx.x + (SHARED_SIZE_LIMIT / 2)] =
+      d_SrcKey[(SHARED_SIZE_LIMIT / 2)];
+  s_val[threadIdx.x + (SHARED_SIZE_LIMIT / 2)] =
+      d_SrcVal[(SHARED_SIZE_LIMIT / 2)];
+
+  for (uint size = 2; size < arrayLength; size <<= 1) {
+    // Bitonic merge
+    uint dir = (threadIdx.x & (size / 2)) != 0;
+
+    for (uint stride = size / 2; stride > 0; stride >>= 1) {
+      cg::sync(cta);
+      uint pos = 2 * threadIdx.x - (threadIdx.x & (stride - 1));
+      Comparator(s_key[pos + 0], s_val[pos + 0], s_key[pos + stride],
+                 s_val[pos + stride], dir);
+    }
+  }
+
+  // ddd == sortDir for the last bitonic merge step
+  {
+    for (uint stride = arrayLength / 2; stride > 0; stride >>= 1) {
+      cg::sync(cta);
+      uint pos = 2 * threadIdx.x - (threadIdx.x & (stride - 1));
+      Comparator(s_key[pos + 0], s_val[pos + 0], s_key[pos + stride],
+                 s_val[pos + stride], sortDir);
+    }
+  }
+
+  cg::sync(cta);
+  d_DstKey[0] = s_key[threadIdx.x + 0];
+  d_DstVal[0] = s_val[threadIdx.x + 0];
+  d_DstKey[(SHARED_SIZE_LIMIT / 2)] =
+      s_key[threadIdx.x + (SHARED_SIZE_LIMIT / 2)];
+  d_DstVal[(SHARED_SIZE_LIMIT / 2)] =
+      s_val[threadIdx.x + (SHARED_SIZE_LIMIT / 2)];
+}
+
+// Helper function (also used by odd-even merge sort)
+extern "C" uint factorRadix2(uint *log2L, uint L) {
+  if (!L) {
+    *log2L = 0;
+    return 0;
+  }
+ else {
+    for (*log2L = 0; (L & 1) == 0; L >>= 1, *log2L++) ;
+    return L;
+  }
+}
+
+extern "C" void bitonicSortShared(uint *d_DstKey, uint *d_DstVal,
+                                  uint *d_SrcKey, uint *d_SrcVal,
+                                  uint batchSize, uint arrayLength,
+                                  uint sortDir) {
+  // Nothing to sort
+  if (arrayLength < 2) {
+    return;
+  }
+
+  // Only power-of-two array lengths are supported by this implementation
+  uint log2L;
+  uint factorizationRemainder = factorRadix2(&log2L, arrayLength);
+  assert(factorizationRemainder == 1);
+
+  uint blockCount = batchSize * arrayLength / SHARED_SIZE_LIMIT;
+  uint threadCount = SHARED_SIZE_LIMIT / 2;
+
+  assert(arrayLength <= SHARED_SIZE_LIMIT);
+  assert((batchSize * arrayLength) % SHARED_SIZE_LIMIT == 0);
+
+  bitonicSortSharedKernel<<<blockCount, threadCount>>>(
+      d_DstKey, d_DstVal, d_SrcKey, d_SrcVal, arrayLength, sortDir);
+  getLastCudaError("bitonicSortSharedKernel<<<>>> failed!\n");
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Merge step 3: merge elementary intervals
+////////////////////////////////////////////////////////////////////////////////
+static inline __host__ __device__ uint iDivUp(uint a, uint b) {
+  return ((a % b) == 0) ? (a / b) : (a / b + 1);
+}
+
+static inline __host__ __device__ uint getSampleCount(uint dividend) {
+  return iDivUp(dividend, SAMPLE_STRIDE);
+}
+
+template <uint sortDir>
+static inline __device__ void ComparatorExtended(uint &keyA, uint &valA,
+                                                 uint &flagA, uint &keyB,
+                                                 uint &valB, uint &flagB,
+                                                 uint arrowDir) {
+  uint t;
+
+  if ((!(flagA || flagB) && ((keyA > keyB) == arrowDir)) ||
+      ((arrowDir == sortDir) && (flagA == 1)) ||
+      ((arrowDir != sortDir) && (flagB == 1))) {
+    t = keyA;
+    keyA = keyB;
+    keyB = t;
+    t = valA;
+    valA = valB;
+    valB = t;
+    t = flagA;
+    flagA = flagB;
+    flagB = t;
+  }
+}
+
+template <uint sortDir>
+__global__ void bitonicMergeElementaryIntervalsKernel(
+    uint *d_DstKey, uint *d_DstVal, uint *d_SrcKey, uint *d_SrcVal,
+    uint *d_LimitsA, uint *d_LimitsB, uint stride, uint N) {
+  // Handle to thread block group
+  cg::thread_block cta = cg::this_thread_block();
+  __shared__ uint s_key[2 * SAMPLE_STRIDE];
+  __shared__ uint s_val[2 * SAMPLE_STRIDE];
+  __shared__ uint s_inf[2 * SAMPLE_STRIDE];
+
+  const uint intervalI = blockIdx.x & ((2 * stride) / SAMPLE_STRIDE - 1);
+  const uint segmentBase = (blockIdx.x - intervalI) * SAMPLE_STRIDE;
+  d_SrcKey += segmentBase;
+  d_SrcVal += segmentBase;
+  d_DstKey += segmentBase;
+  d_DstVal += segmentBase;
+
+  // Set up threadblock-wide parameters
+  __shared__ uint startSrcA, lenSrcA, startSrcB, lenSrcB, startDst;
+
+  if (threadIdx.x == 0) {
+    uint segmentElementsA = stride;
+    uint segmentElementsB =min(stride, N - segmentBase - stride);
+    uint segmentSamplesA = stride / SAMPLE_STRIDE;
+    uint segmentSamplesB = getSampleCount(segmentElementsB);
+    uint segmentSamples = segmentSamplesA + segmentSamplesB;
+
+    startSrcA = d_LimitsA[blockIdx.x];
+    startSrcB = d_LimitsB[blockIdx.x];
+    startDst = startSrcA + startSrcB;
+
+    uint endSrcA = (intervalI + 1 < segmentSamples) ? d_LimitsA[blockIdx.x + 1]
+                                                    : segmentElementsA;
+    uint endSrcB = (intervalI + 1 < segmentSamples) ? d_LimitsB[blockIdx.x + 1]
+                                                    : segmentElementsB;
+    lenSrcA = endSrcA - startSrcA;
+    lenSrcB = endSrcB - startSrcB;
+  }
+
+  s_inf[threadIdx.x + 0] = 1;
+  s_inf[threadIdx.x + SAMPLE_STRIDE] = 1;
+
+  // Load input data
+  cg::sync(cta);
+
+  if (threadIdx.x < lenSrcA) {
+    s_key[threadIdx.x] = d_SrcKey[0 + startSrcA + threadIdx.x];
+    s_val[threadIdx.x] = d_SrcVal[0 + startSrcA + threadIdx.x];
+    s_inf[threadIdx.x] = 0;
+  }
+
+  // Prepare for bitonic merge by inversing the ordering
+  if (threadIdx.x < lenSrcB) {
+    s_key[2 * SAMPLE_STRIDE - 1 - threadIdx.x] =
+        d_SrcKey[stride + startSrcB + threadIdx.x];
+    s_val[2 * SAMPLE_STRIDE - 1 - threadIdx.x] =
+        d_SrcVal[stride + startSrcB + threadIdx.x];
+    s_inf[2 * SAMPLE_STRIDE - 1 - threadIdx.x] = 0;
+  }
+
+  //"Extended" bitonic merge
+  for (uint stride = SAMPLE_STRIDE; stride > 0; stride >>= 1) {
+    cg::sync(cta);
+    uint pos = 2 * threadIdx.x - (threadIdx.x & (stride - 1));
+    ComparatorExtended<sortDir>(s_key[pos + 0], s_val[pos + 0], s_inf[pos + 0],
+                                s_key[pos + stride], s_val[pos + stride],
+                                s_inf[pos + stride], sortDir);
+  }
+
+  // Store sorted data
+  cg::sync(cta);
+  d_DstKey += startDst;
+  d_DstVal += startDst;
+
+  if (threadIdx.x < lenSrcA) {
+    d_DstKey[threadIdx.x] = s_key[threadIdx.x];
+    d_DstVal[threadIdx.x] = s_val[threadIdx.x];
+  }
+
+  if (threadIdx.x < lenSrcB) {
+    d_DstKey[lenSrcA + threadIdx.x] = s_key[lenSrcA + threadIdx.x];
+    d_DstVal[lenSrcA + threadIdx.x] = s_val[lenSrcA + threadIdx.x];
+  }
+}
+
+extern "C" void bitonicMergeElementaryIntervals(uint *d_DstKey, uint *d_DstVal,
+                                                uint *d_SrcKey, uint *d_SrcVal,
+                                                uint *d_LimitsA,
+                                                uint *d_LimitsB, uint stride,
+                                                uint N, uint sortDir) {
+  uint lastSegmentElements = N % (2 * stride);
+
+  uint mergePairs = (lastSegmentElements > stride)
+                        ? getSampleCount(N)
+                        : (N - lastSegmentElements) / SAMPLE_STRIDE;
+
+  if (sortDir) {
+    bitonicMergeElementaryIntervalsKernel<1U><<<mergePairs, SAMPLE_STRIDE>>>(
+        d_DstKey, d_DstVal, d_SrcKey, d_SrcVal, d_LimitsA, d_LimitsB, stride,
+        N);
+    getLastCudaError("mergeElementaryIntervalsKernel<1> failed\n");
+  } else {
+    bitonicMergeElementaryIntervalsKernel<0U><<<mergePairs, SAMPLE_STRIDE>>>(
+        d_DstKey, d_DstVal, d_SrcKey, d_SrcVal, d_LimitsA, d_LimitsB, stride,
+        N);
+    getLastCudaError("mergeElementaryIntervalsKernel<0> failed\n");
+  }
+}
diff --git a/src/samples/Samples/0_Introduction/mergeSort/mergeSort.cu.hip b/src/samples/Samples/0_Introduction/mergeSort/mergeSort.cu.hip
index 98047f5..c7702c0 100755
--- a/src/samples/Samples/0_Introduction/mergeSort/mergeSort.cu.hip
+++ b/src/samples/Samples/0_Introduction/mergeSort/mergeSort.cu.hip
@@ -36,7 +36,7 @@
 
 #include <assert.h>
 #include <hip/hip_cooperative_groups.h>
-#inlcude "HIPCHECK.h"
+
 namespace cg = cooperative_groups;
 
 #include "helper_cuda_hipified.h"
diff --git a/src/samples/Samples/0_Introduction/simpleAWBarrier/simpleAWBarrier.cu.hip b/src/samples/Samples/0_Introduction/simpleAWBarrier/simpleAWBarrier.cu.hip
index bc7610f..d71a5b0 100755
--- a/src/samples/Samples/0_Introduction/simpleAWBarrier/simpleAWBarrier.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleAWBarrier/simpleAWBarrier.cu.hip
@@ -145,7 +145,7 @@ int main(int argc, char **argv) {
   int dev = findCudaDevice(argc, (const char **)argv);
 
   int major = 0;
-  checkCudaErrors(
+  HIPCHECK(
       hipDeviceGetAttribute(&major, hipDeviceAttributeComputeCapabilityMajor, dev));
 
   // Arrive-Wait Barrier require a GPU of Volta (SM7X) architecture or higher.
@@ -155,7 +155,7 @@ int main(int argc, char **argv) {
   }
 
   int supportsCooperativeLaunch = 0;
-  checkCudaErrors(hipDeviceGetAttribute(&supportsCooperativeLaunch,
+  HIPCHECK(hipDeviceGetAttribute(&supportsCooperativeLaunch,
                                          hipDeviceAttributeCooperativeLaunch, dev));
 
   if (!supportsCooperativeLaunch) {
@@ -178,11 +178,11 @@ int runNormVecByDotProductAWBarrier(int argc, char **argv, int deviceId) {
   double *d_partialResults;
   int size = 10000000;
 
-  checkCudaErrors(hipHostMalloc(&vecA, sizeof(float) * size));
-  checkCudaErrors(hipHostMalloc(&vecB, sizeof(float) * size));
+  HIPCHECK(hipHostMalloc(&vecA, sizeof(float) * size));
+  HIPCHECK(hipHostMalloc(&vecB, sizeof(float) * size));
 
-  checkCudaErrors(hipMalloc(&d_vecA, sizeof(float) * size));
-  checkCudaErrors(hipMalloc(&d_vecB, sizeof(float) * size));
+  HIPCHECK(hipMalloc(&d_vecA, sizeof(float) * size));
+  HIPCHECK(hipMalloc(&d_vecB, sizeof(float) * size));
 
   float baseVal = 2.0;
   for (int i = 0; i < size; i++) {
@@ -190,31 +190,31 @@ int runNormVecByDotProductAWBarrier(int argc, char **argv, int deviceId) {
   }
 
   hipStream_t stream;
-  checkCudaErrors(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
+  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
 
-  checkCudaErrors(hipMemcpyAsync(d_vecA, vecA, sizeof(float) * size,
+  HIPCHECK(hipMemcpyAsync(d_vecA, vecA, sizeof(float) * size,
                                   hipMemcpyHostToDevice, stream));
-  checkCudaErrors(hipMemcpyAsync(d_vecB, vecB, sizeof(float) * size,
+  HIPCHECK(hipMemcpyAsync(d_vecB, vecB, sizeof(float) * size,
                                   hipMemcpyHostToDevice, stream));
 
   // Kernel configuration, where a one-dimensional
   // grid and one-dimensional blocks are configured.
   int minGridSize = 0, blockSize = 0;
-  checkCudaErrors(hipOccupancyMaxPotentialBlockSize(
+  HIPCHECK(hipOccupancyMaxPotentialBlockSize(
       &minGridSize, &blockSize, (void *)normVecByDotProductAWBarrier, 0, size));
 
   int smemSize = ((blockSize / 32) + 1) * sizeof(double);
 
   int numBlocksPerSm = 0;
-  checkCudaErrors(hipOccupancyMaxActiveBlocksPerMultiprocessor(
+  HIPCHECK(hipOccupancyMaxActiveBlocksPerMultiprocessor(
       &numBlocksPerSm, normVecByDotProductAWBarrier, blockSize, smemSize));
 
   int multiProcessorCount = 0;
-  checkCudaErrors(hipDeviceGetAttribute(
+  HIPCHECK(hipDeviceGetAttribute(
       &multiProcessorCount, hipDeviceAttributeMultiprocessorCount, deviceId));
 
   minGridSize = multiProcessorCount * numBlocksPerSm;
-  checkCudaErrors(hipMalloc(&d_partialResults, minGridSize * sizeof(double)));
+  HIPCHECK(hipMalloc(&d_partialResults, minGridSize * sizeof(double)));
 
   printf(
       "Launching normVecByDotProductAWBarrier kernel with numBlocks = %d "
@@ -226,13 +226,13 @@ int runNormVecByDotProductAWBarrier(int argc, char **argv, int deviceId) {
   void *kernelArgs[] = {(void *)&d_vecA, (void *)&d_vecB,
                         (void *)&d_partialResults, (void *)&size};
 
-  checkCudaErrors(
+  HIPCHECK(
       hipLaunchCooperativeKernel((void *)normVecByDotProductAWBarrier, dimGrid,
                                   dimBlock, kernelArgs, smemSize, stream));
 
-  checkCudaErrors(hipMemcpyAsync(vecA, d_vecA, sizeof(float) * size,
+  HIPCHECK(hipMemcpyAsync(vecA, d_vecA, sizeof(float) * size,
                                   hipMemcpyDeviceToHost, stream));
-  checkCudaErrors(hipStreamSynchronize(stream));
+  HIPCHECK(hipStreamSynchronize(stream));
 
   float expectedResult = (baseVal / sqrt(size * baseVal * baseVal));
   unsigned int matches = 0;
@@ -246,9 +246,15 @@ int runNormVecByDotProductAWBarrier(int argc, char **argv, int deviceId) {
   }
 
   printf("Result = %s\n", matches == size ? "PASSED" : "FAILED");
-  checkCudaErrors(hipFree(d_vecA));
-  checkCudaErrors(hipFree(d_vecB));
-  checkCudaErrors(hipFree(d_partialResults));
+  HIPCHECK(hipFree(d_vecA));
+  HIPCHECK(hipFree(d_vecB));
+  HIPCHECK(hipFree(d_partialResults));
+
+  HIPCHECK(hipHostFree(vecA));
+  HIPCHECK(hipHostFree(vecB));
+  return matches == size;
+}
+eckCudaErrors(hipFree(d_partialResults));
 
   checkCudaErrors(hipHostFree(vecA));
   checkCudaErrors(hipHostFree(vecB));
diff --git a/src/samples/Samples/0_Introduction/simpleAssert/simpleAssert.cu.hip b/src/samples/Samples/0_Introduction/simpleAssert/simpleAssert.cu.hip
index 91432f5..cb3a67b 100755
--- a/src/samples/Samples/0_Introduction/simpleAssert/simpleAssert.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleAssert/simpleAssert.cu.hip
@@ -36,6 +36,8 @@
 
 // Includes, system
 #include <stdio.h>
+#include "rocprofiler.h"
+#include "HIPCHECK.h"
 #include <cassert>
 
 // Includes CUDA
diff --git a/src/samples/Samples/0_Introduction/simpleAssert/simpleAssert.out b/src/samples/Samples/0_Introduction/simpleAssert/simpleAssert.out
index fb58ac5..479bdeb 100755
Binary files a/src/samples/Samples/0_Introduction/simpleAssert/simpleAssert.out and b/src/samples/Samples/0_Introduction/simpleAssert/simpleAssert.out differ
diff --git a/src/samples/Samples/0_Introduction/simpleAtomicIntrinsics/simpleAtomicIntrinsics.out b/src/samples/Samples/0_Introduction/simpleAtomicIntrinsics/simpleAtomicIntrinsics.out
index 58b791f..64df673 100755
Binary files a/src/samples/Samples/0_Introduction/simpleAtomicIntrinsics/simpleAtomicIntrinsics.out and b/src/samples/Samples/0_Introduction/simpleAtomicIntrinsics/simpleAtomicIntrinsics.out differ
diff --git a/src/samples/Samples/0_Introduction/simpleCUDA2GL/simpleCUDA2GL.cu.hip b/src/samples/Samples/0_Introduction/simpleCUDA2GL/simpleCUDA2GL.cu.hip
index e69de29..7f0c62c 100755
--- a/src/samples/Samples/0_Introduction/simpleCUDA2GL/simpleCUDA2GL.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleCUDA2GL/simpleCUDA2GL.cu.hip
@@ -0,0 +1,63 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// Utilities and system includes
+
+#include "helper_cuda_hipified.h"
+
+// clamp x to range [a, b]
+__device__ float clamp(float x, float a, float b) { return max(a, min(b, x)); }
+
+__device__ int clamp(int x, int a, int b) { return max(a, min(b, x)); }
+
+// convert floating point rgb color to 8-bit integer
+__device__ int rgbToInt(float r, float g, float b) {
+  r = clamp(r, 0.0f, 255.0f);
+  g = clamp(g, 0.0f, 255.0f);
+  b = clamp(b, 0.0f, 255.0f);
+  return (int(b) << 16) | (int(g) << 8) | int(r);
+}
+
+__global__ void cudaProcess(unsigned int *g_odata, int imgw) {
+  extern __shared__ uchar4 sdata[];
+
+  int tx = threadIdx.x;
+  int ty = threadIdx.y;
+  int bw = blockDim.x;
+  int bh = blockDim.y;
+  int x = blockIdx.x * bw + tx;
+  int y = blockIdx.y * bh + ty;
+
+  uchar4 c4 = make_uchar4((x & 0x20) ? 100 : 0, 0, (y & 0x20) ? 100 : 0, 0);
+  g_odata[y * imgw + x] = rgbToInt(c4.z, c4.y, c4.x);
+}
+
+extern "C" void launch_cudaProcess(dim3 grid, dim3 block, int sbytes,
+                                   unsigned int *g_odata, int imgw) {
+  cudaProcess<<<grid, block, sbytes>>>(g_odata, imgw);
+}
diff --git a/src/samples/Samples/0_Introduction/simpleCallback/simpleCallback.cu.hip b/src/samples/Samples/0_Introduction/simpleCallback/simpleCallback.cu.hip
index 3180829..cc39628 100755
--- a/src/samples/Samples/0_Introduction/simpleCallback/simpleCallback.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleCallback/simpleCallback.cu.hip
@@ -142,8 +142,7 @@ CUT_THREADPROC postprocess(void *void_arg) {
   CUT_THREADEND;
 }
 
-void myStreamCallback(hipStream_t stream, hipError_t status,
-                                void *data) {
+void (CUDART_CB* myStreamCallback)(hipStream_t stream, hipError_t status, void *data) {
   // Check status of GPU after stream operations are done
   HIPCHECK(status);
 
diff --git a/src/samples/Samples/0_Introduction/simpleDrvRuntime/vectorAdd_kernel.cu.hip b/src/samples/Samples/0_Introduction/simpleDrvRuntime/vectorAdd_kernel.cu.hip
index e69de29..72e20f6 100755
--- a/src/samples/Samples/0_Introduction/simpleDrvRuntime/vectorAdd_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleDrvRuntime/vectorAdd_kernel.cu.hip
@@ -0,0 +1,43 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* Vector addition: C = A + B.
+ *
+ * This sample is a very basic sample that implements element by element
+ * vector addition. It is the same as the sample illustrating Chapter 3
+ * of the programming guide with some additions like error checking.
+ *
+ */
+
+// Device code
+extern "C" __global__ void VecAdd_kernel(const float *A, const float *B,
+                                         float *C, int N) {
+  int i = blockDim.x * blockIdx.x + threadIdx.x;
+
+  if (i < N) C[i] = A[i] + B[i];
+}
diff --git a/src/samples/Samples/0_Introduction/simpleHyperQ/simpleHyperQ.out b/src/samples/Samples/0_Introduction/simpleHyperQ/simpleHyperQ.out
index cdde666..c98a8c6 100755
Binary files a/src/samples/Samples/0_Introduction/simpleHyperQ/simpleHyperQ.out and b/src/samples/Samples/0_Introduction/simpleHyperQ/simpleHyperQ.out differ
diff --git a/src/samples/Samples/0_Introduction/simpleLayeredTexture/simpleLayeredTexture.out b/src/samples/Samples/0_Introduction/simpleLayeredTexture/simpleLayeredTexture.out
index 484e31c..596cd6a 100755
Binary files a/src/samples/Samples/0_Introduction/simpleLayeredTexture/simpleLayeredTexture.out and b/src/samples/Samples/0_Introduction/simpleLayeredTexture/simpleLayeredTexture.out differ
diff --git a/src/samples/Samples/0_Introduction/simpleMultiCopy/simpleMultiCopy.cu.hip b/src/samples/Samples/0_Introduction/simpleMultiCopy/simpleMultiCopy.cu.hip
index c35b52a..1a1290a 100755
--- a/src/samples/Samples/0_Introduction/simpleMultiCopy/simpleMultiCopy.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleMultiCopy/simpleMultiCopy.cu.hip
@@ -1,4 +1,3 @@
-
 /* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -130,12 +129,12 @@ int main(int argc, char *argv[]) {
   } else {
     // Otherwise pick the device with the highest Gflops/s
     cuda_device = gpuGetMaxGflopsDeviceId();
-    checkCudaErrors(hipSetDevice(cuda_device));
-    checkCudaErrors(hipGetDeviceProperties(&deviceProp, cuda_device));
+    HIPCHECK(hipSetDevice(cuda_device));
+    HIPCHECK(hipGetDeviceProperties(&deviceProp, cuda_device));
     printf("> Using CUDA device [%d]: %s\n", cuda_device, deviceProp.name);
   }
 
-  checkCudaErrors(hipGetDeviceProperties(&deviceProp, cuda_device));
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, cuda_device));
   printf("[%s] has %d MP(s) x %d (Cores/MP) = %d (Cores)\n", deviceProp.name,
          deviceProp.multiProcessorCount,
          _ConvertSMVer2Cores(deviceProp.major, deviceProp.minor),
@@ -193,7 +192,7 @@ int main(int argc, char *argv[]) {
 
   // Time copies and kernel
   hipEventRecord(start, 0);
-  checkCudaErrors(hipMemcpyAsync(d_data_in[0], h_data_in[0], memsize,
+  HIPCHECK(hipMemcpyAsync(d_data_in[0], h_data_in[0], memsize,
                                   hipMemcpyHostToDevice, 0));
   hipEventRecord(stop, 0);
   hipEventSynchronize(stop);
@@ -202,7 +201,7 @@ int main(int argc, char *argv[]) {
   hipEventElapsedTime(&memcpy_h2d_time, start, stop);
 
   hipEventRecord(start, 0);
-  checkCudaErrors(hipMemcpyAsync(h_data_out[0], d_data_out[0], memsize,
+  HIPCHECK(hipMemcpyAsync(h_data_out[0], d_data_out[0], memsize,
                                   hipMemcpyDeviceToHost, 0));
   hipEventRecord(stop, 0);
   hipEventSynchronize(stop);
@@ -220,18 +219,20 @@ int main(int argc, char *argv[]) {
 
   printf("\n");
   printf("Relevant properties of this CUDA device\n");
+  /*
   printf(
       "(%s) Can overlap one CPU<>GPU data transfer with GPU kernel execution "
       "(device property \"deviceOverlap\")\n",
       deviceProp.deviceOverlap ? "X" : " ");
+*/
   // printf("(%s) Can execute several GPU kernels simultaneously (compute
   // capability >= 2.0)\n", deviceProp.major >= 2 ? "X": " ");
-  printf(
+ /* printf(
       "(%s) Can overlap two CPU<>GPU data transfers with GPU kernel execution\n"
       "    (Compute Capability >= 2.0 AND (Tesla product OR Quadro "
       "4000/5000/6000/K5000)\n",
       (deviceProp.major >= 2 && deviceProp.asyncEngineCount > 1) ? "X" : " ");
-
+*/
   printf("\n");
   printf("Measured timings (throughput):\n");
   printf(" Memcpy host to device\t: %f ms (%f GB/s)\n", memcpy_h2d_time,
@@ -329,16 +330,16 @@ float processWithStreams(int streams_used) {
         d_data_out[current_stream], d_data_in[current_stream], N, inner_reps);
 
     // Upload next frame
-    checkCudaErrors(
+    HIPCHECK(
         hipMemcpyAsync(d_data_in[next_stream], h_data_in[next_stream], memsize,
                         hipMemcpyHostToDevice, stream[next_stream]));
 
     // Download current frame
-    checkCudaErrors(hipMemcpyAsync(
+    HIPCHECK(hipMemcpyAsync(
         h_data_out[current_stream], d_data_out[current_stream], memsize,
         hipMemcpyDeviceToHost, stream[current_stream]));
 
-    checkCudaErrors(
+    HIPCHECK(
         hipEventRecord(cycleDone[current_stream], stream[current_stream]));
 
     current_stream = next_stream;
diff --git a/src/samples/Samples/0_Introduction/simpleMultiGPU/simpleMultiGPU.out b/src/samples/Samples/0_Introduction/simpleMultiGPU/simpleMultiGPU.out
deleted file mode 100755
index 62d020b..0000000
Binary files a/src/samples/Samples/0_Introduction/simpleMultiGPU/simpleMultiGPU.out and /dev/null differ
diff --git a/src/samples/Samples/0_Introduction/simpleOccupancy/simpleOccupancy.cu.hip b/src/samples/Samples/0_Introduction/simpleOccupancy/simpleOccupancy.cu.hip
index 867d277..b44e5fc 100755
--- a/src/samples/Samples/0_Introduction/simpleOccupancy/simpleOccupancy.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleOccupancy/simpleOccupancy.cu.hip
@@ -28,7 +28,7 @@
 
 #include <iostream>
 #include <helper_cuda.h>  // helper functions for CUDA error check
-#include "HIPCHECK.h"
+
 const int manualBlockSize = 32;
 
 ////////////////////////////////////////////////////////////////////////////////
diff --git a/src/samples/Samples/0_Introduction/simpleP2P/simpleP2P.out b/src/samples/Samples/0_Introduction/simpleP2P/simpleP2P.out
index 32121a5..b8f8622 100755
Binary files a/src/samples/Samples/0_Introduction/simpleP2P/simpleP2P.out and b/src/samples/Samples/0_Introduction/simpleP2P/simpleP2P.out differ
diff --git a/src/samples/Samples/0_Introduction/simpleSeparateCompilation/simpleSeparateCompilation.cu.hip b/src/samples/Samples/0_Introduction/simpleSeparateCompilation/simpleSeparateCompilation.cu.hip
index 3f01638..f800259 100755
--- a/src/samples/Samples/0_Introduction/simpleSeparateCompilation/simpleSeparateCompilation.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleSeparateCompilation/simpleSeparateCompilation.cu.hip
@@ -30,7 +30,7 @@
 #include <stdio.h>
 //#include "rocprofiler.h"
 #include <iostream>
+#include "HIPCHECK.h"
+
 // STL.
 #include <vector>
 
diff --git a/src/samples/Samples/0_Introduction/simpleStreams/simpleStreams.out b/src/samples/Samples/0_Introduction/simpleStreams/simpleStreams.out
index aa0690c..4c1962b 100755
Binary files a/src/samples/Samples/0_Introduction/simpleStreams/simpleStreams.out and b/src/samples/Samples/0_Introduction/simpleStreams/simpleStreams.out differ
diff --git a/src/samples/Samples/0_Introduction/simpleSurfaceWrite/simpleSurfaceWrite.out b/src/samples/Samples/0_Introduction/simpleSurfaceWrite/simpleSurfaceWrite.out
index bc992e7..f1bd3ec 100755
Binary files a/src/samples/Samples/0_Introduction/simpleSurfaceWrite/simpleSurfaceWrite.out and b/src/samples/Samples/0_Introduction/simpleSurfaceWrite/simpleSurfaceWrite.out differ
diff --git a/src/samples/Samples/0_Introduction/simpleTemplates_nvrtc/simpleTemplates_kernel.cu.hip b/src/samples/Samples/0_Introduction/simpleTemplates_nvrtc/simpleTemplates_kernel.cu.hip
index e69de29..89eed5d 100755
--- a/src/samples/Samples/0_Introduction/simpleTemplates_nvrtc/simpleTemplates_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleTemplates_nvrtc/simpleTemplates_kernel.cu.hip
@@ -0,0 +1,71 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// includes, kernels
+#include "sharedmem.cuh"
+
+////////////////////////////////////////////////////////////////////////////////
+//! Simple test kernel for device functionality
+//! @param g_idata  input data in global memory
+//! @param g_odata  output data in global memory
+////////////////////////////////////////////////////////////////////////////////
+
+template <class T>
+__device__ void testKernel(T *g_idata, T *g_odata) {
+  // Shared mem size is determined by the host app at run time
+  SharedMemory<T> smem;
+
+  T *sdata = smem.getPointer();
+
+  // access thread id
+  const unsigned int tid = threadIdx.x;
+
+  // access number of threads in this block
+  const unsigned int num_threads = blockDim.x;
+
+  // read in input data from global memory
+  sdata[tid] = g_idata[tid];
+
+  __syncthreads();
+
+  // perform some computations
+  sdata[tid] = (T)num_threads * sdata[tid];
+
+  __syncthreads();
+
+  // write data to global memory
+  g_odata[tid] = sdata[tid];
+}
+
+extern "C" __global__ void testFloat(float *p1, float *p2) {
+  testKernel<float>(p1, p2);
+}
+
+extern "C" __global__ void testInt(int *p1, int *p2) {
+  testKernel<int>(p1, p2);
+}
diff --git a/src/samples/Samples/0_Introduction/simpleTexture/simpleTexture.out b/src/samples/Samples/0_Introduction/simpleTexture/simpleTexture.out
index 25c6794..61ef9b2 100755
Binary files a/src/samples/Samples/0_Introduction/simpleTexture/simpleTexture.out and b/src/samples/Samples/0_Introduction/simpleTexture/simpleTexture.out differ
diff --git a/src/samples/Samples/0_Introduction/simpleTextureDrv/simpleTexture_kernel.cu.hip b/src/samples/Samples/0_Introduction/simpleTextureDrv/simpleTexture_kernel.cu.hip
index e69de29..d878b25 100755
--- a/src/samples/Samples/0_Introduction/simpleTextureDrv/simpleTexture_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleTextureDrv/simpleTexture_kernel.cu.hip
@@ -0,0 +1,56 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _SIMPLETEXTURE_KERNEL_H_
+#define _SIMPLETEXTURE_KERNEL_H_
+#include <hip/hip_runtime.h>
+
+////////////////////////////////////////////////////////////////////////////////
+//! Transform an image using texture lookups
+//! @param g_odata  output data in global memory
+////////////////////////////////////////////////////////////////////////////////
+extern "C" __global__ void transformKernel(float *g_odata, int width,
+                                           int height, float theta,
+                                           hipTextureObject_t tex) {
+  // calculate normalized texture coordinates
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  float u = (float)x - (float)width / 2;
+  float v = (float)y - (float)height / 2;
+  float tu = u * cosf(theta) - v * sinf(theta);
+  float tv = v * cosf(theta) + u * sinf(theta);
+
+  tu /= (float)width;
+  tv /= (float)height;
+
+  // read from texture and write to global memory
+  g_odata[y * width + x] = tex2D<float>(tex, tu + 0.5f, tv + 0.5f);
+}
+
+#endif  // #ifndef _SIMPLETEXTURE_KERNEL_H_
diff --git a/src/samples/Samples/0_Introduction/systemWideAtomics/systemWideAtomics.cu.hip b/src/samples/Samples/0_Introduction/systemWideAtomics/systemWideAtomics.cu.hip
index d226320..824e287 100755
--- a/src/samples/Samples/0_Introduction/systemWideAtomics/systemWideAtomics.cu.hip
+++ b/src/samples/Samples/0_Introduction/systemWideAtomics/systemWideAtomics.cu.hip
@@ -36,7 +36,7 @@
 #include <stdint.h>
 #include <cstdio>
 #include <ctime>
+#include "HIPCHECK.h"
+
 #define min(a, b) (a) < (b) ? (a) : (b)
 #define max(a, b) (a) > (b) ? (a) : (b)
 
diff --git a/src/samples/Samples/0_Introduction/template/template.out b/src/samples/Samples/0_Introduction/template/template.out
index 5cb4f8a..f6dd5a9 100755
Binary files a/src/samples/Samples/0_Introduction/template/template.out and b/src/samples/Samples/0_Introduction/template/template.out differ
diff --git a/src/samples/Samples/0_Introduction/vectorAddDrv/vectorAdd_kernel.cu.hip b/src/samples/Samples/0_Introduction/vectorAddDrv/vectorAdd_kernel.cu.hip
index e69de29..8c2afb9 100755
--- a/src/samples/Samples/0_Introduction/vectorAddDrv/vectorAdd_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/vectorAddDrv/vectorAdd_kernel.cu.hip
@@ -0,0 +1,42 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+/* Vector addition: C = A + B.
+ *
+ * This sample is a very basic sample that implements element by element
+ * vector addition. It is the same as the sample illustrating Chapter 3
+ * of the programming guide with some additions like error checking.
+ *
+ */
+
+// Device code
+extern "C" __global__ void VecAdd_kernel(const float *A, const float *B,
+                                         float *C, int N) {
+  int i = blockDim.x * blockIdx.x + threadIdx.x;
+
+  if (i < N) C[i] = A[i] + B[i];
+}
diff --git a/src/samples/Samples/0_Introduction/vectorAddMMAP/vectorAdd_kernel.cu.hip b/src/samples/Samples/0_Introduction/vectorAddMMAP/vectorAdd_kernel.cu.hip
index e69de29..72e20f6 100755
--- a/src/samples/Samples/0_Introduction/vectorAddMMAP/vectorAdd_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/vectorAddMMAP/vectorAdd_kernel.cu.hip
@@ -0,0 +1,43 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* Vector addition: C = A + B.
+ *
+ * This sample is a very basic sample that implements element by element
+ * vector addition. It is the same as the sample illustrating Chapter 3
+ * of the programming guide with some additions like error checking.
+ *
+ */
+
+// Device code
+extern "C" __global__ void VecAdd_kernel(const float *A, const float *B,
+                                         float *C, int N) {
+  int i = blockDim.x * blockIdx.x + threadIdx.x;
+
+  if (i < N) C[i] = A[i] + B[i];
+}
diff --git a/src/samples/Samples/0_Introduction/vectorAdd_nvrtc/vectorAdd_kernel.cu.hip b/src/samples/Samples/0_Introduction/vectorAdd_nvrtc/vectorAdd_kernel.cu.hip
index e69de29..bb459dd 100755
--- a/src/samples/Samples/0_Introduction/vectorAdd_nvrtc/vectorAdd_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/vectorAdd_nvrtc/vectorAdd_kernel.cu.hip
@@ -0,0 +1,43 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/**
+ * CUDA Kernel Device code
+ *
+ * Computes the vector addition of A and B into C. The 3 vectors have the same
+ * number of elements numElements.
+ */
+
+extern "C" __global__ void vectorAdd(const float *A, const float *B, float *C,
+                                     int numElements) {
+  int i = blockDim.x * blockIdx.x + threadIdx.x;
+
+  if (i < numElements) {
+    C[i] = A[i] + B[i];
+  }
+}
diff --git a/src/samples/Samples/1_Utilities/bandwidthTest/bandwidthTest.out b/src/samples/Samples/1_Utilities/bandwidthTest/bandwidthTest.out
index 4e838ee..fea1f94 100755
Binary files a/src/samples/Samples/1_Utilities/bandwidthTest/bandwidthTest.out and b/src/samples/Samples/1_Utilities/bandwidthTest/bandwidthTest.out differ
diff --git a/src/samples/Samples/1_Utilities/topologyQuery/topologyQuery.cu.hip b/src/samples/Samples/1_Utilities/topologyQuery/topologyQuery.cu.hip
index 21c1a8d..47fd6d7 100755
--- a/src/samples/Samples/1_Utilities/topologyQuery/topologyQuery.cu.hip
+++ b/src/samples/Samples/1_Utilities/topologyQuery/topologyQuery.cu.hip
@@ -36,7 +36,7 @@
 // includes, project
 #include "helper_cuda_hipified.h"
 #include <helper_functions.h>  // helper for shared that are common to CUDA Samples
+#include "HIPCHECK.h"
+
 int main(int argc, char **argv) {
   int deviceCount = 0;
   HIPCHECK(hipGetDeviceCount(&deviceCount));
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/FunctionPointers/FunctionPointers_kernels.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/FunctionPointers/FunctionPointers_kernels.cu.hip
index eb0288c..5524cf0 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/FunctionPointers/FunctionPointers_kernels.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/FunctionPointers/FunctionPointers_kernels.cu.hip
@@ -412,4 +412,8 @@ extern "C" void sobelFilter(Pixel *odata, int iw, int ih,
    iw, ih, fScale, blockOperation, pPointOp, tex);
     } break;
   }
+}
+   iw, ih, fScale, blockOperation, pPointOp, tex);
+    } break;
+  }
 }
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiInlineP/src/piestimator.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiInlineP/src/piestimator.cu.hip
index e69de29..c56c703 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiInlineP/src/piestimator.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiInlineP/src/piestimator.cu.hip
@@ -0,0 +1,284 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "../inc/piestimator.h"
+
+#include <string>
+#include <vector>
+#include <numeric>
+#include <stdexcept>
+#include <typeinfo>
+#include <hip/hip_runtime.h>
+#include <hip/hip_cooperative_groups.h>
+
+namespace cg = cooperative_groups;
+#include <hiprand_kernel.h>
+
+using std::string;
+using std::vector;
+
+// RNG init kernel
+__global__ void initRNG(hiprandState *const rngStates, const unsigned int seed) {
+  // Determine thread ID
+  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
+
+  // Initialise the RNG
+  hiprand_init(seed, tid, 0, &rngStates[tid]);
+}
+
+__device__ unsigned int reduce_sum(unsigned int in, cg::thread_block cta) {
+  extern __shared__ unsigned int sdata[];
+
+  // Perform first level of reduction:
+  // - Write to shared memory
+  unsigned int ltid = threadIdx.x;
+
+  sdata[ltid] = in;
+  cg::sync(cta);
+
+  // Do reduction in shared mem
+  for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {
+    if (ltid < s) {
+      sdata[ltid] += sdata[ltid + s];
+    }
+
+    cg::sync(cta);
+  }
+
+  return sdata[0];
+}
+
+__device__ inline void getPoint(float &x, float &y, hiprandState &state) {
+  x = hiprand_uniform(&state);
+  y = hiprand_uniform(&state);
+}
+__device__ inline void getPoint(double &x, double &y, hiprandState &state) {
+  x = hiprand_uniform_double(&state);
+  y = hiprand_uniform_double(&state);
+}
+
+// Estimator kernel
+template <typename Real>
+__global__ void computeValue(unsigned int *const results,
+                             hiprandState *const rngStates,
+                             const unsigned int numSims) {
+  // Handle to thread block group
+  cg::thread_block cta = cg::this_thread_block();
+  // Determine thread ID
+  unsigned int bid = blockIdx.x;
+  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int step = gridDim.x * blockDim.x;
+
+  // Initialise the RNG
+  hiprandState localState = rngStates[tid];
+
+  // Count the number of points which lie inside the unit quarter-circle
+  unsigned int pointsInside = 0;
+
+  for (unsigned int i = tid; i < numSims; i += step) {
+    Real x;
+    Real y;
+    getPoint(x, y, localState);
+    Real l2norm2 = x * x + y * y;
+
+    if (l2norm2 < static_cast<Real>(1)) {
+      pointsInside++;
+    }
+  }
+
+  // Reduce within the block
+  pointsInside = reduce_sum(pointsInside, cta);
+
+  // Store the result
+  if (threadIdx.x == 0) {
+    results[bid] = pointsInside;
+  }
+}
+
+template <typename Real>
+PiEstimator<Real>::PiEstimator(unsigned int numSims, unsigned int device,
+                               unsigned int threadBlockSize, unsigned int seed)
+    : m_numSims(numSims),
+      m_device(device),
+      m_threadBlockSize(threadBlockSize),
+      m_seed(seed) {}
+
+template <typename Real>
+Real PiEstimator<Real>::operator()() {
+  hipError_t cudaResult = hipSuccess;
+  struct hipDeviceProp_t deviceProperties;
+  struct hipFuncAttributes funcAttributes;
+
+  // Get device properties
+  cudaResult = hipGetDeviceProperties(&deviceProperties, m_device);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not get device properties: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Check precision is valid
+  if (typeid(Real) == typeid(double) &&
+      (deviceProperties.major < 1 ||
+       (deviceProperties.major == 1 && deviceProperties.minor < 3))) {
+    throw std::runtime_error("Device does not have double precision support");
+  }
+
+  // Attach to GPU
+  cudaResult = hipSetDevice(m_device);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not set CUDA device: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Determine how to divide the work between cores
+  dim3 block;
+  dim3 grid;
+  block.x = m_threadBlockSize;
+  grid.x = (m_numSims + m_threadBlockSize - 1) / m_threadBlockSize;
+
+  // Aim to launch around ten or more times as many blocks as there
+  // are multiprocessors on the target device.
+  unsigned int blocksPerSM = 10;
+  unsigned int numSMs = deviceProperties.multiProcessorCount;
+
+  while (grid.x > 2 * blocksPerSM * numSMs) {
+    grid.x >>= 1;
+  }
+
+  // Get initRNG function properties and check the maximum block size
+  cudaResult = hipFuncGetAttributes(&funcAttributes, initRNG);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not get function attributes: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  if (block.x > (unsigned int)funcAttributes.maxThreadsPerBlock) {
+    throw std::runtime_error(
+        "Block X dimension is too large for initRNG kernel");
+  }
+
+  // Get computeValue function properties and check the maximum block size
+  cudaResult = hipFuncGetAttributes(&funcAttributes, computeValue<Real>);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not get function attributes: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  if (block.x > (unsigned int)funcAttributes.maxThreadsPerBlock) {
+    throw std::runtime_error(
+        "Block X dimension is too large for computeValue kernel");
+  }
+
+  // Check the dimensions are valid
+  if (block.x > (unsigned int)deviceProperties.maxThreadsDim[0]) {
+    throw std::runtime_error("Block X dimension is too large for device");
+  }
+
+  if (grid.x > (unsigned int)deviceProperties.maxGridSize[0]) {
+    throw std::runtime_error("Grid X dimension is too large for device");
+  }
+
+  // Allocate memory for RNG states
+  hiprandState *d_rngStates = 0;
+  cudaResult =
+      hipMalloc((void **)&d_rngStates, grid.x * block.x * sizeof(hiprandState));
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not allocate memory on device for RNG states: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Allocate memory for result
+  // Each thread block will produce one result
+  unsigned int *d_results = 0;
+  cudaResult = hipMalloc((void **)&d_results, grid.x * sizeof(unsigned int));
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not allocate memory on device for partial results: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Initialise RNG
+  initRNG<<<grid, block>>>(d_rngStates, m_seed);
+
+  // Count the points inside unit quarter-circle
+  computeValue<Real><<<grid, block, block.x * sizeof(unsigned int)>>>(
+      d_results, d_rngStates, m_numSims);
+
+  // Copy partial results back
+  vector<unsigned int> results(grid.x);
+  cudaResult = hipMemcpy(&results[0], d_results, grid.x * sizeof(unsigned int),
+                          hipMemcpyDeviceToHost);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not copy partial results to host: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Complete sum-reduction on host
+  Real value =
+      static_cast<Real>(std::accumulate(results.begin(), results.end(), 0));
+
+  // Determine the proportion of points inside the quarter-circle,
+  // i.e. the area of the unit quarter-circle
+  value /= m_numSims;
+
+  // Value is currently an estimate of the area of a unit quarter-circle, so we
+  // can scale to a full circle by multiplying by four. Now since the area of a
+  // circle is pi * r^2, and r is one, the value will be an estimate for the
+  // value of pi.
+  value *= 4;
+
+  // Cleanup
+  if (d_rngStates) {
+    hipFree(d_rngStates);
+    d_rngStates = 0;
+  }
+
+  if (d_results) {
+    hipFree(d_results);
+    d_results = 0;
+  }
+
+  return value;
+}
+
+// Explicit template instantiation
+template class PiEstimator<float>;
+template class PiEstimator<double>;
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiQ/src/piestimator.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiQ/src/piestimator.cu.hip
index e69de29..581aa2c 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiQ/src/piestimator.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/MC_EstimatePiQ/src/piestimator.cu.hip
@@ -0,0 +1,312 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "../inc/piestimator.h"
+
+#include <string>
+#include <vector>
+#include <numeric>
+#include <stdexcept>
+#include <typeinfo>
+#include <hip/hip_runtime.h>
+#include <hip/hip_cooperative_groups.h>
+
+namespace cg = cooperative_groups;
+#include <hiprand.h>
+
+using std::string;
+using std::vector;
+
+__device__ unsigned int reduce_sum(unsigned int in, cg::thread_block cta) {
+  extern __shared__ unsigned int sdata[];
+
+  // Perform first level of reduction:
+  // - Write to shared memory
+  unsigned int ltid = threadIdx.x;
+
+  sdata[ltid] = in;
+  cg::sync(cta);
+
+  // Do reduction in shared mem
+  for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {
+    if (ltid < s) {
+      sdata[ltid] += sdata[ltid + s];
+    }
+
+    cg::sync(cta);
+  }
+
+  return sdata[0];
+}
+
+// Estimator kernel
+template <typename Real>
+__global__ void computeValue(unsigned int *const results,
+                             const Real *const points,
+                             const unsigned int numSims) {
+  // Handle to thread block group
+  cg::thread_block cta = cg::this_thread_block();
+  // Determine thread ID
+  unsigned int bid = blockIdx.x;
+  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int step = gridDim.x * blockDim.x;
+
+  // Shift the input/output pointers
+  const Real *pointx = points + tid;
+  const Real *pointy = pointx + numSims;
+
+  // Count the number of points which lie inside the unit quarter-circle
+  unsigned int pointsInside = 0;
+
+  for (unsigned int i = tid; i < numSims;
+       i += step, pointx += step, pointy += step) {
+    Real x = *pointx;
+    Real y = *pointy;
+    Real l2norm2 = x * x + y * y;
+
+    if (l2norm2 < static_cast<Real>(1)) {
+      pointsInside++;
+    }
+  }
+
+  // Reduce within the block
+  pointsInside = reduce_sum(pointsInside, cta);
+
+  // Store the result
+  if (threadIdx.x == 0) {
+    results[bid] = pointsInside;
+  }
+}
+
+template <typename Real>
+PiEstimator<Real>::PiEstimator(unsigned int numSims, unsigned int device,
+                               unsigned int threadBlockSize)
+    : m_numSims(numSims),
+      m_device(device),
+      m_threadBlockSize(threadBlockSize) {}
+
+template <typename Real>
+Real PiEstimator<Real>::operator()() {
+  hipError_t cudaResult = hipSuccess;
+  struct hipDeviceProp_t deviceProperties;
+  struct hipFuncAttributes funcAttributes;
+
+  // Get device properties
+  cudaResult = hipGetDeviceProperties(&deviceProperties, m_device);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not get device properties: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Check precision is valid
+  if (typeid(Real) == typeid(double) &&
+      (deviceProperties.major < 1 ||
+       (deviceProperties.major == 1 && deviceProperties.minor < 3))) {
+    throw std::runtime_error("Device does not have double precision support");
+  }
+
+  // Attach to GPU
+  cudaResult = hipSetDevice(m_device);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not set CUDA device: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Determine how to divide the work between cores
+  dim3 block;
+  dim3 grid;
+  block.x = m_threadBlockSize;
+  grid.x = (m_numSims + m_threadBlockSize - 1) / m_threadBlockSize;
+
+  // Aim to launch around ten or more times as many blocks as there
+  // are multiprocessors on the target device.
+  unsigned int blocksPerSM = 10;
+  unsigned int numSMs = deviceProperties.multiProcessorCount;
+
+  while (grid.x > 2 * blocksPerSM * numSMs) {
+    grid.x >>= 1;
+  }
+
+  // Get computeValue function properties and check the maximum block size
+  cudaResult = hipFuncGetAttributes(&funcAttributes, computeValue<Real>);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not get function attributes: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  if (block.x > (unsigned int)funcAttributes.maxThreadsPerBlock) {
+    throw std::runtime_error(
+        "Block X dimension is too large for computeValue kernel");
+  }
+
+  // Check the dimensions are valid
+  if (block.x > (unsigned int)deviceProperties.maxThreadsDim[0]) {
+    throw std::runtime_error("Block X dimension is too large for device");
+  }
+
+  if (grid.x > (unsigned int)deviceProperties.maxGridSize[0]) {
+    throw std::runtime_error("Grid X dimension is too large for device");
+  }
+
+  // Allocate memory for points
+  // Each simulation has two random numbers to give X and Y coordinate
+  Real *d_points = 0;
+  cudaResult = hipMalloc((void **)&d_points, 2 * m_numSims * sizeof(Real));
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not allocate memory on device for random numbers: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Allocate memory for result
+  // Each thread block will produce one result
+  unsigned int *d_results = 0;
+  cudaResult = hipMalloc((void **)&d_results, grid.x * sizeof(unsigned int));
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not allocate memory on device for partial results: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Generate random points in unit square
+  hiprandStatus_t curandResult;
+  hiprandGenerator_t qrng;
+
+  if (typeid(Real) == typeid(float)) {
+    curandResult = hiprandCreateGenerator(&qrng, HIPRAND_RNG_QUASI_SOBOL32);
+  } else if (typeid(Real) == typeid(double)) {
+    curandResult = hiprandCreateGenerator(&qrng, HIPRAND_RNG_QUASI_SOBOL64);
+  } else {
+    string msg("Could not create random number generator of specified type");
+    throw std::runtime_error(msg);
+  }
+
+  if (curandResult != HIPRAND_STATUS_SUCCESS) {
+    string msg("Could not create quasi-random number generator: ");
+    msg += curandResult;
+    throw std::runtime_error(msg);
+  }
+
+  curandResult = hiprandSetQuasiRandomGeneratorDimensions(qrng, 2);
+
+  if (curandResult != HIPRAND_STATUS_SUCCESS) {
+    string msg(
+        "Could not set number of dimensions for quasi-random number "
+        "generator: ");
+    msg += curandResult;
+    throw std::runtime_error(msg);
+  }
+
+  curandResult =
+      curandSetGeneratorOrdering(qrng, CURAND_ORDERING_QUASI_DEFAULT);
+
+  if (curandResult != HIPRAND_STATUS_SUCCESS) {
+    string msg("Could not set order for quasi-random number generator: ");
+    msg += curandResult;
+    throw std::runtime_error(msg);
+  }
+
+  if (typeid(Real) == typeid(float)) {
+    curandResult =
+        hiprandGenerateUniform(qrng, (float *)d_points, 2 * m_numSims);
+  } else if (typeid(Real) == typeid(double)) {
+    curandResult =
+        hiprandGenerateUniformDouble(qrng, (double *)d_points, 2 * m_numSims);
+  } else {
+    string msg("Could not generate random numbers of specified type");
+    throw std::runtime_error(msg);
+  }
+
+  if (curandResult != HIPRAND_STATUS_SUCCESS) {
+    string msg("Could not generate quasi-random numbers: ");
+    msg += curandResult;
+    throw std::runtime_error(msg);
+  }
+
+  curandResult = hiprandDestroyGenerator(qrng);
+
+  if (curandResult != HIPRAND_STATUS_SUCCESS) {
+    string msg("Could not destroy quasi-random number generator: ");
+    msg += curandResult;
+    throw std::runtime_error(msg);
+  }
+
+  // Count the points inside unit quarter-circle
+  computeValue<Real><<<grid, block, block.x * sizeof(unsigned int)>>>(
+      d_results, d_points, m_numSims);
+
+  // Copy partial results back
+  vector<unsigned int> results(grid.x);
+  cudaResult = hipMemcpy(&results[0], d_results, grid.x * sizeof(unsigned int),
+                          hipMemcpyDeviceToHost);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not copy partial results to host: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Complete sum-reduction on host
+  Real value =
+      static_cast<Real>(std::accumulate(results.begin(), results.end(), 0));
+
+  // Determine the proportion of points inside the quarter-circle,
+  // i.e. the area of the unit quarter-circle
+  value /= m_numSims;
+
+  // Value is currently an estimate of the area of a unit quarter-circle, so we
+  // can scale to a full circle by multiplying by four. Now since the area of a
+  // circle is pi * r^2, and r is one, the value will be an estimate for the
+  // value of pi.
+  value *= 4;
+
+  // Cleanup
+  if (d_points) {
+    hipFree(d_points);
+    d_points = 0;
+  }
+
+  if (d_results) {
+    hipFree(d_results);
+    d_results = 0;
+  }
+
+  return value;
+}
+
+// Explicit template instantiation
+template class PiEstimator<float>;
+template class PiEstimator<double>;
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/MC_SingleAsianOptionP/src/pricingengine.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/MC_SingleAsianOptionP/src/pricingengine.cu.hip
index e69de29..ba594cb 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/MC_SingleAsianOptionP/src/pricingengine.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/MC_SingleAsianOptionP/src/pricingengine.cu.hip
@@ -0,0 +1,375 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "../inc/pricingengine.h"
+
+#include <string>
+#include <vector>
+#include <numeric>
+#include <stdexcept>
+#include <typeinfo>
+#include <hip/hip_runtime.h>
+#include <hip/hip_cooperative_groups.h>
+
+namespace cg = cooperative_groups;
+#include <hiprand_kernel.h>
+
+#include "../inc/asianoption.h"
+#include "../inc/cudasharedmem.h"
+
+using std::string;
+using std::vector;
+
+// RNG init kernel
+__global__ void initRNG(hiprandState *const rngStates, const unsigned int seed) {
+  // Determine thread ID
+  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
+
+  // Initialise the RNG
+  hiprand_init(seed, tid, 0, &rngStates[tid]);
+}
+
+__device__ inline float getPathStep(float &drift, float &diffusion,
+                                    hiprandState &state) {
+  return expf(drift + diffusion * hiprand_normal(&state));
+}
+__device__ inline double getPathStep(double &drift, double &diffusion,
+                                     hiprandState &state) {
+  return exp(drift + diffusion * hiprand_normal_double(&state));
+}
+
+// Path generation kernel
+template <typename Real>
+__global__ void generatePaths(Real *const paths, hiprandState *const rngStates,
+                              const AsianOption<Real> *const option,
+                              const unsigned int numSims,
+                              const unsigned int numTimesteps) {
+  // Determine thread ID
+  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int step = gridDim.x * blockDim.x;
+
+  // Compute parameters
+  Real drift =
+      (option->r - static_cast<Real>(0.5) * option->sigma * option->sigma) *
+      option->dt;
+  Real diffusion = option->sigma * sqrt(option->dt);
+
+  // Initialise the RNG
+  hiprandState localState = rngStates[tid];
+
+  for (unsigned int i = tid; i < numSims; i += step) {
+    // Shift the output pointer
+    Real *output = paths + i;
+
+    // Simulate the path
+    Real s = static_cast<Real>(1);
+
+    for (unsigned int t = 0; t < numTimesteps; t++, output += numSims) {
+      s *= getPathStep(drift, diffusion, localState);
+      *output = s;
+    }
+  }
+}
+
+template <typename Real>
+__device__ Real reduce_sum(Real in, cg::thread_block cta) {
+  SharedMemory<Real> sdata;
+
+  // Perform first level of reduction:
+  // - Write to shared memory
+  unsigned int ltid = threadIdx.x;
+
+  sdata[ltid] = in;
+  cg::sync(cta);
+
+  // Do reduction in shared mem
+  for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {
+    if (ltid < s) {
+      sdata[ltid] += sdata[ltid + s];
+    }
+
+    cg::sync(cta);
+  }
+
+  return sdata[0];
+}
+
+// Valuation kernel
+template <typename Real>
+__global__ void computeValue(Real *const values, const Real *const paths,
+                             const AsianOption<Real> *const option,
+                             const unsigned int numSims,
+                             const unsigned int numTimesteps) {
+  // Handle to thread block group
+  cg::thread_block cta = cg::this_thread_block();
+  // Determine thread ID
+  unsigned int bid = blockIdx.x;
+  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int step = gridDim.x * blockDim.x;
+
+  Real sumPayoffs = static_cast<Real>(0);
+
+  for (unsigned int i = tid; i < numSims; i += step) {
+    // Shift the input pointer
+    const Real *path = paths + i;
+    // Compute the arithmetic average
+    Real avg = static_cast<Real>(0);
+
+    for (unsigned int t = 0; t < numTimesteps; t++, path += numSims) {
+      avg += *path;
+    }
+
+    avg = avg * option->spot / numTimesteps;
+    // Compute the payoff
+    Real payoff = avg - option->strike;
+
+    if (option->type == AsianOption<Real>::Put) {
+      payoff = -payoff;
+    }
+
+    payoff = max(static_cast<Real>(0), payoff);
+    // Accumulate payoff locally
+    sumPayoffs += payoff;
+  }
+
+  // Reduce within the block
+  sumPayoffs = reduce_sum<Real>(sumPayoffs, cta);
+
+  // Store the result
+  if (threadIdx.x == 0) {
+    values[bid] = sumPayoffs;
+  }
+}
+
+template <typename Real>
+PricingEngine<Real>::PricingEngine(unsigned int numSims, unsigned int device,
+                                   unsigned int threadBlockSize,
+                                   unsigned int seed)
+    : m_numSims(numSims),
+      m_device(device),
+      m_threadBlockSize(threadBlockSize),
+      m_seed(seed) {}
+
+template <typename Real>
+void PricingEngine<Real>::operator()(AsianOption<Real> &option) {
+  hipError_t cudaResult = hipSuccess;
+  struct hipDeviceProp_t deviceProperties;
+  struct hipFuncAttributes funcAttributes;
+
+  // Get device properties
+  cudaResult = hipGetDeviceProperties(&deviceProperties, m_device);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not get device properties: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Check precision is valid
+  unsigned int deviceVersion =
+      deviceProperties.major * 10 + deviceProperties.minor;
+
+  if (typeid(Real) == typeid(double) && deviceVersion < 13) {
+    throw std::runtime_error("Device does not have double precision support");
+  }
+
+  // Attach to GPU
+  cudaResult = hipSetDevice(m_device);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not set CUDA device: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Determine how to divide the work between cores
+  dim3 block;
+  dim3 grid;
+  block.x = m_threadBlockSize;
+  grid.x = (m_numSims + m_threadBlockSize - 1) / m_threadBlockSize;
+
+  // Aim to launch around ten or more times as many blocks as there
+  // are multiprocessors on the target device.
+  unsigned int blocksPerSM = 10;
+  unsigned int numSMs = deviceProperties.multiProcessorCount;
+
+  while (grid.x > 2 * blocksPerSM * numSMs) {
+    grid.x >>= 1;
+  }
+
+  // Get initRNG function properties and check the maximum block size
+  cudaResult = hipFuncGetAttributes(&funcAttributes,(const void*) initRNG);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not get function attributes: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  if (block.x > (unsigned int)funcAttributes.maxThreadsPerBlock) {
+    throw std::runtime_error(
+        "Block X dimension is too large for initRNG kernel");
+  }
+
+  // Get generatePaths function properties and check the maximum block size
+  cudaResult = hipFuncGetAttributes(&funcAttributes, (const void*)generatePaths<Real>);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not get function attributes: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  if (block.x > (unsigned int)funcAttributes.maxThreadsPerBlock) {
+    throw std::runtime_error(
+        "Block X dimension is too large for generatePaths kernel");
+  }
+
+  // Get computeValue function properties and check the maximum block size
+  cudaResult = hipFuncGetAttributes(&funcAttributes,(const void*) computeValue<Real>);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not get function attributes: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  if (block.x > (unsigned int)funcAttributes.maxThreadsPerBlock) {
+    throw std::runtime_error(
+        "Block X dimension is too large for computeValue kernel");
+  }
+
+  // Setup problem on GPU
+  AsianOption<Real> *d_option = 0;
+  cudaResult = hipMalloc((void **)&d_option, sizeof(AsianOption<Real>));
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not allocate memory on device for option data: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  cudaResult = hipMemcpy(d_option, &option, sizeof(AsianOption<Real>),
+                          hipMemcpyHostToDevice);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not copy data to device: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Allocate memory for paths
+  Real *d_paths = 0;
+  int numTimesteps = static_cast<int>(option.tenor / option.dt);
+  cudaResult =
+      hipMalloc((void **)&d_paths, m_numSims * numTimesteps * sizeof(Real));
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not allocate memory on device for paths: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Allocate memory for RNG states
+  hiprandState *d_rngStates = 0;
+  cudaResult =
+      hipMalloc((void **)&d_rngStates, grid.x * block.x * sizeof(hiprandState));
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not allocate memory on device for RNG state: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Allocate memory for result
+  Real *d_values = 0;
+  cudaResult = hipMalloc((void **)&d_values, grid.x * sizeof(Real));
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not allocate memory on device for partial results: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Initialise RNG
+  initRNG<<<grid, block>>>(d_rngStates, m_seed);
+
+  // Generate paths
+  generatePaths<Real><<<grid, block>>>(d_paths, d_rngStates, d_option,
+                                       m_numSims, numTimesteps);
+
+  // Compute value
+  computeValue<<<grid, block, block.x * sizeof(Real)>>>(
+      d_values, d_paths, d_option, m_numSims, numTimesteps);
+
+  // Copy partial results back
+  vector<Real> values(grid.x);
+  cudaResult = hipMemcpy(&values[0], d_values, grid.x * sizeof(Real),
+                          hipMemcpyDeviceToHost);
+
+  if (cudaResult != hipSuccess) {
+    string msg("Could not copy partial results to host: ");
+    msg += hipGetErrorString(cudaResult);
+    throw std::runtime_error(msg);
+  }
+
+  // Complete sum-reduction on host
+  option.value =
+      std::accumulate(values.begin(), values.end(), static_cast<Real>(0));
+
+  // Compute the mean
+  option.value /= m_numSims;
+
+  // Discount to present value
+  option.value *= exp(-option.r * option.tenor);
+
+  // Cleanup
+  if (d_option) {
+    hipFree(d_option);
+    d_option = 0;
+  }
+
+  if (d_paths) {
+    hipFree(d_paths);
+    d_paths = 0;
+  }
+
+  if (d_rngStates) {
+    hipFree(d_rngStates);
+    d_rngStates = 0;
+  }
+
+  if (d_values) {
+    hipFree(d_values);
+    d_values = 0;
+  }
+}
+
+// Explicit template instantiation
+template class PricingEngine<float>;
+template class PricingEngine<double>;
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_large.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_large.cu.hip
index e69de29..f98a3a3 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_large.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_large.cu.hip
@@ -0,0 +1,370 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* Computation of eigenvalues of a large symmetric, tridiagonal matrix */
+
+// includes, system
+#include <stdlib.h>
+#include <stdio.h>
+#include "rocprofiler.h"
+#include "HIPCHECK.h"
+#include <string.h>
+#include <math.h>
+#include <float.h>
+
+// includes, project
+#include "helper_functions.h"
+#include "helper_cuda_hipified.h"
+#include "config.h"
+#include "structs.h"
+#include "util.h"
+#include "matlab.h"
+
+#include "bisect_large.cuh"
+
+// includes, kernels
+#include "bisect_kernel_large.cuh"
+#include "bisect_kernel_large_onei.cuh"
+#include "bisect_kernel_large_multi.cuh"
+
+////////////////////////////////////////////////////////////////////////////////
+//! Initialize variables and memory for result
+//! @param  result handles to memory
+//! @param  matrix_size  size of the matrix
+////////////////////////////////////////////////////////////////////////////////
+void initResultDataLargeMatrix(ResultDataLarge &result,
+                               const unsigned int mat_size) {
+  // helper variables to initialize memory
+  unsigned int zero = 0;
+  unsigned int mat_size_f = sizeof(float) * mat_size;
+  unsigned int mat_size_ui = sizeof(unsigned int) * mat_size;
+
+  float *tempf = (float *)malloc(mat_size_f);
+  unsigned int *tempui = (unsigned int *)malloc(mat_size_ui);
+
+  for (unsigned int i = 0; i < mat_size; ++i) {
+    tempf[i] = 0.0f;
+    tempui[i] = 0;
+  }
+
+  // number of intervals containing only one eigenvalue after the first step
+  HIPCHECK(hipMalloc((void **)&result.g_num_one, sizeof(unsigned int)));
+  HIPCHECK(hipMemcpy(result.g_num_one, &zero, sizeof(unsigned int),
+                             hipMemcpyHostToDevice));
+
+  // number of (thread) blocks of intervals with multiple eigenvalues after
+  // the first iteration
+  HIPCHECK(
+      hipMalloc((void **)&result.g_num_blocks_mult, sizeof(unsigned int)));
+  HIPCHECK(hipMemcpy(result.g_num_blocks_mult, &zero,
+                             sizeof(unsigned int), hipMemcpyHostToDevice));
+
+  HIPCHECK(hipMalloc((void **)&result.g_left_one, mat_size_f));
+  HIPCHECK(hipMalloc((void **)&result.g_right_one, mat_size_f));
+  HIPCHECK(hipMalloc((void **)&result.g_pos_one, mat_size_ui));
+
+  HIPCHECK(hipMalloc((void **)&result.g_left_mult, mat_size_f));
+  HIPCHECK(hipMalloc((void **)&result.g_right_mult, mat_size_f));
+  HIPCHECK(hipMalloc((void **)&result.g_left_count_mult, mat_size_ui));
+  HIPCHECK(hipMalloc((void **)&result.g_right_count_mult, mat_size_ui));
+
+  HIPCHECK(
+      hipMemcpy(result.g_left_one, tempf, mat_size_f, hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(result.g_right_one, tempf, mat_size_f,
+                             hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(result.g_pos_one, tempui, mat_size_ui,
+                             hipMemcpyHostToDevice));
+
+  HIPCHECK(hipMemcpy(result.g_left_mult, tempf, mat_size_f,
+                             hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(result.g_right_mult, tempf, mat_size_f,
+                             hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(result.g_left_count_mult, tempui, mat_size_ui,
+                             hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(result.g_right_count_mult, tempui, mat_size_ui,
+                             hipMemcpyHostToDevice));
+
+  HIPCHECK(hipMalloc((void **)&result.g_blocks_mult, mat_size_ui));
+  HIPCHECK(hipMemcpy(result.g_blocks_mult, tempui, mat_size_ui,
+                             hipMemcpyHostToDevice));
+  HIPCHECK(hipMalloc((void **)&result.g_blocks_mult_sum, mat_size_ui));
+  HIPCHECK(hipMemcpy(result.g_blocks_mult_sum, tempui, mat_size_ui,
+                             hipMemcpyHostToDevice));
+
+  HIPCHECK(hipMalloc((void **)&result.g_lambda_mult, mat_size_f));
+  HIPCHECK(hipMemcpy(result.g_lambda_mult, tempf, mat_size_f,
+                             hipMemcpyHostToDevice));
+  HIPCHECK(hipMalloc((void **)&result.g_pos_mult, mat_size_ui));
+  HIPCHECK(hipMemcpy(result.g_pos_mult, tempf, mat_size_ui,
+                             hipMemcpyHostToDevice));
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Cleanup result memory
+//! @param result  handles to memory
+////////////////////////////////////////////////////////////////////////////////
+void cleanupResultDataLargeMatrix(ResultDataLarge &result) {
+  HIPCHECK(hipFree(result.g_num_one));
+  HIPCHECK(hipFree(result.g_num_blocks_mult));
+  HIPCHECK(hipFree(result.g_left_one));
+  HIPCHECK(hipFree(result.g_right_one));
+  HIPCHECK(hipFree(result.g_pos_one));
+  HIPCHECK(hipFree(result.g_left_mult));
+  HIPCHECK(hipFree(result.g_right_mult));
+  HIPCHECK(hipFree(result.g_left_count_mult));
+  HIPCHECK(hipFree(result.g_right_count_mult));
+  HIPCHECK(hipFree(result.g_blocks_mult));
+  HIPCHECK(hipFree(result.g_blocks_mult_sum));
+  HIPCHECK(hipFree(result.g_lambda_mult));
+  HIPCHECK(hipFree(result.g_pos_mult));
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Run the kernels to compute the eigenvalues for large matrices
+//! @param  input   handles to input data
+//! @param  result  handles to result data
+//! @param  mat_size  matrix size
+//! @param  precision  desired precision of eigenvalues
+//! @param  lg  lower limit of Gerschgorin interval
+//! @param  ug  upper limit of Gerschgorin interval
+//! @param  iterations  number of iterations (for timing)
+////////////////////////////////////////////////////////////////////////////////
+void computeEigenvaluesLargeMatrix(const InputData &input,
+                                   const ResultDataLarge &result,
+                                   const unsigned int mat_size,
+                                   const float precision, const float lg,
+                                   const float ug,
+                                   const unsigned int iterations) {
+  dim3 blocks(1, 1, 1);
+  dim3 threads(MAX_THREADS_BLOCK, 1, 1);
+
+  StopWatchInterface *timer_step1 = NULL;
+  StopWatchInterface *timer_step2_one = NULL;
+  StopWatchInterface *timer_step2_mult = NULL;
+  StopWatchInterface *timer_total = NULL;
+  sdkCreateTimer(&timer_step1);
+  sdkCreateTimer(&timer_step2_one);
+  sdkCreateTimer(&timer_step2_mult);
+  sdkCreateTimer(&timer_total);
+
+  sdkStartTimer(&timer_total);
+
+  // do for multiple iterations to improve timing accuracy
+  for (unsigned int iter = 0; iter < iterations; ++iter) {
+    sdkStartTimer(&timer_step1);
+    bisectKernelLarge<<<blocks, threads>>>(
+        input.g_a, input.g_b, mat_size, lg, ug, 0, mat_size, precision,
+        result.g_num_one, result.g_num_blocks_mult, result.g_left_one,
+        result.g_right_one, result.g_pos_one, result.g_left_mult,
+        result.g_right_mult, result.g_left_count_mult,
+        result.g_right_count_mult, result.g_blocks_mult,
+        result.g_blocks_mult_sum);
+
+    getLastCudaError("Kernel launch failed.");
+    HIPCHECK(hipDeviceSynchronize());
+    sdkStopTimer(&timer_step1);
+
+    // get the number of intervals containing one eigenvalue after the first
+    // processing step
+    unsigned int num_one_intervals;
+    HIPCHECK(hipMemcpy(&num_one_intervals, result.g_num_one,
+                               sizeof(unsigned int), hipMemcpyDeviceToHost));
+
+    dim3 grid_onei;
+    grid_onei.x = getNumBlocksLinear(num_one_intervals, MAX_THREADS_BLOCK);
+    dim3 threads_onei;
+    // use always max number of available threads to better balance load times
+    // for matrix data
+    threads_onei.x = MAX_THREADS_BLOCK;
+
+    // compute eigenvalues for intervals that contained only one eigenvalue
+    // after the first processing step
+    sdkStartTimer(&timer_step2_one);
+
+    bisectKernelLarge_OneIntervals<<<grid_onei, threads_onei>>>(
+        input.g_a, input.g_b, mat_size, num_one_intervals, result.g_left_one,
+        result.g_right_one, result.g_pos_one, precision);
+
+    getLastCudaError("bisectKernelLarge_OneIntervals() FAILED.");
+    HIPCHECK(hipDeviceSynchronize());
+    sdkStopTimer(&timer_step2_one);
+
+    // process intervals that contained more than one eigenvalue after
+    // the first processing step
+
+    // get the number of blocks of intervals that contain, in total when
+    // each interval contains only one eigenvalue, not more than
+    // MAX_THREADS_BLOCK threads
+    unsigned int num_blocks_mult = 0;
+    HIPCHECK(hipMemcpy(&num_blocks_mult, result.g_num_blocks_mult,
+                               sizeof(unsigned int), hipMemcpyDeviceToHost));
+
+    // setup the execution environment
+    dim3 grid_mult(num_blocks_mult, 1, 1);
+    dim3 threads_mult(MAX_THREADS_BLOCK, 1, 1);
+
+    sdkStartTimer(&timer_step2_mult);
+
+    bisectKernelLarge_MultIntervals<<<grid_mult, threads_mult>>>(
+        input.g_a, input.g_b, mat_size, result.g_blocks_mult,
+        result.g_blocks_mult_sum, result.g_left_mult, result.g_right_mult,
+        result.g_left_count_mult, result.g_right_count_mult,
+        result.g_lambda_mult, result.g_pos_mult, precision);
+
+    getLastCudaError("bisectKernelLarge_MultIntervals() FAILED.");
+    HIPCHECK(hipDeviceSynchronize());
+    sdkStopTimer(&timer_step2_mult);
+  }
+
+  sdkStopTimer(&timer_total);
+
+  printf("Average time step 1: %f ms\n",
+         sdkGetTimerValue(&timer_step1) / (float)iterations);
+  printf("Average time step 2, one intervals: %f ms\n",
+         sdkGetTimerValue(&timer_step2_one) / (float)iterations);
+  printf("Average time step 2, mult intervals: %f ms\n",
+         sdkGetTimerValue(&timer_step2_mult) / (float)iterations);
+
+  printf("Average time TOTAL: %f ms\n",
+         sdkGetTimerValue(&timer_total) / (float)iterations);
+
+  sdkDeleteTimer(&timer_step1);
+  sdkDeleteTimer(&timer_step2_one);
+  sdkDeleteTimer(&timer_step2_mult);
+  sdkDeleteTimer(&timer_total);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Process the result, that is obtain result from device and do simple sanity
+//! checking
+//! @param  input   handles to input data
+//! @param  result  handles to result data
+//! @param  mat_size  matrix size
+//! @param  filename  output filename
+////////////////////////////////////////////////////////////////////////////////
+bool processResultDataLargeMatrix(const InputData &input,
+                                  const ResultDataLarge &result,
+                                  const unsigned int mat_size,
+                                  const char *filename,
+                                  const unsigned int user_defined,
+                                  char *exec_path) {
+  bool bCompareResult = false;
+  const unsigned int mat_size_ui = sizeof(unsigned int) * mat_size;
+  const unsigned int mat_size_f = sizeof(float) * mat_size;
+
+  // copy data from intervals that contained more than one eigenvalue after
+  // the first processing step
+  float *lambda_mult = (float *)malloc(sizeof(float) * mat_size);
+  HIPCHECK(hipMemcpy(lambda_mult, result.g_lambda_mult,
+                             sizeof(float) * mat_size, hipMemcpyDeviceToHost));
+  unsigned int *pos_mult =
+      (unsigned int *)malloc(sizeof(unsigned int) * mat_size);
+  HIPCHECK(hipMemcpy(pos_mult, result.g_pos_mult,
+                             sizeof(unsigned int) * mat_size,
+                             hipMemcpyDeviceToHost));
+
+  unsigned int *blocks_mult_sum =
+      (unsigned int *)malloc(sizeof(unsigned int) * mat_size);
+  HIPCHECK(hipMemcpy(blocks_mult_sum, result.g_blocks_mult_sum,
+                             sizeof(unsigned int) * mat_size,
+                             hipMemcpyDeviceToHost));
+
+  unsigned int num_one_intervals;
+  HIPCHECK(hipMemcpy(&num_one_intervals, result.g_num_one,
+                             sizeof(unsigned int), hipMemcpyDeviceToHost));
+
+  unsigned int sum_blocks_mult = mat_size - num_one_intervals;
+
+  // copy data for intervals that contained one eigenvalue after the first
+  // processing step
+  float *left_one = (float *)malloc(mat_size_f);
+  float *right_one = (float *)malloc(mat_size_f);
+  unsigned int *pos_one = (unsigned int *)malloc(mat_size_ui);
+  HIPCHECK(hipMemcpy(left_one, result.g_left_one, mat_size_f,
+                             hipMemcpyDeviceToHost));
+  HIPCHECK(hipMemcpy(right_one, result.g_right_one, mat_size_f,
+                             hipMemcpyDeviceToHost));
+  HIPCHECK(hipMemcpy(pos_one, result.g_pos_one, mat_size_ui,
+                             hipMemcpyDeviceToHost));
+
+  // extract eigenvalues
+  float *eigenvals = (float *)malloc(mat_size_f);
+
+  // singleton intervals generated in the second step
+  for (unsigned int i = 0; i < sum_blocks_mult; ++i) {
+    eigenvals[pos_mult[i] - 1] = lambda_mult[i];
+  }
+
+  // singleton intervals generated in the first step
+  unsigned int index = 0;
+
+  for (unsigned int i = 0; i < num_one_intervals; ++i, ++index) {
+    eigenvals[pos_one[i] - 1] = left_one[i];
+  }
+
+  if (1 == user_defined) {
+    // store result
+    writeTridiagSymMatlab(filename, input.a, input.b + 1, eigenvals, mat_size);
+    // getLastCudaError( sdkWriteFilef( filename, eigenvals, mat_size, 0.0f));
+
+    printf("User requests non-default argument(s), skipping self-check!\n");
+    bCompareResult = true;
+  } else {
+    // compare with reference solution
+
+    float *reference = NULL;
+    unsigned int input_data_size = 0;
+
+    char *ref_path = sdkFindFilePath("reference.dat", exec_path);
+    assert(NULL != ref_path);
+    sdkReadFile(ref_path, &reference, &input_data_size, false);
+    assert(input_data_size == mat_size);
+
+    // there's an imprecision of Sturm count computation which makes an
+    // additional offset necessary
+    float tolerance = 1.0e-5f + 5.0e-6f;
+
+    if (sdkCompareL2fe(reference, eigenvals, mat_size, tolerance) == true) {
+      bCompareResult = true;
+    } else {
+      bCompareResult = false;
+    }
+
+    free(ref_path);
+    free(reference);
+  }
+
+  freePtr(eigenvals);
+  freePtr(lambda_mult);
+  freePtr(pos_mult);
+  freePtr(blocks_mult_sum);
+  freePtr(left_one);
+  freePtr(right_one);
+  freePtr(pos_one);
+
+  return bCompareResult;
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_small.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_small.cu.hip
index e69de29..75c2b72 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_small.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_small.cu.hip
@@ -0,0 +1,184 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* Computation of eigenvalues of a small symmetric, tridiagonal matrix */
+
+// includes, system
+#include <stdlib.h>
+#include <stdio.h>
+#include "rocprofiler.h"
+#include "HIPCHECK.h"
+#include <string.h>
+#include <math.h>
+#include <float.h>
+
+// includes, project
+#include "helper_functions.h"
+#include "helper_cuda_hipified.h"
+#include "config.h"
+#include "structs.h"
+#include "matlab.h"
+
+// includes, kernels
+#include "bisect_kernel_small.cuh"
+
+// includes, file
+#include "bisect_small.cuh"
+
+////////////////////////////////////////////////////////////////////////////////
+//! Determine eigenvalues for matrices smaller than MAX_SMALL_MATRIX
+//! @param TimingIterations  number of iterations for timing
+//! @param  input  handles to input data of kernel
+//! @param  result handles to result of kernel
+//! @param  mat_size  matrix size
+//! @param  lg  lower limit of Gerschgorin interval
+//! @param  ug  upper limit of Gerschgorin interval
+//! @param  precision  desired precision of eigenvalues
+//! @param  iterations  number of iterations for timing
+////////////////////////////////////////////////////////////////////////////////
+void computeEigenvaluesSmallMatrix(const InputData &input,
+                                   ResultDataSmall &result,
+                                   const unsigned int mat_size, const float lg,
+                                   const float ug, const float precision,
+                                   const unsigned int iterations) {
+  StopWatchInterface *timer = NULL;
+  sdkCreateTimer(&timer);
+  sdkStartTimer(&timer);
+
+  for (unsigned int i = 0; i < iterations; ++i) {
+    dim3 blocks(1, 1, 1);
+    dim3 threads(MAX_THREADS_BLOCK_SMALL_MATRIX, 1, 1);
+
+    bisectKernel<<<blocks, threads>>>(input.g_a, input.g_b, mat_size,
+                                      result.g_left, result.g_right,
+                                      result.g_left_count, result.g_right_count,
+                                      lg, ug, 0, mat_size, precision);
+  }
+
+  HIPCHECK(hipDeviceSynchronize());
+  sdkStopTimer(&timer);
+  getLastCudaError("Kernel launch failed");
+  printf("Average time: %f ms (%i iterations)\n",
+         sdkGetTimerValue(&timer) / (float)iterations, iterations);
+
+  sdkDeleteTimer(&timer);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Initialize variables and memory for the result for small matrices
+//! @param result  handles to the necessary memory
+//! @param  mat_size  matrix_size
+////////////////////////////////////////////////////////////////////////////////
+void initResultSmallMatrix(ResultDataSmall &result,
+                           const unsigned int mat_size) {
+  result.mat_size_f = sizeof(float) * mat_size;
+  result.mat_size_ui = sizeof(unsigned int) * mat_size;
+
+  result.eigenvalues = (float *)malloc(result.mat_size_f);
+
+  // helper variables
+  result.zero_f = (float *)malloc(result.mat_size_f);
+  result.zero_ui = (unsigned int *)malloc(result.mat_size_ui);
+
+  for (unsigned int i = 0; i < mat_size; ++i) {
+    result.zero_f[i] = 0.0f;
+    result.zero_ui[i] = 0;
+
+    result.eigenvalues[i] = 0.0f;
+  }
+
+  HIPCHECK(hipMalloc((void **)&result.g_left, result.mat_size_f));
+  HIPCHECK(hipMalloc((void **)&result.g_right, result.mat_size_f));
+
+  HIPCHECK(
+      hipMalloc((void **)&result.g_left_count, result.mat_size_ui));
+  HIPCHECK(
+      hipMalloc((void **)&result.g_right_count, result.mat_size_ui));
+
+  // initialize result memory
+  HIPCHECK(hipMemcpy(result.g_left, result.zero_f, result.mat_size_f,
+                             hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(result.g_right, result.zero_f, result.mat_size_f,
+                             hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(result.g_right_count, result.zero_ui,
+                             result.mat_size_ui, hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(result.g_left_count, result.zero_ui,
+                             result.mat_size_ui, hipMemcpyHostToDevice));
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Cleanup memory and variables for result for small matrices
+//! @param  result  handle to variables
+////////////////////////////////////////////////////////////////////////////////
+void cleanupResultSmallMatrix(ResultDataSmall &result) {
+  freePtr(result.eigenvalues);
+  freePtr(result.zero_f);
+  freePtr(result.zero_ui);
+
+  HIPCHECK(hipFree(result.g_left));
+  HIPCHECK(hipFree(result.g_right));
+  HIPCHECK(hipFree(result.g_left_count));
+  HIPCHECK(hipFree(result.g_right_count));
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Process the result obtained on the device, that is transfer to host and
+//! perform basic sanity checking
+//! @param  input  handles to input data
+//! @param  result  handles to result data
+//! @param  mat_size   matrix size
+//! @param  filename  output filename
+////////////////////////////////////////////////////////////////////////////////
+void processResultSmallMatrix(const InputData &input,
+                              const ResultDataSmall &result,
+                              const unsigned int mat_size,
+                              const char *filename) {
+  const unsigned int mat_size_f = sizeof(float) * mat_size;
+  const unsigned int mat_size_ui = sizeof(unsigned int) * mat_size;
+
+  // copy data back to host
+  float *left = (float *)malloc(mat_size_f);
+  unsigned int *left_count = (unsigned int *)malloc(mat_size_ui);
+
+  HIPCHECK(
+      hipMemcpy(left, result.g_left, mat_size_f, hipMemcpyDeviceToHost));
+  HIPCHECK(hipMemcpy(left_count, result.g_left_count, mat_size_ui,
+                             hipMemcpyDeviceToHost));
+
+  float *eigenvalues = (float *)malloc(mat_size_f);
+
+  for (unsigned int i = 0; i < mat_size; ++i) {
+    eigenvalues[left_count[i]] = left[i];
+  }
+
+  // save result in matlab format
+  writeTridiagSymMatlab(filename, input.a, input.b + 1, eigenvalues, mat_size);
+
+  freePtr(left);
+  freePtr(left_count);
+  freePtr(eigenvalues);
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_util.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_util.cu.hip
index e69de29..1061c2a 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_util.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_util.cu.hip
@@ -0,0 +1,530 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* Utility / shared functionality for bisection kernels */
+
+#ifndef _BISECT_UTIL_H_
+#define _BISECT_UTIL_H_
+
+#include <hip/hip_runtime.h>
+#include <hip/hip_cooperative_groups.h>
+
+namespace cg = cooperative_groups;
+
+// includes, project
+#include "config.h"
+#include "util.h"
+
+////////////////////////////////////////////////////////////////////////////////
+//! Compute the next lower power of two of n
+//! @param  n  number for which next higher power of two is sought
+////////////////////////////////////////////////////////////////////////////////
+__device__ inline int floorPow2(int n) {
+  // early out if already power of two
+  if (0 == (n & (n - 1))) {
+    return n;
+  }
+
+  int exp;
+  frexp((float)n, &exp);
+  return (1 << (exp - 1));
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Compute the next higher power of two of n
+//! @param  n  number for which next higher power of two is sought
+////////////////////////////////////////////////////////////////////////////////
+__device__ inline int ceilPow2(int n) {
+  // early out if already power of two
+  if (0 == (n & (n - 1))) {
+    return n;
+  }
+
+  int exp;
+  frexp((float)n, &exp);
+  return (1 << exp);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Compute midpoint of interval [\a left, \a right] avoiding overflow if
+//! possible
+//! @param left   left / lower limit of interval
+//! @param right  right / upper limit of interval
+////////////////////////////////////////////////////////////////////////////////
+__device__ inline float computeMidpoint(const float left, const float right) {
+  float mid;
+
+  if (sign_f(left) == sign_f(right)) {
+    mid = left + (right - left) * 0.5f;
+  } else {
+    mid = (left + right) * 0.5f;
+  }
+
+  return mid;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Check if interval converged and store appropriately
+//! @param  addr    address where to store the information of the interval
+//! @param  s_left  shared memory storage for left interval limits
+//! @param  s_right  shared memory storage for right interval limits
+//! @param  s_left_count  shared memory storage for number of eigenvalues less
+//!                       than left interval limits
+//! @param  s_right_count  shared memory storage for number of eigenvalues less
+//!                       than right interval limits
+//! @param  left   lower limit of interval
+//! @param  right  upper limit of interval
+//! @param  left_count  eigenvalues less than \a left
+//! @param  right_count  eigenvalues less than \a right
+//! @param  precision  desired precision for eigenvalues
+////////////////////////////////////////////////////////////////////////////////
+template <class S, class T>
+__device__ void storeInterval(unsigned int addr, float *s_left, float *s_right,
+                              T *s_left_count, T *s_right_count, float left,
+                              float right, S left_count, S right_count,
+                              float precision) {
+  s_left_count[addr] = left_count;
+  s_right_count[addr] = right_count;
+
+  // check if interval converged
+  float t0 = abs(right - left);
+  float t1 = max(abs(left), abs(right)) * precision;
+
+  if (t0 <= max(MIN_ABS_INTERVAL, t1)) {
+    // compute mid point
+    float lambda = computeMidpoint(left, right);
+
+    // mark as converged
+    s_left[addr] = lambda;
+    s_right[addr] = lambda;
+  } else {
+    // store current limits
+    s_left[addr] = left;
+    s_right[addr] = right;
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Compute number of eigenvalues that are smaller than x given a symmetric,
+//! real, and tridiagonal matrix
+//! @param  g_d  diagonal elements stored in global memory
+//! @param  g_s  superdiagonal elements stored in global memory
+//! @param  n    size of matrix
+//! @param  x    value for which the number of eigenvalues that are smaller is
+//!              seeked
+//! @param  tid  thread identified (e.g. threadIdx.x or gtid)
+//! @param  num_intervals_active  number of active intervals / threads that
+//!                               currently process an interval
+//! @param  s_d  scratch space to store diagonal entries of the tridiagonal
+//!              matrix in shared memory
+//! @param  s_s  scratch space to store superdiagonal entries of the tridiagonal
+//!              matrix in shared memory
+//! @param  converged  flag if the current thread is already converged (that
+//!         is count does not have to be computed)
+////////////////////////////////////////////////////////////////////////////////
+__device__ inline unsigned int computeNumSmallerEigenvals(
+    float *g_d, float *g_s, const unsigned int n, const float x,
+    const unsigned int tid, const unsigned int num_intervals_active, float *s_d,
+    float *s_s, unsigned int converged, cg::thread_block cta) {
+  float delta = 1.0f;
+  unsigned int count = 0;
+
+  cg::sync(cta);
+
+  // read data into shared memory
+  if (threadIdx.x < n) {
+    s_d[threadIdx.x] = *(g_d + threadIdx.x);
+    s_s[threadIdx.x] = *(g_s + threadIdx.x - 1);
+  }
+
+  cg::sync(cta);
+
+  // perform loop only for active threads
+  if ((tid < num_intervals_active) && (0 == converged)) {
+    // perform (optimized) Gaussian elimination to determine the number
+    // of eigenvalues that are smaller than n
+    for (unsigned int k = 0; k < n; ++k) {
+      delta = s_d[k] - x - (s_s[k] * s_s[k]) / delta;
+      count += (delta < 0) ? 1 : 0;
+    }
+
+  }  // end if thread currently processing an interval
+
+  return count;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Compute number of eigenvalues that are smaller than x given a symmetric,
+//! real, and tridiagonal matrix
+//! @param  g_d  diagonal elements stored in global memory
+//! @param  g_s  superdiagonal elements stored in global memory
+//! @param  n    size of matrix
+//! @param  x    value for which the number of eigenvalues that are smaller is
+//!              seeked
+//! @param  tid  thread identified (e.g. threadIdx.x or gtid)
+//! @param  num_intervals_active  number of active intervals / threads that
+//!                               currently process an interval
+//! @param  s_d  scratch space to store diagonal entries of the tridiagonal
+//!              matrix in shared memory
+//! @param  s_s  scratch space to store superdiagonal entries of the tridiagonal
+//!              matrix in shared memory
+//! @param  converged  flag if the current thread is already converged (that
+//!         is count does not have to be computed)
+////////////////////////////////////////////////////////////////////////////////
+__device__ inline unsigned int computeNumSmallerEigenvalsLarge(
+    float *g_d, float *g_s, const unsigned int n, const float x,
+    const unsigned int tid, const unsigned int num_intervals_active, float *s_d,
+    float *s_s, unsigned int converged, cg::thread_block cta) {
+  float delta = 1.0f;
+  unsigned int count = 0;
+
+  unsigned int rem = n;
+
+  // do until whole diagonal and superdiagonal has been loaded and processed
+  for (unsigned int i = 0; i < n; i += blockDim.x) {
+    cg::sync(cta);
+
+    // read new chunk of data into shared memory
+    if ((i + threadIdx.x) < n) {
+      s_d[threadIdx.x] = *(g_d + i + threadIdx.x);
+      s_s[threadIdx.x] = *(g_s + i + threadIdx.x - 1);
+    }
+
+    cg::sync(cta);
+
+    if (tid < num_intervals_active) {
+      // perform (optimized) Gaussian elimination to determine the number
+      // of eigenvalues that are smaller than n
+      for (unsigned int k = 0; k < min((int)rem, (int)blockDim.x); ++k) {
+        delta = s_d[k] - x - (s_s[k] * s_s[k]) / delta;
+        // delta = (abs( delta) < (1.0e-10)) ? -(1.0e-10) : delta;
+        count += (delta < 0) ? 1 : 0;
+      }
+
+    }  // end if thread currently processing an interval
+
+    rem -= blockDim.x;
+  }
+
+  return count;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+//! Store all non-empty intervals resulting from the subdivision of the interval
+//! currently processed by the thread
+//! @param  addr  base address for storing intervals
+//! @param  num_threads_active  number of threads / intervals in current sweep
+//! @param  s_left  shared memory storage for left interval limits
+//! @param  s_right  shared memory storage for right interval limits
+//! @param  s_left_count  shared memory storage for number of eigenvalues less
+//!                       than left interval limits
+//! @param  s_right_count  shared memory storage for number of eigenvalues less
+//!                       than right interval limits
+//! @param  left   lower limit of interval
+//! @param  mid    midpoint of interval
+//! @param  right  upper limit of interval
+//! @param  left_count  eigenvalues less than \a left
+//! @param  mid_count  eigenvalues less than \a mid
+//! @param  right_count  eigenvalues less than \a right
+//! @param  precision  desired precision for eigenvalues
+//! @param  compact_second_chunk  shared mem flag if second chunk is used and
+//!                               ergo requires compaction
+//! @param  s_compaction_list_exc  helper array for stream compaction,
+//!                                s_compaction_list_exc[tid] = 1 when the
+//!                                thread generated two child intervals
+//! @is_active_interval  mark is thread has a second non-empty child interval
+////////////////////////////////////////////////////////////////////////////////
+template <class S, class T>
+__device__ void storeNonEmptyIntervals(
+    unsigned int addr, const unsigned int num_threads_active, float *s_left,
+    float *s_right, T *s_left_count, T *s_right_count, float left, float mid,
+    float right, const S left_count, const S mid_count, const S right_count,
+    float precision, unsigned int &compact_second_chunk,
+    T *s_compaction_list_exc, unsigned int &is_active_second) {
+  // check if both child intervals are valid
+  if ((left_count != mid_count) && (mid_count != right_count)) {
+    // store the left interval
+    storeInterval(addr, s_left, s_right, s_left_count, s_right_count, left, mid,
+                  left_count, mid_count, precision);
+
+    // mark that a second interval has been generated, only stored after
+    // stream compaction of second chunk
+    is_active_second = 1;
+    s_compaction_list_exc[threadIdx.x] = 1;
+    atomicExch(&compact_second_chunk, 1);
+  } else {
+    // only one non-empty child interval
+
+    // mark that no second child
+    is_active_second = 0;
+    s_compaction_list_exc[threadIdx.x] = 0;
+
+    // store the one valid child interval
+    if (left_count != mid_count) {
+      storeInterval(addr, s_left, s_right, s_left_count, s_right_count, left,
+                    mid, left_count, mid_count, precision);
+    } else {
+      storeInterval(addr, s_left, s_right, s_left_count, s_right_count, mid,
+                    right, mid_count, right_count, precision);
+    }
+  }
+}
+////////////////////////////////////////////////////////////////////////////////
+//! Create indices for compaction, that is process \a s_compaction_list_exc
+//! which is 1 for intervals that generated a second child and 0 otherwise
+//! and create for each of the non-zero elements the index where the new
+//! interval belongs to in a compact representation of all generated second
+//! childs
+//! @param   s_compaction_list_exc  list containing the flags which threads
+//!                                 generated two children
+//! @param   num_threads_compaction number of threads to employ for compaction
+////////////////////////////////////////////////////////////////////////////////
+template <class T>
+__device__ void createIndicesCompaction(T *s_compaction_list_exc,
+                                        unsigned int num_threads_compaction,
+                                        cg::thread_block cta) {
+  unsigned int offset = 1;
+  const unsigned int tid = threadIdx.x;
+
+  // higher levels of scan tree
+  for (int d = (num_threads_compaction >> 1); d > 0; d >>= 1) {
+    cg::sync(cta);
+
+    if (tid < d) {
+      unsigned int ai = offset * (2 * tid + 1) - 1;
+      unsigned int bi = offset * (2 * tid + 2) - 1;
+
+      s_compaction_list_exc[bi] =
+          s_compaction_list_exc[bi] + s_compaction_list_exc[ai];
+    }
+
+    offset <<= 1;
+  }
+
+  // traverse down tree: first down to level 2 across
+  for (int d = 2; d < num_threads_compaction; d <<= 1) {
+    offset >>= 1;
+    cg::sync(cta);
+
+    if (tid < (d - 1)) {
+      unsigned int ai = offset * (tid + 1) - 1;
+      unsigned int bi = ai + (offset >> 1);
+
+      s_compaction_list_exc[bi] =
+          s_compaction_list_exc[bi] + s_compaction_list_exc[ai];
+    }
+  }
+
+  cg::sync(cta);
+}
+
+///////////////////////////////////////////////////////////////////////////////
+//! Perform stream compaction for second child intervals
+//! @param  s_left  shared
+//! @param  s_left  shared memory storage for left interval limits
+//! @param  s_right  shared memory storage for right interval limits
+//! @param  s_left_count  shared memory storage for number of eigenvalues less
+//!                       than left interval limits
+//! @param  s_right_count  shared memory storage for number of eigenvalues less
+//!                       than right interval limits
+//! @param  mid    midpoint of current interval (left of new interval)
+//! @param  right  upper limit of interval
+//! @param  mid_count  eigenvalues less than \a mid
+//! @param  s_compaction_list  list containing the indices where the data has
+//!         to be stored
+//! @param  num_threads_active  number of active threads / intervals
+//! @is_active_interval  mark is thread has a second non-empty child interval
+///////////////////////////////////////////////////////////////////////////////
+template <class T>
+__device__ void compactIntervals(float *s_left, float *s_right, T *s_left_count,
+                                 T *s_right_count, float mid, float right,
+                                 unsigned int mid_count,
+                                 unsigned int right_count, T *s_compaction_list,
+                                 unsigned int num_threads_active,
+                                 unsigned int is_active_second) {
+  const unsigned int tid = threadIdx.x;
+
+  // perform compaction / copy data for all threads where the second
+  // child is not dead
+  if ((tid < num_threads_active) && (1 == is_active_second)) {
+    unsigned int addr_w = num_threads_active + s_compaction_list[tid];
+
+    s_left[addr_w] = mid;
+    s_right[addr_w] = right;
+    s_left_count[addr_w] = mid_count;
+    s_right_count[addr_w] = right_count;
+  }
+}
+
+///////////////////////////////////////////////////////////////////////////////
+//! Store intervals that have already converged (w.r.t. the desired precision),
+//! duplicating intervals that contain multiple eigenvalues
+//! @param  s_left  shared memory storage for left interval limits
+//! @param  s_right  shared memory storage for right interval limits
+//! @param  s_left_count  shared memory storage for number of eigenvalues less
+//!                       than left interval limits
+//! @param  s_right_count  shared memory storage for number of eigenvalues less
+//!                       than right interval limits
+//! @param  left   lower limit of interval
+//! @param  mid    midpoint of interval (updated if split is necessary)
+//! @param  right  upper limit of interval
+//! @param  left_count  eigenvalues less than \a left
+//! @param  mid_count  eigenvalues less than \a mid
+//! @param  right_count  eigenvalues less than \a right
+//! @param  s_compaction_list_exc  helper array for stream compaction, updated
+//!                                at tid if split is necessary
+//! @param  compact_second_chunk  shared mem flag if second chunk is used and
+//!                               ergo requires compaction
+//! @param  num_threads_active  number of active threads / intervals
+///////////////////////////////////////////////////////////////////////////////
+template <class T, class S>
+__device__ void storeIntervalConverged(float *s_left, float *s_right,
+                                       T *s_left_count, T *s_right_count,
+                                       float &left, float &mid, float &right,
+                                       S &left_count, S &mid_count,
+                                       S &right_count, T *s_compaction_list_exc,
+                                       unsigned int &compact_second_chunk,
+                                       const unsigned int num_threads_active) {
+  const unsigned int tid = threadIdx.x;
+  const unsigned int multiplicity = right_count - left_count;
+
+  // check multiplicity of eigenvalue
+  if (1 == multiplicity) {
+    // just re-store intervals, simple eigenvalue
+    s_left[tid] = left;
+    s_right[tid] = right;
+    s_left_count[tid] = left_count;
+    s_right_count[tid] = right_count;
+
+    // mark that no second child / clear
+    s_right_count[tid + num_threads_active] = 0;
+    s_compaction_list_exc[tid] = 0;
+  } else {
+    // number of eigenvalues after the split less than mid
+    mid_count = left_count + (multiplicity >> 1);
+
+    // store left interval
+    s_left[tid] = left;
+    s_right[tid] = right;
+    s_left_count[tid] = left_count;
+    s_right_count[tid] = mid_count;
+
+    mid = left;
+
+    // mark that second child interval exists
+    s_right_count[tid + num_threads_active] = right_count;
+    s_compaction_list_exc[tid] = 1;
+    compact_second_chunk = 1;
+  }
+}
+
+template <class T, class S>
+__device__ void storeIntervalConverged(float *s_left, float *s_right,
+                                       T *s_left_count, T *s_right_count,
+                                       float &left, float &mid, float &right,
+                                       S &left_count, S &mid_count,
+                                       S &right_count, T *s_compaction_list_exc,
+                                       unsigned int &compact_second_chunk,
+                                       const unsigned int num_threads_active,
+                                       unsigned int &is_active_second) {
+  const unsigned int tid = threadIdx.x;
+  const unsigned int multiplicity = right_count - left_count;
+
+  // check multiplicity of eigenvalue
+  if (1 == multiplicity) {
+    // just re-store intervals, simple eigenvalue
+    s_left[tid] = left;
+    s_right[tid] = right;
+    s_left_count[tid] = left_count;
+    s_right_count[tid] = right_count;
+
+    // mark that no second child / clear
+    is_active_second = 0;
+    s_compaction_list_exc[tid] = 0;
+  } else {
+    // number of eigenvalues after the split less than mid
+    mid_count = left_count + (multiplicity >> 1);
+
+    // store left interval
+    s_left[tid] = left;
+    s_right[tid] = right;
+    s_left_count[tid] = left_count;
+    s_right_count[tid] = mid_count;
+
+    mid = left;
+
+    // mark that second child interval exists
+    is_active_second = 1;
+    s_compaction_list_exc[tid] = 1;
+    compact_second_chunk = 1;
+  }
+}
+
+///////////////////////////////////////////////////////////////////////////////
+//! Subdivide interval if active and not already converged
+//! @param tid  id of thread
+//! @param  s_left  shared memory storage for left interval limits
+//! @param  s_right  shared memory storage for right interval limits
+//! @param  s_left_count  shared memory storage for number of eigenvalues less
+//!                       than left interval limits
+//! @param  s_right_count  shared memory storage for number of eigenvalues less
+//!                       than right interval limits
+//! @param  num_threads_active  number of active threads in warp
+//! @param  left   lower limit of interval
+//! @param  right  upper limit of interval
+//! @param  left_count  eigenvalues less than \a left
+//! @param  right_count  eigenvalues less than \a right
+//! @param  all_threads_converged  shared memory flag if all threads are
+//!                                 converged
+///////////////////////////////////////////////////////////////////////////////
+template <class T>
+__device__ void subdivideActiveInterval(
+    const unsigned int tid, float *s_left, float *s_right, T *s_left_count,
+    T *s_right_count, const unsigned int num_threads_active, float &left,
+    float &right, unsigned int &left_count, unsigned int &right_count,
+    float &mid, unsigned int &all_threads_converged) {
+  // for all active threads
+  if (tid < num_threads_active) {
+    left = s_left[tid];
+    right = s_right[tid];
+    left_count = s_left_count[tid];
+    right_count = s_right_count[tid];
+
+    // check if thread already converged
+    if (left != right) {
+      mid = computeMidpoint(left, right);
+      atomicExch(&all_threads_converged, 0);
+    } else if ((right_count - left_count) > 1) {
+      // mark as not converged if multiple eigenvalues enclosed
+      // duplicate interval in storeIntervalsConverged()
+      atomicExch(&all_threads_converged, 0);
+    }
+
+  }  // end for all active threads
+}
+
+#endif  // #ifndef _BISECT_UTIL_H_
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX_nvrtc/inlinePTX_kernel.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX_nvrtc/inlinePTX_kernel.cu.hip
index e69de29..18a6fe6 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX_nvrtc/inlinePTX_kernel.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX_nvrtc/inlinePTX_kernel.cu.hip
@@ -0,0 +1,40 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+extern "C" __global__ void sequence_gpu(int *d_ptr, int length) {
+  int elemID = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (elemID < length) {
+    unsigned int laneid;
+
+    // This command gets the lane ID within the current warp
+    asm("mov.u32  %0, %%laneid;" : "=r"(laneid));
+
+    d_ptr[elemID] = laneid;
+  }
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/interval/interval.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/interval/interval.cu.hip
index e69de29..79002e1 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/interval/interval.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/interval/interval.cu.hip
@@ -0,0 +1,165 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* Example of program using the interval_gpu<T> template class and operators:
+ * Search for roots of a function using an interval Newton method.
+  *
+ * Use the command-line argument "--n=<N>" to select which GPU implementation to
+ * use,
+ * otherwise the naive implementation will be used by default.
+ * 0: the naive implementation
+ * 1: the optimized implementation
+ * 2: the recursive implementation
+ *
+ */
+
+const static char *sSDKsample = "Interval Computing";
+
+#include <iostream>
+#include <stdio.h>
+#include "rocprofiler.h"
+#include "HIPCHECK.h"
+#include "helper_cuda_hipified.h"
+#include "interval_hipified.h"
+#include "cuda_interval_hipified.h"
+#include "cpu_interval_hipified.h"
+
+int main(int argc, char *argv[]) {
+  int implementation_choice = 0;
+
+  printf("[%s]  starting ...\n\n", sSDKsample);
+
+  if (checkCmdLineFlag(argc, (const char **)argv, "n")) {
+    implementation_choice =
+        getCmdLineArgumentInt(argc, (const char **)argv, "n");
+  }
+
+  // Pick the best GPU available, or if the developer selects one at the command
+  // line
+  int devID = findCudaDevice(argc, (const char **)argv);
+  hipDeviceProp_t deviceProp;
+  hipGetDeviceProperties(&deviceProp, devID);
+  printf("> GPU Device has Compute Capabilities SM %d.%d\n\n", deviceProp.major,
+         deviceProp.minor);
+
+  switch (implementation_choice) {
+    case 0:
+      printf("GPU naive implementation\n");
+      break;
+
+    case 1:
+      printf("GPU optimized implementation\n");
+      break;
+
+    case 2:
+      printf("GPU recursive implementation (requires Compute SM 2.0+)\n");
+      break;
+
+    default:
+      printf("GPU naive implementation\n");
+  }
+
+  interval_gpu<T> *d_result;
+  int *d_nresults;
+  int *h_nresults = new int[THREADS];
+  hipEvent_t start, stop;
+
+  CHECKED_CALL(hipSetDevice(devID));
+  CHECKED_CALL(hipMalloc((void **)&d_result,
+                          THREADS * DEPTH_RESULT * sizeof(*d_result)));
+  CHECKED_CALL(hipMalloc((void **)&d_nresults, THREADS * sizeof(*d_nresults)));
+  CHECKED_CALL(hipEventCreate(&start));
+  CHECKED_CALL(hipEventCreate(&stop));
+
+  // We need L1 cache to store the stack (only applicable to sm_20 and higher)
+  CHECKED_CALL(
+      hipFuncSetCacheConfig((const void*)test_interval_newton<T>, hipFuncCachePreferL1));
+
+  // Increase the stack size large enough for the non-inlined and recursive
+  // function calls (only applicable to sm_20 and higher)
+  CHECKED_CALL(hipDeviceSetLimit(hipLimitStackSize, 8192));
+
+  interval_gpu<T> i(0.01f, 4.0f);
+  std::cout << "Searching for roots in [" << i.lower() << ", " << i.upper()
+            << "]...\n";
+
+  CHECKED_CALL(hipEventRecord(start, 0));
+
+  for (int it = 0; it < NUM_RUNS; ++it) {
+    test_interval_newton<T><<<GRID_SIZE, BLOCK_SIZE>>>(d_result, d_nresults, i,
+                                                       implementation_choice);
+    CHECKED_CALL(hipGetLastError());
+  }
+
+  CHECKED_CALL(hipEventRecord(stop, 0));
+  CHECKED_CALL(hipDeviceSynchronize());
+
+  I_CPU *h_result = new I_CPU[THREADS * DEPTH_RESULT];
+  CHECKED_CALL(hipMemcpy(h_result, d_result,
+                          THREADS * DEPTH_RESULT * sizeof(*d_result),
+                          hipMemcpyDeviceToHost));
+  CHECKED_CALL(hipMemcpy(h_nresults, d_nresults, THREADS * sizeof(*d_nresults),
+                          hipMemcpyDeviceToHost));
+
+  std::cout << "Found " << h_nresults[0]
+            << " intervals that may contain the root(s)\n";
+  std::cout.precision(15);
+
+  for (int i = 0; i != h_nresults[0]; ++i) {
+    std::cout << " i[" << i << "] ="
+              << " [" << h_result[THREADS * i + 0].lower() << ", "
+              << h_result[THREADS * i + 0].upper() << "]\n";
+  }
+
+  float time;
+  CHECKED_CALL(hipEventElapsedTime(&time, start, stop));
+  std::cout << "Number of equations solved: " << THREADS << "\n";
+  std::cout << "Time per equation: "
+            << 1000000.0f * (time / (float)(THREADS)) / NUM_RUNS << " us\n";
+
+  CHECKED_CALL(hipEventDestroy(start));
+  CHECKED_CALL(hipEventDestroy(stop));
+  CHECKED_CALL(hipFree(d_result));
+  CHECKED_CALL(hipFree(d_nresults));
+
+  // Compute the results using a CPU implementation based on the Boost library
+  I_CPU i_cpu(0.01f, 4.0f);
+  I_CPU *h_result_cpu = new I_CPU[THREADS * DEPTH_RESULT];
+  int *h_nresults_cpu = new int[THREADS];
+  test_interval_newton_cpu<I_CPU>(h_result_cpu, h_nresults_cpu, i_cpu);
+
+  // Compare the CPU and GPU results
+  bool bTestResult =
+      checkAgainstHost(h_nresults, h_nresults_cpu, h_result, h_result_cpu);
+
+  delete[] h_result_cpu;
+  delete[] h_nresults_cpu;
+  delete[] h_result;
+  delete[] h_nresults;
+
+  exit(bTestResult ? EXIT_SUCCESS : EXIT_FAILURE);
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.cu.hip
index 42fc161..417fcf4 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.cu.hip
@@ -34,7 +34,7 @@
 #include <thrust/generate.h>
 #include <thrust/detail/type_traits.h>
 
-#include <helper_cuda.h>
+#include "helper_cuda_hipified.h"
 
 #include <algorithm>
 #include <time.h>
@@ -142,8 +142,8 @@ bool testSort(int argc, char **argv) {
 
   // run multiple iterations to compute an average sort time
   hipEvent_t start_event, stop_event;
-  checkCudaErrors(hipEventCreate(&start_event));
-  checkCudaErrors(hipEventCreate(&stop_event));
+  HIPCHECK(hipEventCreate(&start_event));
+  HIPCHECK(hipEventCreate(&stop_event));
 
   float totalTime = 0;
 
@@ -153,18 +153,18 @@ bool testSort(int argc, char **argv) {
 
     if (!keysOnly) d_values = h_values;
 
-    checkCudaErrors(hipEventRecord(start_event, 0));
+    HIPCHECK(hipEventRecord(start_event, 0));
 
     if (keysOnly)
       thrust::sort(d_keys.begin(), d_keys.end());
     else
       thrust::sort_by_key(d_keys.begin(), d_keys.end(), d_values.begin());
 
-    checkCudaErrors(hipEventRecord(stop_event, 0));
-    checkCudaErrors(hipEventSynchronize(stop_event));
+    HIPCHECK(hipEventRecord(stop_event, 0));
+    HIPCHECK(hipEventSynchronize(stop_event));
 
     float time = 0;
-    checkCudaErrors(hipEventElapsedTime(&time, start_event, stop_event));
+    HIPCHECK(hipEventElapsedTime(&time, start_event, stop_event));
     totalTime += time;
   }
 
@@ -188,8 +188,8 @@ bool testSort(int argc, char **argv) {
   bool bTestResult =
       thrust::is_sorted(h_keysSorted.begin(), h_keysSorted.end());
 
-  checkCudaErrors(hipEventDestroy(start_event));
-  checkCudaErrors(hipEventDestroy(stop_event));
+  HIPCHECK(hipEventDestroy(start_event));
+  HIPCHECK(hipEventDestroy(stop_event));
 
   if (!bTestResult && !quiet) {
     return false;
@@ -213,5 +213,3 @@ int main(int argc, char **argv) {
 
   printf(bTestResult ? "Test passed\n" : "Test failed!\n");
 }
-tf(bTestResult ? "Test passed\n" : "Test failed!\n");
-}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.out b/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.out
index b4ce3f4..25e4468 100755
Binary files a/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.out and b/src/samples/Samples/2_Concepts_and_Techniques/radixSortThrust/radixSortThrust.out differ
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/reductionMultiBlockCG/reductionMultiBlockCG.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/reductionMultiBlockCG/reductionMultiBlockCG.cu.hip
index 82853ad..31bf6ca 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/reductionMultiBlockCG/reductionMultiBlockCG.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/reductionMultiBlockCG/reductionMultiBlockCG.cu.hip
@@ -177,7 +177,7 @@ int main(int argc, char **argv) {
   printf("%s Starting...\n\n", sSDKsample);
 
   dev = findCudaDevice(argc, (const char **)argv);
-  checkCudaErrors(hipGetDeviceProperties(&deviceProp, dev));
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, dev));
   if (!deviceProp.cooperativeLaunch) {
     printf(
         "\nSelected GPU (%d) does not support Cooperative Kernel Launch, "
@@ -235,7 +235,7 @@ void getNumBlocksAndThreads(int n, int maxBlocks, int maxThreads, int &blocks,
     threads = 1;
     blocks = 1;
   } else {
-    checkCudaErrors(hipOccupancyMaxPotentialBlockSize(
+    HIPCHECK(hipOccupancyMaxPotentialBlockSize(
         &blocks, &threads, reduceSinglePassMultiBlockCG));
   }
 
@@ -267,7 +267,7 @@ float benchmarkReduce(int n, int numThreads, int numBlocks, int maxThreads,
   // copy final sum from device to host
   error =
       hipMemcpy(&gpu_result, d_odata, sizeof(float), hipMemcpyDeviceToHost);
-  checkCudaErrors(error);
+  HIPCHECK(error);
 
   return gpu_result;
 }
@@ -287,8 +287,8 @@ bool runTest(int argc, char **argv, int device) {
 
   // Set the device to be used
   hipDeviceProp_t prop = {0};
-  checkCudaErrors(hipSetDevice(device));
-  checkCudaErrors(hipGetDeviceProperties(&prop, device));
+  HIPCHECK(hipSetDevice(device));
+  HIPCHECK(hipGetDeviceProperties(&prop, device));
 
   // create random input data on CPU
   unsigned int bytes = size * sizeof(float);
@@ -324,7 +324,7 @@ bool runTest(int argc, char **argv, int device) {
   // We calculate the occupancy to know how many block can actually fit on the
   // GPU
   int numBlocksPerSm = 0;
-  checkCudaErrors(hipOccupancyMaxActiveBlocksPerMultiprocessor(
+  HIPCHECK(hipOccupancyMaxActiveBlocksPerMultiprocessor(
       &numBlocksPerSm, reduceSinglePassMultiBlockCG, numThreads,
       numThreads * sizeof(double)));
 
@@ -342,12 +342,12 @@ bool runTest(int argc, char **argv, int device) {
   float *d_idata = NULL;
   float *d_odata = NULL;
 
-  checkCudaErrors(hipMalloc((void **)&d_idata, bytes));
-  checkCudaErrors(hipMalloc((void **)&d_odata, numBlocks * sizeof(float)));
+  HIPCHECK(hipMalloc((void **)&d_idata, bytes));
+  HIPCHECK(hipMalloc((void **)&d_odata, numBlocks * sizeof(float)));
 
   // copy data directly to device memory
-  checkCudaErrors(hipMemcpy(d_idata, h_idata, bytes, hipMemcpyHostToDevice));
-  checkCudaErrors(hipMemcpy(d_odata, h_idata, numBlocks * sizeof(float),
+  HIPCHECK(hipMemcpy(d_idata, h_idata, bytes, hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(d_odata, h_idata, numBlocks * sizeof(float),
                              hipMemcpyHostToDevice));
 
   int testIterations = 100;
@@ -403,3 +403,9 @@ ta);
 
   return bTestPassed;
 }
+ta);
+  hipFree(d_idata);
+  hipFree(d_odata);
+
+  return bTestPassed;
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/scan/scan.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/scan/scan.cu.hip
index 2dbdf6a..c3e1a8d 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/scan/scan.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/scan/scan.cu.hip
@@ -32,7 +32,7 @@
 namespace cg = cooperative_groups;
 #include "helper_cuda_hipified.h"
 #include "scan_common.h"
+#include "HIPCHECK.h"
+
 // All three kernels run 512 threads per workgroup
 // Must be a power of two
 #define THREADBLOCK_SIZE 256
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/segmentationTreeThrust/segmentationTree.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/segmentationTreeThrust/segmentationTree.cu.hip
index 27d51ae..634c5f4 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/segmentationTreeThrust/segmentationTree.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/segmentationTreeThrust/segmentationTree.cu.hip
@@ -71,9 +71,9 @@
 #include <thrust/device_free.h>
 
 // Sample framework includes.
-#include <helper_functions.h>
-#include <helper_cuda.h>
-
+#include "helper_functions.h"
+#include "helper_cuda_hipified.h"
+#include "HIPCHECK.h"
 // Project includes.
 #include "common.cuh"
 
@@ -265,13 +265,13 @@ class Pyramid
                     thrust::device_ptr<uint> superVerticesOffsets,
                     thrust::device_ptr<uint> verticesIDs)
                 {
-                    checkCudaErrors(
+                    HIPCHECK(
                         hipMemcpy(&(superNodesOffsets_[0]),
                                    superVerticesOffsets.get(),
                                    sizeof(uint) * superNodesOffsets_.size(),
                                    hipMemcpyDeviceToHost));
 
-                    checkCudaErrors(
+                    HIPCHECK(
                         hipMemcpy(&(nodes_[0]),
                                    verticesIDs.get(),
                                    sizeof(uint) * nodes_.size(),
@@ -515,15 +515,15 @@ class SegmentationTreeBuilder
             dOutputEdgesFlags_ = pools.uintEdges.get();
 
             // Copy graph to the device memory
-            checkCudaErrors(hipMemcpy(dVertices_.get(),
+            HIPCHECK(hipMemcpy(dVertices_.get(),
                                        &(graph.vertices[0]),
                                        sizeof(uint) * verticesCount_,
                                        hipMemcpyHostToDevice));
-            checkCudaErrors(hipMemcpy(dEdges_.get(),
+            HIPCHECK(hipMemcpy(dEdges_.get(),
                                        &(graph.edges[0]),
                                        sizeof(uint) * edgesCount_,
                                        hipMemcpyHostToDevice));
-            checkCudaErrors(hipMemcpy(dWeights_.get(),
+            HIPCHECK(hipMemcpy(dWeights_.get(),
                                        &(graph.weights[0]),
                                        sizeof(float) * edgesCount_,
                                        hipMemcpyHostToDevice));
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/segmentationTreeThrust/segmentationTreeThrust.out b/src/samples/Samples/2_Concepts_and_Techniques/segmentationTreeThrust/segmentationTreeThrust.out
index d5fea33..d202192 100755
Binary files a/src/samples/Samples/2_Concepts_and_Techniques/segmentationTreeThrust/segmentationTreeThrust.out and b/src/samples/Samples/2_Concepts_and_Techniques/segmentationTreeThrust/segmentationTreeThrust.out differ
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/shfl_scan/shfl_scan.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/shfl_scan/shfl_scan.cu.hip
index 05c1860..97e704a 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/shfl_scan/shfl_scan.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/shfl_scan/shfl_scan.cu.hip
@@ -213,9 +213,9 @@ bool shuffle_simple_test(int argc, char **argv) {
   cuda_device = findCudaDevice(argc, (const char **)argv);
 
   hipDeviceProp_t deviceProp;
-  checkCudaErrors(hipGetDevice(&cuda_device));
+  HIPCHECK(hipGetDevice(&cuda_device));
 
-  checkCudaErrors(hipGetDeviceProperties(&deviceProp, cuda_device));
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, cuda_device));
 
   printf("> Detected Compute SM %d.%d hardware with %d multi-processors\n",
          deviceProp.major, deviceProp.minor, deviceProp.multiProcessorCount);
@@ -227,9 +227,9 @@ bool shuffle_simple_test(int argc, char **argv) {
     exit(EXIT_WAIVED);
   }
 
-  checkCudaErrors(hipHostMalloc(reinterpret_cast<void **>(&h_data),
+  HIPCHECK(hipHostMalloc(reinterpret_cast<void **>(&h_data),
                                  sizeof(int) * n_elements));
-  checkCudaErrors(hipHostMalloc(reinterpret_cast<void **>(&h_result),
+  HIPCHECK(hipHostMalloc(reinterpret_cast<void **>(&h_result),
                                  sizeof(int) * n_elements));
 
   // initialize data:
@@ -259,32 +259,32 @@ bool shuffle_simple_test(int argc, char **argv) {
 
   // initialize a timer
   hipEvent_t start, stop;
-  checkCudaErrors(hipEventCreate(&start));
-  checkCudaErrors(hipEventCreate(&stop));
+  HIPCHECK(hipEventCreate(&start));
+  HIPCHECK(hipEventCreate(&stop));
   float et = 0;
   float inc = 0;
 
-  checkCudaErrors(hipMalloc(reinterpret_cast<void **>(&d_data), sz));
-  checkCudaErrors(
+  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_data), sz));
+  HIPCHECK(
       hipMalloc(reinterpret_cast<void **>(&d_partial_sums), partial_sz));
-  checkCudaErrors(hipMemset(d_partial_sums, 0, partial_sz));
+  HIPCHECK(hipMemset(d_partial_sums, 0, partial_sz));
 
-  checkCudaErrors(
+  HIPCHECK(
       hipHostMalloc(reinterpret_cast<void **>(&h_partial_sums), partial_sz));
-  checkCudaErrors(hipMemcpy(d_data, h_data, sz, hipMemcpyHostToDevice));
+  HIPCHECK(hipMemcpy(d_data, h_data, sz, hipMemcpyHostToDevice));
 
-  checkCudaErrors(hipEventRecord(start, 0));
+  HIPCHECK(hipEventRecord(start, 0));
   shfl_scan_test<<<gridSize, blockSize, shmem_sz>>>(d_data, 32, d_partial_sums);
   shfl_scan_test<<<p_gridSize, p_blockSize, shmem_sz>>>(d_partial_sums, 32);
   uniform_add<<<gridSize - 1, blockSize>>>(d_data + blockSize, d_partial_sums,
                                            n_elements);
-  checkCudaErrors(hipEventRecord(stop, 0));
-  checkCudaErrors(hipEventSynchronize(stop));
-  checkCudaErrors(hipEventElapsedTime(&inc, start, stop));
+  HIPCHECK(hipEventRecord(stop, 0));
+  HIPCHECK(hipEventSynchronize(stop));
+  HIPCHECK(hipEventElapsedTime(&inc, start, stop));
   et += inc;
 
-  checkCudaErrors(hipMemcpy(h_result, d_data, sz, hipMemcpyDeviceToHost));
-  checkCudaErrors(hipMemcpy(h_partial_sums, d_partial_sums, partial_sz,
+  HIPCHECK(hipMemcpy(h_result, d_data, sz, hipMemcpyDeviceToHost));
+  HIPCHECK(hipMemcpy(h_partial_sums, d_partial_sums, partial_sz,
                              hipMemcpyDeviceToHost));
 
   printf("Test Sum: %d\n", h_partial_sums[n_partialSums - 1]);
@@ -294,11 +294,11 @@ bool shuffle_simple_test(int argc, char **argv) {
 
   bool bTestResult = CPUverify(h_data, h_result, n_elements);
 
-  checkCudaErrors(hipHostFree(h_data));
-  checkCudaErrors(hipHostFree(h_result));
-  checkCudaErrors(hipHostFree(h_partial_sums));
-  checkCudaErrors(hipFree(d_data));
-  checkCudaErrors(hipFree(d_partial_sums));
+  HIPCHECK(hipHostFree(h_data));
+  HIPCHECK(hipHostFree(h_result));
+  HIPCHECK(hipHostFree(h_partial_sums));
+  HIPCHECK(hipFree(d_data));
+  HIPCHECK(hipFree(d_partial_sums));
 
   return bTestResult;
 }
@@ -317,7 +317,7 @@ bool shuffle_integral_image_test() {
   printf("\nComputing Integral Image Test on size %d x %d synthetic data\n", w,
          h);
   printf("---------------------------------------------------\n");
-  checkCudaErrors(hipHostMalloc(reinterpret_cast<void **>(&h_image), sz));
+  HIPCHECK(hipHostMalloc(reinterpret_cast<void **>(&h_image), sz));
   // fill test "image" with synthetic 1's data
   memset(h_image, 0, sz);
 
@@ -327,11 +327,11 @@ bool shuffle_integral_image_test() {
   int gridSize = h;
 
   // Create a synthetic image for testing
-  checkCudaErrors(hipMalloc(reinterpret_cast<void **>(&d_data), sz));
-  checkCudaErrors(hipMalloc(reinterpret_cast<void **>(&d_integral_image),
+  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_data), sz));
+  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_integral_image),
                              n_elements * sizeof(int) * 4));
-  checkCudaErrors(hipMemset(d_data, 1, sz));
-  checkCudaErrors(hipMemset(d_integral_image, 0, sz));
+  HIPCHECK(hipMemset(d_data, 1, sz));
+  HIPCHECK(hipMemset(d_integral_image, 0, sz));
 
   hipEvent_t start, stop;
   hipEventCreate(&start);
@@ -345,12 +345,12 @@ bool shuffle_integral_image_test() {
       reinterpret_cast<uint4 *>(d_data),
       reinterpret_cast<uint4 *>(d_integral_image));
   hipEventRecord(stop);
-  checkCudaErrors(hipEventSynchronize(stop));
-  checkCudaErrors(hipEventElapsedTime(&et, start, stop));
+  HIPCHECK(hipEventSynchronize(stop));
+  HIPCHECK(hipEventElapsedTime(&et, start, stop));
   printf("Method: Fast  Time (GPU Timer): %f ms ", et);
 
   // verify the scan line results
-  checkCudaErrors(
+  HIPCHECK(
       hipMemcpy(h_image, d_integral_image, sz, hipMemcpyDeviceToHost));
   err = verifyDataRowSums(h_image, w, h);
   printf("Diff = %d\n", err);
@@ -363,21 +363,21 @@ bool shuffle_integral_image_test() {
   shfl_vertical_shfl<<<testGrid, blockSz>>>((unsigned int *)d_integral_image, w,
                                             h);
   hipEventRecord(stop);
-  checkCudaErrors(hipEventSynchronize(stop));
-  checkCudaErrors(hipEventElapsedTime(&et, start, stop));
+  HIPCHECK(hipEventSynchronize(stop));
+  HIPCHECK(hipEventElapsedTime(&et, start, stop));
   printf("Method: Vertical Scan  Time (GPU Timer): %f ms ", et);
 
   // Verify the column results
-  checkCudaErrors(
+  HIPCHECK(
       hipMemcpy(h_image, d_integral_image, sz, hipMemcpyDeviceToHost));
   printf("\n");
 
   int finalSum = h_image[w * h - 1];
   printf("CheckSum: %d, (expect %dx%d=%d)\n", finalSum, w, h, w * h);
 
-  checkCudaErrors(hipFree(d_data));
-  checkCudaErrors(hipFree(d_integral_image));
-  checkCudaErrors(hipHostFree(h_image));
+  HIPCHECK(hipFree(d_data));
+  HIPCHECK(hipFree(d_integral_image));
+  HIPCHECK(hipHostFree(h_image));
   // verify final sum: if the final value in the corner is the same as the size
   // of the buffer (all 1's) then the integral image was generated successfully
   return (finalSum == w * h) ? true : false;
@@ -395,9 +395,9 @@ int main(int argc, char *argv[]) {
   cuda_device = findCudaDevice(argc, (const char **)argv);
 
   hipDeviceProp_t deviceProp;
-  checkCudaErrors(hipGetDevice(&cuda_device));
+  HIPCHECK(hipGetDevice(&cuda_device));
 
-  checkCudaErrors(hipGetDeviceProperties(&deviceProp, cuda_device));
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, cuda_device));
 
   printf("> Detected Compute SM %d.%d hardware with %d multi-processors\n",
          deviceProp.major, deviceProp.minor, deviceProp.multiProcessorCount);
@@ -477,3 +477,27 @@ ing test.\n");
 
   exit((bTestResult) ? EXIT_SUCCESS : EXIT_FAILURE);
 }
+ing test.\n");
+    exit(EXIT_WAIVED);
+  }
+
+  bool bTestResult = true;
+  bool simpleTest = shuffle_simple_test(argc, argv);
+  bool intTest = shuffle_integral_image_test();
+
+  bTestResult = simpleTest & intTest;
+
+  exit((bTestResult) ? EXIT_SUCCESS : EXIT_FAILURE);
+}
+ing test.\n");
+    exit(EXIT_WAIVED);
+  }
+
+  bool bTestResult = true;
+  bool simpleTest = shuffle_simple_test(argc, argv);
+  bool intTest = shuffle_integral_image_test();
+
+  bTestResult = simpleTest & intTest;
+
+  exit((bTestResult) ? EXIT_SUCCESS : EXIT_FAILURE);
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationIPC/streamOrderedAllocationIPC.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationIPC/streamOrderedAllocationIPC.cu.hip
index f549e15..f0e371d 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationIPC/streamOrderedAllocationIPC.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationIPC/streamOrderedAllocationIPC.cu.hip
@@ -475,3 +475,15 @@ d(_WIN32) || defined(WIN64) || defined(_WIN64)
   return EXIT_SUCCESS;
 #endif
 }
+d(_WIN32) || defined(WIN64) || defined(_WIN64)
+  printf("Not supported on ARM\n");
+  return EXIT_WAIVED;
+#else
+  if (argc == 1) {
+    parentProcess(argv[0]);
+  } else {
+    childProcess(atoi(argv[1]));
+  }
+  return EXIT_SUCCESS;
+#endif
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationP2P/streamOrderedAllocationP2P.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationP2P/streamOrderedAllocationP2P.cu.hip
index e69de29..7e94df3 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationP2P/streamOrderedAllocationP2P.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationP2P/streamOrderedAllocationP2P.cu.hip
@@ -0,0 +1,255 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * This sample demonstrates peer-to-peer access of stream ordered memory
+ * allocated with hipMallocAsync and cudaMemPool family of APIs through simple
+ * kernel which does peer-to-peer to access & scales vector elements.
+ */
+
+// System includes
+#include <assert.h>
+#include <stdio.h>
+#include "rocprofiler.h"
+#include "HIPCHECK.h"
+#include <iostream>
+#include <map>
+#include <set>
+#include <utility>
+
+// CUDA runtime
+#include <hip/hip_runtime.h>
+
+// helper functions and utilities to work with CUDA
+#include "helper_cuda_hipified.h"
+#include "helper_functions.h"
+
+// Simple kernel to demonstrate copying hipMallocAsync memory via P2P to peer
+// device
+__global__ void copyP2PAndScale(const int *src, int *dst, int N) {
+  int idx = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (idx < N) {
+    // scale & store src vector.
+    dst[idx] = 2 * src[idx];
+  }
+}
+
+// Map of device version to device number
+std::multimap<std::pair<int, int>, int> getIdenticalGPUs() {
+  int numGpus = 0;
+  HIPCHECK(hipGetDeviceCount(&numGpus));
+
+  std::multimap<std::pair<int, int>, int> identicalGpus;
+
+  for (int i = 0; i < numGpus; i++) {
+    int isMemPoolSupported = 0;
+    HIPCHECK(hipDeviceGetAttribute(&isMemPoolSupported,
+                                           hipDeviceAttributeMemoryPoolsSupported, i));
+
+    // Filter unsupported devices
+    if (isMemPoolSupported) {
+      int major = 0, minor = 0;
+      HIPCHECK(
+          hipDeviceGetAttribute(&major, hipDeviceAttributeComputeCapabilityMajor, i));
+      HIPCHECK(
+          hipDeviceGetAttribute(&minor, hipDeviceAttributeComputeCapabilityMinor, i));
+      identicalGpus.emplace(std::make_pair(major, minor), i);
+    }
+  }
+
+  return identicalGpus;
+}
+
+std::pair<int, int> getP2PCapableGpuPair() {
+  constexpr size_t kNumGpusRequired = 2;
+
+  auto gpusByArch = getIdenticalGPUs();
+
+  auto it = gpusByArch.begin();
+  auto end = gpusByArch.end();
+
+  auto bestFit = std::make_pair(it, it);
+  // use std::distance to find the largest number of GPUs amongst architectures
+  auto distance = [](decltype(bestFit) p) {
+    return std::distance(p.first, p.second);
+  };
+
+  // Read each unique key/pair element in order
+  for (; it != end; it = gpusByArch.upper_bound(it->first)) {
+    // first and second are iterators bounded within the architecture group
+    auto testFit = gpusByArch.equal_range(it->first);
+    // Always use devices with highest architecture version or whichever has the
+    // most devices available
+    if (distance(bestFit) <= distance(testFit)) bestFit = testFit;
+  }
+
+  if (distance(bestFit) < kNumGpusRequired) {
+    printf(
+        "No Two or more GPUs with same architecture capable of cuda Memory "
+        "Pools found."
+        "\nWaiving the sample\n");
+    exit(EXIT_WAIVED);
+  }
+
+  std::set<int> bestFitDeviceIds;
+
+  // check & select peer-to-peer access capable GPU devices.
+  int devIds[2];
+  for (auto itr = bestFit.first; itr != bestFit.second; itr++) {
+    int deviceId = itr->second;
+    HIPCHECK(hipSetDevice(deviceId));
+
+    std::for_each(itr, bestFit.second, [&deviceId, &bestFitDeviceIds,
+                                        &kNumGpusRequired](
+                                           decltype(*itr) mapPair) {
+      if (deviceId != mapPair.second) {
+        int access = 0;
+        HIPCHECK(
+            hipDeviceCanAccessPeer(&access, deviceId, mapPair.second));
+        printf("Device=%d %s Access Peer Device=%d\n", deviceId,
+               access ? "CAN" : "CANNOT", mapPair.second);
+        if (access && bestFitDeviceIds.size() < kNumGpusRequired) {
+          bestFitDeviceIds.emplace(deviceId);
+          bestFitDeviceIds.emplace(mapPair.second);
+        } else {
+          printf("Ignoring device %i (max devices exceeded)\n", mapPair.second);
+        }
+      }
+    });
+
+    if (bestFitDeviceIds.size() >= kNumGpusRequired) {
+      printf("Selected p2p capable devices - ");
+      int i = 0;
+      for (auto devicesItr = bestFitDeviceIds.begin();
+           devicesItr != bestFitDeviceIds.end(); devicesItr++) {
+        devIds[i++] = *devicesItr;
+        printf("deviceId = %d  ", *devicesItr);
+      }
+      printf("\n");
+      break;
+    }
+  }
+
+  // if bestFitDeviceIds.size() == 0 it means the GPUs in system are not p2p
+  // capable, hence we add it without p2p capability check.
+  if (!bestFitDeviceIds.size()) {
+    printf("No Two or more Devices p2p capable found.. exiting..\n");
+    exit(EXIT_WAIVED);
+  }
+
+  auto p2pGpuPair = std::make_pair(devIds[0], devIds[1]);
+
+  return p2pGpuPair;
+}
+
+int memPoolP2PCopy() {
+  int *dev0_srcVec, *dev1_dstVec;  // Device buffers
+  hipStream_t stream1, stream2;
+  hipMemPool_t memPool;
+  hipEvent_t waitOnStream1;
+
+  // Allocate CPU memory.
+  size_t nelem = 1048576;
+  size_t bytes = nelem * sizeof(int);
+
+  int *a = (int *)malloc(bytes);
+  int *output = (int *)malloc(bytes);
+
+  /* Initialize the vectors. */
+  for (int n = 0; n < nelem; n++) {
+    a[n] = rand() / (int)RAND_MAX;
+  }
+
+  auto p2pDevices = getP2PCapableGpuPair();
+  printf("selected devices = %d & %d\n", p2pDevices.first, p2pDevices.second);
+  HIPCHECK(hipSetDevice(p2pDevices.first));
+  HIPCHECK(hipEventCreate(&waitOnStream1));
+
+  HIPCHECK(hipStreamCreateWithFlags(&stream1, hipStreamNonBlocking));
+
+  // Get the default mempool for device p2pDevices.first from the pair
+  HIPCHECK(hipDeviceGetDefaultMemPool(&memPool, p2pDevices.first));
+
+  // Allocate memory in a stream from the pool set above.
+  HIPCHECK(hipMallocAsync(&dev0_srcVec, bytes, stream1));
+
+  HIPCHECK(
+      hipMemcpyAsync(dev0_srcVec, a, bytes, hipMemcpyHostToDevice, stream1));
+  HIPCHECK(hipEventRecord(waitOnStream1, stream1));
+
+  HIPCHECK(hipSetDevice(p2pDevices.second));
+  HIPCHECK(hipStreamCreateWithFlags(&stream2, hipStreamNonBlocking));
+
+  // Allocate memory in p2pDevices.second device
+  HIPCHECK(hipMallocAsync(&dev1_dstVec, bytes, stream2));
+
+  // Setup peer mappings for p2pDevices.second device
+  hipMemAccessDesc desc;
+  memset(&desc, 0, sizeof(hipMemAccessDesc));
+  desc.location.type = hipMemLocationTypeDevice;
+  desc.location.id = p2pDevices.second;
+  desc.flags = hipMemAccessFlagsProtReadWrite;
+  HIPCHECK(hipMemPoolSetAccess(memPool, &desc, 1));
+
+  printf("> copyP2PAndScale kernel running ...\n");
+  dim3 block(256);
+  dim3 grid((unsigned int)ceil(nelem / (int)block.x));
+  HIPCHECK(hipStreamWaitEvent(stream2, waitOnStream1,3));
+  copyP2PAndScale<<<grid, block, 0, stream2>>>(dev0_srcVec, dev1_dstVec, nelem);
+
+  HIPCHECK(hipMemcpyAsync(output, dev1_dstVec, bytes,
+                                  hipMemcpyDeviceToHost, stream2));
+  HIPCHECK(hipFreeAsync(dev0_srcVec, stream2));
+  HIPCHECK(hipFreeAsync(dev1_dstVec, stream2));
+  HIPCHECK(hipStreamSynchronize(stream2));
+
+  /* Compare the results */
+  printf("> Checking the results from copyP2PAndScale() ...\n");
+
+  for (int n = 0; n < nelem; n++) {
+    if ((2 * a[n]) != output[n]) {
+      printf("mismatch i = %d expected = %d val = %d\n", n, 2 * a[n],
+             output[n]);
+      return EXIT_FAILURE;
+    }
+  }
+
+  free(a);
+  free(output);
+  HIPCHECK(hipStreamDestroy(stream1));
+  HIPCHECK(hipStreamDestroy(stream2));
+  printf("PASSED\n");
+
+  return EXIT_SUCCESS;
+}
+
+int main(int argc, char **argv) {
+  int ret = memPoolP2PCopy();
+  return ret;
+}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationP2P/streamOrderedAllocationP2P.out b/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationP2P/streamOrderedAllocationP2P.out
index a14f0e8..3672df7 100755
Binary files a/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationP2P/streamOrderedAllocationP2P.out and b/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationP2P/streamOrderedAllocationP2P.out differ
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/threadMigration/threadMigration.out b/src/samples/Samples/2_Concepts_and_Techniques/threadMigration/threadMigration.out
index 12b9aed..6e20d01 100755
Binary files a/src/samples/Samples/2_Concepts_and_Techniques/threadMigration/threadMigration.out and b/src/samples/Samples/2_Concepts_and_Techniques/threadMigration/threadMigration.out differ
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/threadMigration/threadMigration_kernel.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/threadMigration/threadMigration_kernel.cu.hip
index e69de29..42cc050 100755
--- a/src/samples/Samples/2_Concepts_and_Techniques/threadMigration/threadMigration_kernel.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/threadMigration/threadMigration_kernel.cu.hip
@@ -0,0 +1,31 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+extern "C" __global__ void kernelFunction(int *input) {
+  input[threadIdx.x] = 32 - threadIdx.x;
+}
diff --git a/src/samples/Samples/3_CUDA_Features/StreamPriorities/StreamPriorities.cu.hip b/src/samples/Samples/3_CUDA_Features/StreamPriorities/StreamPriorities.cu.hip
index 2736b81..245cb2b 100755
--- a/src/samples/Samples/3_CUDA_Features/StreamPriorities/StreamPriorities.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/StreamPriorities/StreamPriorities.cu.hip
@@ -28,7 +28,7 @@
 
 // std::system includes
 #include <cstdio>
+#include "HIPCHECK.h"
+
 // CUDA-C includes
 #include <hip/hip_runtime.h>
 
diff --git a/src/samples/Samples/3_CUDA_Features/bf16TensorCoreGemm/bf16TensorCoreGemm.cu.hip b/src/samples/Samples/3_CUDA_Features/bf16TensorCoreGemm/bf16TensorCoreGemm.cu.hip
index 5719083..e573fb1 100755
--- a/src/samples/Samples/3_CUDA_Features/bf16TensorCoreGemm/bf16TensorCoreGemm.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/bf16TensorCoreGemm/bf16TensorCoreGemm.cu.hip
@@ -651,7 +651,7 @@ int main(int argc, char **argv)
     int dev = findCudaDevice(argc, (const char **)argv);
 
     hipDeviceProp_t deviceProp;
-    checkCudaErrors(hipGetDeviceProperties(&deviceProp, dev));
+    HIPCHECK(hipGetDeviceProperties(&deviceProp, dev));
 
     // Tensor cores require a GPU of Volta (SM8X) architecture or higher.
     if (deviceProp.major < 8) {
@@ -684,10 +684,10 @@ int main(int argc, char **argv)
     float *C = NULL;
     float *D = NULL;
 
-    checkCudaErrors(hipMalloc((void**)&A, sizeof(__nv_bfloat16) * M_GLOBAL * K_GLOBAL));
-    checkCudaErrors(hipMalloc((void**)&B, sizeof(__nv_bfloat16) * N_GLOBAL * K_GLOBAL));
-    checkCudaErrors(hipMalloc((void**)&C, sizeof(float) * M_GLOBAL * N_GLOBAL));
-    checkCudaErrors(hipMalloc((void**)&D, sizeof(float) * M_GLOBAL * N_GLOBAL));
+    HIPCHECK(hipMalloc((void**)&A, sizeof(__nv_bfloat16) * M_GLOBAL * K_GLOBAL));
+    HIPCHECK(hipMalloc((void**)&B, sizeof(__nv_bfloat16) * N_GLOBAL * K_GLOBAL));
+    HIPCHECK(hipMalloc((void**)&C, sizeof(float) * M_GLOBAL * N_GLOBAL));
+    HIPCHECK(hipMalloc((void**)&D, sizeof(float) * M_GLOBAL * N_GLOBAL));
 
     assert(((unsigned long long)A) % 128 == 0);
     assert(((unsigned long long)B) % 128 == 0);
@@ -698,10 +698,10 @@ int main(int argc, char **argv)
 
     printf("Preparing data for GPU...\n");
 
-    checkCudaErrors(hipMemcpy(A, A_h, sizeof(__nv_bfloat16) * M_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
-    checkCudaErrors(hipMemcpy(B, B_h, sizeof(__nv_bfloat16) * N_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
-    checkCudaErrors(hipMemcpy(C, C_h, sizeof(float) * M_GLOBAL * N_GLOBAL, hipMemcpyHostToDevice));
-    checkCudaErrors(hipMemset(D, 0, sizeof(float) * M_GLOBAL * N_GLOBAL));
+    HIPCHECK(hipMemcpy(A, A_h, sizeof(__nv_bfloat16) * M_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
+    HIPCHECK(hipMemcpy(B, B_h, sizeof(__nv_bfloat16) * N_GLOBAL * K_GLOBAL, hipMemcpyHostToDevice));
+    HIPCHECK(hipMemcpy(C, C_h, sizeof(float) * M_GLOBAL * N_GLOBAL, hipMemcpyHostToDevice));
+    HIPCHECK(hipMemset(D, 0, sizeof(float) * M_GLOBAL * N_GLOBAL));
 
     enum {
         // Compute the right amount of shared memory to request.
@@ -719,9 +719,9 @@ int main(int argc, char **argv)
 
     hipEvent_t start, stop;
 
-    checkCudaErrors(hipEventCreate(&start));
-    checkCudaErrors(hipEventCreate(&stop));
-    checkCudaErrors(hipEventRecord(start));
+    HIPCHECK(hipEventCreate(&start));
+    HIPCHECK(hipEventCreate(&stop));
+    HIPCHECK(hipEventRecord(start));
 
     // kernel to run - default (b16mma_shmem_gemm_async_copy == 0)
     kernels selected_kernel = bf16mma_shmem_gemm_async_copy;
@@ -745,16 +745,16 @@ int main(int argc, char **argv)
         {
             case bf16mma_shmem_gemm_async_copy :
             default:
-                checkCudaErrors(hipFuncSetAttribute(compute_bf16gemm_async_copy, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
+                HIPCHECK(hipFuncSetAttribute((const void *)compute_bf16gemm_async_copy, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
                 checkKernelErrors((compute_bf16gemm_async_copy<<<deviceProp.multiProcessorCount*2, THREADS_PER_BLOCK, SHMEM_SZ>>>(A, B, C, D, alpha, beta)));
                 break;
             case bf16mma_shmem_gemm :
-                checkCudaErrors(hipFuncSetAttribute(compute_bf16gemm, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
+                HIPCHECK(hipFuncSetAttribute((const void *)compute_bf16gemm, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
                 checkKernelErrors((compute_bf16gemm<<<deviceProp.multiProcessorCount*2, THREADS_PER_BLOCK, SHMEM_SZ>>>(A, B, C, D, alpha, beta)));
                 break;
         }
 #if CPU_DEBUG
-        checkCudaErrors(hipMemcpy(result_hD, D, sizeof(float)*M_GLOBAL*N_GLOBAL, hipMemcpyDeviceToHost));
+        HIPCHECK(hipMemcpy(result_hD, D, sizeof(float)*M_GLOBAL*N_GLOBAL, hipMemcpyDeviceToHost));
 #endif
     }
     else {
@@ -772,12 +772,12 @@ int main(int argc, char **argv)
         printf("Computing... using simple_wmma_gemm kernel\n");
         simple_wmma_bf16gemm<<<gridDim, blockDim>>>(A, B, C, D, M_GLOBAL, N_GLOBAL, K_GLOBAL, alpha, beta);
 #if CPU_DEBUG
-        checkCudaErrors(hipMemcpy(result_hD, D, sizeof(float) * M_GLOBAL * N_GLOBAL, hipMemcpyDeviceToHost));
+        HIPCHECK(hipMemcpy(result_hD, D, sizeof(float) * M_GLOBAL * N_GLOBAL, hipMemcpyDeviceToHost));
 #endif
     }
 
-    checkCudaErrors(hipEventRecord(stop));
-    checkCudaErrors(hipEventSynchronize(stop));
+    HIPCHECK(hipEventRecord(stop));
+    HIPCHECK(hipEventSynchronize(stop));
 
 #if CPU_DEBUG
     printf("Verifying correctness of the computations...\n");
@@ -801,7 +801,7 @@ int main(int argc, char **argv)
 
     float milliseconds = 0;
 
-    checkCudaErrors(hipEventElapsedTime(&milliseconds, start, stop));
+    HIPCHECK(hipEventElapsedTime(&milliseconds, start, stop));
 
     printf("Time: %f ms\n", milliseconds);
     printf("TFLOPS: %.2f\n", (((double)M_GLOBAL * N_GLOBAL * K_GLOBAL * 2)/(milliseconds/1000.)) / 1e12);
diff --git a/src/samples/Samples/3_CUDA_Features/cdpQuadtree/cdpQuadtree.cu.hip b/src/samples/Samples/3_CUDA_Features/cdpQuadtree/cdpQuadtree.cu.hip
index a4ffd7e..aff7fa7 100755
--- a/src/samples/Samples/3_CUDA_Features/cdpQuadtree/cdpQuadtree.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cdpQuadtree/cdpQuadtree.cu.hip
@@ -32,7 +32,7 @@
 #include <hip/hip_cooperative_groups.h>
 
 namespace cg = cooperative_groups;
-#include <helper_cuda.h>
+#include "helper_cuda_hipified.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 // A structure of 2D points (structure of arrays).
@@ -654,8 +654,8 @@ bool cdpQuadtree(int warp_size) {
 
   // Allocate memory to store points.
   Points *points;
-  checkCudaErrors(hipMalloc((void **)&points, 2 * sizeof(Points)));
-  checkCudaErrors(hipMemcpy(points, points_init, 2 * sizeof(Points),
+  HIPCHECK(hipMalloc((void **)&points, 2 * sizeof(Points)));
+  HIPCHECK(hipMemcpy(points, points_init, 2 * sizeof(Points),
                              hipMemcpyHostToDevice));
 
   // We could use a close form...
@@ -669,14 +669,11 @@ bool cdpQuadtree(int warp_size) {
   Quadtree_node root;
   root.set_range(0, num_points);
   Quadtree_node *nodes;
-  checkCudaErrors(
+  HIPCHECK(
       hipMalloc((void **)&nodes, max_nodes * sizeof(Quadtree_node)));
-  checkCudaErrors(
+  HIPCHECK(
       hipMemcpy(nodes, &root, sizeof(Quadtree_node), hipMemcpyHostToDevice));
 
-  // We set the recursion limit for CDP to max_depth.
-  hipDeviceSetLimit(cudaLimitDevRuntimeSyncDepth, max_depth);
-
   // Build the quadtree.
   Parameters params(max_depth, min_points_per_node);
   std::cout << "Launching CDP kernel to build the quadtree" << std::endl;
@@ -686,7 +683,7 @@ bool cdpQuadtree(int warp_size) {
   build_quadtree_kernel<
       NUM_THREADS_PER_BLOCK><<<1, NUM_THREADS_PER_BLOCK, smem_size>>>(
       nodes, points, params);
-  checkCudaErrors(hipGetLastError());
+  HIPCHECK(hipGetLastError());
 
   // Copy points to CPU.
   thrust::host_vector<float> x_h(x_d0);
@@ -697,7 +694,7 @@ bool cdpQuadtree(int warp_size) {
 
   // Copy nodes to CPU.
   Quadtree_node *host_nodes = new Quadtree_node[max_nodes];
-  checkCudaErrors(hipMemcpy(host_nodes, nodes,
+  HIPCHECK(hipMemcpy(host_nodes, nodes,
                              max_nodes * sizeof(Quadtree_node),
                              hipMemcpyDeviceToHost));
 
@@ -709,8 +706,8 @@ bool cdpQuadtree(int warp_size) {
   delete[] host_nodes;
 
   // Free memory.
-  checkCudaErrors(hipFree(nodes));
-  checkCudaErrors(hipFree(points));
+  HIPCHECK(hipFree(nodes));
+  HIPCHECK(hipFree(points));
 
   return ok;
 }
@@ -723,7 +720,7 @@ int main(int argc, char **argv) {
   // The test requires an architecture SM35 or greater (CDP capable).
   int cuda_device = findCudaDevice(argc, (const char **)argv);
   hipDeviceProp_t deviceProps;
-  checkCudaErrors(hipGetDeviceProperties(&deviceProps, cuda_device));
+  HIPCHECK(hipGetDeviceProperties(&deviceProps, cuda_device));
   int cdpCapable = (deviceProps.major == 3 && deviceProps.minor >= 5) ||
                    deviceProps.major >= 4;
 
@@ -753,3 +750,7 @@ ops.warpSize);
 
   return (ok ? EXIT_SUCCESS : EXIT_FAILURE);
 }
+ops.warpSize);
+
+  return (ok ? EXIT_SUCCESS : EXIT_FAILURE);
+}
diff --git a/src/samples/Samples/3_CUDA_Features/cudaCompressibleMemory/saxpy.cu.hip b/src/samples/Samples/3_CUDA_Features/cudaCompressibleMemory/saxpy.cu.hip
index d2da2fc..a908033 100755
--- a/src/samples/Samples/3_CUDA_Features/cudaCompressibleMemory/saxpy.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cudaCompressibleMemory/saxpy.cu.hip
@@ -121,22 +121,22 @@ int main(int argc, char **argv)
 
     findCudaDevice(argc, (const char**)argv);
     hipDevice_t currentDevice;
-    checkCudaErrors(hipCtxGetDevice(&currentDevice));
+    HIPCHECK(hipCtxGetDevice(&currentDevice));
 
     // Check that the selected device supports virtual memory management
     int vmm_supported = -1;
-    checkCudaErrors(hipDeviceGetAttribute(&vmm_supported,
-                          CU_DEVICE_ATTRIBUTE_VIRTUAL_ADDRESS_MANAGEMENT_SUPPORTED,
-                          currentDevice));
+   // HIPCHECK(hipDeviceGetAttribute(&vmm_supported,
+   //                       CU_DEVICE_ATTRIBUTE_VIRTUAL_ADDRESS_MANAGEMENT_SUPPORTED,
+   //                       currentDevice));
     if (vmm_supported == 0) {
         printf("Device %d doesn't support Virtual Memory Management, waiving the execution.\n", currentDevice);
         exit(EXIT_WAIVED);
     }
 
     int isCompressionAvailable;
-    checkCudaErrors(hipDeviceGetAttribute(&isCompressionAvailable,
-                             CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED,
-                             currentDevice));
+   // HIPCHECK(hipDeviceGetAttribute(&isCompressionAvailable,
+   //                          CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED,
+   //                          currentDevice));
     if (isCompressionAvailable == 0)
     {
         printf("Device %d doesn't support Generic memory compression, waiving the execution.\n", currentDevice);
diff --git a/src/samples/Samples/3_CUDA_Features/cudaTensorCoreGemm/cudaTensorCoreGemm.cu.hip b/src/samples/Samples/3_CUDA_Features/cudaTensorCoreGemm/cudaTensorCoreGemm.cu.hip
index 7eebfb0..ad486b0 100755
--- a/src/samples/Samples/3_CUDA_Features/cudaTensorCoreGemm/cudaTensorCoreGemm.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cudaTensorCoreGemm/cudaTensorCoreGemm.cu.hip
@@ -660,3 +660,9 @@ rpret_cast<void *>(B)));
 
   return 0;
 }
+rpret_cast<void *>(B)));
+  checkCudaErrors(hipFree(reinterpret_cast<void *>(C)));
+  checkCudaErrors(hipFree(reinterpret_cast<void *>(D)));
+
+  return 0;
+}
diff --git a/src/samples/Samples/3_CUDA_Features/graphMemoryNodes/graphMemoryNodes.cu.hip b/src/samples/Samples/3_CUDA_Features/graphMemoryNodes/graphMemoryNodes.cu.hip
index e69de29..e1d90f4 100755
--- a/src/samples/Samples/3_CUDA_Features/graphMemoryNodes/graphMemoryNodes.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/graphMemoryNodes/graphMemoryNodes.cu.hip
@@ -0,0 +1,555 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// System includes
+#include <assert.h>
+#include <stdio.h>
+
+#include <climits>
+#include <vector>
+
+// CUDA runtime
+#include <hip/hip_runtime.h>
+
+// helper functions and utilities to work with CUDA
+#include <helper_cuda.h>
+#include <helper_functions.h>
+
+#define THREADS_PER_BLOCK 512
+#define ALLOWABLE_VARIANCE 1.e-6f
+#define NUM_ELEMENTS 8000000
+
+// Stores the square of each input element in output array
+__global__ void squareArray(const float *input, float *output,
+                            int numElements) {
+  int idx = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (idx < numElements) {
+    output[idx] = input[idx] * input[idx];
+  }
+}
+
+// Stores the negative of each input element in output array
+__global__ void negateArray(const float *input, float *output,
+                            int numElements) {
+  int idx = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (idx < numElements) {
+    output[idx] = input[idx] * -1;
+  }
+}
+
+struct negSquareArrays {
+  float *input;
+  float *square;
+  float *negSquare;
+  int numElements;
+  size_t bytes;
+  size_t numBlocks;
+};
+
+void fillRandomly(float *array, int numElements) {
+  for (int n = 0; n < numElements; n++) {
+    array[n] = rand() / (float)RAND_MAX;
+  }
+}
+
+void resetOutputArrays(negSquareArrays *hostArrays) {
+  fillRandomly(hostArrays->square, hostArrays->numElements);
+  fillRandomly(hostArrays->negSquare, hostArrays->numElements);
+}
+
+void prepareHostArrays(negSquareArrays *hostArrays) {
+  hostArrays->numElements = NUM_ELEMENTS;
+  size_t bytes = hostArrays->numElements * sizeof(float);
+
+  size_t numBlocks = hostArrays->numElements / (size_t)THREADS_PER_BLOCK;
+  if ((numBlocks % (size_t)THREADS_PER_BLOCK) != 0) {
+    numBlocks++;
+  }
+
+  hostArrays->input = (float *)malloc(bytes);
+  hostArrays->square = (float *)malloc(bytes);
+  hostArrays->negSquare = (float *)malloc(bytes);
+  hostArrays->bytes = bytes;
+  hostArrays->numBlocks = numBlocks;
+
+  fillRandomly(hostArrays->input, hostArrays->numElements);
+  fillRandomly(hostArrays->square, hostArrays->numElements);
+  fillRandomly(hostArrays->negSquare, hostArrays->numElements);
+}
+
+void createFreeGraph(hipGraphExec_t *graphExec, float *dPtr) {
+  hipGraph_t graph;
+  hipGraphNode_t freeNode;
+
+  checkCudaErrors(hipGraphCreate(&graph, 0));
+
+  checkCudaErrors(
+      cudaGraphAddMemFreeNode(&freeNode, graph, NULL, 0, (void *)dPtr));
+
+  checkCudaErrors(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
+  checkCudaErrors(hipGraphDestroy(graph));
+}
+
+/**
+ * Demonstrates explicitly creating a CUDA graph including memory nodes.
+ * createNegateSquaresGraphWithStreamCapture constructs an equivalent graph
+ * using stream capture.
+ *
+ * If d_negSquare_out is non null, then:
+ * 1) d_negSquare will not be freed;
+ * 2) the value of d_negSquare_out will be set to d_negSquare.
+ *
+ * Diagram of the graph constructed by createNegateSquaresGraphExplicitly:
+ *
+ * alloc d_input
+ *       |
+ * alloc d_square
+ *       |
+ * Memcpy a to device
+ *       |
+ * launch kernel squareArray ------->---- Memcpy d_square to host
+ *       |                                      |
+ * free d_input                                 |
+ *       |                                      |
+ * allocate d_negSquare                         |
+ *       |                                      |
+ * launch kernel negateArray -------->--- free d_square
+ *       |
+ * Memcpy d_negSquare to host
+ *       |
+ * free d_negSquare
+ */
+void createNegateSquaresGraphExplicitly(hipGraphExec_t *graphExec, int device,
+                                        negSquareArrays *hostArrays,
+                                        float **d_negSquare_out = NULL) {
+  // Array buffers on device
+  float *d_input, *d_square, *d_negSquare;
+
+  // Memory allocation parameters
+  cudaMemAllocNodeParams allocParams;
+  memset(&allocParams, 0, sizeof(allocParams));
+  allocParams.bytesize = hostArrays->bytes;
+  allocParams.poolProps.allocType = hipMemAllocationTypePinned;
+  allocParams.poolProps.location.id = device;
+  allocParams.poolProps.location.type = hipMemLocationTypeDevice;
+
+  // Kernel launch parameters
+  hipKernelNodeParams kernelNodeParams = {0};
+  kernelNodeParams.gridDim = dim3(hostArrays->numBlocks, 1, 1);
+  kernelNodeParams.blockDim = dim3(THREADS_PER_BLOCK, 1, 1);
+  kernelNodeParams.sharedMemBytes = 0;
+  kernelNodeParams.extra = NULL;
+
+  hipGraph_t graph;
+  hipGraphNode_t allocNodeInput, allocNodeSquare, allocNodeNegSquare;
+  hipGraphNode_t copyNodeInput, copyNodeSquare, copyNodeNegSquare;
+  hipGraphNode_t squareKernelNode, negateKernelNode;
+  hipGraphNode_t freeNodeInput, freeNodeSquare;
+
+  // Buffer for storing graph node dependencies
+  std::vector<hipGraphNode_t> nodeDependencies;
+
+  checkCudaErrors(hipGraphCreate(&graph, 0));
+
+  checkCudaErrors(
+      cudaGraphAddMemAllocNode(&allocNodeInput, graph, NULL, 0, &allocParams));
+  d_input = (float *)allocParams.dptr;
+
+  // To keep the graph structure simple (fewer branching dependencies),
+  // allocNodeSquare should depend on allocNodeInput
+  checkCudaErrors(cudaGraphAddMemAllocNode(&allocNodeSquare, graph,
+                                           &allocNodeInput, 1, &allocParams));
+  d_square = (float *)allocParams.dptr;
+
+  // copyNodeInput needs to depend on allocNodeInput because copyNodeInput
+  // writes to d_input. It does so here indirectly through allocNodeSquare.
+  checkCudaErrors(hipGraphAddMemcpyNode1D(
+      &copyNodeInput, graph, &allocNodeSquare, 1, d_input, hostArrays->input,
+      hostArrays->bytes, hipMemcpyHostToDevice));
+
+  void *squareKernelArgs[3] = {(void *)&d_input, (void *)&d_square,
+                               (void *)&(hostArrays->numElements)};
+  kernelNodeParams.func = (void *)squareArray;
+  kernelNodeParams.kernelParams = (void **)squareKernelArgs;
+
+  // Square kernel depends on copyNodeInput to ensure all data is on the device
+  // before kernel launch.
+  checkCudaErrors(hipGraphAddKernelNode(&squareKernelNode, graph,
+                                         &copyNodeInput, 1, &kernelNodeParams));
+
+  checkCudaErrors(hipGraphAddMemcpyNode1D(
+      &copyNodeSquare, graph, &squareKernelNode, 1, hostArrays->square,
+      d_square, hostArrays->bytes, hipMemcpyDeviceToHost));
+
+  // Free of d_input depends on the square kernel to ensure that d_input is not
+  // freed while being read by the kernel. It also depends on the alloc of
+  // d_input via squareKernelNode > copyNodeInput > allocNodeSquare >
+  // allocNodeInput.
+  checkCudaErrors(cudaGraphAddMemFreeNode(&freeNodeInput, graph,
+                                          &squareKernelNode, 1, d_input));
+
+  // Allocation of C depends on free of A so CUDA can reuse the virtual address.
+  checkCudaErrors(cudaGraphAddMemAllocNode(&allocNodeNegSquare, graph,
+                                           &freeNodeInput, 1, &allocParams));
+  d_negSquare = (float *)allocParams.dptr;
+
+  if (d_negSquare == d_input) {
+    printf(
+        "Check verified that d_negSquare and d_input share a virtual "
+        "address.\n");
+  }
+
+  void *negateKernelArgs[3] = {(void *)&d_square, (void *)&d_negSquare,
+                               (void *)&(hostArrays->numElements)};
+  kernelNodeParams.func = (void *)negateArray;
+  kernelNodeParams.kernelParams = (void **)negateKernelArgs;
+
+  checkCudaErrors(hipGraphAddKernelNode(
+      &negateKernelNode, graph, &allocNodeNegSquare, 1, &kernelNodeParams));
+
+  nodeDependencies.push_back(copyNodeSquare);
+  nodeDependencies.push_back(negateKernelNode);
+  checkCudaErrors(cudaGraphAddMemFreeNode(&freeNodeSquare, graph,
+                                          nodeDependencies.data(),
+                                          nodeDependencies.size(), d_square));
+  nodeDependencies.clear();
+
+  checkCudaErrors(hipGraphAddMemcpyNode1D(
+      &copyNodeNegSquare, graph, &negateKernelNode, 1, hostArrays->negSquare,
+      d_negSquare, hostArrays->bytes, hipMemcpyDeviceToHost));
+
+  if (d_negSquare_out == NULL) {
+    hipGraphNode_t freeNodeNegSquare;
+    checkCudaErrors(cudaGraphAddMemFreeNode(
+        &freeNodeNegSquare, graph, &copyNodeNegSquare, 1, d_negSquare));
+  } else {
+    *d_negSquare_out = d_negSquare;
+  }
+
+  checkCudaErrors(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
+  checkCudaErrors(hipGraphDestroy(graph));
+}
+
+/**
+ * Adds work to a CUDA stream which negates the square of values in the input
+ * array.
+ *
+ * If d_negSquare_out is non null, then:
+ * 1) d_negSquare will not be freed;
+ * 2) the value of d_negSquare_out will be set to d_negSquare.
+ *
+ * Diagram of the stream operations in doNegateSquaresInStream
+ * ---------------------------------------------------------------------
+ * | STREAM                             | STREAM2                      |
+ * ---------------------------------------------------------------------
+ *
+ * alloc d_input
+ *       |
+ * alloc d_square
+ *       |
+ * Memcpy a to device
+ *       |
+ * launch kernel squareArray
+ *       |
+ * record squareKernelCompleteEvent -->-- wait squareKernelCompleteEvent
+ *       |                                      |
+ * free d_input                                 |
+ *       |                                      |
+ * allocate d_negSquare                   Memcpy d_square to host
+ *       |                                      |
+ * launch kernel negateArray                    |
+ *       |                                      |
+ * record negateKernelCompleteEvent -->-- wait negateKernelCompleteEvent
+ *       |                                      |
+ * Memcpy d_negSquare to host                   |
+ *       |                                free d_square
+ * free d_negSquare                             |
+ *       |                                      |
+ * wait squareFreeEvent --------------<---- record squareFreeEvent
+ */
+void doNegateSquaresInStream(hipStream_t stream1, negSquareArrays *hostArrays,
+                             float **d_negSquare_out = NULL) {
+  float *d_input, *d_square, *d_negSquare;
+  hipStream_t stream2;
+  hipEvent_t squareKernelCompleteEvent, negateKernelCompleteEvent,
+      squareFreeEvent;
+
+  checkCudaErrors(hipStreamCreateWithFlags(&stream2, hipStreamNonBlocking));
+
+  checkCudaErrors(hipEventCreate(&squareKernelCompleteEvent));
+  checkCudaErrors(hipEventCreate(&negateKernelCompleteEvent));
+  checkCudaErrors(hipEventCreate(&squareFreeEvent));
+
+  // Virtual addresses are assigned synchronously when hipMallocAsync is
+  // called, thus there is no performace benefit gained by separating the
+  // allocations into two streams.
+  checkCudaErrors(hipMallocAsync(&d_input, hostArrays->bytes, stream1));
+  checkCudaErrors(hipMallocAsync(&d_square, hostArrays->bytes, stream1));
+
+  checkCudaErrors(hipMemcpyAsync(d_input, hostArrays->input, hostArrays->bytes,
+                                  hipMemcpyHostToDevice, stream1));
+  squareArray<<<hostArrays->numBlocks, THREADS_PER_BLOCK, 0, stream1>>>(
+      d_input, d_square, hostArrays->numElements);
+  checkCudaErrors(hipEventRecord(squareKernelCompleteEvent, stream1));
+
+  checkCudaErrors(hipStreamWaitEvent(stream2, squareKernelCompleteEvent, 0));
+  checkCudaErrors(hipMemcpyAsync(hostArrays->square, d_square,
+                                  hostArrays->bytes, hipMemcpyDeviceToHost,
+                                  stream2));
+
+  checkCudaErrors(hipFreeAsync(d_input, stream1));
+  checkCudaErrors(hipMallocAsync(&d_negSquare, hostArrays->bytes, stream1));
+  negateArray<<<hostArrays->numBlocks, THREADS_PER_BLOCK, 0, stream1>>>(
+      d_square, d_negSquare, hostArrays->numElements);
+  checkCudaErrors(hipEventRecord(negateKernelCompleteEvent, stream1));
+  checkCudaErrors(hipMemcpyAsync(hostArrays->negSquare, d_negSquare,
+                                  hostArrays->bytes, hipMemcpyDeviceToHost,
+                                  stream1));
+  if (d_negSquare_out == NULL) {
+    checkCudaErrors(hipFreeAsync(d_negSquare, stream1));
+  } else {
+    *d_negSquare_out = d_negSquare;
+  }
+
+  checkCudaErrors(hipStreamWaitEvent(stream2, negateKernelCompleteEvent, 0));
+  checkCudaErrors(hipFreeAsync(d_square, stream2));
+  checkCudaErrors(hipEventRecord(squareFreeEvent, stream2));
+
+  checkCudaErrors(hipStreamWaitEvent(stream1, squareFreeEvent, 0));
+
+  checkCudaErrors(hipStreamDestroy(stream2));
+  checkCudaErrors(hipEventDestroy(squareKernelCompleteEvent));
+  checkCudaErrors(hipEventDestroy(negateKernelCompleteEvent));
+  checkCudaErrors(hipEventDestroy(squareFreeEvent));
+}
+
+/**
+ * Demonstrates creating a CUDA graph including memory nodes using stream
+ * capture. createNegateSquaresGraphExplicitly constructs an equivalent graph
+ * without stream capture.
+ */
+void createNegateSquaresGraphWithStreamCapture(hipGraphExec_t *graphExec,
+                                               negSquareArrays *hostArrays,
+                                               float **d_negSquare_out = NULL) {
+  hipGraph_t graph;
+  hipStream_t stream;
+
+  checkCudaErrors(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
+
+  checkCudaErrors(hipStreamBeginCapture(stream, hipStreamCaptureModeGlobal));
+  doNegateSquaresInStream(stream, hostArrays, d_negSquare_out);
+  checkCudaErrors(hipStreamEndCapture(stream, &graph));
+
+  checkCudaErrors(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
+  checkCudaErrors(hipStreamDestroy(stream));
+  checkCudaErrors(hipGraphDestroy(graph));
+}
+
+void prepareRefArrays(negSquareArrays *hostArrays,
+                      negSquareArrays *deviceRefArrays,
+                      bool **foundValidationFailure) {
+  deviceRefArrays->bytes = hostArrays->bytes;
+  deviceRefArrays->numElements = hostArrays->numElements;
+
+  for (int i = 0; i < hostArrays->numElements; i++) {
+    hostArrays->square[i] = hostArrays->input[i] * hostArrays->input[i];
+    hostArrays->negSquare[i] = hostArrays->square[i] * -1;
+  }
+
+  checkCudaErrors(
+      hipMalloc((void **)&deviceRefArrays->negSquare, deviceRefArrays->bytes));
+  checkCudaErrors(hipMemcpy(deviceRefArrays->negSquare, hostArrays->negSquare,
+                             hostArrays->bytes, hipMemcpyHostToDevice));
+
+  checkCudaErrors(
+      hipMallocManaged((void **)foundValidationFailure, sizeof(bool)));
+}
+
+int checkValidationFailure(bool *foundValidationFailure) {
+  if (*foundValidationFailure) {
+    printf("Validation FAILURE!\n\n");
+    *foundValidationFailure = false;
+    return EXIT_FAILURE;
+  } else {
+    printf("Validation PASSED!\n\n");
+    return EXIT_SUCCESS;
+  }
+}
+
+__global__ void validateGPU(float *d_negSquare, negSquareArrays devRefArrays,
+                            bool *foundValidationFailure) {
+  int idx = blockIdx.x * blockDim.x + threadIdx.x;
+  float ref, diff;
+
+  if (idx < devRefArrays.numElements) {
+    ref = devRefArrays.negSquare[idx];
+    diff = d_negSquare[idx] - ref;
+    diff *= diff;
+    ref *= ref;
+    if (diff / ref > ALLOWABLE_VARIANCE) {
+      *foundValidationFailure = true;
+    }
+  }
+}
+
+void validateHost(negSquareArrays *hostArrays, bool *foundValidationFailure) {
+  float ref, diff;
+
+  for (int i = 0; i < hostArrays->numElements; i++) {
+    ref = hostArrays->input[i] * hostArrays->input[i] * -1;
+    diff = hostArrays->negSquare[i] - ref;
+    diff *= diff;
+    ref *= ref;
+    if (diff / ref > ALLOWABLE_VARIANCE) {
+      *foundValidationFailure = true;
+    }
+  }
+}
+
+int main(int argc, char **argv) {
+  negSquareArrays hostArrays, deviceRefArrays;
+  hipStream_t stream;
+  hipGraphExec_t graphExec, graphExecFreeC;
+
+  // Declare pointers for GPU buffers
+  float *d_negSquare = NULL;
+  bool *foundValidationFailure = NULL;
+
+  srand(time(0));
+  int device = findCudaDevice(argc, (const char **)argv);
+
+  int driverVersion = 0;
+  int deviceSupportsMemoryPools = 0;
+
+  hipDriverGetVersion(&driverVersion);
+  printf("Driver version is: %d.%d\n", driverVersion / 1000,
+         (driverVersion % 100) / 10);
+
+  if (driverVersion < 11040) {
+    printf("Waiving execution as driver does not support Graph Memory Nodes\n");
+    exit(EXIT_WAIVED);
+  }
+
+  hipDeviceGetAttribute(&deviceSupportsMemoryPools,
+                         hipDeviceAttributeMemoryPoolsSupported, device);
+  if (!deviceSupportsMemoryPools) {
+    printf("Waiving execution as device does not support Memory Pools\n");
+    exit(EXIT_WAIVED);
+  } else {
+    printf("Setting up sample.\n");
+  }
+
+  prepareHostArrays(&hostArrays);
+  prepareRefArrays(&hostArrays, &deviceRefArrays, &foundValidationFailure);
+  checkCudaErrors(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
+  printf("Setup complete.\n\n");
+
+  printf("Running negateSquares in a stream.\n");
+  doNegateSquaresInStream(stream, &hostArrays);
+  checkCudaErrors(hipStreamSynchronize(stream));
+  printf("Validating negateSquares in a stream...\n");
+  validateHost(&hostArrays, foundValidationFailure);
+  checkValidationFailure(foundValidationFailure);
+  resetOutputArrays(&hostArrays);
+
+  printf("Running negateSquares in a stream-captured graph.\n");
+  createNegateSquaresGraphWithStreamCapture(&graphExec, &hostArrays);
+  checkCudaErrors(hipGraphLaunch(graphExec, stream));
+  checkCudaErrors(hipStreamSynchronize(stream));
+  printf("Validating negateSquares in a stream-captured graph...\n");
+  validateHost(&hostArrays, foundValidationFailure);
+  checkValidationFailure(foundValidationFailure);
+  resetOutputArrays(&hostArrays);
+
+  printf("Running negateSquares in an explicitly constructed graph.\n");
+  createNegateSquaresGraphExplicitly(&graphExec, device, &hostArrays);
+  checkCudaErrors(hipGraphLaunch(graphExec, stream));
+  checkCudaErrors(hipStreamSynchronize(stream));
+  printf("Validating negateSquares in an explicitly constructed graph...\n");
+  validateHost(&hostArrays, foundValidationFailure);
+  checkValidationFailure(foundValidationFailure);
+  resetOutputArrays(&hostArrays);
+
+  // Each of the three examples below free d_negSquare outside the graph. As
+  // demonstrated by validateGPU, d_negSquare can be accessed by outside the
+  // graph before d_negSquare is freed.
+
+  printf("Running negateSquares with d_negSquare freed outside the stream.\n");
+  createNegateSquaresGraphExplicitly(&graphExec, device, &hostArrays,
+                                     &d_negSquare);
+  checkCudaErrors(hipGraphLaunch(graphExec, stream));
+  validateGPU<<<hostArrays.numBlocks, THREADS_PER_BLOCK, 0, stream>>>(
+      d_negSquare, deviceRefArrays, foundValidationFailure);
+  // Since hipFree is synchronous, the stream must synchronize before freeing
+  // d_negSquare to ensure d_negSquare no longer being accessed.
+  checkCudaErrors(hipStreamSynchronize(stream));
+  checkCudaErrors(hipFree(d_negSquare));
+  printf(
+      "Validating negateSquares with d_negSquare freed outside the "
+      "stream...\n");
+  validateHost(&hostArrays, foundValidationFailure);
+  checkValidationFailure(foundValidationFailure);
+  resetOutputArrays(&hostArrays);
+
+  printf("Running negateSquares with d_negSquare freed outside the graph.\n");
+  checkCudaErrors(hipGraphLaunch(graphExec, stream));
+  validateGPU<<<hostArrays.numBlocks, THREADS_PER_BLOCK, 0, stream>>>(
+      d_negSquare, deviceRefArrays, foundValidationFailure);
+  checkCudaErrors(hipFreeAsync(d_negSquare, stream));
+  checkCudaErrors(hipStreamSynchronize(stream));
+  printf(
+      "Validating negateSquares with d_negSquare freed outside the graph...\n");
+  checkValidationFailure(foundValidationFailure);
+  resetOutputArrays(&hostArrays);
+
+  printf(
+      "Running negateSquares with d_negSquare freed in a different graph.\n");
+  createFreeGraph(&graphExecFreeC, d_negSquare);
+  checkCudaErrors(hipGraphLaunch(graphExec, stream));
+  validateGPU<<<hostArrays.numBlocks, THREADS_PER_BLOCK, 0, stream>>>(
+      d_negSquare, deviceRefArrays, foundValidationFailure);
+  checkCudaErrors(hipGraphLaunch(graphExecFreeC, stream));
+  checkCudaErrors(hipStreamSynchronize(stream));
+  printf(
+      "Validating negateSquares with d_negSquare freed in a different "
+      "graph...\n");
+  checkValidationFailure(foundValidationFailure);
+
+  printf("Cleaning up sample.\n");
+  checkCudaErrors(hipGraphExecDestroy(graphExec));
+  checkCudaErrors(hipGraphExecDestroy(graphExecFreeC));
+  checkCudaErrors(hipStreamDestroy(stream));
+  checkCudaErrors(hipFree(foundValidationFailure));
+  checkCudaErrors(hipFree(deviceRefArrays.negSquare));
+  free(hostArrays.input);
+  free(hostArrays.square);
+  free(hostArrays.negSquare);
+  printf("Cleanup complete. Exiting sample.\n");
+}
\ No newline at end of file
diff --git a/src/samples/Samples/3_CUDA_Features/jacobiCudaGraphs/jacobi.cu.hip b/src/samples/Samples/3_CUDA_Features/jacobiCudaGraphs/jacobi.cu.hip
index 35a8da9..f671fe4 100755
--- a/src/samples/Samples/3_CUDA_Features/jacobiCudaGraphs/jacobi.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/jacobiCudaGraphs/jacobi.cu.hip
@@ -29,7 +29,6 @@
 #include <hip/hip_cooperative_groups.h>
 #include <hip/hip_runtime.h>
 #include "helper_cuda_hipified.h"
+#include "HIPCHECK.h"
 #include <vector>
 #include "jacobi.h"
 
diff --git a/src/samples/Samples/3_CUDA_Features/memMapIPCDrv/memMapIpc_kernel.cu.hip b/src/samples/Samples/3_CUDA_Features/memMapIPCDrv/memMapIpc_kernel.cu.hip
index e69de29..41b1784 100755
--- a/src/samples/Samples/3_CUDA_Features/memMapIPCDrv/memMapIpc_kernel.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/memMapIPCDrv/memMapIpc_kernel.cu.hip
@@ -0,0 +1,38 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+
+// Device code
+extern "C" __global__ void memMapIpc_kernel(char *ptr, int sz, char val)
+{
+    // Dummy kernel
+    int idx = blockIdx.x * blockDim.x + threadIdx.x;
+    for (; idx < sz; idx += (gridDim.x * blockDim.x)) {
+        ptr[idx] = val;
+    }
+}
diff --git a/src/samples/Samples/3_CUDA_Features/newdelete/newdelete.out b/src/samples/Samples/3_CUDA_Features/newdelete/newdelete.out
index 2a5db23..88d5885 100755
Binary files a/src/samples/Samples/3_CUDA_Features/newdelete/newdelete.out and b/src/samples/Samples/3_CUDA_Features/newdelete/newdelete.out differ
diff --git a/src/samples/Samples/3_CUDA_Features/simpleCudaGraphs/simpleCudaGraphs.out b/src/samples/Samples/3_CUDA_Features/simpleCudaGraphs/simpleCudaGraphs.out
index ba74a4b..3d5954e 100755
Binary files a/src/samples/Samples/3_CUDA_Features/simpleCudaGraphs/simpleCudaGraphs.out and b/src/samples/Samples/3_CUDA_Features/simpleCudaGraphs/simpleCudaGraphs.out differ
diff --git a/src/samples/Samples/4_CUDA_Libraries/conjugateGradientMultiDeviceCG/conjugateGradientMultiDeviceCG.cu.hip b/src/samples/Samples/4_CUDA_Libraries/conjugateGradientMultiDeviceCG/conjugateGradientMultiDeviceCG.cu.hip
index 2751c9b..43cc9bd 100755
--- a/src/samples/Samples/4_CUDA_Libraries/conjugateGradientMultiDeviceCG/conjugateGradientMultiDeviceCG.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/conjugateGradientMultiDeviceCG/conjugateGradientMultiDeviceCG.cu.hip
@@ -432,13 +432,13 @@ extern "C" __global__ void multiGpuConjugateGradient(
 // Map of device version to device number
 std::multimap<std::pair<int, int>, int> getIdenticalGPUs() {
   int numGpus = 0;
-  checkCudaErrors(hipGetDeviceCount(&numGpus));
+  HIPCHECK(hipGetDeviceCount(&numGpus));
 
   std::multimap<std::pair<int, int>, int> identicalGpus;
 
   for (int i = 0; i < numGpus; i++) {
     hipDeviceProp_t deviceProp;
-    checkCudaErrors(hipGetDeviceProperties(&deviceProp, i));
+    HIPCHECK(hipGetDeviceProperties(&deviceProp, i));
 
     // Filter unsupported devices
     if (deviceProp.cooperativeLaunch && deviceProp.concurrentManagedAccess) {
@@ -497,7 +497,7 @@ int main(int argc, char **argv) {
   // access between participating GPUs gives better performance.
   for (auto itr = bestFit.first; itr != bestFit.second; itr++) {
     int deviceId = itr->second;
-    checkCudaErrors(hipSetDevice(deviceId));
+    HIPCHECK(hipSetDevice(deviceId));
 
     std::for_each(
         itr, bestFit.second,
@@ -505,7 +505,7 @@ int main(int argc, char **argv) {
          &kNumGpusRequired](decltype(*itr) mapPair) {
           if (deviceId != mapPair.second) {
             int access = 0;
-            checkCudaErrors(
+            HIPCHECK(
                 hipDeviceCanAccessPeer(&access, deviceId, mapPair.second));
             printf("Device=%d %s Access Peer Device=%d\n", deviceId,
                    access ? "CAN" : "CANNOT", mapPair.second);
@@ -551,12 +551,12 @@ int main(int argc, char **argv) {
     // participating devices.
     for (auto p1_itr = bestFitDeviceIds.begin();
          p1_itr != bestFitDeviceIds.end(); p1_itr++) {
-      checkCudaErrors(hipSetDevice(*p1_itr));
+      HIPCHECK(hipSetDevice(*p1_itr));
       for (auto p2_itr = bestFitDeviceIds.begin();
            p2_itr != bestFitDeviceIds.end(); p2_itr++) {
         if (*p1_itr != *p2_itr) {
-          checkCudaErrors(hipDeviceEnablePeerAccess(*p2_itr, 0));
-          checkCudaErrors(hipSetDevice(*p1_itr));
+          HIPCHECK(hipDeviceEnablePeerAccess(*p2_itr, 0));
+          HIPCHECK(hipSetDevice(*p1_itr));
         }
       }
     }
@@ -566,33 +566,33 @@ int main(int argc, char **argv) {
   N = 10485760 * 2;
   nz = (N - 2) * 3 + 4;
 
-  checkCudaErrors(hipMallocManaged((void **)&I, sizeof(int) * (N + 1)));
-  checkCudaErrors(hipMallocManaged((void **)&J, sizeof(int) * nz));
-  checkCudaErrors(hipMallocManaged((void **)&val, sizeof(float) * nz));
+  HIPCHECK(hipMallocManaged((void **)&I, sizeof(int) * (N + 1)));
+  HIPCHECK(hipMallocManaged((void **)&J, sizeof(int) * nz));
+  HIPCHECK(hipMallocManaged((void **)&val, sizeof(float) * nz));
 
   float *val_cpu = (float *)malloc(sizeof(float) * nz);
 
   genTridiag(I, J, val_cpu, N, nz);
 
   memcpy(val, val_cpu, sizeof(float) * nz);
-  checkCudaErrors(
+  HIPCHECK(
       hipMemAdvise(I, sizeof(int) * (N + 1), hipMemAdviseSetReadMostly, 0));
-  checkCudaErrors(
+  HIPCHECK(
       hipMemAdvise(J, sizeof(int) * nz, hipMemAdviseSetReadMostly, 0));
-  checkCudaErrors(
+  HIPCHECK(
       hipMemAdvise(val, sizeof(float) * nz, hipMemAdviseSetReadMostly, 0));
 
-  checkCudaErrors(hipMallocManaged((void **)&x, sizeof(float) * N));
+  HIPCHECK(hipMallocManaged((void **)&x, sizeof(float) * N));
 
   double *dot_result;
-  checkCudaErrors(hipMallocManaged((void **)&dot_result, sizeof(double)));
+  HIPCHECK(hipMallocManaged((void **)&dot_result, sizeof(double)));
 
-  checkCudaErrors(hipMemset(dot_result, 0, sizeof(double)));
+  HIPCHECK(hipMemset(dot_result, 0, sizeof(double)));
 
   // temp memory for ConjugateGradient
-  checkCudaErrors(hipMallocManaged((void **)&r, N * sizeof(float)));
-  checkCudaErrors(hipMallocManaged((void **)&p, N * sizeof(float)));
-  checkCudaErrors(hipMallocManaged((void **)&Ax, N * sizeof(float)));
+  HIPCHECK(hipMallocManaged((void **)&r, N * sizeof(float)));
+  HIPCHECK(hipMallocManaged((void **)&p, N * sizeof(float)));
+  HIPCHECK(hipMallocManaged((void **)&Ax, N * sizeof(float)));
 
   std::cout << "\nRunning on GPUs = " << kNumGpusRequired << std::endl;
   hipStream_t nStreams[kNumGpusRequired];
@@ -606,11 +606,11 @@ int main(int argc, char **argv) {
   // set numSms & numBlocksPerSm to be lowest of 2 devices
   while (deviceId != bestFitDeviceIds.end()) {
     hipDeviceProp_t deviceProp;
-    checkCudaErrors(hipSetDevice(*deviceId));
-    checkCudaErrors(hipGetDeviceProperties(&deviceProp, *deviceId));
+    HIPCHECK(hipSetDevice(*deviceId));
+    HIPCHECK(hipGetDeviceProperties(&deviceProp, *deviceId));
 
     int numBlocksPerSm_current = 0;
-    checkCudaErrors(hipOccupancyMaxActiveBlocksPerMultiprocessor(
+    HIPCHECK(hipOccupancyMaxActiveBlocksPerMultiprocessor(
         &numBlocksPerSm_current, multiGpuConjugateGradient, numThreads,
         sMemSize));
 
@@ -634,8 +634,8 @@ int main(int argc, char **argv) {
   int totalThreadsPerGPU = numSms * numBlocksPerSm * THREADS_PER_BLOCK;
   deviceId = bestFitDeviceIds.begin();
   while (deviceId != bestFitDeviceIds.end()) {
-    checkCudaErrors(hipSetDevice(*deviceId));
-    checkCudaErrors(hipStreamCreate(&nStreams[device_count]));
+    HIPCHECK(hipSetDevice(*deviceId));
+    HIPCHECK(hipStreamCreate(&nStreams[device_count]));
 
     int perGPUIter = N / (totalThreadsPerGPU * kNumGpusRequired);
     int offset_Ax = device_count * totalThreadsPerGPU;
@@ -643,11 +643,11 @@ int main(int argc, char **argv) {
     int offset_p = device_count * totalThreadsPerGPU;
     int offset_x = device_count * totalThreadsPerGPU;
 
-    checkCudaErrors(hipMemPrefetchAsync(I, sizeof(int) * N, *deviceId,
+    HIPCHECK(hipMemPrefetchAsync(I, sizeof(int) * N, *deviceId,
                                          nStreams[device_count]));
-    checkCudaErrors(hipMemPrefetchAsync(val, sizeof(float) * nz, *deviceId,
+    HIPCHECK(hipMemPrefetchAsync(val, sizeof(float) * nz, *deviceId,
                                          nStreams[device_count]));
-    checkCudaErrors(hipMemPrefetchAsync(J, sizeof(float) * nz, *deviceId,
+    HIPCHECK(hipMemPrefetchAsync(J, sizeof(float) * nz, *deviceId,
                                          nStreams[device_count]));
 
     if (offset_Ax <= N) {
@@ -704,7 +704,7 @@ int main(int argc, char **argv) {
 
   // Structure used for cross-grid synchronization.
   MultiDeviceData multi_device_data;
-  checkCudaErrors(hipHostAlloc(
+  HIPCHECK(hipHostAlloc(
       &multi_device_data.hostMemoryArrivedList,
       (kNumGpusRequired - 1) * sizeof(*multi_device_data.hostMemoryArrivedList),
       hipHostMallocPortable));
@@ -725,23 +725,23 @@ int main(int argc, char **argv) {
   deviceId = bestFitDeviceIds.begin();
   device_count = 0;
   while (deviceId != bestFitDeviceIds.end()) {
-    checkCudaErrors(hipSetDevice(*deviceId));
-    checkCudaErrors(hipLaunchCooperativeKernel(
+    HIPCHECK(hipSetDevice(*deviceId));
+    HIPCHECK(hipLaunchCooperativeKernel(
         (void *)multiGpuConjugateGradient, dimGrid, dimBlock, kernelArgs,
         sMemSize, nStreams[device_count++]));
     multi_device_data.deviceRank++;
     deviceId++;
   }
 
-  checkCudaErrors(hipMemPrefetchAsync(x, sizeof(float) * N, hipCpuDeviceId));
-  checkCudaErrors(
+  HIPCHECK(hipMemPrefetchAsync(x, sizeof(float) * N, hipCpuDeviceId));
+  HIPCHECK(
       hipMemPrefetchAsync(dot_result, sizeof(double), hipCpuDeviceId));
 
   deviceId = bestFitDeviceIds.begin();
   device_count = 0;
   while (deviceId != bestFitDeviceIds.end()) {
-    checkCudaErrors(hipSetDevice(*deviceId));
-    checkCudaErrors(hipStreamSynchronize(nStreams[device_count++]));
+    HIPCHECK(hipSetDevice(*deviceId));
+    HIPCHECK(hipStreamSynchronize(nStreams[device_count++]));
     deviceId++;
   }
 
@@ -769,15 +769,15 @@ int main(int argc, char **argv) {
     }
   }
 
-  checkCudaErrors(hipHostFree(multi_device_data.hostMemoryArrivedList));
-  checkCudaErrors(hipFree(I));
-  checkCudaErrors(hipFree(J));
-  checkCudaErrors(hipFree(val));
-  checkCudaErrors(hipFree(x));
-  checkCudaErrors(hipFree(r));
-  checkCudaErrors(hipFree(p));
-  checkCudaErrors(hipFree(Ax));
-  checkCudaErrors(hipFree(dot_result));
+  HIPCHECK(hipHostFree(multi_device_data.hostMemoryArrivedList));
+  HIPCHECK(hipFree(I));
+  HIPCHECK(hipFree(J));
+  HIPCHECK(hipFree(val));
+  HIPCHECK(hipFree(x));
+  HIPCHECK(hipFree(r));
+  HIPCHECK(hipFree(p));
+  HIPCHECK(hipFree(Ax));
+  HIPCHECK(hipFree(dot_result));
   free(val_cpu);
 
 #if ENABLE_CPU_DEBUG_CODE
@@ -828,3 +828,15 @@ G_CODE
           (sqrt(r1) < tol) ? "PASSED" : "FAILED");
   exit((sqrt(r1) < tol) ? EXIT_SUCCESS : EXIT_FAILURE);
 }
+G_CODE
+  free(Ax_cpu);
+  free(r_cpu);
+  free(p_cpu);
+  free(x_cpu);
+#endif
+
+  printf("Test Summary:  Error amount = %f \n", err);
+  fprintf(stdout, "&&&& conjugateGradientMultiDeviceCG %s\n",
+          (sqrt(r1) < tol) ? "PASSED" : "FAILED");
+  exit((sqrt(r1) < tol) ? EXIT_SUCCESS : EXIT_FAILURE);
+}
diff --git a/src/samples/Samples/4_CUDA_Libraries/cuDLAErrorReporting/main.cu.hip b/src/samples/Samples/4_CUDA_Libraries/cuDLAErrorReporting/main.cu.hip
index e69de29..c38223b 100755
--- a/src/samples/Samples/4_CUDA_Libraries/cuDLAErrorReporting/main.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/cuDLAErrorReporting/main.cu.hip
@@ -0,0 +1,431 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "cudla.h"
+#include "hip/hip_runtime.h"
+
+#include <cstdio>
+#include <cstdlib>
+#include <cstring>
+#include <sys/stat.h>
+#include <fstream>
+#include <sstream>
+
+#define DPRINTF(...) printf(__VA_ARGS__)
+
+static void printTensorDesc(cudlaModuleTensorDescriptor* tensorDesc) {
+  DPRINTF("\tTENSOR NAME : %s\n", tensorDesc->name);
+  DPRINTF("\tsize: %lu\n", tensorDesc->size);
+
+  DPRINTF("\tdims: [%lu, %lu, %lu, %lu]\n", tensorDesc->n, tensorDesc->c,
+          tensorDesc->h, tensorDesc->w);
+
+  DPRINTF("\tdata fmt: %d\n", tensorDesc->dataFormat);
+  DPRINTF("\tdata type: %d\n", tensorDesc->dataType);
+  DPRINTF("\tdata category: %d\n", tensorDesc->dataCategory);
+  DPRINTF("\tpixel fmt: %d\n", tensorDesc->pixelFormat);
+  DPRINTF("\tpixel mapping: %d\n", tensorDesc->pixelMapping);
+  DPRINTF("\tstride[0]: %d\n", tensorDesc->stride[0]);
+  DPRINTF("\tstride[1]: %d\n", tensorDesc->stride[1]);
+  DPRINTF("\tstride[2]: %d\n", tensorDesc->stride[2]);
+  DPRINTF("\tstride[3]: %d\n", tensorDesc->stride[3]);
+}
+
+typedef struct {
+  cudlaDevHandle devHandle;
+  cudlaModule moduleHandle;
+  unsigned char* loadableData;
+  hipStream_t stream;
+  unsigned char* inputBuffer;
+  unsigned char* outputBuffer;
+  void* inputBufferGPU;
+  void* outputBufferGPU;
+  cudlaModuleTensorDescriptor* inputTensorDesc;
+  cudlaModuleTensorDescriptor* outputTensorDesc;
+} ResourceList;
+
+void cleanUp(ResourceList* resourceList);
+
+void cleanUp(ResourceList* resourceList) {
+  if (resourceList->inputTensorDesc != NULL) {
+    free(resourceList->inputTensorDesc);
+    resourceList->inputTensorDesc = NULL;
+  }
+  if (resourceList->outputTensorDesc != NULL) {
+    free(resourceList->outputTensorDesc);
+    resourceList->outputTensorDesc = NULL;
+  }
+
+  if (resourceList->loadableData != NULL) {
+    free(resourceList->loadableData);
+    resourceList->loadableData = NULL;
+  }
+
+  if (resourceList->moduleHandle != NULL) {
+    cudlaModuleUnload(resourceList->moduleHandle, 0);
+    resourceList->moduleHandle = NULL;
+  }
+
+  if (resourceList->devHandle != NULL) {
+    cudlaDestroyDevice(resourceList->devHandle);
+    resourceList->devHandle = NULL;
+  }
+
+  if (resourceList->inputBufferGPU != 0) {
+    hipFree(resourceList->inputBufferGPU);
+    resourceList->inputBufferGPU = 0;
+  }
+  if (resourceList->outputBufferGPU != 0) {
+    hipFree(resourceList->outputBufferGPU);
+    resourceList->outputBufferGPU = 0;
+  }
+
+  if (resourceList->inputBuffer != NULL) {
+    free(resourceList->inputBuffer);
+    resourceList->inputBuffer = NULL;
+  }
+  if (resourceList->outputBuffer != NULL) {
+    free(resourceList->outputBuffer);
+    resourceList->outputBuffer = NULL;
+  }
+
+  if (resourceList->stream != NULL) {
+    hipStreamDestroy(resourceList->stream);
+    resourceList->stream = NULL;
+  }
+}
+
+int main(int argc, char** argv) {
+  cudlaDevHandle devHandle;
+  cudlaModule moduleHandle;
+  cudlaStatus err;
+  FILE* fp = NULL;
+  struct stat st;
+  size_t file_size;
+  size_t actually_read = 0;
+  unsigned char* loadableData = NULL;
+
+  hipStream_t stream;
+  hipError_t result;
+  const char* errPtr = NULL;
+
+  ResourceList resourceList;
+
+  memset(&resourceList, 0x00, sizeof(ResourceList));
+
+  if (argc != 2) {
+    DPRINTF("Usage : ./cuDLAErrorReporting <loadable>\n");
+    return 1;
+  }
+
+  // Read loadable into buffer.
+  fp = fopen(argv[1], "rb");
+  if (fp == NULL) {
+    DPRINTF("Cannot open file %s\n", argv[1]);
+    return 1;
+  }
+
+  if (stat(argv[1], &st) != 0) {
+    DPRINTF("Cannot stat file\n");
+    return 1;
+  }
+
+  file_size = st.st_size;
+  DPRINTF("The file size = %ld\n", file_size);
+
+  loadableData = (unsigned char*)malloc(file_size);
+  if (loadableData == NULL) {
+    DPRINTF("Cannot Allocate memory for loadable\n");
+    return 1;
+  }
+
+  actually_read = fread(loadableData, 1, file_size, fp);
+  if (actually_read != file_size) {
+    free(loadableData);
+    DPRINTF("Read wrong size\n");
+    return 1;
+  }
+  fclose(fp);
+
+  resourceList.loadableData = loadableData;
+
+  // Initialize CUDA.
+  result = hipFree(0);
+  if (result != hipSuccess) {
+    errPtr = hipGetErrorName(result);
+    DPRINTF("Error in creating hipFree = %s\n", errPtr);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  result = hipSetDevice(0);
+  if (result != hipSuccess) {
+    errPtr = hipGetErrorName(result);
+    DPRINTF("Error in creating hipSetDevice = %s\n", errPtr);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  err = cudlaCreateDevice(0, &devHandle, CUDLA_CUDA_DLA);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in cuDLA create device = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  DPRINTF("Device created successfully\n");
+  resourceList.devHandle = devHandle;
+
+  err = cudlaModuleLoadFromMemory(devHandle, loadableData, file_size,
+                                  &moduleHandle, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in cudlaModuleLoadFromMemory = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  } else {
+    DPRINTF("Successfully loaded module\n");
+  }
+
+  resourceList.moduleHandle = moduleHandle;
+
+  // Create CUDA stream.
+  result = hipStreamCreateWithFlags(&stream, hipStreamNonBlocking);
+
+  if (result != hipSuccess) {
+    errPtr = hipGetErrorName(result);
+    DPRINTF("Error in creating cuda stream = %s\n", errPtr);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.stream = stream;
+
+  // Get tensor attributes.
+  uint32_t numInputTensors = 0;
+  uint32_t numOutputTensors = 0;
+  cudlaModuleAttribute attribute;
+
+  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_NUM_INPUT_TENSORS,
+                                 &attribute);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in getting numInputTensors = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  numInputTensors = attribute.numInputTensors;
+  DPRINTF("numInputTensors = %d\n", numInputTensors);
+
+  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_NUM_OUTPUT_TENSORS,
+                                 &attribute);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in getting numOutputTensors = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  numOutputTensors = attribute.numOutputTensors;
+  DPRINTF("numOutputTensors = %d\n", numOutputTensors);
+
+  cudlaModuleTensorDescriptor* inputTensorDesc =
+      (cudlaModuleTensorDescriptor*)malloc(sizeof(cudlaModuleTensorDescriptor) *
+                                           numInputTensors);
+  cudlaModuleTensorDescriptor* outputTensorDesc =
+      (cudlaModuleTensorDescriptor*)malloc(sizeof(cudlaModuleTensorDescriptor) *
+                                           numOutputTensors);
+
+  if ((inputTensorDesc == NULL) || (outputTensorDesc == NULL)) {
+    if (inputTensorDesc != NULL) {
+      free(inputTensorDesc);
+      inputTensorDesc = NULL;
+    }
+
+    if (outputTensorDesc != NULL) {
+      free(outputTensorDesc);
+      outputTensorDesc = NULL;
+    }
+
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.inputTensorDesc = inputTensorDesc;
+  resourceList.outputTensorDesc = outputTensorDesc;
+
+  attribute.inputTensorDesc = inputTensorDesc;
+  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_INPUT_TENSOR_DESCRIPTORS,
+                                 &attribute);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in getting input tensor descriptor = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("Printing input tensor descriptor\n");
+  printTensorDesc(inputTensorDesc);
+
+  attribute.outputTensorDesc = outputTensorDesc;
+  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_OUTPUT_TENSOR_DESCRIPTORS,
+                                 &attribute);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in getting output tensor descriptor = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("Printing output tensor descriptor\n");
+  printTensorDesc(outputTensorDesc);
+
+  // Setup the input and output buffers which will be used as an input to CUDA.
+  unsigned char* inputBuffer = (unsigned char*)malloc(inputTensorDesc[0].size);
+  if (inputBuffer == NULL) {
+    DPRINTF("Error in allocating input memory\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.inputBuffer = inputBuffer;
+
+  unsigned char* outputBuffer =
+      (unsigned char*)malloc(outputTensorDesc[0].size);
+  if (outputBuffer == NULL) {
+    DPRINTF("Error in allocating output memory\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.outputBuffer = outputBuffer;
+
+  memset(inputBuffer, 0x01, inputTensorDesc[0].size);
+  memset(outputBuffer, 0x00, outputTensorDesc[0].size);
+
+  // Allocate memory on GPU.
+  void* inputBufferGPU;
+  void* outputBufferGPU;
+  result = hipMalloc(&inputBufferGPU, inputTensorDesc[0].size);
+  if (result != hipSuccess) {
+    DPRINTF("Error in allocating input memory on GPU\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.inputBufferGPU = inputBufferGPU;
+
+  result = hipMalloc(&outputBufferGPU, outputTensorDesc[0].size);
+  if (result != hipSuccess) {
+    DPRINTF("Error in allocating output memory on GPU\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.outputBufferGPU = outputBufferGPU;
+
+  // Register the CUDA-allocated buffers.
+  uint64_t* inputBufferRegisteredPtr = NULL;
+  uint64_t* outputBufferRegisteredPtr = NULL;
+
+  err = cudlaMemRegister(devHandle, (uint64_t*)inputBufferGPU,
+                         inputTensorDesc[0].size, &inputBufferRegisteredPtr, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in registering input memory = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  err =
+      cudlaMemRegister(devHandle, (uint64_t*)outputBufferGPU,
+                       outputTensorDesc[0].size, &outputBufferRegisteredPtr, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in registering output memory = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("ALL MEMORY REGISTERED SUCCESSFULLY\n");
+
+  // Copy data from CPU buffers to GPU buffers.
+  result = hipMemcpyAsync(inputBufferGPU, inputBuffer, inputTensorDesc[0].size,
+                           hipMemcpyHostToDevice, stream);
+  if (result != hipSuccess) {
+    DPRINTF("Error in enqueueing memcpy for input\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+  result =
+      hipMemsetAsync(outputBufferGPU, 0, outputTensorDesc[0].size, stream);
+  if (result != hipSuccess) {
+    DPRINTF("Error in enqueueing memset for output\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  // Enqueue a cuDLA task.
+  cudlaTask task;
+  task.moduleHandle = moduleHandle;
+  task.outputTensor = &outputBufferRegisteredPtr;
+  task.numOutputTensors = 1;
+  task.numInputTensors = 1;
+  task.inputTensor = &inputBufferRegisteredPtr;
+  task.waitEvents = NULL;
+  task.signalEvents = NULL;
+  err = cudlaSubmitTask(devHandle, &task, 1, stream, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in submitting task\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("SUBMIT IS DONE !!!\n");
+
+  // Wait for stream operations to finish and bring output buffer to CPU.
+  result =
+      hipMemcpyAsync(outputBuffer, outputBufferGPU, outputTensorDesc[0].size,
+                      hipMemcpyDeviceToHost, stream);
+  if (result != hipSuccess) {
+    if (result != cudaErrorExternalDevice) {
+      DPRINTF("Error in bringing result back to CPU\n");
+      cleanUp(&resourceList);
+      return 1;
+    } else {
+      cudlaStatus hwStatus = cudlaGetLastError(devHandle);
+      if (hwStatus != cudlaSuccess) {
+        DPRINTF("Asynchronous error in HW = %u\n", hwStatus);
+      }
+    }
+  }
+
+  result = hipStreamSynchronize(stream);
+  if (result != hipSuccess) {
+    DPRINTF("Error in synchronizing stream = %s\n", hipGetErrorName(result));
+
+    if (result == cudaErrorExternalDevice) {
+      cudlaStatus hwStatus = cudlaGetLastError(devHandle);
+      if (hwStatus != cudlaSuccess) {
+        DPRINTF("Asynchronous error in HW = %u\n", hwStatus);
+      }
+    }
+  }
+
+  cleanUp(&resourceList);
+
+  DPRINTF("cuDLAErrorReporting DONE !!!\n");
+
+  return 0;
+}
diff --git a/src/samples/Samples/4_CUDA_Libraries/cuDLAHybridMode/main.cu.hip b/src/samples/Samples/4_CUDA_Libraries/cuDLAHybridMode/main.cu.hip
index e69de29..ecaf605 100755
--- a/src/samples/Samples/4_CUDA_Libraries/cuDLAHybridMode/main.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/cuDLAHybridMode/main.cu.hip
@@ -0,0 +1,496 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "cudla.h"
+#include "hip/hip_runtime.h"
+
+#include <cstdio>
+#include <cstdlib>
+#include <cstring>
+#include <sys/stat.h>
+#include <fstream>
+#include <sstream>
+
+#define DPRINTF(...) printf(__VA_ARGS__)
+
+static void printTensorDesc(cudlaModuleTensorDescriptor* tensorDesc) {
+  DPRINTF("\tTENSOR NAME : %s\n", tensorDesc->name);
+  DPRINTF("\tsize: %lu\n", tensorDesc->size);
+
+  DPRINTF("\tdims: [%lu, %lu, %lu, %lu]\n", tensorDesc->n, tensorDesc->c,
+          tensorDesc->h, tensorDesc->w);
+
+  DPRINTF("\tdata fmt: %d\n", tensorDesc->dataFormat);
+  DPRINTF("\tdata type: %d\n", tensorDesc->dataType);
+  DPRINTF("\tdata category: %d\n", tensorDesc->dataCategory);
+  DPRINTF("\tpixel fmt: %d\n", tensorDesc->pixelFormat);
+  DPRINTF("\tpixel mapping: %d\n", tensorDesc->pixelMapping);
+  DPRINTF("\tstride[0]: %d\n", tensorDesc->stride[0]);
+  DPRINTF("\tstride[1]: %d\n", tensorDesc->stride[1]);
+  DPRINTF("\tstride[2]: %d\n", tensorDesc->stride[2]);
+  DPRINTF("\tstride[3]: %d\n", tensorDesc->stride[3]);
+}
+
+static int initializeInputBuffers(char* filePath,
+                                  cudlaModuleTensorDescriptor* tensorDesc,
+                                  unsigned char* buf) {
+  // Read the file in filePath and fill up 'buf' according to format
+  // specified by the user.
+
+  return 0;
+}
+
+typedef struct {
+  cudlaDevHandle devHandle;
+  cudlaModule moduleHandle;
+  unsigned char* loadableData;
+  hipStream_t stream;
+  unsigned char* inputBuffer;
+  unsigned char* outputBuffer;
+  void* inputBufferGPU;
+  void* outputBufferGPU;
+  cudlaModuleTensorDescriptor* inputTensorDesc;
+  cudlaModuleTensorDescriptor* outputTensorDesc;
+} ResourceList;
+
+void cleanUp(ResourceList* resourceList);
+
+void cleanUp(ResourceList* resourceList) {
+  if (resourceList->inputTensorDesc != NULL) {
+    free(resourceList->inputTensorDesc);
+    resourceList->inputTensorDesc = NULL;
+  }
+  if (resourceList->outputTensorDesc != NULL) {
+    free(resourceList->outputTensorDesc);
+    resourceList->outputTensorDesc = NULL;
+  }
+
+  if (resourceList->loadableData != NULL) {
+    free(resourceList->loadableData);
+    resourceList->loadableData = NULL;
+  }
+
+  if (resourceList->moduleHandle != NULL) {
+    cudlaModuleUnload(resourceList->moduleHandle, 0);
+    resourceList->moduleHandle = NULL;
+  }
+
+  if (resourceList->devHandle != NULL) {
+    cudlaDestroyDevice(resourceList->devHandle);
+    resourceList->devHandle = NULL;
+  }
+
+  if (resourceList->inputBufferGPU != 0) {
+    hipFree(resourceList->inputBufferGPU);
+    resourceList->inputBufferGPU = 0;
+  }
+  if (resourceList->outputBufferGPU != 0) {
+    hipFree(resourceList->outputBufferGPU);
+    resourceList->outputBufferGPU = 0;
+  }
+
+  if (resourceList->inputBuffer != NULL) {
+    free(resourceList->inputBuffer);
+    resourceList->inputBuffer = NULL;
+  }
+  if (resourceList->outputBuffer != NULL) {
+    free(resourceList->outputBuffer);
+    resourceList->outputBuffer = NULL;
+  }
+
+  if (resourceList->stream != NULL) {
+    hipStreamDestroy(resourceList->stream);
+    resourceList->stream = NULL;
+  }
+}
+
+int main(int argc, char** argv) {
+  cudlaDevHandle devHandle;
+  cudlaModule moduleHandle;
+  cudlaStatus err;
+  FILE* fp = NULL;
+  struct stat st;
+  size_t file_size;
+  size_t actually_read = 0;
+  unsigned char* loadableData = NULL;
+
+  hipStream_t stream;
+  hipError_t result;
+  const char* errPtr = NULL;
+
+  ResourceList resourceList;
+
+  memset(&resourceList, 0x00, sizeof(ResourceList));
+
+  if (argc != 3) {
+    DPRINTF("Usage : ./cuDLAHybridMode <loadable> <imageFile>\n");
+    return 1;
+  }
+
+  // Read loadable into buffer.
+  fp = fopen(argv[1], "rb");
+  if (fp == NULL) {
+    DPRINTF("Cannot open file %s\n", argv[1]);
+    return 1;
+  }
+
+  if (stat(argv[1], &st) != 0) {
+    DPRINTF("Cannot stat file\n");
+    return 1;
+  }
+
+  file_size = st.st_size;
+  DPRINTF("The file size = %ld\n", file_size);
+
+  loadableData = (unsigned char*)malloc(file_size);
+  if (loadableData == NULL) {
+    DPRINTF("Cannot Allocate memory for loadable\n");
+    return 1;
+  }
+
+  actually_read = fread(loadableData, 1, file_size, fp);
+  if (actually_read != file_size) {
+    free(loadableData);
+    DPRINTF("Read wrong size\n");
+    return 1;
+  }
+  fclose(fp);
+
+  resourceList.loadableData = loadableData;
+
+  // Initialize CUDA.
+  result = hipFree(0);
+  if (result != hipSuccess) {
+    errPtr = hipGetErrorName(result);
+    DPRINTF("Error in creating hipFree = %s\n", errPtr);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  result = hipSetDevice(0);
+  if (result != hipSuccess) {
+    errPtr = hipGetErrorName(result);
+    DPRINTF("Error in creating hipSetDevice = %s\n", errPtr);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  err = cudlaCreateDevice(0, &devHandle, CUDLA_CUDA_DLA);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in cuDLA create device = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  DPRINTF("Device created successfully\n");
+  resourceList.devHandle = devHandle;
+
+  err = cudlaModuleLoadFromMemory(devHandle, loadableData, file_size,
+                                  &moduleHandle, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in cudlaModuleLoadFromMemory = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  } else {
+    DPRINTF("Successfully loaded module\n");
+  }
+
+  resourceList.moduleHandle = moduleHandle;
+
+  // Create CUDA stream.
+  result = hipStreamCreateWithFlags(&stream, hipStreamNonBlocking);
+
+  if (result != hipSuccess) {
+    errPtr = hipGetErrorName(result);
+    DPRINTF("Error in creating cuda stream = %s\n", errPtr);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.stream = stream;
+
+  // Get tensor attributes.
+  uint32_t numInputTensors = 0;
+  uint32_t numOutputTensors = 0;
+  cudlaModuleAttribute attribute;
+
+  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_NUM_INPUT_TENSORS,
+                                 &attribute);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in getting numInputTensors = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  numInputTensors = attribute.numInputTensors;
+  DPRINTF("numInputTensors = %d\n", numInputTensors);
+
+  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_NUM_OUTPUT_TENSORS,
+                                 &attribute);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in getting numOutputTensors = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  numOutputTensors = attribute.numOutputTensors;
+  DPRINTF("numOutputTensors = %d\n", numOutputTensors);
+
+  cudlaModuleTensorDescriptor* inputTensorDesc =
+      (cudlaModuleTensorDescriptor*)malloc(sizeof(cudlaModuleTensorDescriptor) *
+                                           numInputTensors);
+  cudlaModuleTensorDescriptor* outputTensorDesc =
+      (cudlaModuleTensorDescriptor*)malloc(sizeof(cudlaModuleTensorDescriptor) *
+                                           numOutputTensors);
+
+  if ((inputTensorDesc == NULL) || (outputTensorDesc == NULL)) {
+    if (inputTensorDesc != NULL) {
+      free(inputTensorDesc);
+      inputTensorDesc = NULL;
+    }
+
+    if (outputTensorDesc != NULL) {
+      free(outputTensorDesc);
+      outputTensorDesc = NULL;
+    }
+
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.inputTensorDesc = inputTensorDesc;
+  resourceList.outputTensorDesc = outputTensorDesc;
+
+  attribute.inputTensorDesc = inputTensorDesc;
+  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_INPUT_TENSOR_DESCRIPTORS,
+                                 &attribute);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in getting input tensor descriptor = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("Printing input tensor descriptor\n");
+  printTensorDesc(inputTensorDesc);
+
+  attribute.outputTensorDesc = outputTensorDesc;
+  err = cudlaModuleGetAttributes(moduleHandle, CUDLA_OUTPUT_TENSOR_DESCRIPTORS,
+                                 &attribute);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in getting output tensor descriptor = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("Printing output tensor descriptor\n");
+  printTensorDesc(outputTensorDesc);
+
+  // Setup the input and output buffers which will be used as an input to CUDA.
+  unsigned char* inputBuffer = (unsigned char*)malloc(inputTensorDesc[0].size);
+  if (inputBuffer == NULL) {
+    DPRINTF("Error in allocating input memory\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.inputBuffer = inputBuffer;
+
+  unsigned char* outputBuffer =
+      (unsigned char*)malloc(outputTensorDesc[0].size);
+  if (outputBuffer == NULL) {
+    DPRINTF("Error in allocating output memory\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.outputBuffer = outputBuffer;
+
+  memset(inputBuffer, 0x00, inputTensorDesc[0].size);
+  memset(outputBuffer, 0x00, outputTensorDesc[0].size);
+
+  // Fill up the buffers with data.
+  if (initializeInputBuffers(argv[2], inputTensorDesc, inputBuffer) != 0) {
+    DPRINTF("Error in initializing input buffer\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  // Allocate memory on GPU.
+  void* inputBufferGPU;
+  void* outputBufferGPU;
+  result = hipMalloc(&inputBufferGPU, inputTensorDesc[0].size);
+  if (result != hipSuccess) {
+    DPRINTF("Error in allocating input memory on GPU\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.inputBufferGPU = inputBufferGPU;
+
+  result = hipMalloc(&outputBufferGPU, outputTensorDesc[0].size);
+  if (result != hipSuccess) {
+    DPRINTF("Error in allocating output memory on GPU\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.outputBufferGPU = outputBufferGPU;
+
+  // Register the CUDA-allocated buffers.
+  uint64_t* inputBufferRegisteredPtr = NULL;
+  uint64_t* outputBufferRegisteredPtr = NULL;
+
+  err = cudlaMemRegister(devHandle, (uint64_t*)inputBufferGPU,
+                         inputTensorDesc[0].size, &inputBufferRegisteredPtr, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in registering input memory = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  err =
+      cudlaMemRegister(devHandle, (uint64_t*)outputBufferGPU,
+                       outputTensorDesc[0].size, &outputBufferRegisteredPtr, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in registering output memory = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("ALL MEMORY REGISTERED SUCCESSFULLY\n");
+
+  // Copy data from CPU buffers to GPU buffers.
+  result = hipMemcpyAsync(inputBufferGPU, inputBuffer, inputTensorDesc[0].size,
+                           hipMemcpyHostToDevice, stream);
+  if (result != hipSuccess) {
+    DPRINTF("Error in enqueueing memcpy for input\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+  result =
+      hipMemsetAsync(outputBufferGPU, 0, outputTensorDesc[0].size, stream);
+  if (result != hipSuccess) {
+    DPRINTF("Error in enqueueing memset for output\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  // Enqueue a cuDLA task.
+  cudlaTask task;
+  task.moduleHandle = moduleHandle;
+  task.outputTensor = &outputBufferRegisteredPtr;
+  task.numOutputTensors = 1;
+  task.numInputTensors = 1;
+  task.inputTensor = &inputBufferRegisteredPtr;
+  task.waitEvents = NULL;
+  task.signalEvents = NULL;
+  err = cudlaSubmitTask(devHandle, &task, 1, stream, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in submitting task\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("SUBMIT IS DONE !!!\n");
+
+  // Wait for stream operations to finish and bring output buffer to CPU.
+  result =
+      hipMemcpyAsync(outputBuffer, outputBufferGPU, outputTensorDesc[0].size,
+                      hipMemcpyDeviceToHost, stream);
+  if (result != hipSuccess) {
+    DPRINTF("Error in bringing result back to CPU\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+  result = hipStreamSynchronize(stream);
+  if (result != hipSuccess) {
+    DPRINTF("Error in synchronizing stream\n");
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  // Output is available in outputBuffer.
+
+  // Teardown.
+  err = cudlaMemUnregister(devHandle, inputBufferRegisteredPtr);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in unregistering input memory = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  err = cudlaMemUnregister(devHandle, outputBufferRegisteredPtr);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in registering output memory = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  }
+  DPRINTF("ALL MEMORY UNREGISTERED SUCCESSFULLY\n");
+
+  free(inputTensorDesc);
+  free(outputTensorDesc);
+  free(loadableData);
+  free(inputBuffer);
+  free(outputBuffer);
+  hipFree(inputBufferGPU);
+  hipFree(outputBufferGPU);
+
+  resourceList.inputTensorDesc = NULL;
+  resourceList.outputTensorDesc = NULL;
+  resourceList.loadableData = NULL;
+  resourceList.inputBuffer = NULL;
+  resourceList.outputBuffer = NULL;
+  resourceList.inputBufferGPU = 0;
+  resourceList.outputBufferGPU = 0;
+
+  result = hipStreamDestroy(stream);
+  if (result != hipSuccess) {
+    errPtr = hipGetErrorName(result);
+    DPRINTF("Error in destroying cuda stream = %s\n", errPtr);
+    cleanUp(&resourceList);
+    return 1;
+  }
+
+  resourceList.stream = NULL;
+
+  err = cudlaModuleUnload(moduleHandle, 0);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in cudlaModuleUnload = %d\n", err);
+    cleanUp(&resourceList);
+    return 1;
+  } else {
+    DPRINTF("Successfully unloaded module\n");
+  }
+
+  resourceList.moduleHandle = NULL;
+
+  err = cudlaDestroyDevice(devHandle);
+  if (err != cudlaSuccess) {
+    DPRINTF("Error in cuDLA destroy device = %d\n", err);
+    return 1;
+  }
+  DPRINTF("Device destroyed successfully\n");
+
+  resourceList.devHandle = NULL;
+
+  DPRINTF("cuDLAHybridMode DONE !!!\n");
+
+  return 0;
+}
diff --git a/src/samples/Samples/4_CUDA_Libraries/lineOfSight/lineOfSight.cu.hip b/src/samples/Samples/4_CUDA_Libraries/lineOfSight/lineOfSight.cu.hip
index c4dd461..7edb61c 100755
--- a/src/samples/Samples/4_CUDA_Libraries/lineOfSight/lineOfSight.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/lineOfSight/lineOfSight.cu.hip
@@ -152,11 +152,11 @@ int runTest(int argc, char **argv) {
   hipChannelFormatDesc channelDesc =
       hipCreateChannelDesc(32, 0, 0, 0, hipChannelFormatKindFloat);
   hipArray *heightFieldArray;
-  checkCudaErrors(
+  HIPCHECK(
       hipMallocArray(&heightFieldArray, &channelDesc, dim.x, dim.y));
 
   // Initialize device memory
-  checkCudaErrors(hipMemcpy2DToArray(
+  HIPCHECK(hipMemcpy2DToArray(
       heightFieldArray, 0, 0, heightField.height, dim.x * sizeof(float),
       dim.x * sizeof(float), dim.y, hipMemcpyHostToDevice));
 
@@ -175,7 +175,7 @@ int runTest(int argc, char **argv) {
   texDescr.addressMode[1] = hipAddressModeClamp;
   texDescr.readMode = hipReadModeElementType;
 
-  checkCudaErrors(
+  HIPCHECK(
       hipCreateTextureObject(&heightFieldTex, &texRes, &texDescr, NULL));
 
   //////////////////////////////////////////////////////////////////////////////
@@ -256,7 +256,7 @@ int runTest(int argc, char **argv) {
   sdkResetTimer(&timer);
 
   // Cleanup memory
-  checkCudaErrors(hipFreeArray(heightFieldArray));
+  HIPCHECK(hipFreeArray(heightFieldArray));
   return res;
 }
 
diff --git a/src/samples/Samples/4_CUDA_Libraries/oceanFFT/oceanFFT_kernel.cu.hip b/src/samples/Samples/4_CUDA_Libraries/oceanFFT/oceanFFT_kernel.cu.hip
index e69de29..16d309d 100755
--- a/src/samples/Samples/4_CUDA_Libraries/oceanFFT/oceanFFT_kernel.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/oceanFFT/oceanFFT_kernel.cu.hip
@@ -0,0 +1,160 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+///////////////////////////////////////////////////////////////////////////////
+#include <hipfft.h>
+#include <math_constants.h>
+
+// Round a / b to nearest higher integer value
+int cuda_iDivUp(int a, int b) { return (a + (b - 1)) / b; }
+
+// complex math functions
+__device__ float2 conjugate(float2 arg) { return make_float2(arg.x, -arg.y); }
+
+__device__ float2 complex_exp(float arg) {
+  return make_float2(cosf(arg), sinf(arg));
+}
+
+__device__ float2 complex_add(float2 a, float2 b) {
+  return make_float2(a.x + b.x, a.y + b.y);
+}
+
+__device__ float2 complex_mult(float2 ab, float2 cd) {
+  return make_float2(ab.x * cd.x - ab.y * cd.y, ab.x * cd.y + ab.y * cd.x);
+}
+
+// generate wave heightfield at time t based on initial heightfield and
+// dispersion relationship
+__global__ void generateSpectrumKernel(float2 *h0, float2 *ht,
+                                       unsigned int in_width,
+                                       unsigned int out_width,
+                                       unsigned int out_height, float t,
+                                       float patchSize) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+  unsigned int in_index = y * in_width + x;
+  unsigned int in_mindex =
+      (out_height - y) * in_width + (out_width - x);  // mirrored
+  unsigned int out_index = y * out_width + x;
+
+  // calculate wave vector
+  float2 k;
+  k.x = (-(int)out_width / 2.0f + x) * (2.0f * CUDART_PI_F / patchSize);
+  k.y = (-(int)out_width / 2.0f + y) * (2.0f * CUDART_PI_F / patchSize);
+
+  // calculate dispersion w(k)
+  float k_len = sqrtf(k.x * k.x + k.y * k.y);
+  float w = sqrtf(9.81f * k_len);
+
+  if ((x < out_width) && (y < out_height)) {
+    float2 h0_k = h0[in_index];
+    float2 h0_mk = h0[in_mindex];
+
+    // output frequency-space complex values
+    ht[out_index] =
+        complex_add(complex_mult(h0_k, complex_exp(w * t)),
+                    complex_mult(conjugate(h0_mk), complex_exp(-w * t)));
+    // ht[out_index] = h0_k;
+  }
+}
+
+// update height map values based on output of FFT
+__global__ void updateHeightmapKernel(float *heightMap, float2 *ht,
+                                      unsigned int width) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+  unsigned int i = y * width + x;
+
+  // cos(pi * (m1 + m2))
+  float sign_correction = ((x + y) & 0x01) ? -1.0f : 1.0f;
+
+  heightMap[i] = ht[i].x * sign_correction;
+}
+
+// update height map values based on output of FFT
+__global__ void updateHeightmapKernel_y(float *heightMap, float2 *ht,
+                                        unsigned int width) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+  unsigned int i = y * width + x;
+
+  // cos(pi * (m1 + m2))
+  float sign_correction = ((x + y) & 0x01) ? -1.0f : 1.0f;
+
+  heightMap[i] = ht[i].y * sign_correction;
+}
+
+// generate slope by partial differences in spatial domain
+__global__ void calculateSlopeKernel(float *h, float2 *slopeOut,
+                                     unsigned int width, unsigned int height) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+  unsigned int i = y * width + x;
+
+  float2 slope = make_float2(0.0f, 0.0f);
+
+  if ((x > 0) && (y > 0) && (x < width - 1) && (y < height - 1)) {
+    slope.x = h[i + 1] - h[i - 1];
+    slope.y = h[i + width] - h[i - width];
+  }
+
+  slopeOut[i] = slope;
+}
+
+// wrapper functions
+extern "C" void cudaGenerateSpectrumKernel(float2 *d_h0, float2 *d_ht,
+                                           unsigned int in_width,
+                                           unsigned int out_width,
+                                           unsigned int out_height,
+                                           float animTime, float patchSize) {
+  dim3 block(8, 8, 1);
+  dim3 grid(cuda_iDivUp(out_width, block.x), cuda_iDivUp(out_height, block.y),
+            1);
+  generateSpectrumKernel<<<grid, block>>>(d_h0, d_ht, in_width, out_width,
+                                          out_height, animTime, patchSize);
+}
+
+extern "C" void cudaUpdateHeightmapKernel(float *d_heightMap, float2 *d_ht,
+                                          unsigned int width,
+                                          unsigned int height, bool autoTest) {
+  dim3 block(8, 8, 1);
+  dim3 grid(cuda_iDivUp(width, block.x), cuda_iDivUp(height, block.y), 1);
+  if (autoTest) {
+    updateHeightmapKernel_y<<<grid, block>>>(d_heightMap, d_ht, width);
+  } else {
+    updateHeightmapKernel<<<grid, block>>>(d_heightMap, d_ht, width);
+  }
+}
+
+extern "C" void cudaCalculateSlopeKernel(float *hptr, float2 *slopeOut,
+                                         unsigned int width,
+                                         unsigned int height) {
+  dim3 block(8, 8, 1);
+  dim3 grid2(cuda_iDivUp(width, block.x), cuda_iDivUp(height, block.y), 1);
+  calculateSlopeKernel<<<grid2, block>>>(hptr, slopeOut, width, height);
+}
diff --git a/src/samples/Samples/5_Domain_Specific/HSOpticalFlow/flowCUDA.cu.hip b/src/samples/Samples/5_Domain_Specific/HSOpticalFlow/flowCUDA.cu.hip
index e0e1f31..e94f08c 100755
--- a/src/samples/Samples/5_Domain_Specific/HSOpticalFlow/flowCUDA.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/HSOpticalFlow/flowCUDA.cu.hip
@@ -25,17 +25,15 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  */
 
-
-#include <hip/hip_runtime.h>
 #include "common.h"
-#include "helper_cuda_hipified.h"
+
 // include kernels
-#include "downscaleKernel_hipified.cuh"
-#include "upscaleKernel_hipified.cuh"
-#include "warpingKernel_hipified.cuh"
-#include "derivativesKernel_hipified.cuh"
-#include "solverKernel_hipified.cuh"
-#include "addKernel_hipified.cuh"
+#include "downscaleKernel.cuh"
+#include "upscaleKernel.cuh"
+#include "warpingKernel.cuh"
+#include "derivativesKernel.cuh"
+#include "solverKernel.cuh"
+#include "addKernel.cuh"
 
 ///////////////////////////////////////////////////////////////////////////////
 /// \brief method logic
diff --git a/src/samples/Samples/5_Domain_Specific/SobelFilter/SobelFilter_kernels.cu.hip b/src/samples/Samples/5_Domain_Specific/SobelFilter/SobelFilter_kernels.cu.hip
index 1a9e3b2..d4f03ec 100755
--- a/src/samples/Samples/5_Domain_Specific/SobelFilter/SobelFilter_kernels.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/SobelFilter/SobelFilter_kernels.cu.hip
@@ -302,3 +302,4 @@ extern "C" void sobelFilter(Pixel *odata, int iw, int ih,
          iw, ih, fScale, texObject);
     } break;
   }
+}
diff --git a/src/samples/Samples/5_Domain_Specific/SobolQRNG/SobolQRNG.out b/src/samples/Samples/5_Domain_Specific/SobolQRNG/SobolQRNG.out
index c0832b9..3dcbd35 100755
Binary files a/src/samples/Samples/5_Domain_Specific/SobolQRNG/SobolQRNG.out and b/src/samples/Samples/5_Domain_Specific/SobolQRNG/SobolQRNG.out differ
diff --git a/src/samples/Samples/5_Domain_Specific/SobolQRNG/sobol_gpu.cu.hip b/src/samples/Samples/5_Domain_Specific/SobolQRNG/sobol_gpu.cu.hip
index e09b0e8..388ff07 100755
--- a/src/samples/Samples/5_Domain_Specific/SobolQRNG/sobol_gpu.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/SobolQRNG/sobol_gpu.cu.hip
@@ -58,7 +58,7 @@
 #include "sobol.h"
 #include "sobol_gpu.h"
 #include <hip/hip_cooperative_groups.h>
-
+#include "HIPCHECK.h"
 namespace cg = cooperative_groups;
 #include "helper_cuda_hipified.h"
 
diff --git a/src/samples/Samples/5_Domain_Specific/binomialOptions/binomialOptions.out b/src/samples/Samples/5_Domain_Specific/binomialOptions/binomialOptions.out
index ef6fb2b..674e891 100755
Binary files a/src/samples/Samples/5_Domain_Specific/binomialOptions/binomialOptions.out and b/src/samples/Samples/5_Domain_Specific/binomialOptions/binomialOptions.out differ
diff --git a/src/samples/Samples/5_Domain_Specific/binomialOptions_nvrtc/binomialOptions_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/binomialOptions_nvrtc/binomialOptions_kernel.cu.hip
index e69de29..24fd6b5 100755
--- a/src/samples/Samples/5_Domain_Specific/binomialOptions_nvrtc/binomialOptions_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/binomialOptions_nvrtc/binomialOptions_kernel.cu.hip
@@ -0,0 +1,108 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "common_gpu_header.h"
+#include "binomialOptions_common.h"
+#include "realtype.h"
+
+// Preprocessed input option data
+typedef struct {
+  real S;
+  real X;
+  real vDt;
+  real puByDf;
+  real pdByDf;
+} __TOptionData;
+static __constant__ __TOptionData d_OptionData[MAX_OPTIONS];
+__device__ real d_CallValue[MAX_OPTIONS];
+
+#define THREADBLOCK_SIZE 128
+#define ELEMS_PER_THREAD (NUM_STEPS / THREADBLOCK_SIZE)
+#if NUM_STEPS % THREADBLOCK_SIZE
+#error Bad constants
+#endif
+
+////////////////////////////////////////////////////////////////////////////////
+// Overloaded shortcut functions for different precision modes
+////////////////////////////////////////////////////////////////////////////////
+
+#ifndef DOUBLE_PRECISION
+__device__ inline float expiryCallValue(float S, float X, float vDt, int i) {
+  float d = S * __expf(vDt * (2.0f * i - NUM_STEPS)) - X;
+  return (d > 0.0F) ? d : 0.0F;
+}
+
+#else
+__device__ inline double expiryCallValue(double S, double X, double vDt,
+                                         int i) {
+  double d = S * exp(vDt * (2.0 * i - NUM_STEPS)) - X;
+  return (d > 0.0) ? d : 0.0;
+}
+#endif
+
+////////////////////////////////////////////////////////////////////////////////
+// GPU kernel
+////////////////////////////////////////////////////////////////////////////////
+extern "C" __global__ void binomialOptionsKernel() {
+  __shared__ real call_exchange[THREADBLOCK_SIZE + 1];
+
+  const int tid = threadIdx.x;
+  const real S = d_OptionData[blockIdx.x].S;
+  const real X = d_OptionData[blockIdx.x].X;
+  const real vDt = d_OptionData[blockIdx.x].vDt;
+  const real puByDf = d_OptionData[blockIdx.x].puByDf;
+  const real pdByDf = d_OptionData[blockIdx.x].pdByDf;
+
+  real call[ELEMS_PER_THREAD + 1];
+#pragma unroll
+  for (int i = 0; i < ELEMS_PER_THREAD; ++i)
+    call[i] = expiryCallValue(S, X, vDt, tid * ELEMS_PER_THREAD + i);
+
+  if (tid == 0)
+    call_exchange[THREADBLOCK_SIZE] = expiryCallValue(S, X, vDt, NUM_STEPS);
+
+  int final_it = max(0, tid * ELEMS_PER_THREAD - 1);
+
+#pragma unroll 16
+  for (int i = NUM_STEPS; i > 0; --i) {
+    call_exchange[tid] = call[0];
+    __syncthreads();
+    call[ELEMS_PER_THREAD] = call_exchange[tid + 1];
+    __syncthreads();
+
+    if (i > final_it) {
+#pragma unroll
+      for (int j = 0; j < ELEMS_PER_THREAD; ++j)
+        call[j] = puByDf * call[j + 1] + pdByDf * call[j];
+    }
+  }
+
+  if (tid == 0) {
+    d_CallValue[blockIdx.x] = call[0];
+  }
+}
diff --git a/src/samples/Samples/5_Domain_Specific/convolutionFFT2D/convolutionFFT2D.cu.hip b/src/samples/Samples/5_Domain_Specific/convolutionFFT2D/convolutionFFT2D.cu.hip
index e69de29..25f8f07 100755
--- a/src/samples/Samples/5_Domain_Specific/convolutionFFT2D/convolutionFFT2D.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/convolutionFFT2D/convolutionFFT2D.cu.hip
@@ -0,0 +1,321 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <assert.h>
+#include <stdio.h>
+#include "rocprofiler.h"
+#include "HIPCHECK.h"
+#include <stdlib.h>
+#include <string.h>
+#include "helper_cuda_hipified.h"
+#include "convolutionFFT2D_common.h"
+#include "convolutionFFT2D.cuh"
+
+////////////////////////////////////////////////////////////////////////////////
+/// Position convolution kernel center at (0, 0) in the image
+////////////////////////////////////////////////////////////////////////////////
+extern "C" void padKernel(float *d_Dst, float *d_Src, int fftH, int fftW,
+                          int kernelH, int kernelW, int kernelY, int kernelX) {
+  assert(d_Src != d_Dst);
+  dim3 threads(32, 8);
+  dim3 grid(iDivUp(kernelW, threads.x), iDivUp(kernelH, threads.y));
+
+  SET_FLOAT_BASE;
+#if (USE_TEXTURE)
+  hipTextureObject_t texFloat;
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeLinear;
+  texRes.res.linear.devPtr = d_Src;
+  texRes.res.linear.sizeInBytes = sizeof(float) * kernelH * kernelW;
+  texRes.res.linear.desc = hipCreateChannelDesc<float>();
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+
+  checkCudaErrors(hipCreateTextureObject(&texFloat, &texRes, &texDescr, NULL));
+#endif
+
+  padKernel_kernel<<<grid, threads>>>(d_Dst, d_Src, fftH, fftW, kernelH,
+                                      kernelW, kernelY, kernelX
+#if (USE_TEXTURE)
+                                      ,
+                                      texFloat
+#endif
+                                      );
+  getLastCudaError("padKernel_kernel<<<>>> execution failed\n");
+
+#if (USE_TEXTURE)
+  checkCudaErrors(hipDestroyTextureObject(texFloat));
+#endif
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Prepare data for "pad to border" addressing mode
+////////////////////////////////////////////////////////////////////////////////
+extern "C" void padDataClampToBorder(float *d_Dst, float *d_Src, int fftH,
+                                     int fftW, int dataH, int dataW,
+                                     int kernelW, int kernelH, int kernelY,
+                                     int kernelX) {
+  assert(d_Src != d_Dst);
+  dim3 threads(32, 8);
+  dim3 grid(iDivUp(fftW, threads.x), iDivUp(fftH, threads.y));
+
+#if (USE_TEXTURE)
+  hipTextureObject_t texFloat;
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeLinear;
+  texRes.res.linear.devPtr = d_Src;
+  texRes.res.linear.sizeInBytes = sizeof(float) * dataH * dataW;
+  texRes.res.linear.desc = hipCreateChannelDesc<float>();
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+
+  checkCudaErrors(hipCreateTextureObject(&texFloat, &texRes, &texDescr, NULL));
+#endif
+
+  padDataClampToBorder_kernel<<<grid, threads>>>(
+      d_Dst, d_Src, fftH, fftW, dataH, dataW, kernelH, kernelW, kernelY, kernelX
+#if (USE_TEXTURE)
+      ,
+      texFloat
+#endif
+      );
+  getLastCudaError("padDataClampToBorder_kernel<<<>>> execution failed\n");
+
+#if (USE_TEXTURE)
+  checkCudaErrors(hipDestroyTextureObject(texFloat));
+#endif
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Modulate Fourier image of padded data by Fourier image of padded kernel
+// and normalize by FFT size
+////////////////////////////////////////////////////////////////////////////////
+extern "C" void modulateAndNormalize(fComplex *d_Dst, fComplex *d_Src, int fftH,
+                                     int fftW, int padding) {
+  assert(fftW % 2 == 0);
+  const int dataSize = fftH * (fftW / 2 + padding);
+
+  modulateAndNormalize_kernel<<<iDivUp(dataSize, 256), 256>>>(
+      d_Dst, d_Src, dataSize, 1.0f / (float)(fftW * fftH));
+  getLastCudaError("modulateAndNormalize() execution failed\n");
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// 2D R2C / C2R post/preprocessing kernels
+////////////////////////////////////////////////////////////////////////////////
+static const double PI = 3.1415926535897932384626433832795;
+static const uint BLOCKDIM = 256;
+
+extern "C" void spPostprocess2D(void *d_Dst, void *d_Src, uint DY, uint DX,
+                                uint padding, int dir) {
+  assert(d_Src != d_Dst);
+  assert(DX % 2 == 0);
+
+#if (POWER_OF_TWO)
+  uint log2DX, log2DY;
+  uint factorizationRemX = factorRadix2(log2DX, DX);
+  uint factorizationRemY = factorRadix2(log2DY, DY);
+  assert(factorizationRemX == 1 && factorizationRemY == 1);
+#endif
+
+  const uint threadCount = DY * (DX / 2);
+  const double phaseBase = dir * PI / (double)DX;
+
+#if (USE_TEXTURE)
+  hipTextureObject_t texComplex;
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeLinear;
+  texRes.res.linear.devPtr = d_Src;
+  texRes.res.linear.sizeInBytes = sizeof(fComplex) * DY * (DX + padding);
+  texRes.res.linear.desc = hipCreateChannelDesc<fComplex>();
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+
+  checkCudaErrors(
+      hipCreateTextureObject(&texComplex, &texRes, &texDescr, NULL));
+#endif
+
+  spPostprocess2D_kernel<<<iDivUp(threadCount, BLOCKDIM), BLOCKDIM>>>(
+      (fComplex *)d_Dst, (fComplex *)d_Src, DY, DX, threadCount, padding,
+      (float)phaseBase
+#if (USE_TEXTURE)
+      ,
+      texComplex
+#endif
+      );
+  getLastCudaError("spPostprocess2D_kernel<<<>>> execution failed\n");
+
+#if (USE_TEXTURE)
+  checkCudaErrors(hipDestroyTextureObject(texComplex));
+#endif
+}
+
+extern "C" void spPreprocess2D(void *d_Dst, void *d_Src, uint DY, uint DX,
+                               uint padding, int dir) {
+  assert(d_Src != d_Dst);
+  assert(DX % 2 == 0);
+
+#if (POWER_OF_TWO)
+  uint log2DX, log2DY;
+  uint factorizationRemX = factorRadix2(log2DX, DX);
+  uint factorizationRemY = factorRadix2(log2DY, DY);
+  assert(factorizationRemX == 1 && factorizationRemY == 1);
+#endif
+
+  const uint threadCount = DY * (DX / 2);
+  const double phaseBase = -dir * PI / (double)DX;
+
+#if (USE_TEXTURE)
+  hipTextureObject_t texComplex;
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeLinear;
+  texRes.res.linear.devPtr = d_Src;
+  texRes.res.linear.sizeInBytes = sizeof(fComplex) * DY * (DX + padding);
+  texRes.res.linear.desc = hipCreateChannelDesc<fComplex>();
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+
+  checkCudaErrors(
+      hipCreateTextureObject(&texComplex, &texRes, &texDescr, NULL));
+#endif
+  spPreprocess2D_kernel<<<iDivUp(threadCount, BLOCKDIM), BLOCKDIM>>>(
+      (fComplex *)d_Dst, (fComplex *)d_Src, DY, DX, threadCount, padding,
+      (float)phaseBase
+#if (USE_TEXTURE)
+      ,
+      texComplex
+#endif
+      );
+  getLastCudaError("spPreprocess2D_kernel<<<>>> execution failed\n");
+
+#if (USE_TEXTURE)
+  checkCudaErrors(hipDestroyTextureObject(texComplex));
+#endif
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Combined spPostprocess2D + modulateAndNormalize + spPreprocess2D
+////////////////////////////////////////////////////////////////////////////////
+extern "C" void spProcess2D(void *d_Dst, void *d_SrcA, void *d_SrcB, uint DY,
+                            uint DX, int dir) {
+  assert(DY % 2 == 0);
+
+#if (POWER_OF_TWO)
+  uint log2DX, log2DY;
+  uint factorizationRemX = factorRadix2(log2DX, DX);
+  uint factorizationRemY = factorRadix2(log2DY, DY);
+  assert(factorizationRemX == 1 && factorizationRemY == 1);
+#endif
+
+  const uint threadCount = (DY / 2) * DX;
+  const double phaseBase = dir * PI / (double)DX;
+
+#if (USE_TEXTURE)
+  hipTextureObject_t texComplexA, texComplexB;
+  hipResourceDesc texRes;
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeLinear;
+  texRes.res.linear.devPtr = d_SrcA;
+  texRes.res.linear.sizeInBytes = sizeof(fComplex) * DY * DX;
+  texRes.res.linear.desc = hipCreateChannelDesc<fComplex>();
+
+  hipTextureDesc texDescr;
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+
+  checkCudaErrors(
+      hipCreateTextureObject(&texComplexA, &texRes, &texDescr, NULL));
+
+  memset(&texRes, 0, sizeof(hipResourceDesc));
+
+  texRes.resType = hipResourceTypeLinear;
+  texRes.res.linear.devPtr = d_SrcB;
+  texRes.res.linear.sizeInBytes = sizeof(fComplex) * DY * DX;
+  texRes.res.linear.desc = hipCreateChannelDesc<fComplex>();
+
+  memset(&texDescr, 0, sizeof(hipTextureDesc));
+
+  texDescr.normalizedCoords = false;
+  texDescr.filterMode = hipFilterModeLinear;
+  texDescr.addressMode[0] = hipAddressModeWrap;
+  texDescr.readMode = hipReadModeElementType;
+
+  checkCudaErrors(
+      hipCreateTextureObject(&texComplexB, &texRes, &texDescr, NULL));
+#endif
+  spProcess2D_kernel<<<iDivUp(threadCount, BLOCKDIM), BLOCKDIM>>>(
+      (fComplex *)d_Dst, (fComplex *)d_SrcA, (fComplex *)d_SrcB, DY, DX,
+      threadCount, (float)phaseBase, 0.5f / (float)(DY * DX)
+#if (USE_TEXTURE)
+                                         ,
+      texComplexA, texComplexB
+#endif
+      );
+  getLastCudaError("spProcess2D_kernel<<<>>> execution failed\n");
+
+#if (USE_TEXTURE)
+  checkCudaErrors(hipDestroyTextureObject(texComplexA));
+  checkCudaErrors(hipDestroyTextureObject(texComplexB));
+#endif
+}
diff --git a/src/samples/Samples/5_Domain_Specific/dwtHaar1D/dwtHaar1D.out b/src/samples/Samples/5_Domain_Specific/dwtHaar1D/dwtHaar1D.out
deleted file mode 100755
index e0b3ec8..0000000
Binary files a/src/samples/Samples/5_Domain_Specific/dwtHaar1D/dwtHaar1D.out and /dev/null differ
diff --git a/src/samples/Samples/5_Domain_Specific/dxtc/dxtc.cu.hip b/src/samples/Samples/5_Domain_Specific/dxtc/dxtc.cu.hip
index 662ba28..bc333f3 100755
--- a/src/samples/Samples/5_Domain_Specific/dxtc/dxtc.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/dxtc/dxtc.cu.hip
@@ -37,9 +37,9 @@ namespace cg = cooperative_groups;
 #include <helper_math.h>
 #include <float.h>  // for FLT_MAX
 
-#include "CudaMath_hipified.h"
-#include "dds_hipified.h"
-#include "permutations_hipified.h"
+#include "CudaMath.h"
+#include "dds.h"
+#include "permutations.h"
 
 // Definitions
 #define INPUT_IMAGE "teapot512_std.ppm"
@@ -611,12 +611,12 @@ int main(int argc, char **argv) {
 
   // copy into global mem
   uint *d_data = NULL;
-  checkCudaErrors(hipMalloc((void **)&d_data, memSize));
+  HIPCHECK(hipMalloc((void **)&d_data, memSize));
 
   // Result
   uint *d_result = NULL;
   const uint compressedSize = (w / 4) * (h / 4) * 8;
-  checkCudaErrors(hipMalloc((void **)&d_result, compressedSize));
+  HIPCHECK(hipMalloc((void **)&d_result, compressedSize));
   uint *h_result = (uint *)malloc(compressedSize);
 
   // Compute permutations.
@@ -625,8 +625,8 @@ int main(int argc, char **argv) {
 
   // Copy permutations host to devie.
   uint *d_permutations = NULL;
-  checkCudaErrors(hipMalloc((void **)&d_permutations, 1024 * sizeof(uint)));
-  checkCudaErrors(hipMemcpy(d_permutations, permutations, 1024 * sizeof(uint),
+  HIPCHECK(hipMalloc((void **)&d_permutations, 1024 * sizeof(uint)));
+  HIPCHECK(hipMemcpy(d_permutations, permutations, 1024 * sizeof(uint),
                              hipMemcpyHostToDevice));
 
   // create a timer
@@ -634,7 +634,7 @@ int main(int argc, char **argv) {
   sdkCreateTimer(&timer);
 
   // Copy image from host to device
-  checkCudaErrors(
+  HIPCHECK(
       hipMemcpy(d_data, block_image, memSize, hipMemcpyHostToDevice));
 
   // Determine launch configuration and run timed computation numIterations
@@ -646,8 +646,8 @@ int main(int argc, char **argv) {
   hipDeviceProp_t deviceProp;
 
   // get number of SMs on this GPU
-  checkCudaErrors(hipGetDevice(&devID));
-  checkCudaErrors(hipGetDeviceProperties(&deviceProp, devID));
+  HIPCHECK(hipGetDevice(&devID));
+  HIPCHECK(hipGetDeviceProperties(&deviceProp, devID));
 
   // Restrict the numbers of blocks to launch on low end GPUs to avoid kernel
   // timeout
@@ -660,7 +660,7 @@ int main(int argc, char **argv) {
 
   for (int i = -1; i < numIterations; ++i) {
     if (i == 0) {
-      checkCudaErrors(hipDeviceSynchronize());
+      HIPCHECK(hipDeviceSynchronize());
       sdkStartTimer(&timer);
     }
 
@@ -673,7 +673,7 @@ int main(int argc, char **argv) {
   getLastCudaError("compress");
 
   // sync to host, stop timer, record perf
-  checkCudaErrors(hipDeviceSynchronize());
+  HIPCHECK(hipDeviceSynchronize());
   sdkStopTimer(&timer);
   double dAvgTime = 1.0e-3 * sdkGetTimerValue(&timer) / (double)numIterations;
   printf(
@@ -682,7 +682,7 @@ int main(int argc, char **argv) {
       (1.0e-6 * (double)(W * H) / dAvgTime), dAvgTime, (W * H), 1, NUM_THREADS);
 
   // copy result data from device to host
-  checkCudaErrors(
+  HIPCHECK(
       hipMemcpy(h_result, d_result, compressedSize, hipMemcpyDeviceToHost));
 
   // Write out result data to DDS file
@@ -770,9 +770,9 @@ int main(int argc, char **argv) {
   rms /= w * h * 3;
 
   // Free allocated resources and exit
-  checkCudaErrors(hipFree(d_permutations));
-  checkCudaErrors(hipFree(d_data));
-  checkCudaErrors(hipFree(d_result));
+  HIPCHECK(hipFree(d_permutations));
+  HIPCHECK(hipFree(d_data));
+  HIPCHECK(hipFree(d_result));
   free(image_path);
   free(data);
   free(block_image);
@@ -797,3 +797,7 @@ int main(int argc, char **argv) {
   /* Return zero if test passed, one otherwise */
   return rms > ERROR_THRESHOLD;
 }
+!\n");
+  /* Return zero if test passed, one otherwise */
+  return rms > ERROR_THRESHOLD;
+}
diff --git a/src/samples/Samples/5_Domain_Specific/fastWalshTransform/fastWalshTransform.out b/src/samples/Samples/5_Domain_Specific/fastWalshTransform/fastWalshTransform.out
index a713fe2..a678a1b 100755
Binary files a/src/samples/Samples/5_Domain_Specific/fastWalshTransform/fastWalshTransform.out and b/src/samples/Samples/5_Domain_Specific/fastWalshTransform/fastWalshTransform.out differ
diff --git a/src/samples/Samples/5_Domain_Specific/nbody/bodysystemcuda.cu.hip b/src/samples/Samples/5_Domain_Specific/nbody/bodysystemcuda.cu.hip
index e69de29..8823fdd 100755
--- a/src/samples/Samples/5_Domain_Specific/nbody/bodysystemcuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/nbody/bodysystemcuda.cu.hip
@@ -0,0 +1,288 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "helper_cuda_hipified.h"
+#include <math.h>
+
+#if defined(__APPLE__) || defined(MACOSX)
+#pragma clang diagnostic ignored "-Wdeprecated-declarations"
+#include <GLUT/glut.h>
+#else
+#include <GL/freeglut.h>
+#endif
+
+// CUDA standard includes
+#include <hip/hip_runtime.h>
+#include <cuda_gl_interop.h>
+
+#include <hip/hip_cooperative_groups.h>
+
+namespace cg = cooperative_groups;
+
+#include "bodysystem.h"
+
+__constant__ float softeningSquared;
+__constant__ double softeningSquared_fp64;
+
+hipError_t setSofteningSquared(float softeningSq) {
+  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared), &softeningSq, sizeof(float), 0,
+                            hipMemcpyHostToDevice);
+}
+
+hipError_t setSofteningSquared(double softeningSq) {
+  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared_fp64), &softeningSq, sizeof(double),
+                            0, hipMemcpyHostToDevice);
+}
+
+template <class T>
+struct SharedMemory {
+  __device__ inline operator T *() {
+    extern __shared__ int __smem[];
+    return (T *)__smem;
+  }
+
+  __device__ inline operator const T *() const {
+    extern __shared__ int __smem[];
+    return (T *)__smem;
+  }
+};
+
+template <typename T>
+__device__ T rsqrt_T(T x) {
+  return rsqrt(x);
+}
+
+template <>
+__device__ float rsqrt_T<float>(float x) {
+  return rsqrtf(x);
+}
+
+template <>
+__device__ double rsqrt_T<double>(double x) {
+  return rsqrt(x);
+}
+
+// Macros to simplify shared memory addressing
+#define SX(i) sharedPos[i + blockDim.x * threadIdx.y]
+// This macro is only used when multithreadBodies is true (below)
+#define SX_SUM(i, j) sharedPos[i + blockDim.x * j]
+
+template <typename T>
+__device__ T getSofteningSquared() {
+  return softeningSquared;
+}
+template <>
+__device__ double getSofteningSquared<double>() {
+  return softeningSquared_fp64;
+}
+
+template <typename T>
+struct DeviceData {
+  T *dPos[2];  // mapped host pointers
+  T *dVel;
+  hipEvent_t event;
+  unsigned int offset;
+  unsigned int numBodies;
+};
+
+template <typename T>
+__device__ typename vec3<T>::Type bodyBodyInteraction(
+    typename vec3<T>::Type ai, typename vec4<T>::Type bi,
+    typename vec4<T>::Type bj) {
+  typename vec3<T>::Type r;
+
+  // r_ij  [3 FLOPS]
+  r.x = bj.x - bi.x;
+  r.y = bj.y - bi.y;
+  r.z = bj.z - bi.z;
+
+  // distSqr = dot(r_ij, r_ij) + EPS^2  [6 FLOPS]
+  T distSqr = r.x * r.x + r.y * r.y + r.z * r.z;
+  distSqr += getSofteningSquared<T>();
+
+  // invDistCube =1/distSqr^(3/2)  [4 FLOPS (2 mul, 1 sqrt, 1 inv)]
+  T invDist = rsqrt_T(distSqr);
+  T invDistCube = invDist * invDist * invDist;
+
+  // s = m_j * invDistCube [1 FLOP]
+  T s = bj.w * invDistCube;
+
+  // a_i =  a_i + s * r_ij [6 FLOPS]
+  ai.x += r.x * s;
+  ai.y += r.y * s;
+  ai.z += r.z * s;
+
+  return ai;
+}
+
+template <typename T>
+__device__ typename vec3<T>::Type computeBodyAccel(
+    typename vec4<T>::Type bodyPos, typename vec4<T>::Type *positions,
+    int numTiles, cg::thread_block cta) {
+  typename vec4<T>::Type *sharedPos = SharedMemory<typename vec4<T>::Type>();
+
+  typename vec3<T>::Type acc = {0.0f, 0.0f, 0.0f};
+
+  for (int tile = 0; tile < numTiles; tile++) {
+    sharedPos[threadIdx.x] = positions[tile * blockDim.x + threadIdx.x];
+
+    cg::sync(cta);
+
+// This is the "tile_calculation" from the GPUG3 article.
+#pragma unroll 128
+
+    for (unsigned int counter = 0; counter < blockDim.x; counter++) {
+      acc = bodyBodyInteraction<T>(acc, bodyPos, sharedPos[counter]);
+    }
+
+    cg::sync(cta);
+  }
+
+  return acc;
+}
+
+template <typename T>
+__global__ void integrateBodies(typename vec4<T>::Type *__restrict__ newPos,
+                                typename vec4<T>::Type *__restrict__ oldPos,
+                                typename vec4<T>::Type *vel,
+                                unsigned int deviceOffset,
+                                unsigned int deviceNumBodies, float deltaTime,
+                                float damping, int numTiles) {
+  // Handle to thread block group
+  cg::thread_block cta = cg::this_thread_block();
+  int index = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (index >= deviceNumBodies) {
+    return;
+  }
+
+  typename vec4<T>::Type position = oldPos[deviceOffset + index];
+
+  typename vec3<T>::Type accel =
+      computeBodyAccel<T>(position, oldPos, numTiles, cta);
+
+  // acceleration = force / mass;
+  // new velocity = old velocity + acceleration * deltaTime
+  // note we factor out the body's mass from the equation, here and in
+  // bodyBodyInteraction
+  // (because they cancel out).  Thus here force == acceleration
+  typename vec4<T>::Type velocity = vel[deviceOffset + index];
+
+  velocity.x += accel.x * deltaTime;
+  velocity.y += accel.y * deltaTime;
+  velocity.z += accel.z * deltaTime;
+
+  velocity.x *= damping;
+  velocity.y *= damping;
+  velocity.z *= damping;
+
+  // new position = old position + velocity * deltaTime
+  position.x += velocity.x * deltaTime;
+  position.y += velocity.y * deltaTime;
+  position.z += velocity.z * deltaTime;
+
+  // store new position and velocity
+  newPos[deviceOffset + index] = position;
+  vel[deviceOffset + index] = velocity;
+}
+
+template <typename T>
+void integrateNbodySystem(DeviceData<T> *deviceData,
+                          hipGraphicsResource **pgres,
+                          unsigned int currentRead, float deltaTime,
+                          float damping, unsigned int numBodies,
+                          unsigned int numDevices, int blockSize,
+                          bool bUsePBO) {
+  if (bUsePBO) {
+    HIPCHECK(cudaGraphicsResourceSetMapFlags(
+        pgres[currentRead], cudaGraphicsMapFlagsReadOnly));
+    HIPCHECK(cudaGraphicsResourceSetMapFlags(
+        pgres[1 - currentRead], cudaGraphicsMapFlagsWriteDiscard));
+    HIPCHECK(hipGraphicsMapResources(2, pgres, 0));
+    size_t bytes;
+    HIPCHECK(hipGraphicsResourceGetMappedPointer(
+        (void **)&(deviceData[0].dPos[currentRead]), &bytes,
+        pgres[currentRead]));
+    HIPCHECK(hipGraphicsResourceGetMappedPointer(
+        (void **)&(deviceData[0].dPos[1 - currentRead]), &bytes,
+        pgres[1 - currentRead]));
+  }
+
+  for (unsigned int dev = 0; dev != numDevices; dev++) {
+    if (numDevices > 1) {
+      hipSetDevice(dev);
+    }
+
+    int numBlocks = (deviceData[dev].numBodies + blockSize - 1) / blockSize;
+    int numTiles = (numBodies + blockSize - 1) / blockSize;
+    int sharedMemSize = blockSize * 4 * sizeof(T);  // 4 floats for pos
+
+    integrateBodies<T><<<numBlocks, blockSize, sharedMemSize>>>(
+        (typename vec4<T>::Type *)deviceData[dev].dPos[1 - currentRead],
+        (typename vec4<T>::Type *)deviceData[dev].dPos[currentRead],
+        (typename vec4<T>::Type *)deviceData[dev].dVel, deviceData[dev].offset,
+        deviceData[dev].numBodies, deltaTime, damping, numTiles);
+
+    if (numDevices > 1) {
+      HIPCHECK(hipEventRecord(deviceData[dev].event));
+      // MJH: Hack on older driver versions to force kernel launches to flush!
+      hipStreamQuery(0);
+    }
+
+    // check if kernel invocation generated an error
+    getLastCudaError("Kernel execution failed");
+  }
+
+  if (numDevices > 1) {
+    for (unsigned int dev = 0; dev < numDevices; dev++) {
+      HIPCHECK(hipEventSynchronize(deviceData[dev].event));
+    }
+  }
+
+  if (bUsePBO) {
+    HIPCHECK(hipGraphicsUnmapResources(2, pgres, 0));
+  }
+}
+
+// Explicit specializations needed to generate code
+template void integrateNbodySystem<float>(DeviceData<float> *deviceData,
+                                          hipGraphicsResource **pgres,
+                                          unsigned int currentRead,
+                                          float deltaTime, float damping,
+                                          unsigned int numBodies,
+                                          unsigned int numDevices,
+                                          int blockSize, bool bUsePBO);
+
+template void integrateNbodySystem<double>(DeviceData<double> *deviceData,
+                                           hipGraphicsResource **pgres,
+                                           unsigned int currentRead,
+                                           float deltaTime, float damping,
+                                           unsigned int numBodies,
+                                           unsigned int numDevices,
+                                           int blockSize, bool bUsePBO);
+                          int blockSize, bool bUsePBO);
diff --git a/src/samples/Samples/5_Domain_Specific/nbody_opengles/bodysystemcuda.cu.hip b/src/samples/Samples/5_Domain_Specific/nbody_opengles/bodysystemcuda.cu.hip
index e69de29..986ac9a 100755
--- a/src/samples/Samples/5_Domain_Specific/nbody_opengles/bodysystemcuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/nbody_opengles/bodysystemcuda.cu.hip
@@ -0,0 +1,278 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "helper_cuda_hipified.h"
+#include <math.h>
+
+//#include <GL/glew.h>
+//#include <GL/freeglut.h>
+
+// CUDA standard includes
+#include <hip/hip_runtime.h>
+//#include <cuda_gl_interop.h>
+
+#include "bodysystem.h"
+
+__constant__ float softeningSquared;
+__constant__ double softeningSquared_fp64;
+
+hipError_t setSofteningSquared(float softeningSq) {
+  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared), &softeningSq, sizeof(float), 0,
+                            hipMemcpyHostToDevice);
+}
+
+hipError_t setSofteningSquared(double softeningSq) {
+  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared_fp64), &softeningSq, sizeof(double),
+                            0, hipMemcpyHostToDevice);
+}
+
+template <class T>
+struct SharedMemory {
+  __device__ inline operator T *() {
+    extern __shared__ int __smem[];
+    return (T *)__smem;
+  }
+
+  __device__ inline operator const T *() const {
+    extern __shared__ int __smem[];
+    return (T *)__smem;
+  }
+};
+
+template <typename T>
+__device__ T rsqrt_T(T x) {
+  return rsqrt(x);
+}
+
+template <>
+__device__ float rsqrt_T<float>(float x) {
+  return rsqrtf(x);
+}
+
+template <>
+__device__ double rsqrt_T<double>(double x) {
+  return rsqrt(x);
+}
+
+// Macros to simplify shared memory addressing
+#define SX(i) sharedPos[i + blockDim.x * threadIdx.y]
+// This macro is only used when multithreadBodies is true (below)
+#define SX_SUM(i, j) sharedPos[i + blockDim.x * j]
+
+template <typename T>
+__device__ T getSofteningSquared() {
+  return softeningSquared;
+}
+template <>
+__device__ double getSofteningSquared<double>() {
+  return softeningSquared_fp64;
+}
+
+template <typename T>
+struct DeviceData {
+  T *dPos[2];  // mapped host pointers
+  T *dVel;
+  hipEvent_t event;
+  unsigned int offset;
+  unsigned int numBodies;
+};
+
+template <typename T>
+__device__ typename vec3<T>::Type bodyBodyInteraction(
+    typename vec3<T>::Type ai, typename vec4<T>::Type bi,
+    typename vec4<T>::Type bj) {
+  typename vec3<T>::Type r;
+
+  // r_ij  [3 FLOPS]
+  r.x = bj.x - bi.x;
+  r.y = bj.y - bi.y;
+  r.z = bj.z - bi.z;
+
+  // distSqr = dot(r_ij, r_ij) + EPS^2  [6 FLOPS]
+  T distSqr = r.x * r.x + r.y * r.y + r.z * r.z;
+  distSqr += getSofteningSquared<T>();
+
+  // invDistCube =1/distSqr^(3/2)  [4 FLOPS (2 mul, 1 sqrt, 1 inv)]
+  T invDist = rsqrt_T(distSqr);
+  T invDistCube = invDist * invDist * invDist;
+
+  // s = m_j * invDistCube [1 FLOP]
+  T s = bj.w * invDistCube;
+
+  // a_i =  a_i + s * r_ij [6 FLOPS]
+  ai.x += r.x * s;
+  ai.y += r.y * s;
+  ai.z += r.z * s;
+
+  return ai;
+}
+
+template <typename T>
+__device__ typename vec3<T>::Type computeBodyAccel(
+    typename vec4<T>::Type bodyPos, typename vec4<T>::Type *positions,
+    int numTiles) {
+  typename vec4<T>::Type *sharedPos = SharedMemory<typename vec4<T>::Type>();
+
+  typename vec3<T>::Type acc = {0.0f, 0.0f, 0.0f};
+
+  for (int tile = 0; tile < numTiles; tile++) {
+    sharedPos[threadIdx.x] = positions[tile * blockDim.x + threadIdx.x];
+
+    __syncthreads();
+
+    // This is the "tile_calculation" from the GPUG3 article.
+#pragma unroll 128
+
+    for (unsigned int counter = 0; counter < blockDim.x; counter++) {
+      acc = bodyBodyInteraction<T>(acc, bodyPos, sharedPos[counter]);
+    }
+
+    __syncthreads();
+  }
+
+  return acc;
+}
+
+template <typename T>
+__global__ void integrateBodies(typename vec4<T>::Type *__restrict__ newPos,
+                                typename vec4<T>::Type *__restrict__ oldPos,
+                                typename vec4<T>::Type *vel,
+                                unsigned int deviceOffset,
+                                unsigned int deviceNumBodies, float deltaTime,
+                                float damping, int numTiles) {
+  int index = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (index >= deviceNumBodies) {
+    return;
+  }
+
+  typename vec4<T>::Type position = oldPos[deviceOffset + index];
+
+  typename vec3<T>::Type accel =
+      computeBodyAccel<T>(position, oldPos, numTiles);
+
+  // acceleration = force / mass;
+  // new velocity = old velocity + acceleration * deltaTime
+  // note we factor out the body's mass from the equation, here and in
+  // bodyBodyInteraction (because they cancel out).  Thus here force ==
+  // acceleration
+  typename vec4<T>::Type velocity = vel[deviceOffset + index];
+
+  velocity.x += accel.x * deltaTime;
+  velocity.y += accel.y * deltaTime;
+  velocity.z += accel.z * deltaTime;
+
+  velocity.x *= damping;
+  velocity.y *= damping;
+  velocity.z *= damping;
+
+  // new position = old position + velocity * deltaTime
+  position.x += velocity.x * deltaTime;
+  position.y += velocity.y * deltaTime;
+  position.z += velocity.z * deltaTime;
+
+  // store new position and velocity
+  newPos[deviceOffset + index] = position;
+  vel[deviceOffset + index] = velocity;
+}
+
+template <typename T>
+void integrateNbodySystem(DeviceData<T> *deviceData,
+                          hipGraphicsResource **pgres,
+                          unsigned int currentRead, float deltaTime,
+                          float damping, unsigned int numBodies,
+                          unsigned int numDevices, int blockSize,
+                          bool bUsePBO) {
+  if (bUsePBO) {
+    HIPCHECK(cudaGraphicsResourceSetMapFlags(
+        pgres[currentRead], cudaGraphicsMapFlagsReadOnly));
+    HIPCHECK(cudaGraphicsResourceSetMapFlags(
+        pgres[1 - currentRead], cudaGraphicsMapFlagsWriteDiscard));
+    HIPCHECK(hipGraphicsMapResources(2, pgres, 0));
+    size_t bytes;
+    HIPCHECK(hipGraphicsResourceGetMappedPointer(
+        (void **)&(deviceData[0].dPos[currentRead]), &bytes,
+        pgres[currentRead]));
+    HIPCHECK(hipGraphicsResourceGetMappedPointer(
+        (void **)&(deviceData[0].dPos[1 - currentRead]), &bytes,
+        pgres[1 - currentRead]));
+  }
+
+  for (unsigned int dev = 0; dev != numDevices; dev++) {
+    if (numDevices > 1) {
+      hipSetDevice(dev);
+    }
+
+    int numBlocks = (deviceData[dev].numBodies + blockSize - 1) / blockSize;
+    int numTiles = (numBodies + blockSize - 1) / blockSize;
+    int sharedMemSize = blockSize * 4 * sizeof(T);  // 4 floats for pos
+
+    integrateBodies<T><<<numBlocks, blockSize, sharedMemSize>>>(
+        (typename vec4<T>::Type *)deviceData[dev].dPos[1 - currentRead],
+        (typename vec4<T>::Type *)deviceData[dev].dPos[currentRead],
+        (typename vec4<T>::Type *)deviceData[dev].dVel, deviceData[dev].offset,
+        deviceData[dev].numBodies, deltaTime, damping, numTiles);
+
+    if (numDevices > 1) {
+      HIPCHECK(hipEventRecord(deviceData[dev].event));
+      // MJH: Hack on older driver versions to force kernel launches to flush!
+      hipStreamQuery(0);
+    }
+
+    // check if kernel invocation generated an error
+    getLastCudaError("Kernel execution failed");
+  }
+
+  if (numDevices > 1) {
+    for (unsigned int dev = 0; dev < numDevices; dev++) {
+      HIPCHECK(hipEventSynchronize(deviceData[dev].event));
+    }
+  }
+
+  if (bUsePBO) {
+    HIPCHECK(hipGraphicsUnmapResources(2, pgres, 0));
+  }
+}
+
+// Explicit specializations needed to generate code
+template void integrateNbodySystem<float>(DeviceData<float> *deviceData,
+                                          hipGraphicsResource **pgres,
+                                          unsigned int currentRead,
+                                          float deltaTime, float damping,
+                                          unsigned int numBodies,
+                                          unsigned int numDevices,
+                                          int blockSize, bool bUsePBO);
+
+template void integrateNbodySystem<double>(DeviceData<double> *deviceData,
+                                           hipGraphicsResource **pgres,
+                                           unsigned int currentRead,
+                                           float deltaTime, float damping,
+                                           unsigned int numBodies,
+                                           unsigned int numDevices,
+                                           int blockSize, bool bUsePBO);
+                          int blockSize, bool bUsePBO);
diff --git a/src/samples/Samples/5_Domain_Specific/nbody_screen/bodysystemcuda.cu.hip b/src/samples/Samples/5_Domain_Specific/nbody_screen/bodysystemcuda.cu.hip
index e69de29..986ac9a 100755
--- a/src/samples/Samples/5_Domain_Specific/nbody_screen/bodysystemcuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/nbody_screen/bodysystemcuda.cu.hip
@@ -0,0 +1,278 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "helper_cuda_hipified.h"
+#include <math.h>
+
+//#include <GL/glew.h>
+//#include <GL/freeglut.h>
+
+// CUDA standard includes
+#include <hip/hip_runtime.h>
+//#include <cuda_gl_interop.h>
+
+#include "bodysystem.h"
+
+__constant__ float softeningSquared;
+__constant__ double softeningSquared_fp64;
+
+hipError_t setSofteningSquared(float softeningSq) {
+  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared), &softeningSq, sizeof(float), 0,
+                            hipMemcpyHostToDevice);
+}
+
+hipError_t setSofteningSquared(double softeningSq) {
+  return hipMemcpyToSymbol(HIP_SYMBOL(softeningSquared_fp64), &softeningSq, sizeof(double),
+                            0, hipMemcpyHostToDevice);
+}
+
+template <class T>
+struct SharedMemory {
+  __device__ inline operator T *() {
+    extern __shared__ int __smem[];
+    return (T *)__smem;
+  }
+
+  __device__ inline operator const T *() const {
+    extern __shared__ int __smem[];
+    return (T *)__smem;
+  }
+};
+
+template <typename T>
+__device__ T rsqrt_T(T x) {
+  return rsqrt(x);
+}
+
+template <>
+__device__ float rsqrt_T<float>(float x) {
+  return rsqrtf(x);
+}
+
+template <>
+__device__ double rsqrt_T<double>(double x) {
+  return rsqrt(x);
+}
+
+// Macros to simplify shared memory addressing
+#define SX(i) sharedPos[i + blockDim.x * threadIdx.y]
+// This macro is only used when multithreadBodies is true (below)
+#define SX_SUM(i, j) sharedPos[i + blockDim.x * j]
+
+template <typename T>
+__device__ T getSofteningSquared() {
+  return softeningSquared;
+}
+template <>
+__device__ double getSofteningSquared<double>() {
+  return softeningSquared_fp64;
+}
+
+template <typename T>
+struct DeviceData {
+  T *dPos[2];  // mapped host pointers
+  T *dVel;
+  hipEvent_t event;
+  unsigned int offset;
+  unsigned int numBodies;
+};
+
+template <typename T>
+__device__ typename vec3<T>::Type bodyBodyInteraction(
+    typename vec3<T>::Type ai, typename vec4<T>::Type bi,
+    typename vec4<T>::Type bj) {
+  typename vec3<T>::Type r;
+
+  // r_ij  [3 FLOPS]
+  r.x = bj.x - bi.x;
+  r.y = bj.y - bi.y;
+  r.z = bj.z - bi.z;
+
+  // distSqr = dot(r_ij, r_ij) + EPS^2  [6 FLOPS]
+  T distSqr = r.x * r.x + r.y * r.y + r.z * r.z;
+  distSqr += getSofteningSquared<T>();
+
+  // invDistCube =1/distSqr^(3/2)  [4 FLOPS (2 mul, 1 sqrt, 1 inv)]
+  T invDist = rsqrt_T(distSqr);
+  T invDistCube = invDist * invDist * invDist;
+
+  // s = m_j * invDistCube [1 FLOP]
+  T s = bj.w * invDistCube;
+
+  // a_i =  a_i + s * r_ij [6 FLOPS]
+  ai.x += r.x * s;
+  ai.y += r.y * s;
+  ai.z += r.z * s;
+
+  return ai;
+}
+
+template <typename T>
+__device__ typename vec3<T>::Type computeBodyAccel(
+    typename vec4<T>::Type bodyPos, typename vec4<T>::Type *positions,
+    int numTiles) {
+  typename vec4<T>::Type *sharedPos = SharedMemory<typename vec4<T>::Type>();
+
+  typename vec3<T>::Type acc = {0.0f, 0.0f, 0.0f};
+
+  for (int tile = 0; tile < numTiles; tile++) {
+    sharedPos[threadIdx.x] = positions[tile * blockDim.x + threadIdx.x];
+
+    __syncthreads();
+
+    // This is the "tile_calculation" from the GPUG3 article.
+#pragma unroll 128
+
+    for (unsigned int counter = 0; counter < blockDim.x; counter++) {
+      acc = bodyBodyInteraction<T>(acc, bodyPos, sharedPos[counter]);
+    }
+
+    __syncthreads();
+  }
+
+  return acc;
+}
+
+template <typename T>
+__global__ void integrateBodies(typename vec4<T>::Type *__restrict__ newPos,
+                                typename vec4<T>::Type *__restrict__ oldPos,
+                                typename vec4<T>::Type *vel,
+                                unsigned int deviceOffset,
+                                unsigned int deviceNumBodies, float deltaTime,
+                                float damping, int numTiles) {
+  int index = blockIdx.x * blockDim.x + threadIdx.x;
+
+  if (index >= deviceNumBodies) {
+    return;
+  }
+
+  typename vec4<T>::Type position = oldPos[deviceOffset + index];
+
+  typename vec3<T>::Type accel =
+      computeBodyAccel<T>(position, oldPos, numTiles);
+
+  // acceleration = force / mass;
+  // new velocity = old velocity + acceleration * deltaTime
+  // note we factor out the body's mass from the equation, here and in
+  // bodyBodyInteraction (because they cancel out).  Thus here force ==
+  // acceleration
+  typename vec4<T>::Type velocity = vel[deviceOffset + index];
+
+  velocity.x += accel.x * deltaTime;
+  velocity.y += accel.y * deltaTime;
+  velocity.z += accel.z * deltaTime;
+
+  velocity.x *= damping;
+  velocity.y *= damping;
+  velocity.z *= damping;
+
+  // new position = old position + velocity * deltaTime
+  position.x += velocity.x * deltaTime;
+  position.y += velocity.y * deltaTime;
+  position.z += velocity.z * deltaTime;
+
+  // store new position and velocity
+  newPos[deviceOffset + index] = position;
+  vel[deviceOffset + index] = velocity;
+}
+
+template <typename T>
+void integrateNbodySystem(DeviceData<T> *deviceData,
+                          hipGraphicsResource **pgres,
+                          unsigned int currentRead, float deltaTime,
+                          float damping, unsigned int numBodies,
+                          unsigned int numDevices, int blockSize,
+                          bool bUsePBO) {
+  if (bUsePBO) {
+    HIPCHECK(cudaGraphicsResourceSetMapFlags(
+        pgres[currentRead], cudaGraphicsMapFlagsReadOnly));
+    HIPCHECK(cudaGraphicsResourceSetMapFlags(
+        pgres[1 - currentRead], cudaGraphicsMapFlagsWriteDiscard));
+    HIPCHECK(hipGraphicsMapResources(2, pgres, 0));
+    size_t bytes;
+    HIPCHECK(hipGraphicsResourceGetMappedPointer(
+        (void **)&(deviceData[0].dPos[currentRead]), &bytes,
+        pgres[currentRead]));
+    HIPCHECK(hipGraphicsResourceGetMappedPointer(
+        (void **)&(deviceData[0].dPos[1 - currentRead]), &bytes,
+        pgres[1 - currentRead]));
+  }
+
+  for (unsigned int dev = 0; dev != numDevices; dev++) {
+    if (numDevices > 1) {
+      hipSetDevice(dev);
+    }
+
+    int numBlocks = (deviceData[dev].numBodies + blockSize - 1) / blockSize;
+    int numTiles = (numBodies + blockSize - 1) / blockSize;
+    int sharedMemSize = blockSize * 4 * sizeof(T);  // 4 floats for pos
+
+    integrateBodies<T><<<numBlocks, blockSize, sharedMemSize>>>(
+        (typename vec4<T>::Type *)deviceData[dev].dPos[1 - currentRead],
+        (typename vec4<T>::Type *)deviceData[dev].dPos[currentRead],
+        (typename vec4<T>::Type *)deviceData[dev].dVel, deviceData[dev].offset,
+        deviceData[dev].numBodies, deltaTime, damping, numTiles);
+
+    if (numDevices > 1) {
+      HIPCHECK(hipEventRecord(deviceData[dev].event));
+      // MJH: Hack on older driver versions to force kernel launches to flush!
+      hipStreamQuery(0);
+    }
+
+    // check if kernel invocation generated an error
+    getLastCudaError("Kernel execution failed");
+  }
+
+  if (numDevices > 1) {
+    for (unsigned int dev = 0; dev < numDevices; dev++) {
+      HIPCHECK(hipEventSynchronize(deviceData[dev].event));
+    }
+  }
+
+  if (bUsePBO) {
+    HIPCHECK(hipGraphicsUnmapResources(2, pgres, 0));
+  }
+}
+
+// Explicit specializations needed to generate code
+template void integrateNbodySystem<float>(DeviceData<float> *deviceData,
+                                          hipGraphicsResource **pgres,
+                                          unsigned int currentRead,
+                                          float deltaTime, float damping,
+                                          unsigned int numBodies,
+                                          unsigned int numDevices,
+                                          int blockSize, bool bUsePBO);
+
+template void integrateNbodySystem<double>(DeviceData<double> *deviceData,
+                                           hipGraphicsResource **pgres,
+                                           unsigned int currentRead,
+                                           float deltaTime, float damping,
+                                           unsigned int numBodies,
+                                           unsigned int numDevices,
+                                           int blockSize, bool bUsePBO);
+                          int blockSize, bool bUsePBO);
diff --git a/src/samples/Samples/5_Domain_Specific/p2pBandwidthLatencyTest/p2pBandwidthLatencyTest.out b/src/samples/Samples/5_Domain_Specific/p2pBandwidthLatencyTest/p2pBandwidthLatencyTest.out
index d81477f..3193a28 100755
Binary files a/src/samples/Samples/5_Domain_Specific/p2pBandwidthLatencyTest/p2pBandwidthLatencyTest.out and b/src/samples/Samples/5_Domain_Specific/p2pBandwidthLatencyTest/p2pBandwidthLatencyTest.out differ
diff --git a/src/samples/Samples/5_Domain_Specific/quasirandomGenerator_nvrtc/quasirandomGenerator_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/quasirandomGenerator_nvrtc/quasirandomGenerator_kernel.cu.hip
index e69de29..f825488 100755
--- a/src/samples/Samples/5_Domain_Specific/quasirandomGenerator_nvrtc/quasirandomGenerator_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/quasirandomGenerator_nvrtc/quasirandomGenerator_kernel.cu.hip
@@ -0,0 +1,160 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef QUASIRANDOMGENERATOR_KERNEL_CUH
+#define QUASIRANDOMGENERATOR_KERNEL_CUH
+
+#include "quasirandomGenerator_common.h"
+
+// Fast integer multiplication
+#define MUL(a, b) __umul24(a, b)
+
+////////////////////////////////////////////////////////////////////////////////
+// Niederreiter quasirandom number generation kernel
+////////////////////////////////////////////////////////////////////////////////
+__constant__ unsigned int c_Table[QRNG_DIMENSIONS][QRNG_RESOLUTION];
+
+extern "C" __global__ void quasirandomGeneratorKernel(float *d_Output,
+                                                      unsigned int seed,
+                                                      unsigned int N) {
+  unsigned int *dimBase = &c_Table[threadIdx.y][0];
+  unsigned int tid = MUL(blockDim.x, blockIdx.x) + threadIdx.x;
+  unsigned int threadN = MUL(blockDim.x, gridDim.x);
+
+  for (unsigned int pos = tid; pos < N; pos += threadN) {
+    unsigned int result = 0;
+    unsigned int data = seed + pos;
+
+    for (int bit = 0; bit < QRNG_RESOLUTION; bit++, data >>= 1)
+      if (data & 1) {
+        result ^= dimBase[bit];
+      }
+
+    d_Output[MUL(threadIdx.y, N) + pos] = (float)(result + 1) * INT_SCALE;
+  }
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Moro's Inverse Cumulative Normal Distribution function approximation
+////////////////////////////////////////////////////////////////////////////////
+__device__ inline float MoroInvCNDgpu(unsigned int x) {
+  const float a1 = 2.50662823884f;
+  const float a2 = -18.61500062529f;
+  const float a3 = 41.39119773534f;
+  const float a4 = -25.44106049637f;
+  const float b1 = -8.4735109309f;
+  const float b2 = 23.08336743743f;
+  const float b3 = -21.06224101826f;
+  const float b4 = 3.13082909833f;
+  const float c1 = 0.337475482272615f;
+  const float c2 = 0.976169019091719f;
+  const float c3 = 0.160797971491821f;
+  const float c4 = 2.76438810333863E-02f;
+  const float c5 = 3.8405729373609E-03f;
+  const float c6 = 3.951896511919E-04f;
+  const float c7 = 3.21767881768E-05f;
+  const float c8 = 2.888167364E-07f;
+  const float c9 = 3.960315187E-07f;
+
+  float z;
+
+  bool negate = false;
+
+  // Ensure the conversion to floating point will give a value in the
+  // range (0,0.5] by restricting the input to the bottom half of the
+  // input domain. We will later reflect the result if the input was
+  // originally in the top half of the input domain
+  if (x >= 0x80000000UL) {
+    x = 0xffffffffUL - x;
+    negate = true;
+  }
+
+  // x is now in the range [0,0x80000000) (i.e. [0,0x7fffffff])
+  // Convert to floating point in (0,0.5]
+  const float x1 = 1.0f / static_cast<float>(0xffffffffUL);
+  const float x2 = x1 / 2.0f;
+  float p1 = x * x1 + x2;
+  // Convert to floating point in (-0.5,0]
+  float p2 = p1 - 0.5f;
+
+  // The input to the Moro inversion is p2 which is in the range
+  // (-0.5,0]. This means that our output will be the negative side
+  // of the bell curve (which we will reflect if "negate" is true).
+
+  // Main body of the bell curve for |p| < 0.42
+  if (p2 > -0.42f) {
+    z = p2 * p2;
+    z = p2 * (((a4 * z + a3) * z + a2) * z + a1) /
+        ((((b4 * z + b3) * z + b2) * z + b1) * z + 1.0f);
+  }
+  // Special case (Chebychev) for tail
+  else {
+    z = __logf(-__logf(p1));
+    z = -(c1 + z * (c2 + z * (c3 + z * (c4 + z * (c5 + z * (c6 + z *
+        (c7 + z * (c8 + z * c9))))))));
+  }
+
+  // If the original input (x) was in the top half of the range, reflect
+  // to get the positive side of the bell curve
+
+  return negate ? -z : z;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Main kernel. Choose between transforming
+// input sequence and uniform ascending (0, 1) sequence
+////////////////////////////////////////////////////////////////////////////////
+
+extern "C" __global__ void inverseCNDKernel(float *d_Output,
+                                            unsigned int pathN) {
+  unsigned int distance = ((unsigned int)-1) / (pathN + 1);
+  unsigned int tid = MUL(blockDim.x, blockIdx.x) + threadIdx.x;
+  unsigned int threadN = MUL(blockDim.x, gridDim.x);
+
+  // Transform input number sequence if it's supplied
+  if (0)  // d_Input)
+  {
+    /*
+      for (unsigned int pos = tid; pos < pathN; pos += threadN)
+      {
+          unsigned int d = d_Input[pos];
+          d_Output[pos] = (float)MoroInvCNDgpu(d);
+      }
+      */
+  }
+  // Else generate input uniformly placed samples on the fly
+  // and write to destination
+  else {
+    for (unsigned int pos = tid; pos < pathN; pos += threadN) {
+      unsigned int d = (pos + 1) * distance;
+      d_Output[pos] = (float)MoroInvCNDgpu(d);
+    }
+  }
+}
+
+#endif
diff --git a/src/samples/Samples/5_Domain_Specific/simpleD3D12/sinewave_cuda.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleD3D12/sinewave_cuda.cu.hip
index e69de29..7220a2e 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleD3D12/sinewave_cuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleD3D12/sinewave_cuda.cu.hip
@@ -0,0 +1,70 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "ShaderStructs.h"
+
+__global__ void sinewave_gen_kernel(Vertex *vertices, unsigned int width,
+                                    unsigned int height, float time) {
+  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
+  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
+
+  // calculate uv coordinates
+  float u = x / (float)width;
+  float v = y / (float)height;
+  u = u * 2.0f - 1.0f;
+  v = v * 2.0f - 1.0f;
+
+  // calculate simple sine wave pattern
+  float freq = 4.0f;
+  float w = sinf(u * freq + time) * cosf(v * freq + time) * 0.5f;
+
+  if (y < height && x < width) {
+    // write output vertex
+    vertices[y * width + x].position.x = u;
+    vertices[y * width + x].position.y = w;
+    vertices[y * width + x].position.z = v;
+    // vertices[y*width+x].position[3] = 1.0f;
+    vertices[y * width + x].color.x = 1.0f;
+    vertices[y * width + x].color.y = 0.0f;
+    vertices[y * width + x].color.z = 0.0f;
+    vertices[y * width + x].color.w = 0.0f;
+  }
+}
+
+// The host CPU Sinewave thread spawner
+void RunSineWaveKernel(size_t mesh_width, size_t mesh_height,
+                       Vertex *cudaDevVertptr, hipStream_t streamToRun,
+                       float AnimTime) {
+  dim3 block(16, 16, 1);
+  dim3 grid(mesh_width / 16, mesh_height / 16, 1);
+  Vertex *vertices = (Vertex *)cudaDevVertptr;
+  sinewave_gen_kernel<<<grid, block, 0, streamToRun>>>(vertices, mesh_width,
+                                                       mesh_height, AnimTime);
+
+  getLastCudaError("sinewave_gen_kernel execution failed.\n");
+}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleVulkanMMAP/MonteCarloPi.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleVulkanMMAP/MonteCarloPi.cu.hip
index 7cc9033..5482a20 100755
--- a/src/samples/Samples/5_Domain_Specific/simpleVulkanMMAP/MonteCarloPi.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleVulkanMMAP/MonteCarloPi.cu.hip
@@ -320,3 +320,4 @@ void MonteCarloPiSimulation::cleanupSimulationAllocations() {
     m_xyVector = nullptr;
     m_pointsInsideCircle = nullptr;
   }
+}
diff --git a/src/samples/Samples/5_Domain_Specific/volumeFiltering/volumeFilter_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/volumeFiltering/volumeFilter_kernel.cu.hip
index e69de29..c397a84 100755
--- a/src/samples/Samples/5_Domain_Specific/volumeFiltering/volumeFilter_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/volumeFiltering/volumeFilter_kernel.cu.hip
@@ -0,0 +1,117 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _VOLUMEFILTER_KERNEL_CU_
+#define _VOLUMEFILTER_KERNEL_CU_
+
+#include "helper_cuda_hipified.h"
+#include <helper_math.h>
+#include "volumeFilter.h"
+
+typedef unsigned int uint;
+typedef unsigned char uchar;
+typedef unsigned short ushort;
+
+__constant__ float4 c_filterData[VOLUMEFILTER_MAXWEIGHTS];
+
+__global__ void d_filter_surface3d(int filterSize, float filter_offset,
+                                   hipExtent volumeSize,
+                                   hipTextureObject_t volumeTexIn,
+                                   hipSurfaceObject_t volumeTexOut) {
+  int x = blockIdx.x * blockDim.x + threadIdx.x;
+  int y = blockIdx.y * blockDim.y + threadIdx.y;
+  int z = blockIdx.z * blockDim.z + threadIdx.z;
+
+  if (x >= volumeSize.width || y >= volumeSize.height ||
+      z >= volumeSize.depth) {
+    return;
+  }
+
+  float filtered = 0;
+  float4 basecoord = make_float4(x, y, z, 0);
+
+  for (int i = 0; i < filterSize; i++) {
+    float4 coord = basecoord + c_filterData[i];
+    filtered += tex3D<float>(volumeTexIn, coord.x, coord.y, coord.z) *
+                c_filterData[i].w;
+  }
+
+  filtered += filter_offset;
+
+  VolumeType output = VolumeTypeInfo<VolumeType>::convert(filtered);
+
+  // surface writes need byte offsets for x!
+  surf3Dwrite(output, volumeTexOut, x * sizeof(VolumeType), y, z);
+}
+
+static unsigned int iDivUp(size_t a, size_t b) {
+  size_t val = (a % b != 0) ? (a / b + 1) : (a / b);
+  if (val > UINT_MAX) {
+    fprintf(stderr, "\nUINT_MAX limit exceeded in iDivUp() exiting.....\n");
+    exit(EXIT_FAILURE);  // val exceeds limit
+  }
+
+  return static_cast<unsigned int>(val);
+}
+
+extern "C" Volume *VolumeFilter_runFilter(Volume *input, Volume *output0,
+                                          Volume *output1, int iterations,
+                                          int numWeights, float4 *weights,
+                                          float postWeightOffset) {
+  Volume *swap = 0;
+  hipExtent size = input->size;
+  unsigned int dim = 32 / sizeof(VolumeType);
+  dim3 blockSize(dim, dim, 1);
+  dim3 gridSize(iDivUp(size.width, blockSize.x),
+                iDivUp(size.height, blockSize.y),
+                iDivUp(size.depth, blockSize.z));
+
+  // set weights
+  HIPCHECK(
+      hipMemcpyToSymbol(HIP_SYMBOL(c_filterData), weights, sizeof(float4) * numWeights));
+
+  for (int i = 0; i < iterations; i++) {
+    d_filter_surface3d<<<gridSize, blockSize>>>(numWeights, postWeightOffset,
+                                                size, input->volumeTex,
+                                                output0->volumeSurf);
+
+    getLastCudaError("filter kernel failed");
+
+    swap = input;
+    input = output0;
+    output0 = swap;
+
+    if (i == 0) {
+      output0 = output1;
+    }
+  }
+
+  return input;
+}
+#endif
+#endif
diff --git a/src/samples/Samples/6_Performance/UnifiedMemoryPerf/commonKernels.cu.hip b/src/samples/Samples/6_Performance/UnifiedMemoryPerf/commonKernels.cu.hip
index e69de29..ed8cd7e 100755
--- a/src/samples/Samples/6_Performance/UnifiedMemoryPerf/commonKernels.cu.hip
+++ b/src/samples/Samples/6_Performance/UnifiedMemoryPerf/commonKernels.cu.hip
@@ -0,0 +1,34 @@
+#include "hip/hip_runtime.h"
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "commonKernels.hpp"
+
+__global__ void spinWhileLessThanOne(volatile unsigned int *latch) {
+  while (latch[0] < 1)
+    ;
+}
diff --git a/src/samples/Samples/6_Performance/UnifiedMemoryPerf/matrixMultiplyPerf.cu.hip b/src/samples/Samples/6_Performance/UnifiedMemoryPerf/matrixMultiplyPerf.cu.hip
old mode 100755
new mode 100644
index 8bdd71e..5964a15
--- a/src/samples/Samples/6_Performance/UnifiedMemoryPerf/matrixMultiplyPerf.cu.hip
+++ b/src/samples/Samples/6_Performance/UnifiedMemoryPerf/matrixMultiplyPerf.cu.hip
@@ -125,7 +125,7 @@ void verifyMatrixData(float *expectedData, float *observedData,
 }
 
 #define BLOCK_SIZE 32
-__global__ void matrixMultiplyKernel(float *C,float *A,float *B,
+__global__ void matrixMultiplyKernel(float *C, float *A, float *B,
                                      unsigned int matrixDim) {
   // Block index
   int bx = blockIdx.x;
@@ -206,13 +206,9 @@ void runMatrixMultiplyKernel(unsigned int matrixDim, int allocType,
                              double *gpuLaunchTransferSyncTimes,
                              double *cpuAccessTimes, double *overallTimes,
                              int device_id) {
-  void *dptrA = NULL,  *hptrA = NULL;
-  void *dptrB = NULL,  *hptrB = NULL;
-  void *dptrC = NULL,  *hptrC = NULL;
-  //float *dptrA = NULL,  *hptrA = NULL;
-  //float *dptrB = NULL,  *hptrB = NULL;
-  //float *dptrC = NULL,  *hptrC = NULL;
-
+  float *dptrA = NULL, *hptrA = NULL;
+  float *dptrB = NULL, *hptrB = NULL;
+  float *dptrC = NULL, *hptrC = NULL;
   float *randValuesX = NULL, *randValuesY = NULL;
   float *randValuesVerifyXmulY = NULL, *randValuesVerifyYmulX = NULL;
   bool copyRequired = false, hintsRequired = false;
@@ -262,7 +258,7 @@ void runMatrixMultiplyKernel(unsigned int matrixDim, int allocType,
       hipMemcpyAsync(dptrA, randValuesX, size, hipMemcpyHostToDevice));
   HIPCHECK(
       hipMemcpyAsync(dptrB, randValuesY, size, hipMemcpyHostToDevice));
-matrixMultiplyKernel<<<grid, threads>>>(dptrC, dptrA, dptrB, matrixDim);
+  matrixMultiplyKernel<<<grid, threads>>>(dptrC, dptrA, dptrB, matrixDim);
   HIPCHECK(hipMemcpyAsync(randValuesVerifyXmulY, dptrC, size,
                                   hipMemcpyDeviceToHost));
   HIPCHECK(hipStreamSynchronize(NULL));
@@ -318,9 +314,9 @@ matrixMultiplyKernel<<<grid, threads>>>(dptrC, dptrA, dptrB, matrixDim);
       HIPCHECK(hipHostMalloc(&hptrA, size));
       HIPCHECK(hipHostMalloc(&hptrB, size));
       HIPCHECK(hipHostMalloc(&hptrC, size));
-      HIPCHECK(hipHostGetDevicePointer(&dptrA, hptrA, 0));
-      HIPCHECK(hipHostGetDevicePointer(&dptrB, hptrB, 0));
-      HIPCHECK(hipHostGetDevicePointer(&dptrC, hptrC, 0));
+      HIPCHECK(hipHostGetDevicePointer((void **)&dptrA, hptrA, 0));
+      HIPCHECK(hipHostGetDevicePointer((void **)&dptrB, hptrB, 0));
+      HIPCHECK(hipHostGetDevicePointer((void **)&dptrC, hptrC, 0));
       break;
 
     case USE_MANAGED_MEMORY:
@@ -700,11 +696,4 @@ int main(int argc, char **argv) {
       "Results may vary when GPU Boost is enabled.\n");
   exit(EXIT_SUCCESS);
 }
-{
-    numKernelRuns =
-        getCmdLineArgumentInt(argc, (const char **)argv, "kernel-iterations");
-  }
 
-  if (checkCmdLineFlag(argc, (const char **)argv, "verbose")) {
-    verboseResults = 1;
-  }
diff --git a/src/samples/Samples/6_Performance/alignedTypes/alignedTypes.out b/src/samples/Samples/6_Performance/alignedTypes/alignedTypes.out
index c506452..c35e3e1 100755
Binary files a/src/samples/Samples/6_Performance/alignedTypes/alignedTypes.out and b/src/samples/Samples/6_Performance/alignedTypes/alignedTypes.out differ
diff --git a/src/samples/Samples/6_Performance/transpose/transpose.out b/src/samples/Samples/6_Performance/transpose/transpose.out
index c0a5ce4..601436a 100755
Binary files a/src/samples/Samples/6_Performance/transpose/transpose.out and b/src/samples/Samples/6_Performance/transpose/transpose.out differ
